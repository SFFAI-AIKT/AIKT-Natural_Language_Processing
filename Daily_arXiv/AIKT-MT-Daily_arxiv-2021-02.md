# Daily arXiv: Machine Translation - February, 2021

# Index

- [2021-02-26](#2021-02-26)
  - [1. LazyFormer: Self Attention with Lazy Update](#2021-02-26-1)
  - [2. IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with Special Tokens, Re-Ranking, Siamese Encoders and Back Translation](#2021-02-26-2)
  - [3. A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives](#2021-02-26-3)
  - [4. Investigating the Limitations of the Transformers with Simple Arithmetic Tasks](#2021-02-26-4)
  - [5. Retrieval Augmentation to Improve Robustness and Interpretability of Deep Neural Networks](#2021-02-26-5)
- [2021-02-25](#2021-02-25)
  - [1. Do Transformer Modifications Transfer Across Implementations and Applications?](#2021-02-25-1)
  - [2. Teach Me to Explain: A Review of Datasets for Explainable NLP](#2021-02-25-2)
  - [3. Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning](#2021-02-25-3)
  - [4. Creolizing the Web](#2021-02-25-4)
  - [5. Task-Specific Pre-Training and Cross Lingual Transfer for Code-Switched Data](#2021-02-25-5)
  - [6. When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute](#2021-02-25-6)
- [2021-02-24](#2021-02-24)
  - [1. RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning](#2021-02-24-1)
  - [2. Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation](#2021-02-24-2)
  - [3. MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture](#2021-02-24-3)
  - [4. Exploring Supervised and Unsupervised Rewards in Machine Translation](#2021-02-24-4)
  - [5. Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations](#2021-02-24-5)
  - [6. Paraphrases do not explain word analogies](#2021-02-24-6)
- [2021-02-23](#2021-02-23)
  - [1. VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining](#2021-02-23-1)
  - [2. Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer](#2021-02-23-2)
  - [3. Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case](#2021-02-23-3)
  - [4. Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging](#2021-02-23-4)
  - [5. Machine Translation Customization via Automatic Training Data Selection from the Web](#2021-02-23-5)
  - [6. CDA: a Cost Efficient Content-based Multilingual Web Document Aligner](#2021-02-23-6)
  - [7. Deep Structured Feature Networks for Table Detection and Tabular Data Extraction from Scanned Financial Document Images](#2021-02-23-7)
  - [8. Understanding and Enhancing the Use of Context for Machine Translation](#2021-02-23-8)
  - [9. Bilingual Language Modeling, A transfer learning technique for Roman Urdu](#2021-02-23-9)
  - [10. Towards Personalised and Document-level Machine Translation of Dialogue](#2021-02-23-10)
  - [11. Multimodal Punctuation Prediction with Contextual Dropout](#2021-02-23-11)
  - [12. Position Information in Transformers: An Overview](#2021-02-23-12)
- [2021-02-22](#2021-02-22)
  - [1. Calibrate Before Use: Improving Few-Shot Performance of Language Models](#2021-02-22-1)
  - [2. Multilingual Augmenter: The Model Chooses](#2021-02-22-2)
- [2021-02-19](#2021-02-19)
  - [1. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](#2021-02-19-1)
  - [2. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](#2021-02-19-2)
- [2021-02-18](#2021-02-18)
  - [1. COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](#2021-02-18-1)
  - [2. Sparsely Factored Neural Machine Translation](#2021-02-18-2)
- [2021-02-17](#2021-02-17)
  - [1. Meta Back-translation](#2021-02-17-1)
  - [2. Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet](#2021-02-17-2)
  - [3. Non-Autoregressive Text Generation with Pre-trained Language Models](#2021-02-17-3)
  - [4. Revisiting Language Encoding in Learning Multilingual Representations](#2021-02-17-4)
- [2021-02-16](#2021-02-16)
  - [1. MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training](#2021-02-16-1)
- [2021-02-15](#2021-02-15)
  - [1. Continuous Learning in Neural Machine Translation using Bilingual Dictionaries](#2021-02-15-1)
  - [2. Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders](#2021-02-15-2)
- [2021-02-12](#2021-02-12)
  - [1. Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation](#2021-02-12-1)
  - [2. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](#2021-02-12-2)
- [2021-02-11](#2021-02-11)
  - [1. Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models](#2021-02-11-1)
- [2021-02-10](#2021-02-10)
  - [1. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](#2021-02-10-1)
- [2021-02-09](#2021-02-09)
  - [1. Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning](#2021-02-09-1)
  - [2. Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models](#2021-02-09-2)
  - [3. Representation Learning for Natural Language Processing](#2021-02-09-3)
  - [4. CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models](#2021-02-09-4)
  - [5. SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning](#2021-02-09-5)
  - [6. Quality Estimation without Human-labeled Data](#2021-02-09-6)
- [2021-02-08](#2021-02-08)
  - [1. Understanding Pre-Editing for Black-Box Neural Machine Translation](#2021-02-08-1)
  - [2. Spell Correction for Azerbaijani Language using Deep Neural Networks](#2021-02-08-2)


- [2021-02-05](#2021-02-05)
- [1. Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models](#2021-02-05-1)
  - [2. Unifying Vision-and-Language Tasks via Text Generation](#2021-02-05-2)
- [2021-02-04](#2021-02-04)

  - [1. The Multilingual TEDx Corpus for Speech Recognition and Translation](#2021-02-04-1)
  - [2. Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation](#2021-02-04-2)
  - [3. When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data](#2021-02-04-3)
- [2021-02-03](#2021-02-03)

  - [1. Two Demonstrations of the Machine Translation Applications to Historical Documents](#2021-02-03-1)
  - [2. CTC-based Compression for Direct Speech Translation](#2021-02-03-2)
  - [3. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics](#2021-02-03-3)
- [2021-02-02](#2021-02-02)

  - [1. Speech Recognition by Simply Fine-tuning BERT](#2021-02-02-1)
  - [2. Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript](#2021-02-02-2)
  - [3. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation](#2021-02-02-3)
  - [4. Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](#2021-02-02-4)
  - [5. Neural OCR Post-Hoc Correction of Historical Corpora](#2021-02-02-5)
  - [6. GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer](#2021-02-02-6)
  - [7. Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](#2021-02-02-7)
  - [8. End2End Acoustic to Semantic Transduction](#2021-02-02-8)
  - [9. Measuring and Improving Consistency in Pretrained Language Models](#2021-02-02-9)
- [2021-02-01](#2021-02-01)

  - [1. Combining pre-trained language models and structured knowledge](#2021-02-01-1)
  - [2. Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning](#2021-02-01-2)
  - [3. Synthesizing Monolingual Data for Neural Machine Translation](#2021-02-01-3)
  - [4. Transition based Graph Decoder for Neural Machine Translation](#2021-02-01-4)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-02-26

[Return to Index](#Index)



<h2 id="2021-02-26-1">1. LazyFormer: Self Attention with Lazy Update</h2>

Title: [LazyFormer: Self Attention with Lazy Update](https://arxiv.org/abs/2102.12702)

Authors: [Chengxuan Ying](https://arxiv.org/search/cs?searchtype=author&query=Ying%2C+C), [Guolin Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+G), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Improving the efficiency of Transformer-based language pre-training is an important task in NLP, especially for the self-attention module, which is computationally expensive. In this paper, we propose a simple but effective solution, called \emph{LazyFormer}, which computes the self-attention distribution infrequently. LazyFormer composes of multiple lazy blocks, each of which contains multiple Transformer layers. In each lazy block, the self-attention distribution is only computed once in the first layer and then is reused in all upper layers. In this way, the cost of computation could be largely saved. We also provide several training tricks for LazyFormer. Extensive experiments demonstrate the effectiveness of the proposed method.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12702](https://arxiv.org/abs/2102.12702) [cs.CL]** |
|           | (or **[arXiv:2102.12702v1](https://arxiv.org/abs/2102.12702v1) [cs.CL]** for this version) |





<h2 id="2021-02-26-2">2. IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with Special Tokens, Re-Ranking, Siamese Encoders and Back Translation</h2>

Title: [IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with Special Tokens, Re-Ranking, Siamese Encoders and Back Translation](https://arxiv.org/abs/2102.12777)

Authors: [Yuqiang Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Wei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y)

> This paper introduces our systems for all three subtasks of SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning. To help our model better represent and understand abstract concepts in natural language, we well-design many simple and effective approaches adapted to the backbone model (RoBERTa). Specifically, we formalize the subtasks into the multiple-choice question answering format and add special tokens to abstract concepts, then, the final prediction of question answering is considered as the result of subtasks. Additionally, we employ many finetuning tricks to improve the performance. Experimental results show that our approaches achieve significant performance compared with the baseline systems. Our approaches achieve eighth rank on subtask-1 and tenth rank on subtask-2.

| Comments: | 5 pages, SemEval-2021 Workshop, ACL-IJCNLP 2021              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.12777](https://arxiv.org/abs/2102.12777) [cs.CL]** |
|           | (or **[arXiv:2102.12777v1](https://arxiv.org/abs/2102.12777v1) [cs.CL]** for this version) |





<h2 id="2021-02-26-3">3. A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives</h2>

Title: [A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives](https://arxiv.org/abs/2102.12982)

Authors: [Nils Rethmeier](https://arxiv.org/search/cs?searchtype=author&query=Rethmeier%2C+N), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

> Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various application tasks. These pretraining methods are frequently extended with recurrence, adversarial or linguistic property masking, and more recently with contrastive learning objectives. Contrastive self-supervised training objectives enabled recent successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar. However, in NLP, automated creation of text input augmentations is still very challenging because a single token can invert the meaning of a sentence. For this reason, some contrastive NLP pretraining methods contrast over input-label pairs, rather than over input-input pairs, using methods from Metric Learning and Energy Based Models. In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks. We introduce key contrastive learning concepts with lessons learned from prior research and structure works by applications and cross-field relations. Finally, we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12982](https://arxiv.org/abs/2102.12982) [cs.CL]** |
|           | (or **[arXiv:2102.12982v1](https://arxiv.org/abs/2102.12982v1) [cs.CL]** for this version) |





<h2 id="2021-02-26-4">4. Investigating the Limitations of the Transformers with Simple Arithmetic Tasks</h2>

Title: [Investigating the Limitations of the Transformers with Simple Arithmetic Tasks](https://arxiv.org/abs/2102.13019)

Authors: [Rodrigo Nogueira](https://arxiv.org/search/cs?searchtype=author&query=Nogueira%2C+R), [Zhiying Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Jimmy Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> The ability to perform arithmetic tasks is a remarkable trait of human intelligence and might form a critical component of more complex reasoning tasks. In this work, we investigate if the surface form of a number has any influence on how sequence-to-sequence language models learn simple arithmetic tasks such as addition and subtraction across a wide range of values. We find that how a number is represented in its surface form has a strong influence on the model's accuracy. In particular, the model fails to learn addition of five-digit numbers when using subwords (e.g., "32"), and it struggles to learn with character-level representations (e.g., "3 2"). By introducing position tokens (e.g., "3 10e1 2"), the model learns to accurately add and subtract numbers up to 60 digits. We conclude that modern pretrained language models can easily learn arithmetic from very few examples, as long as we use the proper surface representation. This result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement. Moreover, we show that regardless of the number of parameters and training examples, models cannot learn addition rules that are independent of the length of the numbers seen during training. Code to reproduce our experiments is available at [this https URL](https://github.com/castorini/transformers-arithmetic)

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.13019](https://arxiv.org/abs/2102.13019) [cs.CL]** |
|           | (or **[arXiv:2102.13019v1](https://arxiv.org/abs/2102.13019v1) [cs.CL]** for this version) |





<h2 id="2021-02-26-5">5. Retrieval Augmentation to Improve Robustness and Interpretability of Deep Neural Networks</h2>

Title: [Retrieval Augmentation to Improve Robustness and Interpretability of Deep Neural Networks](https://arxiv.org/abs/2102.13030)

Authors: [Rita Parada Ramos](https://arxiv.org/search/cs?searchtype=author&query=Ramos%2C+R+P), [Patrícia Pereira](https://arxiv.org/search/cs?searchtype=author&query=Pereira%2C+P), [Helena Moniz](https://arxiv.org/search/cs?searchtype=author&query=Moniz%2C+H), [Joao Paulo Carvalho](https://arxiv.org/search/cs?searchtype=author&query=Carvalho%2C+J+P), [Bruno Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+B)

> Deep neural network models have achieved state-of-the-art results in various tasks related to vision and/or language. Despite the use of large training data, most models are trained by iterating over single input-output pairs, discarding the remaining examples for the current prediction. In this work, we actively exploit the training data to improve the robustness and interpretability of deep neural networks, using the information from nearest training examples to aid the prediction both during training and testing. Specifically, the proposed approach uses the target of the nearest input example to initialize the memory state of an LSTM model or to guide attention mechanisms. We apply this approach to image captioning and sentiment analysis, conducting experiments with both image and text retrieval. Results show the effectiveness of the proposed models for the two tasks, on the widely used Flickr8 and IMDB datasets, respectively. Our code is publicly available [this http URL](http://github.com/RitaRamo/retrieval-augmentation-nn).

| Comments: | Under review as a conference paper at IJCNN 2021             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.13030](https://arxiv.org/abs/2102.13030) [cs.CL]** |
|           | (or **[arXiv:2102.13030v1](https://arxiv.org/abs/2102.13030v1) [cs.CL]** for this version) |



# 2021-02-25

[Return to Index](#Index)



<h2 id="2021-02-25-1">1. Do Transformer Modifications Transfer Across Implementations and Applications?</h2>

Title: [Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/abs/2102.11972)

Authors: [Sharan Narang](https://arxiv.org/search/cs?searchtype=author&query=Narang%2C+S), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W), [Thibault Fevry](https://arxiv.org/search/cs?searchtype=author&query=Fevry%2C+T), [Michael Matena](https://arxiv.org/search/cs?searchtype=author&query=Matena%2C+M), [Karishma Malkan](https://arxiv.org/search/cs?searchtype=author&query=Malkan%2C+K), [Noah Fiedel](https://arxiv.org/search/cs?searchtype=author&query=Fiedel%2C+N), [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N), [Zhenzhong Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Z), [Yanqi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Jake Marcus](https://arxiv.org/search/cs?searchtype=author&query=Marcus%2C+J), [Adam Roberts](https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+A), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C)

> The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.11972](https://arxiv.org/abs/2102.11972) [cs.LG]** |
|           | (or **[arXiv:2102.11972v1](https://arxiv.org/abs/2102.11972v1) [cs.LG]** for this version) |





<h2 id="2021-02-25-2">2. Teach Me to Explain: A Review of Datasets for Explainable NLP</h2>

Title: [Teach Me to Explain: A Review of Datasets for Explainable NLP](https://arxiv.org/abs/2102.12060)

Authors: [Sarah Wiegreffe](https://arxiv.org/search/cs?searchtype=author&query=Wiegreffe%2C+S), [Ana Marasović](https://arxiv.org/search/cs?searchtype=author&query=Marasović%2C+A)

> Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as a loss signal to train models to produce explanations for their predictions, and as a means to evaluate the quality of model-generated explanations. In this review, we identify three predominant classes of explanations (highlights, free-text, and structured), organize the literature on annotating each type, point to what has been learned to date, and give recommendations for collecting ExNLP datasets in the future.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12060](https://arxiv.org/abs/2102.12060) [cs.CL]** |
|           | (or **[arXiv:2102.12060v1](https://arxiv.org/abs/2102.12060v1) [cs.CL]** for this version) |





<h2 id="2021-02-25-3">3. Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning</h2>

Title: [Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning](https://arxiv.org/abs/2102.12266)

Authors: [Gordon Buck](https://arxiv.org/search/cs?searchtype=author&query=Buck%2C+G), [Andreas Vlachos](https://arxiv.org/search/cs?searchtype=author&query=Vlachos%2C+A)

> Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.

| Comments: | Camera ready for the EACL workshop Adapt-NLP 2021            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.12266](https://arxiv.org/abs/2102.12266) [cs.CL]** |
|           | (or **[arXiv:2102.12266v1](https://arxiv.org/abs/2102.12266v1) [cs.CL]** for this version) |





<h2 id="2021-02-25-4">4. Creolizing the Web</h2>

Title: [Creolizing the Web](https://arxiv.org/abs/2102.12382)

Authors: [Abhinav Tamaskar](https://arxiv.org/search/cs?searchtype=author&query=Tamaskar%2C+A), [Roy Rinberg](https://arxiv.org/search/cs?searchtype=author&query=Rinberg%2C+R), [Sunandan Chakraborty](https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+S), [Bud Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+B)

> The evolution of language has been a hotly debated subject with contradicting hypotheses and unreliable claims. Drawing from signalling games, dynamic population mechanics, machine learning and algebraic topology, we present a method for detecting evolutionary patterns in a sociological model of language evolution. We develop a minimalistic model that provides a rigorous base for any generalized evolutionary model for language based on communication between individuals. We also discuss theoretical guarantees of this model, ranging from stability of language representations to fast convergence of language by temporal communication and language drift in an interactive setting. Further we present empirical results and their interpretations on a real world dataset from \rdt to identify communities and echo chambers for opinions, thus placing obstructions to reliable communication among communities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12382](https://arxiv.org/abs/2102.12382) [cs.CL]** |
|           | (or **[arXiv:2102.12382v1](https://arxiv.org/abs/2102.12382v1) [cs.CL]** for this version) |





<h2 id="2021-02-25-5">5. Task-Specific Pre-Training and Cross Lingual Transfer for Code-Switched Data</h2>

Title: [Task-Specific Pre-Training and Cross Lingual Transfer for Code-Switched Data](https://arxiv.org/abs/2102.12407)

Authors: [Akshat Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Sai Krishna Rallabandi](https://arxiv.org/search/cs?searchtype=author&query=Rallabandi%2C+S+K), [Alan Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A)

> Using task-specific pre-training and leveraging cross-lingual transfer are two of the most popular ways to handle code-switched data. In this paper, we aim to compare the effects of both for the task of sentiment analysis. We work with two Dravidian Code-Switched languages - Tamil-Engish and Malayalam-English and four different BERT based models. We compare the effects of task-specific pre-training and cross-lingual transfer and find that task-specific pre-training results in superior zero-shot and supervised performance when compared to performance achieved by leveraging cross-lingual transfer from multilingual BERT models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12407](https://arxiv.org/abs/2102.12407) [cs.CL]** |
|           | (or **[arXiv:2102.12407v1](https://arxiv.org/abs/2102.12407v1) [cs.CL]** for this version) |





<h2 id="2021-02-25-6">6. When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</h2>

Title: [When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute](https://arxiv.org/abs/2102.12459)

Authors: [Tao Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+T)

> Large language models have become increasingly difficult to train because of the required computation time and cost. In this work, we present SRU++, a recurrent unit with optional built-in attention that exhibits state-of-the-art modeling capacity and training efficiency. On standard language modeling benchmarks such as enwik8 and Wiki-103 datasets, our model obtains better perplexity and bits-per-character (bpc) while using 2.5x-10x less training time and cost compared to top-performing Transformer models. Our results reaffirm that attention is not all we need and can be complementary to other sequential modeling modules. Moreover, fast recurrence with little attention can be a leading model architecture.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.12459](https://arxiv.org/abs/2102.12459) [cs.CL]** |
|           | (or **[arXiv:2102.12459v1](https://arxiv.org/abs/2102.12459v1) [cs.CL]** for this version) |



# 2021-02-24

[Return to Index](#Index)



<h2 id="2021-02-24-1">1. RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning</h2>

Title: [RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning](https://arxiv.org/abs/2102.11278)

Authors: [Usama Khalid](https://arxiv.org/search/cs?searchtype=author&query=Khalid%2C+U), [Mirza Omer Beg](https://arxiv.org/search/cs?searchtype=author&query=Beg%2C+M+O), [Muhammad Umair Arshad](https://arxiv.org/search/cs?searchtype=author&query=Arshad%2C+M+U)

> In recent studies, it has been shown that Multilingual language models underperform their monolingual counterparts. It is also a well-known fact that training and maintaining monolingual models for each language is a costly and time-consuming process. Roman Urdu is a resource-starved language used popularly on social media platforms and chat apps. In this research, we propose a novel dataset of scraped tweets containing 54M tokens and 3M sentences. Additionally, we also propose RUBERT a bilingual Roman Urdu model created by additional pretraining of English BERT. We compare its performance with a monolingual Roman Urdu BERT trained from scratch and a multilingual Roman Urdu BERT created by additional pretraining of Multilingual BERT. We show through our experiments that additional pretraining of the English BERT produces the most notable performance improvement.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2102.10958](https://arxiv.org/abs/2102.10958) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.11278](https://arxiv.org/abs/2102.11278) [cs.CL]** |
|           | (or **[arXiv:2102.11278v1](https://arxiv.org/abs/2102.11278v1) [cs.CL]** for this version) |





<h2 id="2021-02-24-2">2. Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation</h2>

Title: [Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation](https://arxiv.org/abs/2102.11387)

Authors: [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Andy Mingren Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+A+M), [Yishu Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y), [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.

| Comments: | Long paper accepted to EACL 2021, Camera-ready version       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.11387](https://arxiv.org/abs/2102.11387) [cs.CL]** |
|           | (or **[arXiv:2102.11387v1](https://arxiv.org/abs/2102.11387v1) [cs.CL]** for this version) |





<h2 id="2021-02-24-3">3. MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture</h2>

Title: [MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture](https://arxiv.org/abs/2102.11402)

Authors: [Wancong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Ieshan Vaidya](https://arxiv.org/search/cs?searchtype=author&query=Vaidya%2C+I)

> MixUp is a computer vision data augmentation technique that uses convex interpolations of input data and their labels to enhance model generalization during training. However, the application of MixUp to the natural language understanding (NLU) domain has been limited, due to the difficulty of interpolating text directly in the input space. In this study, we propose MixUp methods at the Input, Manifold, and sentence embedding levels for the transformer architecture, and apply them to finetune the BERT model for a diverse set of NLU tasks. We find that MixUp can improve model performance, as well as reduce test loss and model calibration error by up to 50%.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.11402](https://arxiv.org/abs/2102.11402) [cs.CL]** |
|           | (or **[arXiv:2102.11402v1](https://arxiv.org/abs/2102.11402v1) [cs.CL]** for this version) |





<h2 id="2021-02-24-4">4. Exploring Supervised and Unsupervised Rewards in Machine Translation</h2>

Title: [Exploring Supervised and Unsupervised Rewards in Machine Translation](https://arxiv.org/abs/2102.11403)

Authors: [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Zixu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Marina Fomicheva](https://arxiv.org/search/cs?searchtype=author&query=Fomicheva%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words.

| Comments: | Long paper accepted to EACL 2021, Camera-ready version       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.11403](https://arxiv.org/abs/2102.11403) [cs.CL]** |
|           | (or **[arXiv:2102.11403v1](https://arxiv.org/abs/2102.11403v1) [cs.CL]** for this version) |





<h2 id="2021-02-24-5">5. Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations</h2>

Title: [Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations](https://arxiv.org/abs/2102.11573)

Authors: [Nikolaos Flemotomos](https://arxiv.org/search/cs?searchtype=author&query=Flemotomos%2C+N), [Victor R. Martinez](https://arxiv.org/search/cs?searchtype=author&query=Martinez%2C+V+R), [Zhuohao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Torrey A. Creed](https://arxiv.org/search/cs?searchtype=author&query=Creed%2C+T+A), [David C. Atkins](https://arxiv.org/search/cs?searchtype=author&query=Atkins%2C+D+C), [Shrikanth Narayanan](https://arxiv.org/search/cs?searchtype=author&query=Narayanan%2C+S)

> During a psychotherapy session, the counselor typically adopts techniques which are codified along specific dimensions (e.g., 'displays warmth and confidence', or 'attempts to set up collaboration') to facilitate the evaluation of the session. Those constructs, traditionally scored by trained human raters, reflect the complex nature of psychotherapy and highly depend on the context of the interaction. Recent advances in deep contextualized language models offer an avenue for accurate in-domain linguistic representations which can lead to robust recognition and scoring of such psychotherapy-relevant behavioral constructs, and support quality assurance and supervision. In this work, a BERT-based model is proposed for automatic behavioral scoring of a specific type of psychotherapy, called Cognitive Behavioral Therapy (CBT), where prior work is limited to frequency-based language features and/or short text excerpts which do not capture the unique elements involved in a spontaneous long conversational interaction. The model is trained in a multi-task manner in order to achieve higher interpretability. BERT-based representations are further augmented with available therapy metadata, providing relevant non-linguistic context and leading to consistent performance improvements.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.11573](https://arxiv.org/abs/2102.11573) [cs.CL]** |
|           | (or **[arXiv:2102.11573v1](https://arxiv.org/abs/2102.11573v1) [cs.CL]** for this version) |





<h2 id="2021-02-24-6">6. Paraphrases do not explain word analogies</h2>

Title: [Paraphrases do not explain word analogies](https://arxiv.org/abs/2102.11749)

Authors: [Louis Fournier](https://arxiv.org/search/cs?searchtype=author&query=Fournier%2C+L), [Ewan Dunbar](https://arxiv.org/search/cs?searchtype=author&query=Dunbar%2C+E)

> Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between "jump" and "jumped" will be in a similar direction to that of "walk" and "walked," and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically.

| Comments: | To appear in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2102.11749](https://arxiv.org/abs/2102.11749) [cs.CL]** |
|           | (or **[arXiv:2102.11749v1](https://arxiv.org/abs/2102.11749v1) [cs.CL]** for this version) |





# 2021-02-23

[Return to Index](#Index)



<h2 id="2021-02-23-1">1. VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining</h2>

Title: [VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining](https://arxiv.org/abs/2102.10407)

Authors: [Jun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Han Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+H), [Kai Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+K), [Boyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Mohamed Elhoseiny](https://arxiv.org/search/cs?searchtype=author&query=Elhoseiny%2C+M)

> In this paper, we aim to improve the data efficiency of image captioning. We propose VisualGPT, a data-efficient image captioning model that leverages the linguistic knowledge from a large pretrained language model (LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining.We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder on a small amount of in-domain training data. The pro-posed self-resurrecting activation unit produces sparse activations but is not susceptible to zero gradients. When trained on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions, the proposed model, VisualGPT, surpasses strong image captioning baselines. VisualGPT outperforms the best baseline model by up to 10.8% CIDEr on MS COCO and up to 5.4% CIDEr on Conceptual Captions.We also perform a series of ablation studies to quantify the utility of each system component. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: [this https URL](https://github.com/Vision-CAIR/VisualGPT).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.10407](https://arxiv.org/abs/2102.10407) [cs.CV]** |
|           | (or **[arXiv:2102.10407v1](https://arxiv.org/abs/2102.10407v1) [cs.CV]** for this version) |





<h2 id="2021-02-23-2">2. Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer</h2>

Title: [Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772)

Authors: [Ronghang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+R), [Amanpreet Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A)

> We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters to all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving comparable performance to well-established prior work on each domain under the same supervision with a compact set of model parameters. Code will be released in MMF at [this https URL](https://mmf.sh/).

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2102.10772](https://arxiv.org/abs/2102.10772) [cs.CV]** |
|           | (or **[arXiv:2102.10772v1](https://arxiv.org/abs/2102.10772v1) [cs.CV]** for this version) |





<h2 id="2021-02-23-3">3. Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case</h2>

Title: [Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case](https://arxiv.org/abs/2102.11115)

Authors: [Adam Dahlgren Lindström](https://arxiv.org/search/cs?searchtype=author&query=Lindström%2C+A+D), [Suna Bensch](https://arxiv.org/search/cs?searchtype=author&query=Bensch%2C+S), [Johanna Björklund](https://arxiv.org/search/cs?searchtype=author&query=Björklund%2C+J), [Frank Drewes](https://arxiv.org/search/cs?searchtype=author&query=Drewes%2C+F)

> Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through machine learning, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion of probing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for embeddings of image-caption pairs, (ii) define three concrete probing tasks within our general framework, (iii) train classifiers to probe for those properties, and (iv) compare various state-of-the-art embeddings under the lens of the proposed probing tasks. Our experiments reveal an up to 12% increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.

| Comments: | Submitted July 1 2020, COLING 2020 main conference           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| DOI:      | [10.18653/v1/2020.coling-main.64](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2020.coling-main.64&v=0ae187d0) |
| Cite as:  | **[arXiv:2102.11115](https://arxiv.org/abs/2102.11115) [cs.LG]** |
|           | (or **[arXiv:2102.11115v1](https://arxiv.org/abs/2102.11115v1) [cs.LG]** for this version) |





<h2 id="2021-02-23-4">4. Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging</h2>

Title: [Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging](https://arxiv.org/abs/2102.10160)

Authors: [Emmanouil Stergiadis](https://arxiv.org/search/cs?searchtype=author&query=Stergiadis%2C+E), [Satendra Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Fedor Kovalev](https://arxiv.org/search/cs?searchtype=author&query=Kovalev%2C+F), [Pavel Levin](https://arxiv.org/search/cs?searchtype=author&query=Levin%2C+P)

> Many modern Neural Machine Translation (NMT) systems are trained on nonhomogeneous datasets with several distinct dimensions of variation (e.g. domain, source, generation method, style, etc.). We describe and empirically evaluate multidimensional tagging (MDT), a simple yet effective method for passing sentence-level information to the model. Our human and BLEU evaluation results show that MDT can be applied to the problem of multi-domain adaptation and significantly reduce training costs without sacrificing the translation quality on any of the constituent domains.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.10160](https://arxiv.org/abs/2102.10160) [cs.CL]** |
|           | (or **[arXiv:2102.10160v1](https://arxiv.org/abs/2102.10160v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-5">5. Machine Translation Customization via Automatic Training Data Selection from the Web</h2>

Title: [Machine Translation Customization via Automatic Training Data Selection from the Web](https://arxiv.org/abs/2102.10243)

Authors: [Thuy Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Alessandro Moschitti](https://arxiv.org/search/cs?searchtype=author&query=Moschitti%2C+A)

> Machine translation (MT) systems, especially when designed for an industrial setting, are trained with general parallel data derived from the Web. Thus, their style is typically driven by word/structure distribution coming from the average of many domains. In contrast, MT customers want translations to be specialized to their domain, for which they are typically able to provide text samples. We describe an approach for customizing MT systems on specific domains by selecting data similar to the target customer data to train neural translation models. We build document classifiers using monolingual target data, e.g., provided by the customers to select parallel training data from Web crawled data. Finally, we train MT models on our automatically selected data, obtaining a system specialized to the target domain. We tested our approach on the benchmark from WMT-18 Translation Task for News domains enabling comparisons with state-of-the-art MT systems. The results show that our models outperform the top systems while using less data and smaller models.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ECIR 2021                                                    |
| Cite as:           | **[arXiv:2102.10243](https://arxiv.org/abs/2102.10243) [cs.CL]** |
|                    | (or **[arXiv:2102.10243v1](https://arxiv.org/abs/2102.10243v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-6">6. CDA: a Cost Efficient Content-based Multilingual Web Document Aligner</h2>

Title: [CDA: a Cost Efficient Content-based Multilingual Web Document Aligner](https://arxiv.org/abs/2102.10246)

Authors: [Thuy Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Alessandro Moschitti](https://arxiv.org/search/cs?searchtype=author&query=Moschitti%2C+A)

> We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TF-IDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EACL 2021                                                    |
| Cite as:           | **[arXiv:2102.10246](https://arxiv.org/abs/2102.10246) [cs.CL]** |
|                    | (or **[arXiv:2102.10246v1](https://arxiv.org/abs/2102.10246v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-7">7. Deep Structured Feature Networks for Table Detection and Tabular Data Extraction from Scanned Financial Document Images</h2>

Title: [Deep Structured Feature Networks for Table Detection and Tabular Data Extraction from Scanned Financial Document Images](https://arxiv.org/abs/2102.10287)

Authors: [Siwen Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Mengting Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M), [Yiwen Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Wanying Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Josiah Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+J)

> Automatic table detection in PDF documents has achieved a great success but tabular data extraction are still challenging due to the integrity and noise issues in detected table areas. The accurate data extraction is extremely crucial in finance area. Inspired by this, the aim of this research is proposing an automated table detection and tabular data extraction from financial PDF documents. We proposed a method that consists of three main processes, which are detecting table areas with a Faster R-CNN (Region-based Convolutional Neural Network) model with Feature Pyramid Network (FPN) on each page image, extracting contents and structures by a compounded layout segmentation technique based on optical character recognition (OCR) and formulating regular expression rules for table header separation. The tabular data extraction feature is embedded with rule-based filtering and restructuring functions that are highly scalable. We annotate a new Financial Documents dataset with table regions for the experiment. The excellent table detection performance of the detection model is obtained from our customized dataset. The main contributions of this paper are proposing the Financial Documents dataset with table-area annotations, the superior detection model and the rule-based layout segmentation technique for the tabular data extraction from PDF files.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.10287](https://arxiv.org/abs/2102.10287) [cs.CL]** |
|           | (or **[arXiv:2102.10287v1](https://arxiv.org/abs/2102.10287v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-8">8. Understanding and Enhancing the Use of Context for Machine Translation</h2>

Title: [Understanding and Enhancing the Use of Context for Machine Translation](https://arxiv.org/abs/2102.10437)

Authors: [Marzieh Fadaee](https://arxiv.org/search/cs?searchtype=author&query=Fadaee%2C+M)

> To understand and infer meaning in language, neural models have to learn complicated nuances. Discovering distinctive linguistic phenomena from data is not an easy task. For instance, lexical ambiguity is a fundamental feature of language which is challenging to learn. Even more prominently, inferring the meaning of rare and unseen lexical units is difficult with neural networks. Meaning is often determined from context. With context, languages allow meaning to be conveyed even when the specific words used are not known by the reader. To model this learning process, a system has to learn from a few instances in context and be able to generalize well to unseen cases. The learning process is hindered when training data is scarce for a task. Even with sufficient data, learning patterns for the long tail of the lexical distribution is challenging. In this thesis, we focus on understanding certain potentials of contexts in neural models and design augmentation models to benefit from them. We focus on machine translation as an important instance of the more general language understanding problem. To translate from a source language to a target language, a neural model has to understand the meaning of constituents in the provided context and generate constituents with the same meanings in the target language. This task accentuates the value of capturing nuances of language and the necessity of generalization from few observations. The main problem we study in this thesis is what neural machine translation models learn from data and how we can devise more focused contexts to enhance this learning. Looking more in-depth into the role of context and the impact of data on learning models is essential to advance the NLP field. Moreover, it helps highlight the vulnerabilities of current neural networks and provides insights into designing more robust models.

| Comments: | PhD dissertation defended on November 10th, 2020             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.10437](https://arxiv.org/abs/2102.10437) [cs.CL]** |
|           | (or **[arXiv:2102.10437v1](https://arxiv.org/abs/2102.10437v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-9">9. Bilingual Language Modeling, A transfer learning technique for Roman Urdu</h2>

Title: [Bilingual Language Modeling, A transfer learning technique for Roman Urdu](https://arxiv.org/abs/2102.10958)

Authors: [Usama Khalid](https://arxiv.org/search/cs?searchtype=author&query=Khalid%2C+U), [Mirza Omer Beg](https://arxiv.org/search/cs?searchtype=author&query=Beg%2C+M+O), [Muhammad Umair Arshad](https://arxiv.org/search/cs?searchtype=author&query=Arshad%2C+M+U)

> Pretrained language models are now of widespread use in Natural Language Processing. Despite their success, applying them to Low Resource languages is still a huge challenge. Although Multilingual models hold great promise, applying them to specific low-resource languages e.g. Roman Urdu can be excessive. In this paper, we show how the code-switching property of languages may be used to perform cross-lingual transfer learning from a corresponding high resource language. We also show how this transfer learning technique termed Bilingual Language Modeling can be used to produce better performing models for Roman Urdu. To enable training and experimentation, we also present a collection of novel corpora for Roman Urdu extracted from various sources and social networking sites, e.g. Twitter. We train Monolingual, Multilingual, and Bilingual models of Roman Urdu - the proposed bilingual model achieves 23% accuracy compared to the 2% and 11% of the monolingual and multilingual models respectively in the Masked Language Modeling (MLM) task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.10958](https://arxiv.org/abs/2102.10958) [cs.CL]** |
|           | (or **[arXiv:2102.10958v1](https://arxiv.org/abs/2102.10958v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-10">10. Towards Personalised and Document-level Machine Translation of Dialogue</h2>

Title: [Towards Personalised and Document-level Machine Translation of Dialogue](https://arxiv.org/abs/2102.10979)

Authors: [Sebastian T. Vincent](https://arxiv.org/search/cs?searchtype=author&query=Vincent%2C+S+T)

> State-of-the-art (SOTA) neural machine translation (NMT) systems translate texts at sentence level, ignoring context: intra-textual information, like the previous sentence, and extra-textual information, like the gender of the speaker. Because of that, some sentences are translated incorrectly. Personalised NMT (PersNMT) and document-level NMT (DocNMT) incorporate this information into the translation process. Both fields are relatively new and previous work within them is limited. Moreover, there are no readily available robust evaluation metrics for them, which makes it difficult to develop better systems, as well as track global progress and compare different methods. This thesis proposal focuses on PersNMT and DocNMT for the domain of dialogue extracted from TV subtitles in five languages: English, Brazilian Portuguese, German, French and Polish. Three main challenges are addressed: (1) incorporating extra-textual information directly into NMT systems; (2) improving the machine translation of cohesion devices; (3) reliable evaluation for PersNMT and DocNMT.

| Comments: | Thesis Proposal, 6 pages, 7 figures, accepted to the EACL2021 Student Workshop |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.10979](https://arxiv.org/abs/2102.10979) [cs.CL]** |
|           | (or **[arXiv:2102.10979v1](https://arxiv.org/abs/2102.10979v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-11">11. Multimodal Punctuation Prediction with Contextual Dropout</h2>

Title: [Multimodal Punctuation Prediction with Contextual Dropout](https://arxiv.org/abs/2102.11012)

Authors: [Andrew Silva](https://arxiv.org/search/cs?searchtype=author&query=Silva%2C+A), [Barry-John Theobald](https://arxiv.org/search/cs?searchtype=author&query=Theobald%2C+B), [Nicholas Apostoloff](https://arxiv.org/search/cs?searchtype=author&query=Apostoloff%2C+N)

> Automatic speech recognition (ASR) is widely used in consumer electronics. ASR greatly improves the utility and accessibility of technology, but usually the output is only word sequences without punctuation. This can result in ambiguity in inferring user-intent. We first present a transformer-based approach for punctuation prediction that achieves 8% improvement on the IWSLT 2012 TED Task, beating the previous state of the art [1]. We next describe our multimodal model that learns from both text and audio, which achieves 8% improvement over the text-only algorithm on an internal dataset for which we have both the audio and transcriptions. Finally, we present an approach to learning a model using contextual dropout that allows us to handle variable amounts of future context at test time.

| Comments:    | Accepted for publication at ICASSP 2021                      |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2102.11012](https://arxiv.org/abs/2102.11012) [cs.CL]** |
|              | (or **[arXiv:2102.11012v1](https://arxiv.org/abs/2102.11012v1) [cs.CL]** for this version) |





<h2 id="2021-02-23-12">12. Position Information in Transformers: An Overview</h2>

Title: [Position Information in Transformers: An Overview](https://arxiv.org/abs/2102.11090)

Authors: [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Martin Schmitt](https://arxiv.org/search/cs?searchtype=author&query=Schmitt%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.11090](https://arxiv.org/abs/2102.11090) [cs.CL]** |
|           | (or **[arXiv:2102.11090v1](https://arxiv.org/abs/2102.11090v1) [cs.CL]** for this version) |





# 2021-02-22

[Return to Index](#Index)



<h2 id="2021-02-22-1">1. Calibrate Before Use: Improving Few-Shot Performance of Language Models</h2>

Title: [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690)

Authors: [Tony Z. Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T+Z), [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Shi Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D), [Sameer Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

> GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.09690](https://arxiv.org/abs/2102.09690) [cs.CL]** |
|           | (or **[arXiv:2102.09690v1](https://arxiv.org/abs/2102.09690v1) [cs.CL]** for this version) |





<h2 id="2021-02-22-2">2. Multilingual Augmenter: The Model Chooses</h2>

Title: [Multilingual Augmenter: The Model Chooses](https://arxiv.org/abs/2102.09708)

Authors: [Matthew Ciolino](https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M), [David Noever](https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D), [Josh Kalin](https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J)

> Natural Language Processing (NLP) relies heavily on training data. Transformers, as they have gotten bigger, have required massive amounts of training data. To satisfy this requirement, text augmentation should be looked at as a way to expand your current dataset and to generalize your models. One text augmentation we will look at is translation augmentation. We take an English sentence and translate it to another language before translating it back to English. In this paper, we look at the effect of 108 different language back translations on various metrics and text embeddings.

| Comments: | 18 Pages, 10 Figures, 4 Tables, 37 References                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.09708](https://arxiv.org/abs/2102.09708) [cs.CL]** |
|           | (or **[arXiv:2102.09708v1](https://arxiv.org/abs/2102.09708v1) [cs.CL]** for this version) |



# 2021-02-19

[Return to Index](#Index)



<h2 id="2021-02-19-1">1. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</h2>

Title: [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)

Authors: [Soravit Changpinyo](https://arxiv.org/search/cs?searchtype=author&query=Changpinyo%2C+S), [Piyush Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+P), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements, inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset, as well as benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. The quantitative and qualitative results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08981](https://arxiv.org/abs/2102.08981) [cs.CV]** |
|           | (or **[arXiv:2102.08981v1](https://arxiv.org/abs/2102.08981v1) [cs.CV]** for this version) |





<h2 id="2021-02-19-2">2. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer</h2>

Title: [Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](https://arxiv.org/abs/2102.09550)

Authors: [Rafał Powalski](https://arxiv.org/search/cs?searchtype=author&query=Powalski%2C+R), [Łukasz Borchmann](https://arxiv.org/search/cs?searchtype=author&query=Borchmann%2C+Ł), [Dawid Jurkiewicz](https://arxiv.org/search/cs?searchtype=author&query=Jurkiewicz%2C+D), [Tomasz Dwojak](https://arxiv.org/search/cs?searchtype=author&query=Dwojak%2C+T), [Michał Pietruszka](https://arxiv.org/search/cs?searchtype=author&query=Pietruszka%2C+M), [Gabriela Pałka](https://arxiv.org/search/cs?searchtype=author&query=Pałka%2C+G)

> We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of solving all problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. We trained our network on real-world documents with different layouts, such as tables, figures, and forms. Our novel approach achieves state-of-the-art in extracting information from documents and answering questions, demanding layout understanding (DocVQA, CORD, WikiOps, SROIE). At the same time, we simplify the process by employing an end-to-end model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.09550](https://arxiv.org/abs/2102.09550) [cs.CL]** |
|           | (or **[arXiv:2102.09550v1](https://arxiv.org/abs/2102.09550v1) [cs.CL]** for this version) |





# 2021-02-18

[Return to Index](#Index)



<h2 id="2021-02-18-1">1. COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</h2>

Title: [COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](https://arxiv.org/abs/2102.08473)

Authors: [Yu Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Chenyan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Saurabh Tiwary](https://arxiv.org/search/cs?searchtype=author&query=Tiwary%2C+S), [Paul Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+P), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X)

> We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: The first task, corrective language modeling, learns to correct the auxiliary model's corruptions by recovering the original tokens. The second task, sequence contrastive learning, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM's advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08473](https://arxiv.org/abs/2102.08473) [cs.CL]** |
|           | (or **[arXiv:2102.08473v1](https://arxiv.org/abs/2102.08473v1) [cs.CL]** for this version) |





<h2 id="2021-02-18-2">2. Sparsely Factored Neural Machine Translation</h2>

Title: [Sparsely Factored Neural Machine Translation](https://arxiv.org/abs/2102.08934)

Authors: [Noe Casas](https://arxiv.org/search/cs?searchtype=author&query=Casas%2C+N), [Jose A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> The standard approach to incorporate linguistic information to neural machine translation systems consists in maintaining separate vocabularies for each of the annotated features to be incorporated (e.g. POS tags, dependency relation label), embed them, and then aggregate them with each subword in the word they belong to. This approach, however, cannot easily accommodate annotation schemes that are not dense for every word.
> We propose a method suited for such a case, showing large improvements in out-of-domain data, and comparable quality for the in-domain data. Experiments are performed in morphologically-rich languages like Basque and German, for the case of low-resource scenarios.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08934](https://arxiv.org/abs/2102.08934) [cs.CL]** |
|           | (or **[arXiv:2102.08934v1](https://arxiv.org/abs/2102.08934v1) [cs.CL]** for this version) |



# 2021-02-17

[Return to Index](#Index)



<h2 id="2021-02-17-1">1. Meta Back-translation</h2>

Title: [Meta Back-translation](https://arxiv.org/abs/2102.07847)

Authors: [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results. In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudo-parallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. Our code will be made available.

| Comments: | Accepted to ICLR 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.07847](https://arxiv.org/abs/2102.07847) [cs.CL]** |
|           | (or **[arXiv:2102.07847v1](https://arxiv.org/abs/2102.07847v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-2">2. Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet</h2>

Title: [Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet](https://arxiv.org/abs/2102.08036)

Authors: [M. Onat Topal](https://arxiv.org/search/cs?searchtype=author&query=Topal%2C+M+O), [Anil Bas](https://arxiv.org/search/cs?searchtype=author&query=Bas%2C+A), [Imke van Heerden](https://arxiv.org/search/cs?searchtype=author&query=van+Heerden%2C+I)

> Recent years have seen a proliferation of attention mechanisms and the rise of Transformers in Natural Language Generation (NLG). Previously, state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing gradient problems; as sentences grew larger, distance between positions remained linear, and sequential computation hindered parallelization since sentences were processed word by word. Transformers usher in a new era. In this paper, we explore three major Transformer-based models, namely GPT, BERT, and XLNet, that carry significant implications for the field. NLG is a burgeoning area that is now bolstered with rapid developments in attention mechanisms. From poetry generation to summarization, text generation derives benefit as Transformer-based language models achieve groundbreaking results.

| Comments: | Accepted as oral presentation to ICIDAAI 2021 - Short Paper  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.08036](https://arxiv.org/abs/2102.08036) [cs.CL]** |
|           | (or **[arXiv:2102.08036v1](https://arxiv.org/abs/2102.08036v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-3">3. Non-Autoregressive Text Generation with Pre-trained Language Models</h2>

Title: [Non-Autoregressive Text Generation with Pre-trained Language Models](https://arxiv.org/abs/2102.08220)

Authors: [Yixuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [David Vandyke](https://arxiv.org/search/cs?searchtype=author&query=Vandyke%2C+D), [Simon Baker](https://arxiv.org/search/cs?searchtype=author&query=Baker%2C+S), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.08220](https://arxiv.org/abs/2102.08220) [cs.CL]** |
|           | (or **[arXiv:2102.08220v1](https://arxiv.org/abs/2102.08220v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-4">4. Revisiting Language Encoding in Learning Multilingual Representations</h2>

Title: [Revisiting Language Encoding in Learning Multilingual Representations](https://arxiv.org/abs/2102.08357)

Authors: [Shengjie Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Kaiyuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+K), [Shuxin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S), [Guolin Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+G), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Transformer has demonstrated its great power to learn contextual word representations for multiple languages in a single model. To process multilingual sentences in the model, a learnable vector is usually assigned to each language, which is called "language embedding". The language embedding can be either added to the word embedding or attached at the beginning of the sentence. It serves as a language-specific signal for the Transformer to capture contextual representations across languages. In this paper, we revisit the use of language embedding and identify several problems in the existing formulations. By investigating the interaction between language embedding and word embedding in the self-attention module, we find that the current methods cannot reflect the language-specific word correlation well. Given these findings, we propose a new approach called Cross-lingual Language Projection (XLP) to replace language embedding. For a sentence, XLP projects the word embeddings into language-specific semantic space, and then the projected embeddings will be fed into the Transformer model to process with their language-specific meanings. In such a way, XLP achieves the purpose of appropriately encoding "language" in a multilingual Transformer model. Experimental results show that XLP can freely and significantly boost the model performance on extensive multilingual benchmark datasets. Codes and models will be released at [this https URL](https://github.com/lsj2408/XLP).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08357](https://arxiv.org/abs/2102.08357) [cs.CL]** |
|           | (or **[arXiv:2102.08357v1](https://arxiv.org/abs/2102.08357v1) [cs.CL]** for this version) |







# 2021-02-16

[Return to Index](#Index)



<h2 id="2021-02-16-1">1. MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training</h2>

Title: [MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training](https://arxiv.org/abs/2102.07380)

Authors: [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N), [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R)

> This paper presents a self-supervised learning method for pointer-generator networks to improve spoken-text normalization. Spoken-text normalization that converts spoken-style text into style normalized text is becoming an important technology for improving subsequent processing such as machine translation and summarization. The most successful spoken-text normalization method to date is sequence-to-sequence (seq2seq) mapping using pointer-generator networks that possess a copy mechanism from an input sequence. However, these models require a large amount of paired data of spoken-style text and style normalized text, and it is difficult to prepare such a volume of data. In order to construct spoken-text normalization model from the limited paired data, we focus on self-supervised learning which can utilize unpaired text data to improve seq2seq models. Unfortunately, conventional self-supervised learning methods do not assume that pointer-generator networks are utilized. Therefore, we propose a novel self-supervised learning method, MAsked Pointer-Generator Network (MAPGN). The proposed method can effectively pre-train the pointer-generator network by learning to fill masked tokens using the copy mechanism. Our experiments demonstrate that MAPGN is more effective for pointer-generator networks than the conventional self-supervised learning methods in two spoken-text normalization tasks.

| Comments: | Accepted at ICASSP 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.07380](https://arxiv.org/abs/2102.07380) [cs.CL]** |
|           | (or **[arXiv:2102.07380v1](https://arxiv.org/abs/2102.07380v1) [cs.CL]** for this version) |







# 2021-02-15

[Return to Index](#Index)



<h2 id="2021-02-15-1">1. Continuous Learning in Neural Machine Translation using Bilingual Dictionaries</h2>

Title: [Continuous Learning in Neural Machine Translation using Bilingual Dictionaries](https://arxiv.org/abs/2102.06558)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

> While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language.
> In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.

| Comments: | 9 pages, EACL 2021                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.06558](https://arxiv.org/abs/2102.06558) [cs.CL]** |
|           | (or **[arXiv:2102.06558v1](https://arxiv.org/abs/2102.06558v1) [cs.CL]** for this version) |





<h2 id="2021-02-15-2">2. Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders</h2>

Title: [Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders](https://arxiv.org/abs/2102.06578)

Authors: [Junwei Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+J), [Yu Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Hong Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Recently, universal neural machine translation (NMT) with shared encoder-decoder gained good performance on zero-shot translation. Unlike universal NMT, jointly trained language-specific encoders-decoders aim to achieve universal representation across non-shared modules, each of which is for a language or language family. The non-shared architecture has the advantage of mitigating internal language competition, especially when the shared vocabulary and model parameters are restricted in their size. However, the performance of using multiple encoders and decoders on zero-shot translation still lags behind universal NMT. In this work, we study zero-shot translation using language-specific encoders-decoders. We propose to generalize the non-shared architecture and universal NMT by differentiating the Transformer layers between language-specific and interlingua. By selectively sharing parameters and applying cross-attentions, we explore maximizing the representation universality and realizing the best alignment of language-agnostic information. We also introduce a denoising auto-encoding (DAE) objective to jointly train the model with the translation task in a multi-task manner. Experiments on two public multilingual parallel datasets show that our proposed model achieves a competitive or better results than universal NMT and strong pivot baseline. Moreover, we experiment incrementally adding new language to the trained model by only updating the new model parameters. With this little effort, the zero-shot translation between this newly added language and existing languages achieves a comparable result with the model trained jointly from scratch on all languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.06578](https://arxiv.org/abs/2102.06578) [cs.CL]** |
|           | (or **[arXiv:2102.06578v1](https://arxiv.org/abs/2102.06578v1) [cs.CL]** for this version) |





# 2021-02-12

[Return to Index](#Index)



<h2 id="2021-02-12-1">1. Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation</h2>

Title: [Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation](https://arxiv.org/abs/2102.05766)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Recently text and speech representation learning has successfully improved many language related tasks. However, all existing methods only learn from one input modality, while a unified acoustic and text representation is desired by many speech-related tasks such as speech translation. We propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly learns a unified representation for both acoustic and text in-put. Within this cross modal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that our proposed speech translation models fine-tuned from FAT-MLM substantially improve translation quality (+5.90 BLEU).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05766](https://arxiv.org/abs/2102.05766) [cs.CL]** |
|           | (or **[arXiv:2102.05766v1](https://arxiv.org/abs/2102.05766v1) [cs.CL]** for this version) |





<h2 id="2021-02-12-2">2. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</h2>

Title: [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)

Authors: [Chao Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+C), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Ye Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Yi-Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Zarana Parekh](https://arxiv.org/search/cs?searchtype=author&query=Parekh%2C+Z), [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V), [Yunhsuan Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+Y), [Zhen Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Tom Duerig](https://arxiv.org/search/cs?searchtype=author&query=Duerig%2C+T)

> Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations also set new state-of-the-art results on Flickr30K and MSCOCO benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05918](https://arxiv.org/abs/2102.05918) [cs.CV]** |
|           | (or **[arXiv:2102.05918v1](https://arxiv.org/abs/2102.05918v1) [cs.CV]** for this version) |





# 2021-02-11

[Return to Index](#Index)



<h2 id="2021-02-11-1">1. Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models</h2>

Title: [Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models](https://arxiv.org/abs/2102.05379)

Authors: [Emiel Hoogeboom](https://arxiv.org/search/stat?searchtype=author&query=Hoogeboom%2C+E), [Didrik Nielsen](https://arxiv.org/search/stat?searchtype=author&query=Nielsen%2C+D), [Priyank Jaini](https://arxiv.org/search/stat?searchtype=author&query=Jaini%2C+P), [Patrick Forré](https://arxiv.org/search/stat?searchtype=author&query=Forré%2C+P), [Max Welling](https://arxiv.org/search/stat?searchtype=author&query=Welling%2C+M)

> The field of language modelling has been largely dominated by autoregressive models, for which sampling is inherently difficult to parallelize. This paper introduces two new classes of generative models for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our models perform competitively on language modelling and modelling of image segmentation maps.

| Subjects: | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05379](https://arxiv.org/abs/2102.05379) [stat.ML]** |
|           | (or **[arXiv:2102.05379v1](https://arxiv.org/abs/2102.05379v1) [stat.ML]** for this version) |













# 2021-02-10

[Return to Index](#Index)



<h2 id="2021-02-10-1">1. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</h2>

Title: [CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](https://arxiv.org/abs/2102.04664)

Authors: [Shuai Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Shuo Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+S), [Junjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Ambrosio Blanco](https://arxiv.org/search/cs?searchtype=author&query=Blanco%2C+A), [Colin Clement](https://arxiv.org/search/cs?searchtype=author&query=Clement%2C+C), [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Ge Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lidong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Michele Tufano](https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N), [Shao Kun Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S+K), [Shengyu Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+S), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S)

> Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.









# 2021-02-09

[Return to Index](#Index)



<h2 id="2021-02-09-1">1. Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning</h2>

Title: [Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning](https://arxiv.org/abs/2102.03554)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> Recent advancements in data-to-text generation largely take on the form of neural end-to-end systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. Our benchmarks show faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.03554](https://arxiv.org/abs/2102.03554) [cs.CL]** |
|           | (or **[arXiv:2102.03554v1](https://arxiv.org/abs/2102.03554v1) [cs.CL]** for this version) |





<h2 id="2021-02-09-2">2. Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models</h2>

Title: [Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models](https://arxiv.org/abs/2102.03596)

Authors: [Lutfi Kerem Senel](https://arxiv.org/search/cs?searchtype=author&query=Senel%2C+L+K), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Recent progress in pretraining language models on large corpora has resulted in large performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with `fill in the blank' style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definition Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.

| Comments: | 5 pages, to appear in EACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.03596](https://arxiv.org/abs/2102.03596) [cs.CL]** |
|           | (or **[arXiv:2102.03596v1](https://arxiv.org/abs/2102.03596v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-3">3. Representation Learning for Natural Language Processing</h2>

Title: [Representation Learning for Natural Language Processing](https://arxiv.org/abs/2102.03732)

Authors: [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> This book aims to review and present the recent advances of distributed representation learning for NLP, including why representation learning can improve NLP, how representation learning takes part in various important topics of NLP, and what challenges are still not well addressed by distributed representation.

| Comments: | Published in Springer                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1007/978-981-15-5573-2](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-981-15-5573-2&v=bf73f6ca) |
| Cite as:  | **[arXiv:2102.03732](https://arxiv.org/abs/2102.03732) [cs.CL]** |
|           | (or **[arXiv:2102.03732v1](https://arxiv.org/abs/2102.03732v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-4">4. CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models</h2>

Title: [CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models](https://arxiv.org/abs/2102.03752)

Authors: [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many low-resource scenarios, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named "CSS-LM") to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings, and outperforms the latest supervised contrastive fine-tuning strategies. Our datasets and source code will be available to provide more details.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.03752](https://arxiv.org/abs/2102.03752) [cs.CL]** |
|           | (or **[arXiv:2102.03752v1](https://arxiv.org/abs/2102.03752v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-5">5. SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning</h2>

Title: [SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning](https://arxiv.org/abs/2102.04009)

Authors: [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Shuo Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Word alignment is essential for the down-streaming cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a super lightweight unsupervised word alignment (SLUA) model, in which bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental results on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while significantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed SLUA. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. Encouragingly, our approach achieves 16.4x speedup against GIZA++, and 50x parameter compression} compared with the Transformer-based alignment methods. We will release our code to facilitate the community.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2102.04009](https://arxiv.org/abs/2102.04009) [cs.CL]** |
|           | (or **[arXiv:2102.04009v1](https://arxiv.org/abs/2102.04009v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-6">6. Quality Estimation without Human-labeled Data</h2>

Title: [Quality Estimation without Human-labeled Data](https://arxiv.org/abs/2102.04020)

Authors: [Yi-Lin Tuan](https://arxiv.org/search/cs?searchtype=author&query=Tuan%2C+Y), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.

| Comments: | Accepted by EACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.04020](https://arxiv.org/abs/2102.04020) [cs.CL]** |
|           | (or **[arXiv:2102.04020v1](https://arxiv.org/abs/2102.04020v1) [cs.CL]** for this version) |









# 2021-02-08

[Return to Index](#Index)



<h2 id="2021-02-08-1">1. Understanding Pre-Editing for Black-Box Neural Machine Translation</h2>

Title: [Understanding Pre-Editing for Black-Box Neural Machine Translation](https://arxiv.org/abs/2102.02955)

Auhors: [Rei Miyata](https://arxiv.org/search/cs?searchtype=author&query=Miyata%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of pre-editing across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.02955](https://arxiv.org/abs/2102.02955) [cs.CL]** |
|           | (or **[arXiv:2102.02955v1](https://arxiv.org/abs/2102.02955v1) [cs.CL]** for this version) |





<h2 id="2021-02-08-2">2. Spell Correction for Azerbaijani Language using Deep Neural Networks</h2>

Title: [Spell Correction for Azerbaijani Language using Deep Neural Networks](https://arxiv.org/abs/2102.03218)

Auhors: [Ahmad Ahmadzade](https://arxiv.org/search/cs?searchtype=author&query=Ahmadzade%2C+A), [Saber Malekzadeh](https://arxiv.org/search/cs?searchtype=author&query=Malekzadeh%2C+S)

> Spell correction is used to detect and correct orthographic mistakes in texts. Most of the time, traditional dictionary lookup with string similarity methods is suitable for the languages that have a less complex structure such as the English language. However, the Azerbaijani language has a more complex structure and due to its morphological structure, the derivation of words is plenty that several words are derived from adding suffices, affixes to the words. Therefore, in this paper sequence to sequence model with an attention mechanism is used to develop spelling correction for Azerbaijani. Total 12000 wrong and correct sentence pairs used for training, and the model is tested on 1000 real-world misspelled words and F1-score results are 75% for distance 0, 90% for distance 1, and 96% for distance 2.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.03218](https://arxiv.org/abs/2102.03218) [cs.CL]** |
|           | (or **[arXiv:2102.03218v1](https://arxiv.org/abs/2102.03218v1) [cs.CL]** for this version) |







# 2021-02-05

[Return to Index](#Index)



<h2 id="2021-02-05-1">1. Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</h2>

Title: [Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models](https://arxiv.org/abs/2102.02503)

Authors: [Alex Tamkin](https://arxiv.org/search/cs?searchtype=author&query=Tamkin%2C+A), [Miles Brundage](https://arxiv.org/search/cs?searchtype=author&query=Brundage%2C+M), [Jack Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+J), [Deep Ganguli](https://arxiv.org/search/cs?searchtype=author&query=Ganguli%2C+D)

> On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.02503](https://arxiv.org/abs/2102.02503) [cs.CL]** |
|           | (or **[arXiv:2102.02503v1](https://arxiv.org/abs/2102.02503v1) [cs.CL]** for this version) |





<h2 id="2021-02-05-2">2. Unifying Vision-and-Language Tasks via Text Generation</h2>

Title: [Unifying Vision-and-Language Tasks via Text Generation](https://arxiv.org/abs/2102.02779)

Authors: [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on answering questions that have rare answers. In addition, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, which achieves similar performance to separately optimized single-task models. Our code will be publicly available at: [this https URL](https://github.com/j-min/VL-T5)

| Comments: | 16 pages, 4 figures, 13 tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.02779](https://arxiv.org/abs/2102.02779) [cs.CL]** |
|           | (or **[arXiv:2102.02779v1](https://arxiv.org/abs/2102.02779v1) [cs.CL]** for this version) |





# 2021-02-04

[Return to Index](#Index)



<h2 id="2021-02-04-1">1. The Multilingual TEDx Corpus for Speech Recognition and Translation</h2>

Title: [The Multilingual TEDx Corpus for Speech Recognition and Translation](https://arxiv.org/abs/2102.01757)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthew Wiesner](https://arxiv.org/search/cs?searchtype=author&query=Wiesner%2C+M), [Jacob Bremerman](https://arxiv.org/search/cs?searchtype=author&query=Bremerman%2C+J), [Roldano Cattoni](https://arxiv.org/search/cs?searchtype=author&query=Cattoni%2C+R), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M), [Douglas W. Oard](https://arxiv.org/search/cs?searchtype=author&query=Oard%2C+D+W), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> We present the Multilingual TEDx corpus, built to support speech recognition (ASR) and speech translation (ST) research across many non-English source languages. The corpus is a collection of audio recordings from TEDx talks in 8 source languages. We segment transcripts into sentences and align them to the source-language audio and target-language translations. The corpus is released along with open-sourced code enabling extension to new talks and languages as they become available. Our corpus creation methodology can be applied to more languages than previous work, and creates multi-way parallel evaluation sets. We provide baselines in multiple ASR and ST settings, including multilingual models to improve translation performance for low-resource language pairs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01757](https://arxiv.org/abs/2102.01757) [cs.CL]** |
|           | (or **[arXiv:2102.01757v1](https://arxiv.org/abs/2102.01757v1) [cs.CL]** for this version) |



<h2 id="2021-02-04-2">2. Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation</h2>

Title: [Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation](https://arxiv.org/abs/2102.01818)

Authors: [Aparna Elangovan](https://arxiv.org/search/cs?searchtype=author&query=Elangovan%2C+A), [Jiayuan He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Karin Verspoor](https://arxiv.org/search/cs?searchtype=author&query=Verspoor%2C+K)

> Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize.

| Comments: | To appear EACL 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.01818](https://arxiv.org/abs/2102.01818) [cs.CL]** |
|           | (or **[arXiv:2102.01818v1](https://arxiv.org/abs/2102.01818v1) [cs.CL]** for this version) |





<h2 id="2021-02-04-3">3. When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data</h2>

Title: [When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data](https://arxiv.org/abs/2102.02201)

Authors: [Peter Hase](https://arxiv.org/search/cs?searchtype=author&query=Hase%2C+P), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Many methods now exist for conditioning model outputs on task instructions, retrieved documents, and user-provided explanations and feedback. Rather than relying solely on examples of task inputs and outputs, these approaches allow for valuable additional data to be used in modeling with the purpose of improving model correctness and aligning learned models with human priors. Meanwhile, a growing body of evidence suggests that some language models can (1) store a large amount of knowledge in their parameters, and (2) perform inference over tasks in unstructured text to solve new tasks at test time. These results raise the possibility that, for some tasks, humans cannot explain to a model any more about the task than it already knows or could infer on its own. In this paper, we study the circumstances under which explanations of individual data points can (or cannot) improve modeling performance. In order to carefully control important properties of the data and explanations, we introduce a synthetic dataset for experiments, and we also make use of three existing datasets with explanations: e-SNLI, TACRED, SemEval. We first give a formal framework for the available modeling approaches, in which explanation data can be used as model inputs, as labels, or as a prior. After arguing that the most promising role for explanation data is as model inputs, we propose to use a retrieval-based method and show that it solves our synthetic task with accuracies upwards of 95%, while baselines without explanation data achieve below 65% accuracy. We then identify properties of datasets for which retrieval-based modeling fails. With the three existing datasets, we find no improvements from explanation retrieval. Drawing on our findings from our synthetic task, we suggest that at least one of six preconditions for successful modeling fails to hold with these datasets.

| Comments: | 25 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.02201](https://arxiv.org/abs/2102.02201) [cs.CL]** |
|           | (or **[arXiv:2102.02201v1](https://arxiv.org/abs/2102.02201v1) [cs.CL]** for this version) |





# 2021-02-03

[Return to Index](#Index)



<h2 id="2021-02-03-1">1. Two Demonstrations of the Machine Translation Applications to Historical Documents</h2>

Title: [Two Demonstrations of the Machine Translation Applications to Historical Documents](https://arxiv.org/abs/2102.01417)

Authors:[Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F)

> We present our demonstration of two machine translation applications to historical documents. The first task consists in generating a new version of a historical document, written in the modern version of its original language. The second application is limited to a document's orthography. It adapts the document's spelling to modern standards in order to achieve an orthography consistency and accounting for the lack of spelling conventions. We followed an interactive, adaptive framework that allows the user to introduce corrections to the system's hypothesis. The system reacts to these corrections by generating a new hypothesis that takes them into account. Once the user is satisfied with the system's hypothesis and validates it, the system adapts its model following an online learning strategy. This system is implemented following a client-server architecture. We developed a website which communicates with the neural models. All code is open-source and publicly available. The demonstration is hosted at [this http URL](http://demosmt.prhlt.upv.es/mthd/).

| Comments: | Presented at the Demos session of ICPR 2020: [this https URL](https://www.micc.unifi.it/icpr2020/index.php/demos/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.01417](https://arxiv.org/abs/2102.01417) [cs.CL]** |
|           | (or **[arXiv:2102.01417v1](https://arxiv.org/abs/2102.01417v1) [cs.CL]** for this version) |





<h2 id="2021-02-03-2">2. CTC-based Compression for Direct Speech Translation</h2>

Title: [CTC-based Compression for Direct Speech Translation](https://arxiv.org/abs/2102.01578)

Authors:[Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input indirect ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%.

| Comments: | Accepted at EACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.01578](https://arxiv.org/abs/2102.01578) [cs.CL]** |
|           | (or **[arXiv:2102.01578v1](https://arxiv.org/abs/2102.01578v1) [cs.CL]** for this version) |





<h2 id="2021-02-03-3">3. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</h2>

Title: [The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics](https://arxiv.org/abs/2102.01672)

Authors:[Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Tosin Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T), [Karmanya Aggarwal](https://arxiv.org/search/cs?searchtype=author&query=Aggarwal%2C+K), [Pawan Sasanka Ammanamanchi](https://arxiv.org/search/cs?searchtype=author&query=Ammanamanchi%2C+P+S), [Aremu Anuoluwapo](https://arxiv.org/search/cs?searchtype=author&query=Anuoluwapo%2C+A), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A), [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Miruna Clinciu](https://arxiv.org/search/cs?searchtype=author&query=Clinciu%2C+M), [Dipanjan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Kaustubh D. Dhole](https://arxiv.org/search/cs?searchtype=author&query=Dhole%2C+K+D), [Wanyu Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+W), [Esin Durmus](https://arxiv.org/search/cs?searchtype=author&query=Durmus%2C+E), [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Chris Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Cristina Garbacea](https://arxiv.org/search/cs?searchtype=author&query=Garbacea%2C+C), [Tatsunori Hashimoto](https://arxiv.org/search/cs?searchtype=author&query=Hashimoto%2C+T), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Yacine Jernite](https://arxiv.org/search/cs?searchtype=author&query=Jernite%2C+Y), [Harsh Jhamtani](https://arxiv.org/search/cs?searchtype=author&query=Jhamtani%2C+H), [Yangfeng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+Y), [Shailza Jolly](https://arxiv.org/search/cs?searchtype=author&query=Jolly%2C+S), [Dhruv Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+D), [Faisal Ladhak](https://arxiv.org/search/cs?searchtype=author&query=Ladhak%2C+F), [Aman Madaan](https://arxiv.org/search/cs?searchtype=author&query=Madaan%2C+A), [Mounica Maddela](https://arxiv.org/search/cs?searchtype=author&query=Maddela%2C+M), [Khyati Mahajan](https://arxiv.org/search/cs?searchtype=author&query=Mahajan%2C+K), [Saad Mahamood](https://arxiv.org/search/cs?searchtype=author&query=Mahamood%2C+S), [Bodhisattwa Prasad Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+B+P), [Pedro Henrique Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H), [Angelina McMillan-Major](https://arxiv.org/search/cs?searchtype=author&query=McMillan-Major%2C+A), [Simon Mille](https://arxiv.org/search/cs?searchtype=author&query=Mille%2C+S), [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Moin Nadeem](https://arxiv.org/search/cs?searchtype=author&query=Nadeem%2C+M), [Shashi Narayan](https://arxiv.org/search/cs?searchtype=author&query=Narayan%2C+S), [Vitaly Nikolaev](https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+V), [Rubungo Andre Niyongabo](https://arxiv.org/search/cs?searchtype=author&query=Niyongabo%2C+R+A), [Salomey Osei](https://arxiv.org/search/cs?searchtype=author&query=Osei%2C+S), [Ankur Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A), [Laura Perez-Beltrachini](https://arxiv.org/search/cs?searchtype=author&query=Perez-Beltrachini%2C+L), [Niranjan Ramesh Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+N+R), [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Juan Diego Rodriguez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez%2C+J+D), [Sashank Santhanam](https://arxiv.org/search/cs?searchtype=author&query=Santhanam%2C+S), [João Sedoc](https://arxiv.org/search/cs?searchtype=author&query=Sedoc%2C+J), [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Samira Shaikh](https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+S), [Anastasia Shimorina](https://arxiv.org/search/cs?searchtype=author&query=Shimorina%2C+A), [Marco Antonio Sobrevilla Cabezudo](https://arxiv.org/search/cs?searchtype=author&query=Cabezudo%2C+M+A+S), [Hendrik Strobelt](https://arxiv.org/search/cs?searchtype=author&query=Strobelt%2C+H), [Nishant Subramani](https://arxiv.org/search/cs?searchtype=author&query=Subramani%2C+N), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Akhila Yerukola](https://arxiv.org/search/cs?searchtype=author&query=Yerukola%2C+A), [Jiawei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. However, due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of corpora and evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models.
> This paper serves as the description of the initial release for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01672](https://arxiv.org/abs/2102.01672) [cs.CL]** |
|           | (or **[arXiv:2102.01672v2](https://arxiv.org/abs/2102.01672v2) [cs.CL]** for this version) |





# 2021-02-02

[Return to Index](#Index)



<h2 id="2021-02-02-1">1. Speech Recognition by Simply Fine-tuning BERT</h2>

Title: [Speech Recognition by Simply Fine-tuning BERT](https://arxiv.org/abs/2102.00291)

Authors: [Wen-Chin Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W), [Chia-Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Shang-Bao Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Kuan-Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Hsin-Min Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Tomoki Toda](https://arxiv.org/search/cs?searchtype=author&query=Toda%2C+T)

> We propose a simple method for automatic speech recognition (ASR) by fine-tuning BERT, which is a language model (LM) trained on large-scale unlabeled text data and can generate rich contextual representations. Our assumption is that given a history context sequence, a powerful LM can narrow the range of possible choices and the speech signal can be used as a simple clue. Hence, comparing to conventional ASR systems that train a powerful acoustic model (AM) from scratch, we believe that speech recognition is possible by simply fine-tuning a BERT model. As an initial study, we demonstrate the effectiveness of the proposed idea on the AISHELL dataset and show that stacking a very simple AM on top of BERT can yield reasonable performance.

| Comments: | Accepted to ICASSP 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2102.00291](https://arxiv.org/abs/2102.00291) [cs.SD]** |
|           | (or **[arXiv:2102.00291v1](https://arxiv.org/abs/2102.00291v1) [cs.SD]** for this version) |





<h2 id="2021-02-02-2">2. Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript</h2>

Title: [Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript](https://arxiv.org/abs/2102.00804)

Authors: [Mukuntha Narayanan Sundararaman](https://arxiv.org/search/eess?searchtype=author&query=Sundararaman%2C+M+N), [Ayush Kumar](https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+A), [Jithendra Vepa](https://arxiv.org/search/eess?searchtype=author&query=Vepa%2C+J)

> Recent years have witnessed significant improvement in ASR systems to recognize spoken utterances. However, it is still a challenging task for noisy and out-of-domain data, where substitution and deletion errors are prevalent in the transcribed text. These errors significantly degrade the performance of downstream tasks. In this work, we propose a BERT-style language model, referred to as PhonemeBERT, that learns a joint language model with phoneme sequence and ASR transcript to learn phonetic-aware representations that are robust to ASR errors. We show that PhonemeBERT can be used on downstream tasks using phoneme sequences as additional features, and also in low-resource setup where we only have ASR-transcripts for the downstream tasks with no phoneme information available. We evaluate our approach extensively by generating noisy data for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS for sentiment, question and intent classification tasks respectively. The results of the proposed approach beats the state-of-the-art baselines comprehensively on each dataset.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.00804](https://arxiv.org/abs/2102.00804) [eess.AS]** |
|           | (or **[arXiv:2102.00804v1](https://arxiv.org/abs/2102.00804v1) [eess.AS]** for this version) |





<h2 id="2021-02-02-3">3. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation</h2>

Title: [Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation](https://arxiv.org/abs/2102.00287)

Authors: [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Matthew Gwilliam](https://arxiv.org/search/cs?searchtype=author&query=Gwilliam%2C+M)

> Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the 'algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: 'machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms - phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and morphological richness in the translations produced by all investigated MT paradigms for two language pairs (EN<=>FR and EN<=>ES).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.00287](https://arxiv.org/abs/2102.00287) [cs.CL]** |
|           | (or **[arXiv:2102.00287v1](https://arxiv.org/abs/2102.00287v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-4">4. Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers</h2>

Title: [Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](https://arxiv.org/abs/2102.00529)

Authors: [Lisa Anne Hendricks](https://arxiv.org/search/cs?searchtype=author&query=Hendricks%2C+L+A), [John Mellor](https://arxiv.org/search/cs?searchtype=author&query=Mellor%2C+J), [Rosalia Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+R), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac%2C+J), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh%2C+A)

> Recently multimodal transformer models have gained popularity because their performance on language and vision tasks suggest they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors which can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers

| Comments: | pre-print of MIT Press Publication version                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2102.00529](https://arxiv.org/abs/2102.00529) [cs.CL]** |
|           | (or **[arXiv:2102.00529v1](https://arxiv.org/abs/2102.00529v1) [cs.CL]** for this version) |







<h2 id="2021-02-02-5">5. Neural OCR Post-Hoc Correction of Historical Corpora</h2>

Title: [Neural OCR Post-Hoc Correction of Historical Corpora](https://arxiv.org/abs/2102.00583)

Authors: [Lijun Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+L), [Maria Koutraki](https://arxiv.org/search/cs?searchtype=author&query=Koutraki%2C+M), [Martin Krickl](https://arxiv.org/search/cs?searchtype=author&query=Krickl%2C+M), [Besnik Fetahu](https://arxiv.org/search/cs?searchtype=author&query=Fetahu%2C+B)

> Optical character recognition (OCR) is crucial for a deeper access to historical collections. OCR needs to account for orthographic variations, typefaces, or language evolution (i.e., new letters, word spellings), as the main source of character, word, or word segmentation transcription errors. For digital corpora of historical prints, the errors are further exacerbated due to low scan quality and lack of language standardization.
> For the task of OCR post-hoc correction, we propose a neural approach based on a combination of recurrent (RNN) and deep convolutional network (ConvNet) to correct OCR transcription errors. At character level we flexibly capture errors, and decode the corrected output based on a novel attention mechanism. Accounting for the input and output similarity, we propose a new loss function that rewards the model's correcting behavior.
> Evaluation on a historical book corpus in German language shows that our models are robust in capturing diverse OCR transcription errors and reduce the word error rate of 32.3% by more than 89%.

| Comments: | To appear at TACL                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.00583](https://arxiv.org/abs/2102.00583) [cs.CL]** |
|           | (or **[arXiv:2102.00583v1](https://arxiv.org/abs/2102.00583v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-6">6. GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer</h2>

Title: [GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer](https://arxiv.org/abs/2102.00769)

Authors: [Yukai Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Sen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Chenxing Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Xiaodan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+X), [Xiaojun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Liang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L)

> Non-parallel text style transfer has attracted increasing research interests in recent years. Despite successes in transferring the style based on the encoder-decoder framework, current approaches still lack the ability to preserve the content and even logic of original sentences, mainly due to the large unconstrained model space or too simplified assumptions on latent embedding space. Since language itself is an intelligent product of humans with certain grammars and has a limited rule-based model space by its nature, relieving this problem requires reconciling the model capacity of deep neural networks with the intrinsic model constraints from human linguistic rules. To this end, we propose a method called Graph Transformer based Auto Encoder (GTAE), which models a sentence as a linguistic graph and performs feature extraction and style transfer at the graph level, to maximally retain the content and the linguistic structure of original sentences. Quantitative experiment results on three non-parallel text style transfer tasks show that our model outperforms state-of-the-art methods in content preservation, while achieving comparable performance on transfer accuracy and sentence naturalness.

| Comments: | The first two authors share equal-authorship; Code:[this https URL](https://github.com/SenZHANG-GitHub/graph-text-style-transfer) ; benchmark: [this https URL](https://github.com/ykshi/text-style-transfer-benchmark) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2102.00769](https://arxiv.org/abs/2102.00769) [cs.CL]** |
|           | (or **[arXiv:2102.00769v1](https://arxiv.org/abs/2102.00769v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-7">7. Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models</h2>

Title: [Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](https://arxiv.org/abs/2102.00894)

Authors: [Nora Kassner](https://arxiv.org/search/cs?searchtype=author&query=Kassner%2C+N), [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as "Paris is the capital of [MASK]" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.00894](https://arxiv.org/abs/2102.00894) [cs.CL]** |
|           | (or **[arXiv:2102.00894v1](https://arxiv.org/abs/2102.00894v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-8">8. End2End Acoustic to Semantic Transduction</h2>

Title: [End2End Acoustic to Semantic Transduction](https://arxiv.org/abs/2102.01013)

Authors: [Valentin Pelloin](https://arxiv.org/search/cs?searchtype=author&query=Pelloin%2C+V), [Nathalie Camelin](https://arxiv.org/search/cs?searchtype=author&query=Camelin%2C+N), [Antoine Laurent](https://arxiv.org/search/cs?searchtype=author&query=Laurent%2C+A), [Renato De Mori](https://arxiv.org/search/cs?searchtype=author&query=De+Mori%2C+R), [Antoine Caubrière](https://arxiv.org/search/cs?searchtype=author&query=Caubrière%2C+A), [Yannick Estève](https://arxiv.org/search/cs?searchtype=author&query=Estève%2C+Y), [Sylvain Meignier](https://arxiv.org/search/cs?searchtype=author&query=Meignier%2C+S)

> In this paper, we propose a novel end-to-end sequence-to-sequence spoken language understanding model using an attention mechanism. It reliably selects contextual acoustic features in order to hypothesize semantic contents. An initial architecture capable of extracting all pronounced words and concepts from acoustic spans is designed and tested. With a shallow fusion language model, this system reaches a 13.6 concept error rate (CER) and an 18.5 concept value error rate (CVER) on the French MEDIA corpus, achieving an absolute 2.8 points reduction compared to the state-of-the-art. Then, an original model is proposed for hypothesizing concepts and their values. This transduction reaches a 15.4 CER and a 21.6 CVER without any new type of context.

| Comments: | Accepted at IEEE ICASSP 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2102.01013](https://arxiv.org/abs/2102.01013) [cs.CL]** |
|           | (or **[arXiv:2102.01013v1](https://arxiv.org/abs/2102.01013v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-9">9. Measuring and Improving Consistency in Pretrained Language Models</h2>

Title: [Measuring and Improving Consistency in Pretrained Language Models](https://arxiv.org/abs/2102.01017)

Authors: [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Nora Kassner](https://arxiv.org/search/cs?searchtype=author&query=Kassner%2C+N), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Abhilasha Ravichander](https://arxiv.org/search/cs?searchtype=author&query=Ravichander%2C+A), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for thirty-eight relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge in a robust way. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01017](https://arxiv.org/abs/2102.01017) [cs.CL]** |
|           | (or **[arXiv:2102.01017v1](https://arxiv.org/abs/2102.01017v1) [cs.CL]** for this version) |











# 2021-02-01

[Return to Index](#Index)



<h2 id="2021-02-01-1">1. Combining pre-trained language models and structured knowledge</h2>

Title: [Combining pre-trained language models and structured knowledge](https://arxiv.org/abs/2101.12294)

Authors: [Pedro Colon-Hernandez](https://arxiv.org/search/cs?searchtype=author&query=Colon-Hernandez%2C+P), [Catherine Havasi](https://arxiv.org/search/cs?searchtype=author&query=Havasi%2C+C), [Jason Alonso](https://arxiv.org/search/cs?searchtype=author&query=Alonso%2C+J), [Matthew Huggins](https://arxiv.org/search/cs?searchtype=author&query=Huggins%2C+M), [Cynthia Breazeal](https://arxiv.org/search/cs?searchtype=author&query=Breazeal%2C+C)

> In recent years, transformer-based language models have achieved state of the art performance in various NLP benchmarks. These models are able to extract mostly distributional information with some semantics from unstructured text, however it has proven challenging to integrate structured information, such as knowledge graphs into these models. We examine a variety of approaches to integrate structured knowledge into current language models and determine challenges, and possible opportunities to leverage both structured and unstructured information sources. From our survey, we find that there are still opportunities at exploiting adapter-based injections and that it may be possible to further combine various of the explored approaches into one system.

| Comments: | Initial Submission                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.12294](https://arxiv.org/abs/2101.12294) [cs.CL]** |
|           | (or **[arXiv:2101.12294v1](https://arxiv.org/abs/2101.12294v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-2">2. Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning</h2>

Title: [Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning](https://arxiv.org/abs/2101.12409)

Authors: [Shengsheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yaping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Chencheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Erhong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+E)

> Most existing Grammatical Error Correction (GEC) methods based on sequence-to-sequence mainly focus on how to generate more pseudo data to obtain better performance. Few work addresses few-shot GEC domain adaptation. In this paper, we treat different GEC domains as different GEC tasks and propose to extend meta-learning to few-shot GEC domain adaptation without using any pseudo data. We exploit a set of data-rich source domains to learn the initialization of model parameters that facilitates fast adaptation on new resource-poor target domains. We adapt GEC model to the first language (L1) of the second language learner. To evaluate the proposed method, we use nine L1s as source domains and five L1s as target domains. Experiment results on the L1 GEC domain adaptation dataset demonstrate that the proposed approach outperforms the multi-task transfer learning baseline by 0.50 F0.5 score on average and enables us to effectively adapt to a new L1 domain with only 200 parallel sentences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.12409](https://arxiv.org/abs/2101.12409) [cs.CL]** |
|           | (or **[arXiv:2101.12409v1](https://arxiv.org/abs/2101.12409v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-3">3. Synthesizing Monolingual Data for Neural Machine Translation</h2>

Title: [Synthesizing Monolingual Data for Neural Machine Translation](https://arxiv.org/abs/2101.12462)

Authors: [Benjamin Marie](https://arxiv.org/search/cs?searchtype=author&query=Marie%2C+B), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> In neural machine translation (NMT), monolingual data in the target language are usually exploited through a method so-called "back-translation" to synthesize additional training parallel data. The synthetic data have been shown helpful to train better NMT, especially for low-resource language pairs and domains. Nonetheless, large monolingual data in the target domains or languages are not always available to generate large synthetic parallel data. In this work, we propose a new method to generate large synthetic parallel data leveraging very small monolingual data in a specific domain. We fine-tune a pre-trained GPT-2 model on such small in-domain monolingual data and use the resulting model to generate a large amount of synthetic in-domain monolingual data. Then, we perform back-translation, or forward translation, to generate synthetic in-domain parallel data. Our preliminary experiments on three language pairs and five domains show the effectiveness of our method to generate fully synthetic but useful in-domain parallel data for improving NMT in all configurations. We also show promising results in extreme adaptation for personalized NMT.

| Comments: | Preliminary work                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.12462](https://arxiv.org/abs/2101.12462) [cs.CL]** |
|           | (or **[arXiv:2101.12462v1](https://arxiv.org/abs/2101.12462v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-4">4. Transition based Graph Decoder for Neural Machine Translation</h2>

Title: [Transition based Graph Decoder for Neural Machine Translation](https://arxiv.org/abs/2101.12640)

Authors: [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O)

> While a number of works showed gains from incorporating source-side symbolic syntactic and semantic structure into neural machine translation (NMT), much fewer works addressed the decoding of such structure.
> We propose a general Transformer-based approach for tree and graph decoding based on generating a sequence of transitions, inspired by a similar approach that uses RNNs by Dyer (2016).
> Experiments with using the proposed decoder with Universal Dependencies syntax on English-German, German-English and English-Russian show improved performance over the standard Transformer decoder, as well as over ablated versions of the model.\tacltxt{\footnote{All code implementing the presented models will be released upon acceptance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.12640](https://arxiv.org/abs/2101.12640) [cs.CL]** |
|           | (or **[arXiv:2101.12640v1](https://arxiv.org/abs/2101.12640v1) [cs.CL]** for this version) |