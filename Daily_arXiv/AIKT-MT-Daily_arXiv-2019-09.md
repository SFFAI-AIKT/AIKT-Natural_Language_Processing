# Daily arXiv: Machine Translation - Sep., 2019

# Index

- [2019-09-30](#2019-09-30)
  - [1. On the use of BERT for Neural Machine Translation](#2019-09-30-1)
  - [2. Improving Pre-Trained Multilingual Models with Vocabulary Expansion](#2019-09-30-2)
- [2019-09-27](#2019-09-27)
  - [1. Attention Forcing for Sequence-to-sequence Model Training](#2019-09-27-1)
  - [2. Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs](#2019-09-27-2)
  - [3. Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation](#2019-09-27-3)
- [2019-09-26](#2019-09-26)
  - [1. Attention Interpretability Across NLP Tasks](#2019-09-26-1)
  - [2. Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training](#2019-09-26-2)
- [2019-09-25](#2019-09-25)
  - [1. Data Ordering Patterns for Neural Machine Translation: An Empirical Study](#2019-09-25-1)
  - [2. Learning ASR-Robust Contextualized Embeddings for Spoken Language Understanding](#2019-09-25-2)
  - [3. Code-switching Language Modeling With Bilingual Word Embeddings: A Case Study for Egyptian Arabic-English](#2019-09-25-3)
  - [4. Transfer Learning across Languages from Someone Else's NMT Model](#2019-09-25-4)
  - [5. Paying Attention to Function Words](#2019-09-25-5)
- [2019-09-24](#2019-09-24)
  - [1. Self-attention based end-to-end Hindi-English Neural Machine Translation](#2019-09-24-1)
  - [2. Using Chinese Glyphs for Named Entity Recognition](#2019-09-24-2)
  - [3. Inducing Constituency Trees through Neural Machine Translation](#2019-09-24-3)
  - [4. Algorithms for certain classes of Tamil Spelling correction](#2019-09-24-4)
  - [5. TinyBERT: Distilling BERT for Natural Language Understanding](#2019-09-24-5)
  - [6. Cross-Lingual Natural Language Generation via Pre-Training](#2019-09-24-6)
- [2019-09-23](#2019-09-23)
  - [1. SANVis: Visual Analytics for Understanding Self-Attention Networks](#2019-09-23-1)
  - [2. Improved Variational Neural Machine Translation by Promoting Mutual Information](#2019-09-23-2)
  - [3. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models](#2019-09-23-3)
  - [4. Towards Neural Language Evaluators](#2019-09-23-4)
  - [5. A simple discriminative training method for machine translation with large-scale features](#2019-09-23-5)
  - [6. Controllable Length Control Neural Encoder-Decoder via Reinforcement Learning](#2019-09-23-6)
  - [7. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages](#2019-09-23-7)
  - [8. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](#2019-09-23-8)
- [2019-09-20](#2019-09-20)
  - [1. Do We Need Neural Models to Explain Human Judgments of Acceptability?](#2019-09-20-1)
  - [2. Cross-Lingual Contextual Word Embeddings Mapping With Multi-Sense Words In Mind](#2019-09-20-2)
  - [3. Low-Resource Parsing with Crosslingual Contextualized Representations](#2019-09-20-3)
  - [4. Improving Generalization by Incorporating Coverage in Natural Language Inference](#2019-09-20-4)
  - [5. CogniVal: A Framework for Cognitive Word Embedding Evaluation](#2019-09-20-5)
- [2019-09-19](#2019-09-19)
  - [1. Relaxed Softmax for learning from Positive and Unlabeled data](#2019-09-19-1)
  - [2. Memory-Augmented Neural Networks for Machine Translation](#2019-09-19-2)
  - [3. Subword ELMo](#2019-09-19-3)
  - [4. Simple, Scalable Adaptation for Neural Machine Translation](#2019-09-19-4)
  - [5. Fine-Tuning Language Models from Human Preferences](#2019-09-19-5)
- [2019-09-18](#2019-09-18)
  - [1. Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation](#2019-09-18-1)
  - [2. Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation](#2019-09-18-2)
  - [3. Learning to Deceive with Attention-Based Explanations](#2019-09-18-3)
- [2019-09-17](#2019-09-17)
  - [1. Adaptive Scheduling for Multi-Task Learning](#2019-09-17-1)
  - [2. Leveraging Out-of-Task Data for End-to-End Automatic Speech Translation](#2019-09-17-2)
  - [3. A Universal Parent Model for Low-Resource Neural Machine Translation Transfer](#2019-09-17-3)
  - [4. ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation](#2019-09-17-4)
  - [5. Tree Transformer: Integrating Tree Structures into Self-Attention](#2019-09-17-5)
  - [6. Beyond BLEU: Training Neural Machine Translation with Semantic Similarity](#2019-09-17-6)
  - [7. Hint-Based Training for Non-Autoregressive Machine Translation](#2019-09-17-7)
  - [8. Natural Language Adversarial Attacks and Defenses in Word Level](#2019-09-17-8)
  - [9. Automatically Extracting Challenge Sets for Non-local Phenomena Neural Machine Translation](#2019-09-17-9)
  - [10. Communication-based Evaluation for Natural Language Generation](#2019-09-17-10)
  - [11. Multilingual Neural Machine Translation for Zero-Resource Languages](#2019-09-17-11)
- [2019-09-16](#2019-09-16)
  - [1. CTRL: A Conditional Transformer Language Model for Controllable Generation](#2019-09-16-1)
  - [2. Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting](#2019-09-16-2)
  - [3. Neural Machine Translation with 4-Bit Precision and Beyond](#2019-09-16-3)
  - [4. A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces](#2019-09-16-4)
- [2019-09-13](#2019-09-13)
  - [1. Entity Projection via Machine-Translation for Cross-Lingual NER](#2019-09-13-1)
  - [2. Problems with automating translation of movie/TV show subtitles](#2019-09-13-2)
  - [3. Speculative Beam Search for Simultaneous Translation](#2019-09-13-3)
  - [4. VizSeq: A Visual Analysis Toolkit for Text Generation Tasks](#2019-09-13-4)
  - [5. Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning](#2019-09-13-5)
  - [6. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction](#2019-09-13-6)
- [2019-09-10](#2019-09-10)
  - [1. Improving Neural Machine Translation with Parent-Scaled Self-Attention](#2019-09-10-1)
  - [2. LAMAL: LAnguage Modeling Is All You Need for Lifelong Language Learning](#2019-09-10-2)
  - [3. Neural Machine Translation with Byte-Level Subwords](#2019-09-10-3)
  - [4. Combining SMT and NMT Back-Translated Data for Efficient NMT](#2019-09-10-4)
- [2019-09-09](#2019-09-09)
  - [1. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction](#2019-09-09-1)
- [2019-09-06](#2019-09-06)
  - [1. Jointly Learning to Align and Translate with Transformer Models](#2019-09-06-1)
  - [2. Investigating Multilingual NMT Representations at Scale](#2019-09-06-2)
  - [3. Multi-Granularity Self-Attention for Neural Machine Translation](#2019-09-06-3)
  - [4. Source Dependency-Aware Transformer with Supervised Self-Attention](#2019-09-06-4)
  - [5. Accelerating Transformer Decoding via a Hybrid of Self-attention and Recurrent Neural Network](#2019-09-06-1)
  - [6. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow](#2019-09-06-6)
- [2019-09-05](#2019-09-05)
  - [1. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives](#2019-09-05-1)
  - [2. Context-Aware Monolingual Repair for Neural Machine Translation](#2019-09-05-2)
  - [3. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation](#2019-09-05-3)
  - [4. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?](#2019-09-05-4)
  - [5. SAO WMT19 Test Suite: Machine Translation of Audit Reports](#2019-09-05-5)
- [2019-09-04](#2019-09-04)
  - [1. Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation](#2019-09-04-1)
  - [2. Handling Syntactic Divergence in Low-resource Machine Translation](#2019-09-04-2)
  - [3. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite](#2019-09-04-3)
  - [4. Improving Back-Translation with Uncertainty-based Confidence Estimation](#2019-09-04-4)
  - [5. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation](#2019-09-04-5)
  - [6. Towards Understanding Neural Machine Translation with Word Importance](#2019-09-04-6)
  - [7. One Model to Learn Both: Zero Pronoun Prediction and Translation](#2019-09-04-7)
  - [8. Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation](#2019-09-04-8)
  - [9. Improving Context-aware Neural Machine Translation with Target-side Context](#2019-09-04-9)
  - [10. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation](#2019-09-04-10)
  - [11. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks](#2019-09-04-11)
  - [12. Multi-agent Learning for Neural Machine Translation](#2019-09-04-12)
  - [13. Bilingual is At Least Monolingual (BALM): A Novel Translation Algorithm that Encodes Monolingual Priors](#2019-09-04-13)
- [2019-09-02](#2019-09-02)
  - [1. Latent Part-of-Speech Sequences for Neural Machine Translation](#2019-09-02-1)
  - [2. Encoders Help You Disambiguate Word Senses in Neural Machine Translation](#2019-09-02-2)
* [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-09-30

[Return to Index](#Index)



<h2 id="2019-09-30-1">1. On the use of BERT for Neural Machine Translation</h2> 

Title: [On the use of BERT for Neural Machine Translation](https://arxiv.org/abs/1909.12744)

Authors:[Stéphane Clinchant](https://arxiv.org/search/cs?searchtype=author&query=Clinchant%2C+S), [Kweon Woo Jung](https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+K+W), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

*(Submitted on 27 Sep 2019)*

> Exploiting large pretrained models for various NMT tasks have gained a lot of visibility recently. In this work we study how BERT pretrained models could be exploited for supervised Neural Machine Translation. We compare various ways to integrate pretrained BERT model with NMT model and study the impact of the monolingual data used for BERT training on the final translation quality. We use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets for these experiments. In addition to standard task test set evaluation, we perform evaluation on out-of-domain test sets and noise injected test sets, in order to assess how BERT pretrained representations affect model robustness.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.12744 [cs.CL]**                                 |
|           | (or **arXiv:1909.12744v1 [cs.CL]** for this version)         |





<h2 id="2019-09-30-2">2. Improving Pre-Trained Multilingual Models with Vocabulary Expansion</h2> 

Title: [Improving Pre-Trained Multilingual Models with Vocabulary Expansion](https://arxiv.org/abs/1909.12440)

Authors:[Hai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Dian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Kai Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+K), [Janshu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

*(Submitted on 26 Sep 2019)*

> Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential.
> In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.

| Comments: | CONLL 2019 final version                             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.12440 [cs.CL]**                         |
|           | (or **arXiv:1909.12440v1 [cs.CL]** for this version) |





# 2019-09-27

[Return to Index](#Index)



<h2 id="2019-09-27-1">1. Attention Forcing for Sequence-to-sequence Model Training</h2> 
Title: [Attention Forcing for Sequence-to-sequence Model Training](https://arxiv.org/abs/1909.12289)

Authors: [Qingyun Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Q), [Yiting Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Joshua Efiong](https://arxiv.org/search/cs?searchtype=author&query=Efiong%2C+J), [Mark J. F. Gales](https://arxiv.org/search/cs?searchtype=author&query=Gales%2C+M+J+F)

*(Submitted on 26 Sep 2019)*

> Auto-regressive sequence-to-sequence models with attention mechanism have achieved state-of-the-art performance in many tasks such as machine translation and speech synthesis. These models can be difficult to train. The standard approach, teacher forcing, guides a model with reference output history during training. The problem is that the model is unlikely to recover from its mistakes during inference, where the reference output is replaced by generated output. Several approaches deal with this problem, largely by guiding the model with generated output history. To make training stable, these approaches often require a heuristic schedule or an auxiliary classifier. This paper introduces attention forcing, which guides the model with generated output history and reference attention. This approach can train the model to recover from its mistakes, in a stable fashion, without the need for a schedule or a classifier. In addition, it allows the model to generate output sequences aligned with the references, which can be important for cascaded systems like many speech synthesis systems. Experiments on speech synthesis show that attention forcing yields significant performance gain. Experiments on machine translation show that for tasks where various re-orderings of the output are valid, guiding the model with generated output history is challenging, while guiding the model with reference attention is beneficial.

| Comments:    | 11 pages, 4 figures, conference                              |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| ACM classes: | I.2                                                          |
| Cite as:     | **arXiv:1909.12289 [cs.LG]**                                 |
|              | (or **arXiv:1909.12289v1 [cs.LG]** for this version)         |



<h2 id="2019-09-27-2">2. Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs</h2> 
Title: [Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs](https://arxiv.org/abs/1909.11861)

Authors: [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiangyuan Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Zijun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Arianna Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

*(Submitted on 26 Sep 2019)*

> In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of WMT17 Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.11861 [cs.CL]**                         |
|           | (or **arXiv:1909.11861v1 [cs.CL]** for this version) |





<h2 id="2019-09-27-3">3. Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation</h2> 
Title: [Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation](https://arxiv.org/abs/1909.12016)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 26 Sep 2019)*

> Neural Machine Translation (NMT) models tend to achieve best performance when larger sets of parallel sentences are provided for training. For this reason, augmenting the training set with artificially-generated sentence pairs can boost performance.
> Nonetheless, the performance can also be improved with a small number of sentences if they are in the same domain as the test set. Accordingly, we want to explore the use of artificially-generated sentences along with data-selection algorithms to improve German-to-English NMT models trained solely with authentic data.
> In this work, we show how artificially-generated sentences can be more beneficial than authentic pairs, and demonstrate their advantages when used in combination with data-selection algorithms.

| Comments:          | Proceedings of the 12th International Conference on Natural Language Generation (INLG 2019) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 12th International Conference on Natural Language Generation (INLG 2019) |
| Cite as:           | **arXiv:1909.12016 [cs.CL]**                                 |
|                    | (or **arXiv:1909.12016v1 [cs.CL]** for this version)         |



# 2019-09-26

[Return to Index](#Index)



<h2 id="2019-09-26-1">1. Attention Interpretability Across NLP Tasks</h2> 
Title: [Attention Interpretability Across NLP Tasks](https://arxiv.org/abs/1909.11218)

Authors: [Shikhar Vashishth](https://arxiv.org/search/cs?searchtype=author&query=Vashishth%2C+S), [Shyam Upadhyay](https://arxiv.org/search/cs?searchtype=author&query=Upadhyay%2C+S), [Gaurav Singh Tomar](https://arxiv.org/search/cs?searchtype=author&query=Tomar%2C+G+S), [Manaal Faruqui](https://arxiv.org/search/cs?searchtype=author&query=Faruqui%2C+M)

*(Submitted on 24 Sep 2019)*

> The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain & Wallace, 2019; Vig & Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.

| Subjects:      | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| -------------- | ------------------------------------------------------------ |
| Report number: | 2019                                                         |
| Cite as:       | **arXiv:1909.11218 [cs.CL]**                                 |
|                | (or **arXiv:1909.11218v1 [cs.CL]** for this version)         |





<h2 id="2019-09-26-2">2. Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training</h2> 
Title: [Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training](https://arxiv.org/abs/1909.11430)

Authors: [Qiao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Q), [Meiyuan Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+M), [Yaqian Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Y), [Jin Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yitao Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+Y)

*(Submitted on 25 Sep 2019)*

> In a pipeline speech translation system, automatic speech recognition (ASR) system will transmit errors in recognition to the downstream machine translation (MT) system. A standard machine translation system is usually trained on parallel corpus composed of clean text and will perform poorly on text with recognition noise, a gap well known in speech translation community. In this paper, we propose a training architecture which aims at making a neural machine translation model more robust against speech recognition errors. Our approach addresses the encoder and the decoder simultaneously using adversarial learning and data augmentation, respectively. Experimental results on IWSLT2018 speech translation task show that our approach can bridge the gap between the ASR output and the MT input, outperforms the baseline by up to 2.83 BLEU on noisy ASR output, while maintaining close performance on clean text.

| Comments: | 7 pages, 5 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1909.11430 [cs.CL]**                                 |
|           | (or **arXiv:1909.11430v1 [cs.CL]** for this version)         |





# 2019-09-25

[Return to Index](#Index)



<h2 id="2019-09-25-1">1. Data Ordering Patterns for Neural Machine Translation: An Empirical Study</h2> 
Title: [Data Ordering Patterns for Neural Machine Translation: An Empirical Study](https://arxiv.org/abs/1909.10642)

Authors: [Siddhant Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S)

*(Submitted on 23 Sep 2019)*

> Recent works show that ordering of the training data affects the model performance for Neural Machine Translation. Several approaches involving dynamic data ordering and data sharding based on curriculum learning have been analysed for the their performance gains and faster convergence. In this work we propose to empirically study several ordering approaches for the training data based on different metrics and evaluate their impact on the model performance. Results from our study show that pre-fixing the ordering of the training data based on perplexity scores from a pre-trained model performs the best and outperforms the default approach of randomly shuffling the training data every epoch.

| Comments: | Submitted to 3rd Workshop on Neural Generation and Translation, EMNLP 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.10642 [cs.CL]**                                 |
|           | (or **arXiv:1909.10642v1 [cs.CL]** for this version)         |





<h2 id="2019-09-25-2">2. Learning ASR-Robust Contextualized Embeddings for Spoken Language Understanding</h2> 
Title: [Learning ASR-Robust Contextualized Embeddings for Spoken Language Understanding](https://arxiv.org/abs/1909.10861)

Authors: [Chao-Wei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

*(Submitted on 24 Sep 2019)*

> Employing pre-trained language models (LM) to extract contextualized word representations has achieved state-of-the-art performance on various NLP tasks. However, applying this technique to noisy transcripts generated by automatic speech recognizer (ASR) is concerned. Therefore, this paper focuses on making contextualized representations more ASR-robust. We propose a novel confusion-aware fine-tuning method to mitigate the impact of ASR errors to pre-trained LMs. Specifically, we fine-tune LMs to produce similar representations for acoustically confusable words that are obtained from word confusion networks (WCNs) produced by ASR. Experiments on the benchmark ATIS dataset show that the proposed method significantly improves the performance of spoken language understanding when performing on ASR transcripts.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.10861 [cs.CL]**                                 |
|           | (or **arXiv:1909.10861v1 [cs.CL]** for this version)         |





<h2 id="2019-09-25-3">3. Code-switching Language Modeling With Bilingual Word Embeddings: A Case Study for Egyptian Arabic-English</h2> 
Title: [Code-switching Language Modeling With Bilingual Word Embeddings: A Case Study for Egyptian Arabic-English](https://arxiv.org/abs/1909.10892)

Authors: [Injy Hamed](https://arxiv.org/search/cs?searchtype=author&query=Hamed%2C+I), [Moritz Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+M), [Mohamed Elmahdy](https://arxiv.org/search/cs?searchtype=author&query=Elmahdy%2C+M), [Slim Abdennadher](https://arxiv.org/search/cs?searchtype=author&query=Abdennadher%2C+S), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

*(Submitted on 24 Sep 2019)*

> Code-switching (CS) is a widespread phenomenon among bilingual and multilingual societies. The lack of CS resources hinders the performance of many NLP tasks. In this work, we explore the potential use of bilingual word embeddings for code-switching (CS) language modeling (LM) in the low resource Egyptian Arabic-English language. We evaluate different state-of-the-art bilingual word embeddings approaches that require cross-lingual resources at different levels and propose an innovative but simple approach that jointly learns bilingual word representations without the use of any parallel data, relying only on monolingual and a small amount of CS data. While all representations improve CS LM, ours performs the best and improves perplexity 33.5% relative over the baseline.

| Comments:          | 11 pages, 1 figure (having 2 sub-figures), submitted to the 21st International Conference on Speech and Computer (SPECOM'19), |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 21st International Conference on Speech and Computer (SPECOM'19), Istanbul, Turkey, August 20-25, 2019 https://link.springer.com/book/10.1007/978-3-030-26061-3 |
| Cite as:           | **arXiv:1909.10892 [cs.CL]**                                 |
|                    | (or **arXiv:1909.10892v1 [cs.CL]** for this version)         |





<h2 id="2019-09-25-4">4. Transfer Learning across Languages from Someone Else's NMT Model</h2> 
Title: [Transfer Learning across Languages from Someone Else's NMT Model](https://arxiv.org/abs/1909.10955)

Authors: [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

*(Submitted on 24 Sep 2019)*

> Neural machine translation is demanding in terms of training time, hardware resources, size, and quantity of parallel sentences. We propose a simple transfer learning method to recycle already trained models for different language pairs with no need for modifications in model architecture, hyper-parameters, or vocabulary. We achieve better translation quality and shorter convergence times than when training from random initialization. To show the applicability of our method, we recycle a Transformer model trained by different researchers for translating English-to-Czech and used it to seed models for seven language pairs. Our translation models are significantly better even when the re-used model's language pair is not linguistically related to the child language pair, especially for low-resource languages. Our approach needs only one pretrained model for all transferring to all various languages pairs. Additionally, we improve this approach with a simple vocabulary transformation. We analyze the behavior of transfer learning to understand the gains from unrelated languages.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.10955 [cs.CL]**                         |
|           | (or **arXiv:1909.10955v1 [cs.CL]** for this version) |





<h2 id="2019-09-25-5">5. Paying Attention to Function Words</h2> 
Title: [Paying Attention to Function Words](https://arxiv.org/abs/1909.11060)

Authors: [Shane Steinert-Threlkeld](https://arxiv.org/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S)

*(Submitted on 24 Sep 2019)*

> All natural languages exhibit a distinction between content words (like nouns and adjectives) and function words (like determiners, auxiliaries, prepositions). Yet surprisingly little has been said about the emergence of this universal architectural feature of natural languages. Why have human languages evolved to exhibit this division of labor between content and function words? How could such a distinction have emerged in the first place? This paper takes steps towards answering these questions by showing how the distinction can emerge through reinforcement learning in agents playing a signaling game across contexts which contain multiple objects that possess multiple perceptually salient gradable properties.

| Comments: | Emergent Communication Workshop @ NeurIPS 2018               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.11060 [cs.CL]**                                 |
|           | (or **arXiv:1909.11060v1 [cs.CL]** for this version)         |



# 2019-09-24

[Return to Index](#Index)



<h2 id="2019-09-24-1">1. Self-attention based end-to-end Hindi-English Neural Machine Translation</h2> 
Title: [Self-attention based end-to-end Hindi-English Neural Machine Translation](https://arxiv.org/abs/1909.09779)

Authors: [Siddhant Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+S), [Ritu Tiwari](https://arxiv.org/search/cs?searchtype=author&query=Tiwari%2C+R)

*(Submitted on 21 Sep 2019)*

> Machine Translation (MT) is a zone of concentrate in Natural Language processing which manages the programmed interpretation of human language, starting with one language then onto the next by the PC. Having a rich research history spreading over about three decades, Machine interpretation is a standout amongst the most looked for after region of research in the computational linguistics network. As a piece of this current ace's proposal, the fundamental center examines the Deep-learning based strategies that have gained critical ground as of late and turning into the de facto strategy in MT. We would like to point out the recent advances that have been put forward in the field of Neural Translation models, different domains under which NMT has replaced conventional SMT models and would also like to mention future avenues in the field. Consequently, we propose an end-to-end self-attention transformer network for Neural Machine Translation, trained on Hindi-English parallel corpus and compare the model's efficiency with other state of art models like encoder-decoder and attention-based encoder-decoder neural models on the basis of BLEU. We conclude this paper with a comparative analysis of the three proposed models.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.09779 [cs.CL]**                         |
|           | (or **arXiv:1909.09779v1 [cs.CL]** for this version) |





<h2 id="2019-09-24-2">2. Using Chinese Glyphs for Named Entity Recognition</h2> 
Title: [Using Chinese Glyphs for Named Entity Recognition](https://arxiv.org/abs/1909.09922)

Authors: [Arijit Sehanobish](https://arxiv.org/search/cs?searchtype=author&query=Sehanobish%2C+A), [Chan Hee Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+C+H)

*(Submitted on 22 Sep 2019)*

> Most Named Entity Recognition (NER) systems use additional features like part-of-speech (POS) tags, shallow parsing, gazetteers, etc. Such kind of information requires external knowledge like unlabeled texts and trained taggers. Adding these features to NER systems have been shown to have a positive impact. However, sometimes creating gazetteers or taggers can take a lot of time and may require extensive data cleaning. In this paper for Chinese NER systems, we do not use these traditional features but we use lexicographic features of Chinese characters. Chinese characters are composed of graphical components called radicals and these components often have some semantic indicators. We propose CNN based models that incorporate this semantic information and use them for NER. Our models show an improvement over the baseline BERT-BiLSTM-CRF model. We set a new baseline score for Chinese OntoNotes v5.0 and show an improvement of +.64 F1 score. We present a state-of-the-art F1 score on Weibo dataset of 71.81 and show a competitive improvement of +0.72 over baseline on ResumeNER dataset.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.09922 [cs.CL]**                                 |
|           | (or **arXiv:1909.09922v1 [cs.CL]** for this version)         |





<h2 id="2019-09-24-3">3. Inducing Constituency Trees through Neural Machine Translation</h2> 
Title: [Inducing Constituency Trees through Neural Machine Translation](https://arxiv.org/abs/1909.10056)

Authors: [Phu Mon Htut](https://arxiv.org/search/cs?searchtype=author&query=Htut%2C+P+M), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Samuel R. Bowman](https://arxiv.org/search/cs?searchtype=author&query=Bowman%2C+S+R)

*(Submitted on 22 Sep 2019)*

> Latent tree learning(LTL) methods learn to parse sentences using only indirect supervision from a downstream task. Recent advances in latent tree learning have made it possible to recover moderately high quality tree structures by training with language modeling or auto-encoding objectives. In this work, we explore the hypothesis that decoding in machine translation, as a conditional language modeling task, will produce better tree structures since it offers a similar training signal as language modeling, but with more semantic signal. We adapt two existing latent-tree language models--PRPN andON-LSTM--for use in translation. We find that they indeed recover trees that are better in F1 score than those seen in language modeling on WSJ test set, while maintaining strong translation quality. We observe that translation is a better objective than language modeling for inducing trees, marking the first success at latent tree learning using a machine translation objective. Additionally, our findings suggest that, although translation provides better signal for inducing trees than language modeling, translation models can perform well without exploiting the latent tree structure.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.10056 [cs.CL]**                         |
|           | (or **arXiv:1909.10056v1 [cs.CL]** for this version) |





<h2 id="2019-09-24-4">4. Algorithms for certain classes of Tamil Spelling correction</h2> 
Title: [Algorithms for certain classes of Tamil Spelling correction](https://arxiv.org/abs/1909.10063)

Authors: [Muthiah Annamalai](https://arxiv.org/search/cs?searchtype=author&query=Annamalai%2C+M), [T. Shrinivasan](https://arxiv.org/search/cs?searchtype=author&query=Shrinivasan%2C+T)

*(Submitted on 22 Sep 2019)*

> Tamil language has an agglutinative, diglossic, alpha-syllabary structure which provides a significant combinatorial explosion of morphological forms all of which are effectively used in Tamil prose, poetry from antiquity to the modern age in an unbroken chain of continuity. However, for the language understanding, spelling correction purposes some of these present challenges as out-of-dictionary words. In this paper the authors propose algorithmic techniques to handle specific problems of conjoined-words (out-of-dictionary) (transliteration)[thendRalkattRu] = [thendRal]+[kattRu] when parts are alone present in word-list in efficient way. Morphological structure of Tamil makes it necessary to depend on synthesis-analysis approach and dictionary lists will never be sufficient to truly capture the language. In this paper we have attempted to make a summary of various known algorithms for specific classes of Tamil spelling errors. We believe this collection of suggestions to improve future spelling checkers. We also note do not cover many important techniques like affix removal and other such techniques of key importance in rule-based spell checkers.

| Comments: | 10 pages, 2 figures, Tamil Internet Conference - 2019, at Anna University, Chennai, India (INFITT) [Sep, 2019] |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1909.10063 [cs.CL]**                                 |
|           | (or **arXiv:1909.10063v1 [cs.CL]** for this version)         |





<h2 id="2019-09-24-5">5. TinyBERT: Distilling BERT for Natural Language Understanding</h2> 
Title: [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

Authors: [Xiaoqi Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+X), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Linlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Fang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 23 Sep 2019)*

> Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT. TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines.

| Comments: | 13 pages, 2 figures, 9 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.10351 [cs.CL]**                                 |
|           | (or **arXiv:1909.10351v1 [cs.CL]** for this version)         |





<h2 id="2019-09-24-6">6. Cross-Lingual Natural Language Generation via Pre-Training</h2> 
Title: [Cross-Lingual Natural Language Generation via Pre-Training](https://arxiv.org/abs/1909.10481)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H)

*(Submitted on 23 Sep 2019)*

> In this work we focus on transferring supervision signals of natural language generation (NLG) tasks between multiple languages. We propose to pretrain the encoder and decoder of a sequence-to-sequence model under both monolingual and cross-lingual settings. The pre-training objective encourages the model to represent different languages in the shared space, so that we can conduct zero-shot cross-lingual transfer. After the pre-training procedure, we use monolingual data to fine-tune the pre-trained model on downstream NLG tasks. Then the sequence-to-sequence model trained in a single language can be directly evaluated beyond that language (i.e., accepting multi-lingual input and producing multi-lingual output). Experimental results on question generation and abstractive summarization show that our model outperforms the machine-translation-based pipeline methods for zero-shot cross-lingual generation. Moreover, cross-lingual transfer improves NLG performance of low-resource languages by leveraging rich-resource language data.

| Comments: | 10 pages                                             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.10481 [cs.CL]**                         |
|           | (or **arXiv:1909.10481v1 [cs.CL]** for this version) |





# 2019-09-23

[Return to Index](#Index)



<h2 id="2019-09-23-1">1. SANVis: Visual Analytics for Understanding Self-Attention Networks</h2> 
Title: [SANVis: Visual Analytics for Understanding Self-Attention Networks](https://arxiv.org/abs/1909.09595)

Authors: [Cheonbok Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C), [Inyoup Na](https://arxiv.org/search/cs?searchtype=author&query=Na%2C+I), [Yongjang Jo](https://arxiv.org/search/cs?searchtype=author&query=Jo%2C+Y), [Sungbok Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+S), [Jaehyo Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+J), [Bum Chul Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+B+C), [Jian Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Hyungjong Noh](https://arxiv.org/search/cs?searchtype=author&query=Noh%2C+H), [Yeonsoo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+Y), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J)

*(Submitted on 13 Sep 2019)*

> Attention networks, a deep neural network architecture inspired by humans' attention mechanism, have seen significant success in image captioning, machine translation, and many other applications. Recently, they have been further evolved into an advanced approach called multi-head self-attention networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether multi-head attention. Meanwhile, the increased model complexity prevents users from easily understanding and manipulating the inner workings of models. To tackle the challenges, we present a visual analytics system called SANVis, which helps users understand the behaviors and the characteristics of multi-head self-attention networks. Using a state-of-the-art self-attention model called Transformer, we demonstrate usage scenarios of SANVis in machine translation tasks. Our system is available at [this http URL](http://short.sanvis.org/)

| Comments: | VAST Short - IEEE VIS 2019                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **arXiv:1909.09595 [cs.CL]**                                 |
|           | (or **arXiv:1909.09595v1 [cs.CL]** for this version)         |





<h2 id="2019-09-23-2">2. Improved Variational Neural Machine Translation by Promoting Mutual Information</h2> 
Title: [Improved Variational Neural Machine Translation by Promoting Mutual Information](https://arxiv.org/abs/1909.09237)

Authors: [Arya D. McCarthy](https://arxiv.org/search/cs?searchtype=author&query=McCarthy%2C+A+D), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Ning Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+N)

*(Submitted on 19 Sep 2019)*

> Posterior collapse plagues VAEs for text, especially for conditional text generation with strong autoregressive decoders. In this work, we address this problem in variational neural machine translation by explicitly promoting mutual information between the latent variables and the data. Our model extends the conditional variational autoencoder (CVAE) with two new ingredients: first, we propose a modified evidence lower bound (ELBO) objective which explicitly promotes mutual information; second, we regularize the probabilities of the decoder by mixing an auxiliary factorized distribution which is directly predicted by the latent variables. We present empirical results on the Transformer architecture and show the proposed model effectively addressed posterior collapse: latent variables are no longer ignored in the presence of powerful decoder. As a result, the proposed model yields improved translation quality while demonstrating superior performance in terms of data efficiency and robustness.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.09237 [cs.CL]**                         |
|           | (or **arXiv:1909.09237v1 [cs.CL]** for this version) |





<h2 id="2019-09-23-3">3. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</h2> 
Title: [AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models](https://arxiv.org/abs/1909.09251)

Authors: [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Jens Tuyls](https://arxiv.org/search/cs?searchtype=author&query=Tuyls%2C+J), [Junlin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Sanjay Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+S), [Matt Gardner](https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+M), [Sameer Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

*(Submitted on 19 Sep 2019)*

> Neural NLP models are increasingly accurate but are imperfect and opaque---they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit's flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at [this https URL](https://allennlp.org/interpret) .

| Comments: | EMNLP 2019 Demo                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.09251 [cs.CL]**                                 |
|           | (or **arXiv:1909.09251v1 [cs.CL]** for this version)         |





<h2 id="2019-09-23-4">4. Towards Neural Language Evaluators</h2> 
Title: [Towards Neural Language Evaluators](https://arxiv.org/abs/1909.09268)

Authors: [Hassan Kané](https://arxiv.org/search/cs?searchtype=author&query=Kané%2C+H), [Yusuf Kocyigit](https://arxiv.org/search/cs?searchtype=author&query=Kocyigit%2C+Y), [Pelkins Ajanoh](https://arxiv.org/search/cs?searchtype=author&query=Ajanoh%2C+P), [Ali Abdalla](https://arxiv.org/search/cs?searchtype=author&query=Abdalla%2C+A), [Mohamed Coulibali](https://arxiv.org/search/cs?searchtype=author&query=Coulibali%2C+M)

*(Submitted on 20 Sep 2019)*

> We review three limitations of BLEU and ROUGE -- the most popular metrics used to assess reference summaries against hypothesis summaries, come up with criteria for what a good metric should behave like and propose concrete ways to use recent Transformers-based Language Models to assess reference summaries against hypothesis summaries.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.09268 [cs.CL]**                                 |
|           | (or **arXiv:1909.09268v1 [cs.CL]** for this version)         |





<h2 id="2019-09-23-5">5. A simple discriminative training method for machine translation with large-scale features</h2> 
Title: [A simple discriminative training method for machine translation with large-scale features](https://arxiv.org/abs/1909.09491)

Authors: [Tian Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+T), [Shaodan Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+S), [Shaojun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S)

*(Submitted on 15 Sep 2019)*

> Margin infused relaxed algorithms (MIRAs) dominate model tuning in statistical machine translation in the case of large scale features, but also they are famous for the complexity in implementation. We introduce a new method, which regards an N-best list as a permutation and minimizes the Plackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate that, the new method is more robust than MERT; though it is only matchable with MIRAs, it has a comparatively advantage, easier to implement.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.09491 [cs.CL]**                         |
|           | (or **arXiv:1909.09491v1 [cs.CL]** for this version) |





<h2 id="2019-09-23-6">6. Controllable Length Control Neural Encoder-Decoder via Reinforcement Learning</h2> 
Title: [Controllable Length Control Neural Encoder-Decoder via Reinforcement Learning](https://arxiv.org/abs/1909.09492)

Authors: [Junyi Bian](https://arxiv.org/search/cs?searchtype=author&query=Bian%2C+J), [Baojun Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B), [Ke Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K), [Zhaohui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Z), [Hong Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H), [Yonghe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

*(Submitted on 17 Sep 2019)*

> Controlling output length in neural language generation is valuable in many scenarios, especially for the tasks that have length constraints. A model with stronger length control capacity can produce sentences with more specific length, however, it usually sacrifices semantic accuracy of the generated sentences. Here, we denote a concept of Controllable Length Control (CLC) for the trade-off between length control capacity and semantic accuracy of the language generation model. More specifically, CLC is to alter length control capacity of the model so as to generate sentence with corresponding quality. This is meaningful in real applications when length control capacity and outputs quality are requested with different priorities, or to overcome unstability of length control during model training. In this paper, we propose two reinforcement learning (RL) methods to adjust the trade-off between length control capacity and semantic accuracy of length control models. Results show that our RL methods improve scores across a wide range of target lengths and achieve the goal of CLC. Additionally, two models LenMC and LenLInit modified on previous length-control models are proposed to obtain better performance in summarization task while still maintain the ability to control length.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.09492 [cs.CL]**                         |
|           | (or **arXiv:1909.09492v1 [cs.CL]** for this version) |





<h2 id="2019-09-23-7">7. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages</h2> 
Title: [Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages](https://arxiv.org/abs/1909.09524)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Petre Petrov](https://arxiv.org/search/cs?searchtype=author&query=Petrov%2C+P), [Pavel Petrushkov](https://arxiv.org/search/cs?searchtype=author&query=Petrushkov%2C+P), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 20 Sep 2019)*

> We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.

| Comments: | EMNLP 2019 camera-ready                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.09524 [cs.CL]**                                 |
|           | (or **arXiv:1909.09524v1 [cs.CL]** for this version)         |





<h2 id="2019-09-23-8">8. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</h2> 
Title: [Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](https://arxiv.org/abs/1909.09587)

Authors: [Tsung-yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Chi-liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

*(Submitted on 15 Sep 2019)*

> Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with a language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.09587 [cs.CL]**                                 |
|           | (or **arXiv:1909.09587v1 [cs.CL]** for this version)         |







# 2019-09-20

[Return to Index](#Index)



<h2 id="2019-09-20-1">1. Do We Need Neural Models to Explain Human Judgments of Acceptability?</h2> 
Title: [Do We Need Neural Models to Explain Human Judgments of Acceptability?](https://arxiv.org/abs/1909.08663)

Authors: [Wang Jing](https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+W) (Beijing Normal University), [M. A. Kelly](https://arxiv.org/search/cs?searchtype=author&query=Kelly%2C+M+A) (The Pennsylvania State University), [David Reitter](https://arxiv.org/search/cs?searchtype=author&query=Reitter%2C+D) (Google Research)

*(Submitted on 18 Sep 2019)*

> Native speakers can judge whether a sentence is an acceptable instance of their language. Acceptability provides a means of evaluating whether computational language models are processing language in a human-like manner. We test the ability of computational language models, simple language features, and word embeddings to predict native English speakers judgments of acceptability on English-language essays written by non-native speakers. We find that much of the sentence acceptability variance can be captured by a combination of features including misspellings, word order, and word similarity (Pearson's r = 0.494). While predictive neural models fit acceptability judgments well (r = 0.527), we find that a 4-gram model with statistical smoothing is just as good (r = 0.528). Thanks to incorporating a count of misspellings, our 4-gram model surpasses both the previous unsupervised state-of-the art (Lau et al., 2015; r = 0.472), and the average non-expert native speaker (r = 0.46). Our results demonstrate that acceptability is well captured by n-gram statistics and simple language features.

| Comments: | 10 pages (8 pages + 2 pages of references), 1 figure, 7 tables |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.08663 [cs.CL]**                                 |
|           | (or **arXiv:1909.08663v1 [cs.CL]** for this version)         |





<h2 id="2019-09-20-2">2. Cross-Lingual Contextual Word Embeddings Mapping With Multi-Sense Words In Mind</h2> 
Title: [Cross-Lingual Contextual Word Embeddings Mapping With Multi-Sense Words In Mind](https://arxiv.org/abs/1909.08681)

Authors: [Zheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Ruiqing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+R), [Jun Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Pierre Zweigenbaum](https://arxiv.org/search/cs?searchtype=author&query=Zweigenbaum%2C+P)

*(Submitted on 18 Sep 2019)*

> Recent work in cross-lingual contextual word embedding learning cannot handle multi-sense words well. In this work, we explore the characteristics of contextual word embeddings and show the link between contextual word embeddings and word senses. We propose two improving solutions by considering contextual multi-sense word embeddings as noise (removal) and by generating cluster level average anchor embeddings for contextual multi-sense word embeddings (replacement). Experiments show that our solutions can improve the supervised contextual word embeddings alignment for multi-sense words in a microscopic perspective without hurting the macroscopic performance on the bilingual lexicon induction task. For unsupervised alignment, our methods significantly improve the performance on the bilingual lexicon induction task for more than 10 points.

| Comments: | 12 pages                                             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.08681 [cs.CL]**                         |
|           | (or **arXiv:1909.08681v1 [cs.CL]** for this version) |





<h2 id="2019-09-20-3">3. Low-Resource Parsing with Crosslingual Contextualized Representations</h2> 
Title: [Low-Resource Parsing with Crosslingual Contextualized Representations](https://arxiv.org/abs/1909.08744)

Authors: [Phoebe Mulcaire](https://arxiv.org/search/cs?searchtype=author&query=Mulcaire%2C+P), [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 19 Sep 2019)*

> Despite advances in dependency parsing, languages with small treebanks still present challenges. We assess recent approaches to multilingual contextual word representations (CWRs), and compare them for crosslingual transfer from a language with a large treebank to a language with a small or nonexistent treebank, by sharing parameters between languages in the parser itself. We experiment with a diverse selection of languages in both simulated and truly low-resource scenarios, and show that multilingual CWRs greatly facilitate low-resource dependency parsing even without crosslingual supervision such as dictionaries or parallel text. Furthermore, we examine the non-contextual part of the learned language models (which we call a "decontextual probe") to demonstrate that polyglot language models better encode crosslingual lexical correspondence compared to aligned monolingual language models. This analysis provides further evidence that polyglot training is an effective approach to crosslingual transfer.

| Comments: | CoNLL 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.08744 [cs.CL]**                         |
|           | (or **arXiv:1909.08744v1 [cs.CL]** for this version) |





<h2 id="2019-09-20-4">4. Improving Generalization by Incorporating Coverage in Natural Language Inference</h2> 
Title: [Improving Generalization by Incorporating Coverage in Natural Language Inference](https://arxiv.org/abs/1909.08940)

Authors: [Nafise Sadat Moosavi](https://arxiv.org/search/cs?searchtype=author&query=Moosavi%2C+N+S), [Prasetya Ajie Utama](https://arxiv.org/search/cs?searchtype=author&query=Utama%2C+P+A), [Andreas Rücklé](https://arxiv.org/search/cs?searchtype=author&query=Rücklé%2C+A), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

*(Submitted on 19 Sep 2019)*

> The task of natural language inference (NLI) is to identify the relation between the given premise and hypothesis. While recent NLI models achieve very high performance on individual datasets, they fail to generalize across similar datasets. This indicates that they are solving NLI datasets instead of the task itself. In order to improve generalization, we propose to extend the input representations with an abstract view of the relation between the hypothesis and the premise, i.e., how well the individual words, or word n-grams, of the hypothesis are covered by the premise. Our experiments show that the use of this information considerably improves generalization across different NLI datasets without requiring any external knowledge or additional data. Finally, we show that using the coverage information is not only beneficial for improving the performance across different datasets of the same task. The resulting generalization improves the performance across datasets that belong to similar but not the same tasks.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.08940 [cs.CL]**                         |
|           | (or **arXiv:1909.08940v1 [cs.CL]** for this version) |





<h2 id="2019-09-20-5">5. CogniVal: A Framework for Cognitive Word Embedding Evaluation</h2> 
Title: [CogniVal: A Framework for Cognitive Word Embedding Evaluation](https://arxiv.org/abs/1909.09001)

Authors: [Nora Hollenstein](https://arxiv.org/search/cs?searchtype=author&query=Hollenstein%2C+N), [Antonio de la Torre](https://arxiv.org/search/cs?searchtype=author&query=de+la+Torre%2C+A), [Nicolas Langer](https://arxiv.org/search/cs?searchtype=author&query=Langer%2C+N), [Ce Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C)

*(Submitted on 19 Sep 2019)*

> An interesting method of evaluating word representations is by how much they reflect the semantic representations in the human brain. However, most, if not all, previous works only focus on small datasets and a single modality. In this paper, we present the first multi-modal framework for evaluating English word representations based on cognitive lexical semantics. Six types of word embeddings are evaluated by fitting them to 15 datasets of eye-tracking, EEG and fMRI signals recorded during language processing. To achieve a global score over all evaluation hypotheses, we apply statistical significance testing accounting for the multiple comparisons problem. This framework is easily extensible and available to include other intrinsic and extrinsic evaluation methods. We find strong correlations in the results between cognitive datasets, across recording modalities and to their performance on extrinsic NLP tasks.

| Comments: | accepted at CoNLL 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.09001 [cs.CL]**                         |
|           | (or **arXiv:1909.09001v1 [cs.CL]** for this version) |





# 2019-09-19

[Return to Index](#Index)





<h2 id="2019-09-19-1">1. Relaxed Softmax for learning from Positive and Unlabeled data</h2> 
Title: [Relaxed Softmax for learning from Positive and Unlabeled data](https://arxiv.org/abs/1909.08079)

Authors: [Ugo Tanielian](https://arxiv.org/search/stat?searchtype=author&query=Tanielian%2C+U), [Flavian Vasile](https://arxiv.org/search/stat?searchtype=author&query=Vasile%2C+F)

*(Submitted on 17 Sep 2019)*

> In recent years, the softmax model and its fast approximations have become the de-facto loss functions for deep neural networks when dealing with multi-class prediction. This loss has been extended to language modeling and recommendation, two fields that fall into the framework of learning from Positive and Unlabeled data. In this paper, we stress the different drawbacks of the current family of softmax losses and sampling schemes when applied in a Positive and Unlabeled learning setup. We propose both a Relaxed Softmax loss (RS) and a new negative sampling scheme based on Boltzmann formulation. We show that the new training objective is better suited for the tasks of density estimation, item similarity and next-event prediction by driving uplifts in performance on textual and recommendation datasets against classical softmax.

| Comments:          | 9 pages, 5 figures, 2 tables, published at RecSys 2019       |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Journal reference: | RecSys 2019 Proceedings of the 13th ACM Conference on Recommender Systems |
| DOI:               | [10.1145/3298689.3347034](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3298689.3347034&v=e3db5db6) |
| Cite as:           | **arXiv:1909.08079 [stat.ML]**                               |
|                    | (or **arXiv:1909.08079v1 [stat.ML]** for this version)       |





<h2 id="2019-09-19-2">2. Memory-Augmented Neural Networks for Machine Translation</h2> 
Title: [Memory-Augmented Neural Networks for Machine Translation](https://arxiv.org/abs/1909.08314)

Authors: [Mark Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+M), [Joeran Beel](https://arxiv.org/search/cs?searchtype=author&query=Beel%2C+J)

*(Submitted on 18 Sep 2019)*

> Memory-augmented neural networks (MANNs) have been shown to outperform other recurrent neural network architectures on a series of artificial sequence learning tasks, yet they have had limited application to real-world tasks. We evaluate direct application of Neural Turing Machines (NTM) and Differentiable Neural Computers (DNC) to machine translation. We further propose and evaluate two models which extend the attentional encoder-decoder with capabilities inspired by memory augmented neural networks. We evaluate our proposed models on IWSLT Vietnamese to English and ACL Romanian to English datasets. Our proposed models and the memory augmented neural networks perform similarly to the attentional encoder-decoder on the Vietnamese to English translation task while have a 0.3-1.9 lower BLEU score for the Romanian to English task. Interestingly, our analysis shows that despite being equipped with additional flexibility and being randomly initialized memory augmented neural networks learn an algorithm for machine translation almost identical to the attentional encoder-decoder.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.08314 [cs.LG]**                                 |
|           | (or **arXiv:1909.08314v1 [cs.LG]** for this version)         |





<h2 id="2019-09-19-3">3. Subword ELMo</h2> 
Title: [Subword ELMo](https://arxiv.org/abs/1909.08357)

Authors: [Jiangtong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Wei Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+W), [Xiaojiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X)

*(Submitted on 18 Sep 2019)*

> Embedding from Language Models (ELMo) has shown to be effective for improving many natural language processing (NLP) tasks, and ELMo takes character information to compose word representation to train language models.However, the character is an insufficient and unnatural linguistic unit for word representation.Thus we introduce Embedding from Subword-aware Language Models (ESuLMo) which learns word representation from subwords using unsupervised segmentation over words.We show that ESuLMo can enhance four benchmark NLP tasks more effectively than ELMo, including syntactic dependency parsing, semantic role labeling, implicit discourse relation recognition and textual entailment, which brings a meaningful improvement over ELMo.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.08357 [cs.CL]**                         |
|           | (or **arXiv:1909.08357v1 [cs.CL]** for this version) |





<h2 id="2019-09-19-4">4. Simple, Scalable Adaptation for Neural Machine Translation</h2> 
Title: [Simple, Scalable Adaptation for Neural Machine Translation](https://arxiv.org/abs/1909.08478)

Authors: [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 18 Sep 2019)*

> Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.08478 [cs.CL]**                                 |
|           | (or **arXiv:1909.08478v1 [cs.CL]** for this version)         |





<h2 id="2019-09-19-5">5. Fine-Tuning Language Models from Human Preferences</h2> 
Title: [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)

Authors: [Daniel M. Ziegler](https://arxiv.org/search/cs?searchtype=author&query=Ziegler%2C+D+M), [Nisan Stiennon](https://arxiv.org/search/cs?searchtype=author&query=Stiennon%2C+N), [Jeffrey Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Tom B. Brown](https://arxiv.org/search/cs?searchtype=author&query=Brown%2C+T+B), [Alec Radford](https://arxiv.org/search/cs?searchtype=author&query=Radford%2C+A), [Dario Amodei](https://arxiv.org/search/cs?searchtype=author&query=Amodei%2C+D), [Paul Christiano](https://arxiv.org/search/cs?searchtype=author&query=Christiano%2C+P), [Geoffrey Irving](https://arxiv.org/search/cs?searchtype=author&query=Irving%2C+G)

*(Submitted on 18 Sep 2019)*

> Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.08593 [cs.CL]**                                 |
|           | (or **arXiv:1909.08593v1 [cs.CL]** for this version)         |





# 2019-09-18

[Return to Index](#Index)



<h2 id="2019-09-18-1">1. Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation</h2> 
Title: [Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation](https://arxiv.org/abs/1909.07575)

Authors: [Chengyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Zhenglu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 17 Sep 2019)*

> End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model outperforms baselines 2.2 BLEU on a large benchmark dataset.

| Comments: | submitted to AAAI2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1909.07575 [cs.CL]**                                 |
|           | (or **arXiv:1909.07575v1 [cs.CL]** for this version)         |





<h2 id="2019-09-18-2">2. Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation</h2> 
Title: [Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation](https://arxiv.org/abs/1909.07907)

Authors: [Jetic Gū](https://arxiv.org/search/cs?searchtype=author&query=Gū%2C+J), [Hassan S. Shavarani](https://arxiv.org/search/cs?searchtype=author&query=Shavarani%2C+H+S), [Anoop Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+A)

*(Submitted on 17 Sep 2019)*

> Neural machine translation (NMT) systems require large amounts of high quality in-domain parallel corpora for training. State-of-the-art NMT systems still face challenges related to out-of-vocabulary words and dealing with low-resource language pairs. In this paper, we propose and compare several models for fusion of bilingual lexicons with an end-to-end trained sequence-to-sequence model for machine translation. The result is a fusion model with two information sources for the decoder: a neural conditional language model and a bilingual lexicon. This fusion model learns how to combine both sources of information in order to produce higher quality translation output. Our experiments show that our proposed models work well in relatively low-resource scenarios, and also effectively reduce the parameter size and training cost for NMT without sacrificing performance.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.07907 [cs.CL]**                         |
|           | (or **arXiv:1909.07907v1 [cs.CL]** for this version) |





<h2 id="2019-09-18-3">3. Learning to Deceive with Attention-Based Explanations</h2> 
Title: [Learning to Deceive with Attention-Based Explanations](https://arxiv.org/abs/1909.07913)

Authors: [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Mansi Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+M), [Bhuwan Dhingra](https://arxiv.org/search/cs?searchtype=author&query=Dhingra%2C+B), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Zachary C. Lipton](https://arxiv.org/search/cs?searchtype=author&query=Lipton%2C+Z+C)

*(Submitted on 17 Sep 2019)*

> Attention mechanisms are ubiquitous components in neural architectures applied in natural language processing. In addition to yielding gains in predictive accuracy, researchers often claim that attention weights confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question, demonstrating a simple method for training models to produce deceptive attention masks, diminishing the total weight assigned to designated impermissible tokens, even as the models are shown to nevertheless rely on these features to drive predictions. Across multiple models and datasets, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Although our results do not rule out potential insights due to organically-trained attention, they cast doubt on attention's reliability as a tool for auditing algorithms, as in the context of fairness and accountability.

| Comments: | Preprint. Ongoing work                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.07913 [cs.CL]**                         |
|           | (or **arXiv:1909.07913v1 [cs.CL]** for this version) |



# 2019-09-17

[Return to Index](#Index)



<h2 id="2019-09-17-1">1. Adaptive Scheduling for Multi-Task Learning</h2> 
Title: [Adaptive Scheduling for Multi-Task Learning](https://arxiv.org/abs/1909.06434)

Authors:[Sébastien Jean](https://arxiv.org/search/cs?searchtype=author&query=Jean%2C+S), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M)

*(Submitted on 13 Sep 2019)*

> To train neural machine translation models simultaneously on multiple tasks (languages), it is common to sample each task uniformly or in proportion to dataset sizes. As these methods offer little control over performance trade-offs, we explore different task scheduling approaches. We first consider existing non-adaptive techniques, then move on to adaptive schedules that over-sample tasks with poorer results compared to their respective baseline. As explicit schedules can be inefficient, especially if one task is highly over-sampled, we also consider implicit schedules, learning to scale learning rates or gradients of individual tasks instead. These techniques allow training multilingual models that perform better for low-resource language pairs (tasks with small amount of data), while minimizing negative effects on high-resource tasks.

| Comments: | Continual Learning Workshop at NeurIPS 2018                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1909.06434 [cs.LG]**                                 |
|           | (or **arXiv:1909.06434v1 [cs.LG]** for this version)         |





<h2 id="2019-09-17-2">2. Leveraging Out-of-Task Data for End-to-End Automatic Speech Translation</h2> 
Title: [Leveraging Out-of-Task Data for End-to-End Automatic Speech Translation](https://arxiv.org/abs/1909.06515)

Authors:[Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Liezl Puzon](https://arxiv.org/search/cs?searchtype=author&query=Puzon%2C+L), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Arya D. McCarthy](https://arxiv.org/search/cs?searchtype=author&query=McCarthy%2C+A+D), [Deepak Gopinath](https://arxiv.org/search/cs?searchtype=author&query=Gopinath%2C+D)

*(Submitted on 14 Sep 2019)*

> For automatic speech translation (AST), end-to-end approaches are outperformed by cascaded models that transcribe with automatic speech recognition (ASR), then translate with machine translation (MT). A major cause of the performance gap is that, while existing AST corpora are small, massive datasets exist for both the ASR and MT subsystems. In this work, we evaluate several data augmentation and pretraining approaches for AST, comparing all on the same datasets. Simple data augmentation by translating ASR transcripts proves most effective on the English--French augmented LibriSpeech dataset, closing the performance gap from 8.2 to 1.4 BLEU, compared to a very strong cascade that could directly utilize copious ASR and MT data. The same end-to-end approach plus fine-tuning closes the gap on the English--Romanian MuST-C dataset from 6.7 to 3.7 BLEU. In addition to these results, we present practical recommendations for augmentation and pretraining approaches. Finally, we decrease the performance gap to 0.01 BLEU using a Transformer-based architecture.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.06515 [cs.CL]**                                 |
|           | (or **arXiv:1909.06515v1 [cs.CL]** for this version)         |





<h2 id="2019-09-17-3">3. A Universal Parent Model for Low-Resource Neural Machine Translation Transfer</h2> 
Title: [A Universal Parent Model for Low-Resource Neural Machine Translation Transfer](https://arxiv.org/abs/1909.06516)

Authors:[Mozhdeh Gheini](https://arxiv.org/search/cs?searchtype=author&query=Gheini%2C+M), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

*(Submitted on 14 Sep 2019)*

> Transfer learning from a high-resource language pair `parent' has been proven to be an effective way to improve neural machine translation quality for low-resource language pairs `children.' However, previous approaches build a custom parent model or at least update an existing parent model's vocabulary for each child language pair they wish to train, in an effort to align parent and child vocabularies. This is not a practical solution. It is wasteful to devote the majority of training time for new language pairs to optimizing parameters on an unrelated data set. Further, this overhead reduces the utility of neural machine translation for deployment in humanitarian assistance scenarios, where extra time to deploy a new language pair can mean the difference between life and death. In this work, we present a `universal' pre-trained neural parent model with constant vocabulary that can be used as a starting point for training practically any new low-resource language to a fixed target language. We demonstrate that our approach, which leverages orthography unification and a broad-coverage approach to subword identification, generalizes well to several languages from a variety of families, and that translation systems built with our approach can be built more quickly than competing methods and with better quality as well.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.06516 [cs.CL]**                         |
|           | (or **arXiv:1909.06516v1 [cs.CL]** for this version) |





<h2 id="2019-09-17-4">4. ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation</h2> 
Title: [ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation](https://arxiv.org/abs/1909.06564)

Authors:[Qiongkai Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Chenchen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Lizhen Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+L)

*(Submitted on 14 Sep 2019)*

> In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation.

| Comments: | EMNLP 2019 (Demo)                                    |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.06564 [cs.CL]**                         |
|           | (or **arXiv:1909.06564v1 [cs.CL]** for this version) |





<h2 id="2019-09-17-5">5. Tree Transformer: Integrating Tree Structures into Self-Attention</h2> 
Title: [Tree Transformer: Integrating Tree Structures into Self-Attention](https://arxiv.org/abs/1909.06639)

Authors:[Yau-Shian Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Hung-Yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

*(Submitted on 14 Sep 2019)*

> Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed ``Constituent Attention'' module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.

| Comments: | accepted by EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.06639 [cs.CL]**                                 |
|           | (or **arXiv:1909.06639v1 [cs.CL]** for this version)         |





<h2 id="2019-09-17-6">6. Beyond BLEU: Training Neural Machine Translation with Semantic Similarity</h2> 
Title: [Beyond BLEU: Training Neural Machine Translation with Semantic Similarity](https://arxiv.org/abs/1909.06694)

Authors:[John Wieting](https://arxiv.org/search/cs?searchtype=author&query=Wieting%2C+J), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 14 Sep 2019)*

> While most neural machine translation (NMT) systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can substantially improve final translation accuracy. However, training with BLEU has some limitations: it doesn't assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages translated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.

| Comments: | Published as a long paper at ACL 2019                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.06694 [cs.CL]**                         |
|           | (or **arXiv:1909.06694v1 [cs.CL]** for this version) |





<h2 id="2019-09-17-7">7. Hint-Based Training for Non-Autoregressive Machine Translation</h2> 
Title: [Hint-Based Training for Non-Autoregressive Machine Translation](https://arxiv.org/abs/1909.06708)

Authors:[Zhuohan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Fei Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+F), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 15 Sep 2019)*

> Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTM-based ART baseline but one order of magnitude faster in inference.

| Comments: | EMNLP-IJCNLP 2019                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1909.06708 [cs.CL]**                                 |
|           | (or **arXiv:1909.06708v1 [cs.CL]** for this version)         |





<h2 id="2019-09-17-8">8. Natural Language Adversarial Attacks and Defenses in Word Level</h2> 
Title: [Natural Language Adversarial Attacks and Defenses in Word Level](https://arxiv.org/abs/1909.06723)

Authors:[Xiaosen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Hao Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+H), [Kun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+K)

*(Submitted on 15 Sep 2019)*

> Up until recent two years, inspired by the big amount of research about adversarial example in the field of computer vision, there has been a growing interest in adversarial attacks for Natural Language Processing (NLP). What followed was a very few works of adversarial defense for NLP. However, there exists no defense method against the successful synonyms substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to perceived by humans. To fill this gap, we postulate the generalization of the model leads to the existence of adversarial examples, and propose an adversarial defense method called Synonyms Encoding Method (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with almost no decay on the accuracy for benign examples. Besides, to better evaluate SEM, we also propose a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic against synonyms substitution based attacks. Compared with existing genetic based adversarial attack, the proposed IGA can achieve higher attack success rate at the same time maintain the transferability of adversarial examples.

| Comments: | 13 pages, 3 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.06723 [cs.CL]**                                 |
|           | (or **arXiv:1909.06723v1 [cs.CL]** for this version)         |





<h2 id="2019-09-17-9">9. Automatically Extracting Challenge Sets for Non-local Phenomena Neural Machine Translation</h2> 
Title: [Automatically Extracting Challenge Sets for Non-local Phenomena Neural Machine Translation](https://arxiv.org/abs/1909.06814)

Authors:[Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O)

*(Submitted on 15 Sep 2019)*

> We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the model. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We, therefore, propose an automatic approach for extracting challenge sets replete with long-distance dependencies, and argue that evaluation using this methodology provides a complementary perspective on system performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.

| Comments: | Accepted for CoNLL                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.06814 [cs.CL]**                                 |
|           | (or **arXiv:1909.06814v1 [cs.CL]** for this version)         |





<h2 id="2019-09-17-10">10. Communication-based Evaluation for Natural Language Generation</h2> 
Title: [Communication-based Evaluation for Natural Language Generation](https://arxiv.org/abs/1909.07290)

Authors:[Benjamin Newman](https://arxiv.org/search/cs?searchtype=author&query=Newman%2C+B), [Reuben Cohn-Gordon](https://arxiv.org/search/cs?searchtype=author&query=Cohn-Gordon%2C+R), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts%2C+C)

*(Submitted on 16 Sep 2019)*

> Natural language generation (NLG) systems are commonly evaluated using n-gram overlap measures (e.g. BLEU, ROUGE). These measures do not directly capture semantics or speaker intentions, and so they often turn out to be misaligned with our true goals for NLG. In this work, we argue instead for communication-based evaluations: assuming the purpose of an NLG system is to convey information to a reader/listener, we can directly evaluate its effectiveness at this task using the Rational Speech Acts model of pragmatic language use. We illustrate with a color reference dataset that contains descriptions in pre-defined quality categories, showing that our method better aligns with these quality categories than do any of the prominent n-gram overlap methods.

| Comments: | 10 pages, 1 figure, SCiL                             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.07290 [cs.CL]**                         |
|           | (or **arXiv:1909.07290v1 [cs.CL]** for this version) |





<h2 id="2019-09-17-11">11. Multilingual Neural Machine Translation for Zero-Resource Languages</h2> 
Title: [Multilingual Neural Machine Translation for Zero-Resource Languages](https://arxiv.org/abs/1909.07342)

Authors:[Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 16 Sep 2019)*

> In recent years, Neural Machine Translation (NMT) has been shown to be more effective than phrase-based statistical methods, thus quickly becoming the state of the art in machine translation (MT). However, NMT systems are limited in translating low-resourced languages, due to the significant amount of parallel data that is required to learn useful mappings between languages. In this work, we show how the so-called multilingual NMT can help to tackle the challenges associated with low-resourced language translation. The underlying principle of multilingual NMT is to force the creation of hidden representations of words in a shared semantic space across multiple languages, thus enabling a positive parameter transfer across languages. Along this direction, we present multilingual translation experiments with three languages (English, Italian, Romanian) covering six translation directions, utilizing both recurrent neural networks and transformer (or self-attentive) neural networks. We then focus on the zero-shot translation problem, that is how to leverage multi-lingual data in order to learn translation directions that are not covered by the available training material. To this aim, we introduce our recently proposed iterative self-training method, which incrementally improves a multilingual NMT on a zero-shot direction by just relying on monolingual data. Our results on TED talks data show that multilingual NMT outperforms conventional bilingual NMT, that the transformer NMT outperforms recurrent NMT, and that zero-shot NMT outperforms conventional pivoting methods and even matches the performance of a fully-trained bilingual system.

| Comments: | 15 pages, Published on Italian Journal of Computational Linguistics (IJCoL) -- Multilingual Neural Machine Translation for Low-Resource Languages, June 2018 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1909.07342 [cs.CL]**                                 |
|           | (or **arXiv:1909.07342v1 [cs.CL]** for this version)         |



# 2019-09-16

[Return to Index](#Index)



<h2 id="2019-09-16-1">1. CTRL: A Conditional Transformer Language Model for Controllable Generation</h2> 
Title: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)

Authors: [Nitish Shirish Keskar](https://arxiv.org/search/cs?searchtype=author&query=Keskar%2C+N+S), [Bryan McCann](https://arxiv.org/search/cs?searchtype=author&query=McCann%2C+B), [Lav R. Varshney](https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+L+R), [Caiming Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C), [Richard Socher](https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R)

*(Submitted on 11 Sep 2019)*

> Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.6 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at [this http URL](http://github.com/salesforce/ctrl).

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.05858 [cs.CL]**                         |
|           | (or **arXiv:1909.05858v1 [cs.CL]** for this version) |



<h2 id="2019-09-16-2">2. Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting</h2> 
Title: [Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting](https://arxiv.org/abs/1909.06002)

Authors: [Yi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

*(Submitted on 13 Sep 2019)*

> We study sequence-to-sequence (seq2seq) pre-training with data augmentation for sentence rewriting. Instead of training a seq2seq model with gold training data and augmented data simultaneously, we separate them to train in different phases: pre-training with the augmented data and fine-tuning with the gold data. We also introduce multiple data augmentation methods to help model pre-training for sentence rewriting. We evaluate our approach in two typical well-defined sentence rewriting tasks: Grammatical Error Correction (GEC) and Formality Style Transfer (FST). Experiments demonstrate our approach can better utilize augmented data without hurting the model's trust in gold data and further improve the model's performance with our proposed data augmentation methods.
> Our approach substantially advances the state-of-the-art results in well-recognized sentence rewriting benchmarks over both GEC and FST. Specifically, it pushes the CoNLL-2014 benchmark's F0.5 score and JFLEG Test GLEU score to 62.61 and 63.54 in the restricted training setting, 66.77 and 65.22 respectively in the unrestricted setting, and advances GYAFC benchmark's BLEU to 74.24 (2.23 absolute improvement) in E&M domain and 77.97 (2.64 absolute improvement) in F&R domain.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.06002 [cs.CL]**                         |
|           | (or **arXiv:1909.06002v1 [cs.CL]** for this version) |





<h2 id="2019-09-16-3">3. Neural Machine Translation with 4-Bit Precision and Beyond</h2> 
Title: [Neural Machine Translation with 4-Bit Precision and Beyond](https://arxiv.org/abs/1909.06091)

Authors: [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

*(Submitted on 13 Sep 2019)*

> Neural Machine Translation (NMT) is resource intensive. We design a quantization procedure to compress fit NMT models better for devices with limited hardware capability. We use logarithmic quantization, instead of the more commonly used fixed-point quantization, based on the empirical fact that parameters distribution is not uniform. We find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. We also propose to use an error-feedback mechanism during retraining, to preserve the compressed model as a stale gradient. We empirically show that NMT models based on Transformer or RNN architecture can be compressed up to 4-bit precision without any noticeable quality degradation. Models can be compressed up to binary precision, albeit with lower quality. RNN architecture seems to be more robust towards compression, compared to the Transformer.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.06091 [cs.CL]**                                 |
|           | (or **arXiv:1909.06091v1 [cs.CL]** for this version)         |





<h2 id="2019-09-16-4">4. A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces</h2> 
Title: [A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces](https://arxiv.org/abs/1909.06092)

Authors: [Anne Lauscher](https://arxiv.org/search/cs?searchtype=author&query=Lauscher%2C+A), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Simone Paolo Ponzetto](https://arxiv.org/search/cs?searchtype=author&query=Ponzetto%2C+S+P), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I)

*(Submitted on 13 Sep 2019)*

> Distributional word vectors have recently been shown to encode many of the human biases, most notably gender and racial biases, and models for attenuating such biases have consequently been proposed. However, existing models and studies (1) operate on under-specified and mutually differing bias definitions, (2) are tailored for a particular bias (e.g., gender bias) and (3) have been evaluated inconsistently and non-rigorously. In this work, we introduce a general framework for debiasing word embeddings. We operationalize the definition of a bias by discerning two types of bias specification: explicit and implicit. We then propose three debiasing models that operate on explicit or implicit bias specifications, and that can be composed towards more robust debiasing. Finally, we devise a full-fledged evaluation framework in which we couple existing bias metrics with newly proposed ones. Experimental findings across three embedding methods suggest that the proposed debiasing models are robust and widely applicable: they often completely remove the bias both implicitly and explicitly, without degradation of semantic information encoded in any of the input distributional spaces. Moreover, we successfully transfer debiasing models, by means of crosslingual embedding spaces, and remove or attenuate biases in distributional word vector spaces of languages that lack readily available bias specifications.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.06092 [cs.CL]**                                 |
|           | (or **arXiv:1909.06092v1 [cs.CL]** for this version)         |





# 2019-09-13

[Return to Index](#Index)



<h2 id="2019-09-13-1">1. Entity Projection via Machine-Translation for Cross-Lingual NER</h2> 
Title: [Entity Projection via Machine-Translation for Cross-Lingual NER](https://arxiv.org/abs/1909.05356)

Authors: [Alankar Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A), [Bhargavi Paranjape](https://arxiv.org/search/cs?searchtype=author&query=Paranjape%2C+B), [Zachary C. Lipton](https://arxiv.org/search/cs?searchtype=author&query=Lipton%2C+Z+C)

*(Submitted on 31 Aug 2019)*

> Although over 100 languages are supported by strong off-the-shelf machine translation systems, only a subset of them possess large annotated corpora for named entity recognition. Motivated by this fact, we leverage machine translation to improve annotation-projection approaches to cross-lingual named entity recognition. We propose a system that improves over prior entity-projection methods by: (a) leveraging machine translation systems twice: first for translating sentences and subsequently for translating entities; (b) matching entities based on orthographic and phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Our approach improves upon current state-of-the-art methods for cross-lingual named entity recognition on 5 diverse languages by an average of 4.1 points. Further, our method achieves state-of-the-art F_1 scores for Armenian, outperforming even a monolingual model trained on Armenian source data.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.05356 [cs.CL]**                                 |
|           | (or **arXiv:1909.05356v1 [cs.CL]** for this version)         |





<h2 id="2019-09-13-2">2. Problems with automating translation of movie/TV show subtitles</h2> 
Title: [Problems with automating translation of movie/TV show subtitles](https://arxiv.org/abs/1909.05362)

Authors: [Prabhakar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Mayank Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+M), [Kartik Pitale](https://arxiv.org/search/cs?searchtype=author&query=Pitale%2C+K), [Keshav Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+K)

*(Submitted on 4 Sep 2019)*

> We present 27 problems encountered in automating the translation of movie/TV show subtitles. We categorize each problem in one of the three categories viz. problems directly related to textual translation, problems related to subtitle creation guidelines, and problems due to adaptability of machine translation (MT) engines. We also present the findings of a translation quality evaluation experiment where we share the frequency of 16 key problems. We show that the systems working at the frontiers of Natural Language Processing do not perform well for subtitles and require some post-processing solutions for redressal of these problems

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.05362 [cs.CL]**                                 |
|           | (or **arXiv:1909.05362v1 [cs.CL]** for this version)         |



<h2 id="2019-09-13-3">3. Speculative Beam Search for Simultaneous Translation</h2> 
Title: [Speculative Beam Search for Simultaneous Translation](https://arxiv.org/abs/1909.05421)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

*(Submitted on 12 Sep 2019)*

> Beam search is universally used in full-sentence translation but its application to simultaneous translation remains non-trivial, where output words are committed on the fly. In particular, the recently proposed wait-k policy (Ma et al., 2019a) is a simple and effective method that (after an initial wait) commits one output word on receiving each input word, making beam search seemingly impossible. To address this challenge, we propose a speculative beam search algorithm that hallucinates several steps into the future in order to reach a more accurate decision, implicitly benefiting from a target language model. This makes beam search applicable for the first time to the generation of a single word in each step. Experiments over diverse language pairs show large improvements over previous work.

| Comments: | accepted by EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.05421 [cs.CL]**                         |
|           | (or **arXiv:1909.05421v1 [cs.CL]** for this version) |





<h2 id="2019-09-13-4">4. VizSeq: A Visual Analysis Toolkit for Text Generation Tasks</h2> 
Title: [VizSeq: A Visual Analysis Toolkit for Text Generation Tasks](https://arxiv.org/abs/1909.05424)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Anirudh Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A), [Danlu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 12 Sep 2019)*

> Automatic evaluation of text generation tasks (e.g. machine translation, text summarization, image captioning and video description) usually relies heavily on task-specific metrics, such as BLEU and ROUGE. They, however, are abstract numbers and are not perfectly aligned with human assessment. This suggests inspecting detailed examples as a complement to identify system error patterns. In this paper, we present VizSeq, a visual analysis toolkit for instance-level and corpus-level system evaluation on a wide variety of text generation tasks. It supports multimodal sources and multiple text references, providing visualization in Jupyter notebook or a web app interface. It can be used locally or deployed onto public servers for centralized data hosting and benchmarking. It covers most common n-gram based metrics accelerated with multiprocessing, and also provides latest embedding-based metrics such as BERTScore.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.05424 [cs.CL]**                         |
|           | (or **arXiv:1909.05424v1 [cs.CL]** for this version) |





<h2 id="2019-09-13-5">5. Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning</h2> 
Title: [Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning](https://arxiv.org/abs/1909.05438)

Authors: [Yibo Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Xiaocheng Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+X), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D)

*(Submitted on 12 Sep 2019)*

> Neural semantic parsing has achieved impressive results in recent years, yet its success relies on the availability of large amounts of supervised data. Our goal is to learn a neural semantic parser when only prior knowledge about a limited number of simple rules is available, without access to either annotated programs or execution results. Our approach is initialized by rules, and improved in a back-translation paradigm using generated question-program pairs from the semantic parser and the question generator. A phrase table with frequent mapping patterns is automatically derived, also updated as training progresses, to measure the quality of generated instances. We train the model with model-agnostic meta-learning to guarantee the accuracy and stability on examples covered by rules, and meanwhile acquire the versatility to generalize well on examples uncovered by rules. Results on three benchmark datasets with different domains and programs show that our approach incrementally improves the accuracy. On WikiSQL, our best model is comparable to the SOTA system learned from denotations.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.05438 [cs.CL]**                         |
|           | (or **arXiv:1909.05438v1 [cs.CL]** for this version) |





<h2 id="2019-09-13-6">6. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction</h2> 
Title: [Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction](https://arxiv.org/abs/1909.05708)

Authors: [Yova Kementchedjhieva](https://arxiv.org/search/cs?searchtype=author&query=Kementchedjhieva%2C+Y), [Mareike Hartmann](https://arxiv.org/search/cs?searchtype=author&query=Hartmann%2C+M), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A)

*(Submitted on 12 Sep 2019)*

> The task of bilingual dictionary induction (BDI) is commonly used for intrinsic evaluation of cross-lingual word embeddings. The largest dataset for BDI was generated automatically, so its quality is dubious. We study the composition and quality of the test sets for five diverse languages from this dataset, with concerning findings: (1) a quarter of the data consists of proper nouns, which can be hardly indicative of BDI performance, and (2) there are pervasive gaps in the gold-standard targets. These issues appear to affect the ranking between cross-lingual embedding systems on individual languages, and the overall degree to which the systems differ in performance. With proper nouns removed from the data, the margin between the top two systems included in the study grows from 3.4% to 17.2%. Manual verification of the predictions, on the other hand, reveals that gaps in the gold standard targets artificially inflate the margin between the two systems on English to Bulgarian BDI from 0.1% to 6.7%. We thus suggest that future research either avoids drawing conclusions from quantitative results on this BDI dataset, or accompanies such evaluation with rigorous error analysis.

| Comments: | Accepted at EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.05708 [cs.CL]**                         |
|           | (or **arXiv:1909.05708v1 [cs.CL]** for this version) |



# 2019-09-12

[Return to Index](#Index)



<h2 id="2019-09-12-1">1. A Quantum Search Decoder for Natural Language Processing</h2> 
Title: [A Quantum Search Decoder for Natural Language Processing](https://arxiv.org/abs/1909.05023)

Authors:[Johannes Bausch](https://arxiv.org/search/quant-ph?searchtype=author&query=Bausch%2C+J), [Sathyawageeswar Subramanian](https://arxiv.org/search/quant-ph?searchtype=author&query=Subramanian%2C+S), [Stephen Piddock](https://arxiv.org/search/quant-ph?searchtype=author&query=Piddock%2C+S)

*(Submitted on 9 Sep 2019)*

> Probabilistic language models, e.g. those based on an LSTM, often face the problem of finding a high probability prediction from a sequence of random variables over a set of words. This is commonly addressed using a form of greedy decoding such as beam search, where a limited number of highest-likelihood paths (the beam width) of the decoder are kept, and at the end the maximum-likelihood path is chosen. The resulting algorithm has linear runtime in the beam width. However, the input is not necessarily distributed such that a high-likelihood input symbol at any given time step also leads to the global optimum. Limiting the beam width can thus result in a failure to recognise long-range dependencies. In practice, only an exponentially large beam width can guarantee that the global optimum is found: for an input of length n and average parser branching ratio R, the baseline classical algorithm needs to query the input on average Rn times. In this work, we construct a quantum algorithm to find the globally optimal parse with high constant success probability. Given the input to the decoder is distributed like a power-law with exponent k>0, our algorithm yields a runtime Rnf(R,k), where f≤1/2, and f→0 exponentially quickly for growing k. This implies that our algorithm always yields a super-Grover type speedup, i.e. it is more than quadratically faster than its classical counterpart. We further modify our procedure to recover a quantum beam search variant, which enables an even stronger empirical speedup, while sacrificing accuracy. Finally, we apply this quantum beam search decoder to Mozilla's implementation of Baidu's DeepSpeech neural net, which we show to exhibit such a power law word rank frequency, underpinning the applicability of our model.

| Comments:    | 36 pages, 9 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Quantum Physics (quant-ph)**; Computation and Language (cs.CL); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG) |
| MSC classes: | 68T50, 68Q12, 68T05                                          |
| Cite as:     | **arXiv:1909.05023 [quant-ph]**                              |
|              | (or **arXiv:1909.05023v1 [quant-ph]** for this version)      |





<h2 id="2019-09-12-2">2. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</h2> 
Title: [MultiFiT: Efficient Multi-lingual Language Model Fine-tuning](https://arxiv.org/abs/1909.04761)

Authors:[Julian Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Piotr Czapla](https://arxiv.org/search/cs?searchtype=author&query=Czapla%2C+P), [Marcin Kardas](https://arxiv.org/search/cs?searchtype=author&query=Kardas%2C+M), [Sylvain Gugger](https://arxiv.org/search/cs?searchtype=author&query=Gugger%2C+S), [Jeremy Howard](https://arxiv.org/search/cs?searchtype=author&query=Howard%2C+J)

*(Submitted on 10 Sep 2019)*

> Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code.

| Comments: | Proceedings of EMNLP-IJCNLP 2019                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.04761 [cs.CL]**                                 |
|           | (or **arXiv:1909.04761v1 [cs.CL]** for this version)         |





<h2 id="2019-09-12-3">3. Dynamic Fusion: Attentional Language Model for Neural Machine Translation</h2> 
Title: [Dynamic Fusion: Attentional Language Model for Neural Machine Translation](https://arxiv.org/abs/1909.04879)

Authors:[Michiki Kurosawa](https://arxiv.org/search/cs?searchtype=author&query=Kurosawa%2C+M), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

*(Submitted on 11 Sep 2019)*

> Neural Machine Translation (NMT) can be used to generate fluent output. As such, language models have been investigated for incorporation with NMT. In prior investigations, two models have been used: a translation model and a language model. The translation model's predictions are weighted by the language model with a hand-crafted ratio in advance. However, these approaches fail to adopt the language model weighting with regard to the translation history. In another line of approach, language model prediction is incorporated into the translation model by jointly considering source and target information. However, this line of approach is limited because it largely ignores the adequacy of the translation output.
> Accordingly, this work employs two mechanisms, the translation model and the language model, with an attentive architecture to the language model as an auxiliary element of the translation model. Compared with previous work in English--Japanese machine translation using a language model, the experimental results obtained with the proposed Dynamic Fusion mechanism improve BLEU and Rank-based Intuitive Bilingual Evaluation Scores (RIBES) scores. Additionally, in the analyses of the attention and predictivity of the language model, the Dynamic Fusion mechanism allows predictive language modeling that conforms to the appropriate grammatical structure.

| Comments: | 13 pages; PACLING 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.04879 [cs.CL]**                         |
|           | (or **arXiv:1909.04879v1 [cs.CL]** for this version) |





<h2 id="2019-09-12-4">4. Getting Gender Right in Neural Machine Translation</h2> 
Title: [Getting Gender Right in Neural Machine Translation](https://arxiv.org/abs/1909.05088)

Authors:[Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E), [Christian Hardmeier](https://arxiv.org/search/cs?searchtype=author&query=Hardmeier%2C+C), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 11 Sep 2019)*

> Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying "I am happy" in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained/recovered. The same sentence would become either "Je suis heureux", for a male speaker or "Je suis heureuse" for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or even on the level of syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is two-fold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.

| Comments:          | Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), October-November, 2018. Brussels, Belgium, pages 3003-3008, URL: [this https URL](https://www.aclweb.org/anthology/D18-1334), DOI: [10.18653/v1/D18-1334](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FD18-1334&v=6f889533) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing |
| DOI:               | [10.18653/v1/D18-1334](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FD18-1334&v=6f889533) |
| Cite as:           | **arXiv:1909.05088 [cs.CL]**                                 |
|                    | (or **arXiv:1909.05088v1 [cs.CL]** for this version)         |







# 2019-09-10

[Return to Index](#Index)



<h2 id="2019-09-10-1">1. Improving Neural Machine Translation with Parent-Scaled Self-Attention</h2> 
Title: [Improving Neural Machine Translation with Parent-Scaled Self-Attention](https://arxiv.org/abs/1909.03149)

Authors:[Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N)

*(Submitted on 6 Sep 2019)*

> Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure. Recent studies have shown that embedding the syntax information of a source sentence in recurrent neural networks can improve their translation accuracy, especially for low-resource language pairs. However, state-of-the-art NMT models are based on self-attention networks (e.g., Transformer), in which it is still not clear how to best embed syntactic information. In this work, we explore different approaches to make such models syntactically aware. Moreover, we propose a novel method to incorporate syntactic information in the self-attention mechanism of the Transformer encoder by introducing attention heads that can attend to the dependency parent of each token. The proposed model is simple yet effective, requiring no additional parameter and improving the translation quality of the Transformer model especially for long sentences and low-resource scenarios. We show the efficacy of the proposed approach on NC11 English-German, WMT16 and WMT17 English-German, WMT18 English-Turkish, and WAT English-Japanese translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.03149 [cs.CL]**                         |
|           | (or **arXiv:1909.03149v1 [cs.CL]** for this version) |





<h2 id="2019-09-10-2">2. LAMAL: LAnguage Modeling Is All You Need for Lifelong Language Learning</h2> 
Title: [LAMAL: LAnguage Modeling Is All You Need for Lifelong Language Learning](https://arxiv.org/abs/1909.03329)

Authors: [Fan-Keng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Cheng-Hao Ho](https://arxiv.org/search/cs?searchtype=author&query=Ho%2C+C), [Hung-Yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

*(Submitted on 7 Sep 2019)*

> Most research on lifelong learning (LLL) applies to images or games, but not language. Here, we introduce LAMAL, a simple yet effective method for LLL based on language modeling. LAMAL replays pseudo samples of previous tasks while requiring no extra memory or model capacity. To be specific, LAMAL is a language model learning to solve the task and generate training samples at the same time. At the beginning of training a new task, the model generates some pseudo samples of previous tasks to train alongside the data of the new task. The results show that LAMAL prevents catastrophic forgetting without any sign of intransigence and can solve up to five very different language tasks sequentially with only one model. Overall, LAMAL outperforms previous methods by a considerable margin and is only 2-3\% worse than multitasking which is usually considered as the upper bound of LLL. Our source code is available at [this https URL](https://github.com/xxx).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.03329 [cs.CL]**                                 |
|           | (or **arXiv:1909.03329v1 [cs.CL]** for this version)         |





<h2 id="2019-09-10-3">3. Neural Machine Translation with Byte-Level Subwords</h2> 
Title: [Neural Machine Translation with Byte-Level Subwords](https://arxiv.org/abs/1909.03341)

Authors:[Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 7 Sep 2019)*

> Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice. In this paper, we investigate byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.03341 [cs.CL]**                         |
|           | (or **arXiv:1909.03341v1 [cs.CL]** for this version) |





<h2 id="2019-09-10-4">4. Combining SMT and NMT Back-Translated Data for Efficient NMT</h2> 
Title: [Combining SMT and NMT Back-Translated Data for Efficient NMT](https://arxiv.org/abs/1909.03750)

Authors:[Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Maja Popovic](https://arxiv.org/search/cs?searchtype=author&query=Popovic%2C+M), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Gideon Maillette de Buy Wenniger](https://arxiv.org/search/cs?searchtype=author&query=de+Buy+Wenniger%2C+G+M), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 9 Sep 2019)*

> Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016), which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model.
> Generally, NMT models are used for back-translation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of Recent Advances in Natural Language Processing (RANLP 2019). pages 922--931 |
| Cite as:           | **arXiv:1909.03750 [cs.CL]**                                 |
|                    | (or **arXiv:1909.03750v1 [cs.CL]** for this version)         |







# 2019-09-09

[Return to Index](#Index)



<h2 id="2019-09-09-1">1. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction</h2> 
Title: [Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction](https://arxiv.org/abs/1909.02855)

Authors: [Paula Czarnowska](https://arxiv.org/search/cs?searchtype=author&query=Czarnowska%2C+P), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Ann Copestake](https://arxiv.org/search/cs?searchtype=author&query=Copestake%2C+A)

*(Submitted on 6 Sep 2019)*

> Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habláramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the state-of-the-art models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.02855 [cs.CL]**                         |
|           | (or **arXiv:1909.02855v1 [cs.CL]** for this version) |



# 2019-09-06

[Return to Index](#Index)



<h2 id="2019-09-06-1">1. Jointly Learning to Align and Translate with Transformer Models</h2> 
Title: [Jointly Learning to Align and Translate with Transformer Models](https://arxiv.org/abs/1909.02074)

Authors: [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Stephan Peitz](https://arxiv.org/search/cs?searchtype=author&query=Peitz%2C+S), [Udhyakumar Nallasamy](https://arxiv.org/search/cs?searchtype=author&query=Nallasamy%2C+U), [Matthias Paulik](https://arxiv.org/search/cs?searchtype=author&query=Paulik%2C+M)

*(Submitted on 4 Sep 2019)*

> The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.

| Comments: | 10 pages, 2 figures. To appear at EMNLP 2019         |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.02074 [cs.CL]**                         |
|           | (or **arXiv:1909.02074v1 [cs.CL]** for this version) |





<h2 id="2019-09-06-2">2. Investigating Multilingual NMT Representations at Scale</h2> 
Title: [Investigating Multilingual NMT Representations at Scale](https://arxiv.org/abs/1909.02197)

Authors: [Sneha Reddy Kudugunta](https://arxiv.org/search/cs?searchtype=author&query=Kudugunta%2C+S+R), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 5 Sep 2019)*

> Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major conclusions from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning.

| Comments: | Paper at EMNLP 2019                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.02197 [cs.CL]**                                 |
|           | (or **arXiv:1909.02197v1 [cs.CL]** for this version)         |





<h2 id="2019-09-06-3">3. Multi-Granularity Self-Attention for Neural Machine Translation</h2> 
Title: [Multi-Granularity Self-Attention for Neural Machine Translation](https://arxiv.org/abs/1909.02222)

Authors: [Jie Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+J), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Jinfeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

*(Submitted on 5 Sep 2019)*

> Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases. In this work, we present multi-granularity self-attention (Mg-Sa): a neural network that combines multi-head self-attention and phrase modeling. Specifically, we train several attention heads to attend to phrases in either n-gram or syntactic formalism. Moreover, we exploit interactions among phrases to enhance the strength of structure modeling - a commonly-cited weakness of self-attention. Experimental results on WMT14 English-to-German and NIST Chinese-to-English translation tasks show the proposed approach consistently improves performance. Targeted linguistic analysis reveals that Mg-Sa indeed captures useful phrase information at various levels of granularities.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.02222 [cs.CL]**                         |
|           | (or **arXiv:1909.02222v1 [cs.CL]** for this version) |





<h2 id="2019-09-06-4">4. Source Dependency-Aware Transformer with Supervised Self-Attention</h2> 
Title: [Source Dependency-Aware Transformer with Supervised Self-Attention](https://arxiv.org/abs/1909.02273)

Authors: [Chengyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Shuangzhi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S)

*(Submitted on 5 Sep 2019)*

> Recently, Transformer has achieved the state-of-the-art performance on many machine translation tasks. However, without syntax knowledge explicitly considered in the encoder, incorrect context information that violates the syntax structure may be integrated into source hidden states, leading to erroneous translations. In this paper, we propose a novel method to incorporate source dependencies into the Transformer. Specifically, we adopt the source dependency tree and define two matrices to represent the dependency relations. Based on the matrices, two heads in the multi-head self-attention module are trained in a supervised manner and two extra cross entropy losses are introduced into the training objective function. Under this training objective, the model is trained to learn the source dependency relations directly. Without requiring pre-parsed input during inference, our model can generate better translations with the dependency-aware context information. Experiments on bi-directional Chinese-to-English, English-to-Japanese and English-to-German translation tasks show that our proposed method can significantly improve the Transformer baseline.

| Comments: | 6 pages                                              |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.02273 [cs.CL]**                         |
|           | (or **arXiv:1909.02273v1 [cs.CL]** for this version) |





<h2 id="2019-09-06-5">5. Accelerating Transformer Decoding via a Hybrid of Self-attention and Recurrent Neural Network</h2> 
Title: [Accelerating Transformer Decoding via a Hybrid of Self-attention and Recurrent Neural Network](https://arxiv.org/abs/1909.02279)

Authors: [Chengyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Shuangzhi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S)

*(Submitted on 5 Sep 2019)*

> Due to the highly parallelizable architecture, Transformer is faster to train than RNN-based models and popularly used in machine translation tasks. However, at inference time, each output word requires all the hidden states of the previously generated words, which limits the parallelization capability, and makes it much slower than RNN-based ones. In this paper, we systematically analyze the time cost of different components of both the Transformer and RNN-based model. Based on it, we propose a hybrid network of self-attention and RNN structures, in which, the highly parallelizable self-attention is utilized as the encoder, and the simpler RNN structure is used as the decoder. Our hybrid network can decode 4-times faster than the Transformer. In addition, with the help of knowledge distillation, our hybrid network achieves comparable translation quality to the original Transformer.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.02279 [cs.CL]**                         |
|           | (or **arXiv:1909.02279v1 [cs.CL]** for this version) |





<h2 id="2019-09-06-6">6. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</h2> 
Title: [FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow](https://arxiv.org/abs/1909.02480)

Authors: [Xuezhe Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

*(Submitted on 5 Sep 2019)*

> Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.

| Comments: | Accepted by EMNLP 2019 (Long Paper)                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.02480 [cs.CL]**                                 |
|           | (or **arXiv:1909.02480v1 [cs.CL]** for this version)         |



# 2019-09-05

[Return to Index](#Index)



<h2 id="2019-09-05-1">1. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</h2> 
Title: [The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives](https://arxiv.org/abs/1909.01380)

Authors: [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I)

*(Submitted on 3 Sep 2019)*

> We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.

| Comments: | EMNLP 2019 (camera-ready)                            |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.01380 [cs.CL]**                         |
|           | (or **arXiv:1909.01380v1 [cs.CL]** for this version) |





<h2 id="2019-09-05-2">2. Context-Aware Monolingual Repair for Neural Machine Translation</h2> 
Title: [Context-Aware Monolingual Repair for Neural Machine Translation](https://arxiv.org/abs/1909.01383)

Authors: [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I)

*(Submitted on 3 Sep 2019)*

> Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.

| Comments: | EMNLP 2019 (camera-ready)                            |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.01383 [cs.CL]**                         |
|           | (or **arXiv:1909.01383v1 [cs.CL]** for this version) |





<h2 id="2019-09-05-3">3. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation</h2> 
Title: [Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation](https://arxiv.org/abs/1909.01559)

Authors: [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

*(Submitted on 4 Sep 2019)*

> Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories: (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ/WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German<->English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.01559 [cs.CL]**                         |
|           | (or **arXiv:1909.01559v1 [cs.CL]** for this version) |





<h2 id="2019-09-05-4">4. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?</h2> 
Title: [Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?](https://arxiv.org/abs/1909.01638)

Authors: [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Roi Reichart](https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A)

*(Submitted on 4 Sep 2019)*

> Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.

| Comments: | EMNLP 2019 (Long paper)                              |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.01638 [cs.CL]**                         |
|           | (or **arXiv:1909.01638v1 [cs.CL]** for this version) |





<h2 id="2019-09-05-5">5. SAO WMT19 Test Suite: Machine Translation of Audit Reports</h2> 
Title: [SAO WMT19 Test Suite: Machine Translation of Audit Reports](https://arxiv.org/abs/1909.01701)

Authors: [Tereza Vojtěchová](https://arxiv.org/search/cs?searchtype=author&query=Vojtěchová%2C+T), [Michal Novák](https://arxiv.org/search/cs?searchtype=author&query=Novák%2C+M), [Miloš Klouček](https://arxiv.org/search/cs?searchtype=author&query=Klouček%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

*(Submitted on 4 Sep 2019)*

> This paper describes a machine translation test set of documents from the auditing domain and its use as one of the "test suites" in the WMT19 News Translation Task for translation directions involving Czech, English and German. 
> Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. 
> Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.

| Comments:          | WMT19 ([this http URL](http://www.statmt.org/wmt19/))        |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Vojt\v{e}chov\'a et al. (2019): SAO WMT19 Test Suite: Machine Translation of Audit Reports. In: Fourth Conference on Machine Translation - Proceedings of the Conference, pp. 680-692, ACL, ISBN 978-1-950737-27-7 |
| Cite as:           | **arXiv:1909.01701 [cs.CL]**                                 |
|                    | (or **arXiv:1909.01701v1 [cs.CL]** for this version)         |





# 2019-09-04

[Return to Index](#Index)



<h2 id="2019-09-04-1">1. Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation</h2> 
Title: [Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation](https://arxiv.org/abs/1909.00562)

Authors: [Junya Ono](https://arxiv.org/search/cs?searchtype=author&query=Ono%2C+J), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 2 Sep 2019)*

> Reduction of training time is an important issue in many tasks like patent translation involving neural networks. Data parallelism and model parallelism are two common approaches for reducing training time using multiple graphics processing units (GPUs) on one machine. In this paper, we propose a hybrid data-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent neural network (RNN) machine translation. We apply a model parallel approach to the RNN encoder-decoder part of the Seq2Seq model and a data parallel approach to the attention-softmax part of the model. We achieved a speed-up of 4.13 to 4.20 times when using 4 GPUs compared with the training speed when using 1 GPU without affecting machine translation accuracy as measured in terms of BLEU scores.

| Comments: | 9 pages, 4 figures, 5 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Distributed, Parallel, and Cluster Computing (cs.DC)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **arXiv:1909.00562 [cs.DC]**                                 |
|           | (or **arXiv:1909.00562v1 [cs.DC]** for this version)         |





<h2 id="2019-09-04-2">2. Handling Syntactic Divergence in Low-resource Machine Translation</h2> 
Title: [Handling Syntactic Divergence in Low-resource Machine Translation](https://arxiv.org/abs/1909.00040)

Authors: [Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Xuezhe Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 30 Aug 2019)*

> Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.

| Comments: | Accepted by EMNLP 2019 (short paper)                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00040 [cs.CL]**                         |
|           | (or **arXiv:1909.00040v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-3">3. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite</h2> 
Title: [Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite](https://arxiv.org/abs/1909.00131)

Authors: [Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Irina Temnikova](https://arxiv.org/search/cs?searchtype=author&query=Temnikova%2C+I), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 31 Aug 2019)*

> The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.

| Comments: | Accepted at EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.00131 [cs.CL]**                                 |
|           | (or **arXiv:1909.00131v1 [cs.CL]** for this version)         |







<h2 id="2019-09-04-4">4. Improving Back-Translation with Uncertainty-based Confidence Estimation</h2> 
Title: [Improving Back-Translation with Uncertainty-based Confidence Estimation](https://arxiv.org/abs/1909.00157)

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Chao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Huanbo Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+H), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

*(Submitted on 31 Aug 2019)*

> While back-translation is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on uncertainty, it is possible for back-translation to better cope with noise in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of back-translation.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00157 [cs.CL]**                         |
|           | (or **arXiv:1909.00157v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-5">5. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation</h2> 
Title: [Explicit Cross-lingual Pre-training for Unsupervised Machine Translation](https://arxiv.org/abs/1909.00180)

Authors: [Shuo Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+S), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Shuai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S)

*(Submitted on 31 Aug 2019)*

> Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our method can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.

| Comments: | Accepted to EMNLP2019; 10 pages, 2 figures           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00180 [cs.CL]**                         |
|           | (or **arXiv:1909.00180v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-6">6. Towards Understanding Neural Machine Translation with Word Importance</h2> 
Title: [Towards Understanding Neural Machine Translation with Word Importance](https://arxiv.org/abs/1909.00326)

Authors: [Shilin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 1 Sep 2019)*

> Although neural machine translation (NMT) has advanced the state-of-the-art on various language pairs, the interpretability of NMT remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.00326 [cs.CL]**                                 |
|           | (or **arXiv:1909.00326v1 [cs.CL]** for this version)         |







<h2 id="2019-09-04-7">7. One Model to Learn Both: Zero Pronoun Prediction and Translation</h2> 
Title: [One Model to Learn Both: Zero Pronoun Prediction and Translation](https://arxiv.org/abs/1909.00369)

Authors: [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 1 Sep 2019)*

> Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should be recalled in non-pro-drop languages. This discourse phenomenon poses a significant challenge for machine translation (MT) when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus translation. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both translation performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00369 [cs.CL]**                         |
|           | (or **arXiv:1909.00369v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-8">8. Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation</h2> 
Title: [Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation](https://arxiv.org/abs/1909.00437)

Authors: [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Henry Tsai](https://arxiv.org/search/cs?searchtype=author&query=Tsai%2C+H), [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Jason Riesa](https://arxiv.org/search/cs?searchtype=author&query=Riesa%2C+J), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Karthik Raman](https://arxiv.org/search/cs?searchtype=author&query=Raman%2C+K)

*(Submitted on 1 Sep 2019)*

> The recently proposed massively multilingual neural machine translation (NMT) system has been shown to be capable of translating over 100 languages to and from English within a single model. Its improved translation performance on low resource languages hints at potential cross-lingual transfer capability for downstream tasks. In this paper, we evaluate the cross-lingual effectiveness of representations from the encoder of a massively multilingual NMT model on 5 downstream classification and sequence labeling tasks covering a diverse set of over 50 languages. We compare against a strong baseline, multilingual BERT (mBERT), in different cross-lingual transfer learning scenarios and show gains in zero-shot transfer in 4 out of these 5 tasks.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.00437 [cs.CL]**                         |
|           | (or **arXiv:1909.00437v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-9">9. Improving Context-aware Neural Machine Translation with Target-side Context</h2> 
Title: [Improving Context-aware Neural Machine Translation with Target-side Context](https://arxiv.org/abs/1909.00531)

Authors: [Hayahide Yamagishi](https://arxiv.org/search/cs?searchtype=author&query=Yamagishi%2C+H), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

*(Submitted on 2 Sep 2019)*

> In recent years, several studies on neural machine translation (NMT) have attempted to use document-level context by using a multi-encoder and two attention mechanisms to read the current and previous sentences to incorporate the context of the previous sentences. These studies concluded that the target-side context is less useful than the source-side context. However, we considered that the reason why the target-side context is less useful lies in the architecture used to model these contexts. 
> Therefore, in this study, we investigate how the target-side context can improve context-aware neural machine translation. We propose a weight sharing method wherein NMT saves decoder states and calculates an attention vector using the saved states when translating a current sentence. Our experiments show that the target-side context is also useful if we plug it into NMT as the decoder state when translating a previous sentence.

| Comments: | 12 pages; PACLING 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00531 [cs.CL]**                         |
|           | (or **arXiv:1909.00531v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-10">10. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation</h2> 
Title: [Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation](https://arxiv.org/abs/1909.00564)

Authors: [Zhengxin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

*(Submitted on 2 Sep 2019)*

> Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However, this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem, we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.

| Comments: | 11 pages, 7 figures, 2019 Conference on Empirical Methods in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1909.00564 [cs.CL]**                                 |
|           | (or **arXiv:1909.00564v1 [cs.CL]** for this version)         |







<h2 id="2019-09-04-11">11. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks</h2> 
Title: [Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks](https://arxiv.org/abs/1909.00964)

Authors: [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 3 Sep 2019 ([v1](https://arxiv.org/abs/1909.00964v1)), last revised 4 Sep 2019 (this version, v2))*

> We present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT and XLM, three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on French and German) is obtained.

| Comments: | Accepted to EMNLP2019; 10 pages, 2 figures           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.00964 [cs.CL]**                         |
|           | (or **arXiv:1909.00964v2 [cs.CL]** for this version) |







<h2 id="2019-09-04-12">12. Multi-agent Learning for Neural Machine Translation</h2> 
Title: [Multi-agent Learning for Neural Machine Translation](https://arxiv.org/abs/1909.01101)

Authors: [Tianchi Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+T), [Hao Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+H), [Zhongjun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

*(Submitted on 3 Sep 2019)*

> Conventional Neural Machine Translation (NMT) models benefit from the training with an additional agent, e.g., dual learning, and bidirectional decoding with one agent decoding from left to right and the other decoding in the opposite direction. In this paper, we extend the training framework to the multi-agent scenario by introducing diverse agents in an interactive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German-English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline systems and shows competitive performance on all tasks.

| Comments: | Accepted by EMNLP2019                                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.01101 [cs.CL]**                         |
|           | (or **arXiv:1909.01101v1 [cs.CL]** for this version) |







<h2 id="2019-09-04-13">13. Bilingual is At Least Monolingual (BALM): A Novel Translation Algorithm that Encodes Monolingual Priors</h2> 
Title: [Bilingual is At Least Monolingual (BALM): A Novel Translation Algorithm that Encodes Monolingual Priors](https://arxiv.org/abs/1909.01146)

Authors: [Jeffrey Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+J), [Chris Callison-Burch](https://arxiv.org/search/cs?searchtype=author&query=Callison-Burch%2C+C)

*(Submitted on 30 Aug 2019)*

> State-of-the-art machine translation (MT) models do not use knowledge of any single language's structure; this is the equivalent of asking someone to translate from English to German while knowing neither language. BALM is a framework incorporates monolingual priors into an MT pipeline; by casting input and output languages into embedded space using BERT, we can solve machine translation with much simpler models. We find that English-to-German translation on the Multi30k dataset can be solved with a simple feedforward network under the BALM framework with near-SOTA BLEU scores.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1909.01146 [cs.CL]**                                 |
|           | (or **arXiv:1909.01146v1 [cs.CL]** for this version)         |







# 2019-09-02

[Return to Index](#Index)



<h2 id="2019-09-02-1">1. Latent Part-of-Speech Sequences for Neural Machine Translation</h2> 
Title: [Latent Part-of-Speech Sequences for Neural Machine Translation](https://arxiv.org/abs/1908.11782)

Authors: [Xuewen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Yingru Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Dongliang Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+D), [Xin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

*(Submitted on 30 Aug 2019)*

> Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.

| Comments: | In proceedings of EMNLP 2019                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **arXiv:1908.11782 [cs.AI]**                                 |
|           | (or **arXiv:1908.11782v1 [cs.AI]** for this version)         |





<h2 id="2019-09-02-2">2. Encoders Help You Disambiguate Word Senses in Neural Machine Translation</h2> 
Title: [Encoders Help You Disambiguate Word Senses in Neural Machine Translation](https://arxiv.org/abs/1908.11771)

Authors: [Gongbo Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+G), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Joakim Nivre](https://arxiv.org/search/cs?searchtype=author&query=Nivre%2C+J)

*(Submitted on 30 Aug 2019)*

> Neural machine translation (NMT) has achieved new state-of-the-art performance in translating ambiguous words. However, it is still unclear which component dominates the process of disambiguation. In this paper, we explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the distributions of self-attention. We train a classifier to predict whether a translation is correct given the representation of an ambiguous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context.

| Comments: | Accepted by EMNLP 2019, camera-ready version         |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.11771 [cs.CL]**                         |
|           | (or **arXiv:1908.11771v1 [cs.CL]** for this version) |
