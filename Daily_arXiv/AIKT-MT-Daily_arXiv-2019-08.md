# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-30](#2019-08-30)
  - [1. Learning a Multitask Curriculum for Neural Machine Translation](#2019-08-30-1)
  - [2. Regularized Context Gates on Transformer for Machine Translation](#2019-08-30-2)

- [2019-08-29](#2019-08-29)
  - [1. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings](#2019-08-29-1)

- [2019-08-28](#2019-08-28)
  - [1. Reference Network for Neural Machine Translation](#2019-08-28-1)
  - [2. On NMT Search Errors and Model Errors: Cat Got Your Tongue?](#2019-08-28-2)
  - [3. Multi-Layer Softmaxing during Training Neural Machine Translation for Flexible Decoding with Fewer Layers](#2019-08-28-3)
  - [4. Bridging the Gap for Tokenizer-Free Language Models](#2019-08-28-4)

- [2019-08-27](#2019-08-27)
  - [1. Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation](#2019-08-27-1)
  - [2. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures](#2019-08-27-2)
  - [3. Multilingual Neural Machine Translation with Language Clustering](#2019-08-27-3)
  - [4. Efficient Bidirectional Neural Machine Translation](#2019-08-27-4)
  - [5. Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation](#2019-08-27-5)
  - [6. An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation](#2019-08-27-6)

- [2019-08-26](#2019-08-26)
  - [1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](#2019-08-26-1)
  - [2. A Lost Croatian Cybernetic Machine Translation Program](#2019-08-26-2)
  - [3. Revealing the Dark Secrets of BERT](#2019-08-26-3)
  
- [2019-08-23](#2019-08-23)
  - [1. Denoising based Sequence-to-Sequence Pre-training for Text Generation](#2019-08-23-1)
  - [2. Dual Skew Divergence Loss for Neural Machine Translation](#2019-08-23-2)

- [2019-08-22](#2019-08-22)
  - [1. Improving Neural Machine Translation with Pre-trained Representation](#2019-08-22-1)
  - [2. On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning](#2019-08-22-2)
  - [3. An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing](#2019-08-22-3)
  - [4. A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation](#2019-08-22-4)
  - [5. Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples](#2019-08-22-5)

- [2019-08-21](#2019-08-21)
  - [1. Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior](#2019-08-21-1)
  - [2. ARAML: A Stable Adversarial Training Framework for Text Generation](#2019-08-21-2)
  - [3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers](#2019-08-21-3)

- [2019-08-20](#2019-08-20)
  - [1. UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task](#2019-08-20-1)
  - [2. Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation](#2019-08-20-2)
  - [3. The Transference Architecture for Automatic Post-Editing](#2019-08-20-3)
  - [4. Language Graph Distillation for Low-Resource Machine Translation](#2019-08-20-4)
  - [5. Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation](#2019-08-20-5)
  - [6. Recurrent Graph Syntax Encoder for Neural Machine Translation](#2019-08-20-6)
  - [7. Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces](#2019-08-20-7)

- [2019-08-19](#2019-08-19)
  - [1. Attending to Future Tokens For Bidirectional Sequence Generation](#2019-08-19-1)
  - [2. Towards Making the Most of BERT in Neural Machine Translation](#2019-08-19-2)
  - [3. Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs](#2019-08-19-3)
  - [4. Simple and Effective Noisy Channel Modeling for Neural Machine Translation](#2019-08-19-4)
  - [5. Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring](#2019-08-19-5)
- [2019-08-15](#2019-08-15)
  - [1. On The Evaluation of Machine Translation Systems Trained With Back-Translation](#2019-08-15-1)
- [2019-08-14](#2019-08-14)
  - [1. Neural Text Generation with Unlikelihood Training](#2019-08-14-1)
  - [2. LSTM vs. GRU vs. Bidirectional RNN for script generation](#2019-08-14-2)
  - [3. Attention is not not Explanation](#2019-08-14-3)
  - [4. Neural Machine Translation with Noisy Lexical Constraints](#2019-08-14-4)
- [2019-08-13](#2019-08-13)
  - [1. On the Validity of Self-Attention as Explanation in Transformer Models](#2019-08-13-1)
- [2019-08-12](#2019-08-12)
  - [1. Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling](#2019-08-12-1)
  - [2. UdS Submission for the WMT 19 Automatic Post-Editing Task](#2019-08-12-2)
- [2019-08-09](#2019-08-09)
  - [1. A Test Suite and Manual Evaluation of Document-Level NMT at WMT19](#2019-08-09-1)
- [2019-08-07](#2019-08-07)
  - [1. MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models](#2019-08-07-1)
  - [2. A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data](#2019-08-07-2)
  - [3. Self-Knowledge Distillation in Natural Language Processing](#2019-08-07-3)
- [2019-08-06](#2019-08-06)
  - [1. Invariance-based Adversarial Attack on Neural Machine Translation Systems](#2019-08-06-1)
  - [2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](#2019-08-06-2)
  - [3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](#2019-08-06-3)
  - [4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](#2019-08-06-4)
  - [5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](#2019-08-06-5)
  - [6. Predicting Actions to Help Predict Translations](#2019-08-06-6)
  - [7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](#2019-08-06-7)
- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-30

[Return to Index](#Index)



<h2 id="2019-08-30-1">1. Learning a Multitask Curriculum for Neural Machine Translation</h2> 
Title: [Learning a Multitask Curriculum for Neural Machine Translation](https://arxiv.org/abs/1908.10940)

Authors: [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Ye Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y), [Jiquan Ngiam](https://arxiv.org/search/cs?searchtype=author&query=Ngiam%2C+J), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Zarana Parekh](https://arxiv.org/search/cs?searchtype=author&query=Parekh%2C+Z)

*(Submitted on 28 Aug 2019)*

> Existing curriculum learning research in neural machine translation (NMT) mostly focuses on a single final task such as selecting data for a domain or for denoising, and considers in-task example selection. This paper studies the data selection problem in multitask setting. We present a method to learn a multitask curriculum on a single, diverse, potentially noisy training dataset. It computes multiple data selection scores for each training example, each score measuring how useful the example is to a certain task. It uses Bayesian optimization to learn a linear weighting of these per-instance scores, and then sorts the data to form a curriculum. We experiment with three domain translation tasks: two specific domains and the general domain, and demonstrate that the learned multitask curriculum delivers results close to individually optimized models and brings solid gains over no curriculum training, across all test sets.

| Comments: | 12 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.10940 [cs.CL]**                                 |
|           | (or **arXiv:1908.10940v1 [cs.CL]** for this version)         |





<h2 id="2019-08-30-2">2. Regularized Context Gates on Transformer for Machine Translation</h2> 
Title: [Regularized Context Gates on Transformer for Machine Translation](https://arxiv.org/abs/1908.11020)

Authors: [Xintong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Max Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+M)

*(Submitted on 29 Aug 2019)*

> Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over strong Transformer baseline.

| Comments: | 7 pages, 2 figures                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.11020 [cs.CL]**                         |
|           | (or **arXiv:1908.11020v1 [cs.CL]** for this version) |



# 2019-08-29

[Return to Index](#Index)



<h2 id="2019-08-29-1">1. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings</h2> 
Title: [Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings](https://arxiv.org/abs/1908.10430)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 27 Aug 2019)*

> The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.10430 [cs.CL]**                         |
|           | (or **arXiv:1908.10430v1 [cs.CL]** for this version) |



# 2019-08-28

[Return to Index](#Index)



<h2 id="2019-08-28-1">1. Reference Network for Neural Machine Translation</h2> 
Title: [Reference Network for Neural Machine Translation](https://arxiv.org/abs/1908.09920)

Authors: [Han Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H), [Chenghao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Jianling Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J)

*(Submitted on 23 Aug 2019)*

> Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a \emph{reference book}, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.

| Comments: | 11 pages, 3 figures, accepted by ACL-2019            |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.09920 [cs.CL]**                         |
|           | (or **arXiv:1908.09920v1 [cs.CL]** for this version) |





<h2 id="2019-08-28-2">2. On NMT Search Errors and Model Errors: Cat Got Your Tongue?</h2> 
Title: [On NMT Search Errors and Model Errors: Cat Got Your Tongue?](https://arxiv.org/abs/1908.10090)

Authors: [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 27 Aug 2019)*

> We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.

| Comments: | EMNLP-2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.10090 [cs.CL]**                         |
|           | (or **arXiv:1908.10090v1 [cs.CL]** for this version) |





<h2 id="2019-08-28-3">3. Multi-Layer Softmaxing during Training Neural Machine Translation for Flexible Decoding with Fewer Layers</h2> 
Title: [Multi-Layer Softmaxing during Training Neural Machine Translation for Flexible Decoding with Fewer Layers](https://arxiv.org/abs/1908.10118)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

*(Submitted on 27 Aug 2019)*

> This paper proposes a novel procedure for training an encoder-decoder based deep neural network which compresses NxM models into a single model enabling us to dynamically choose the number of encoder and decoder layers for decoding. Usually, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute softmax loss. Instead, our method computes a single loss consisting of NxM losses: the softmax loss for the output of each of the M decoder layers derived using the output of each of the N encoder layers. A single model trained by our method can be used for decoding with an arbitrary fewer number of encoder and decoder layers. In practical scenarios, this (a) enables faster decoding with insignificant losses in translation quality and (b) alleviates the need to train NxM models, thereby saving space. We take a case study of neural machine translation and show the advantage and give a cost-benefit analysis of our approach.

| Comments: | Preliminary work. More updates coming very soon              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.10118 [cs.CL]**                                 |
|           | (or **arXiv:1908.10118v1 [cs.CL]** for this version)         |





<h2 id="2019-08-28-4">4. Bridging the Gap for Tokenizer-Free Language Models</h2> 
Title: [Bridging the Gap for Tokenizer-Free Language Models](https://arxiv.org/abs/1908.10322)

Authors: [Dokook Choe](https://arxiv.org/search/cs?searchtype=author&query=Choe%2C+D), [Rami Al-Rfou](https://arxiv.org/search/cs?searchtype=author&query=Al-Rfou%2C+R), [Mandy Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Heeyoung Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N)

*(Submitted on 27 Aug 2019)*

> Purely character-based language models (LMs) have been lagging in quality on large scale datasets, and current state-of-the-art LMs rely on word tokenization. It has been assumed that injecting the prior knowledge of a tokenizer into the model is essential to achieving competitive results. In this paper, we show that contrary to this conventional wisdom, tokenizer-free LMs with sufficient capacity can achieve competitive performance on a large scale dataset. We train a vanilla transformer network with 40 self-attention layers on the One Billion Word (lm1b) benchmark and achieve a new state of the art for tokenizer-free LMs, pushing these models to be on par with their word-based counterparts.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.10322 [cs.CL]**                                 |
|           | (or **arXiv:1908.10322v1 [cs.CL]** for this version)         |



# 2019-08-27

[Return to Index](#Index)



<h2 id="2019-08-27-1">1. Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation</h2> 
Title: [Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation](https://arxiv.org/abs/1908.08962)

Authors: [Iulia Turc](https://arxiv.org/search/cs?searchtype=author&query=Turc%2C+I), [Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M), [Kenton Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Kristina Toutanova](https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K)

*(Submitted on 23 Aug 2019)*

> Recent developments in NLP have been accompanied by large, expensive models. Knowledge distillation is the standard method to realize these gains in applications with limited resources: a compact student is trained to recover the outputs of a powerful teacher. While most prior work investigates student architectures and transfer techniques, we focus on an often-neglected aspect---student initialization. We argue that a random starting point hinders students from fully leveraging the teacher expertise, even in the presence of a large transfer set. We observe that applying language model pre-training to students unlocks their generalization potential, surprisingly even for very compact networks. We conduct experiments on 4 NLP tasks and 24 sizes of Transformer-based students; for sentiment classification on the Amazon Book Reviews dataset, pre-training boosts size reduction and TPU speed-up from 3.1x/1.25x to 31x/16x. Extensive ablation studies dissect the interaction between pre-training and distillation, revealing a compound effect even when they are applied on the same unlabeled dataset.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.08962 [cs.CL]**                         |
|           | (or **arXiv:1908.08962v1 [cs.CL]** for this version) |





<h2 id="2019-08-27-2">2. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</h2> 
Title: [Neural data-to-text generation: A comparison between pipeline and end-to-end architectures](https://arxiv.org/abs/1908.09022)

Authors: [Thiago Castro Ferreira](https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+T+C), [Chris van der Lee](https://arxiv.org/search/cs?searchtype=author&query=van+der+Lee%2C+C), [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Emiel Krahmer](https://arxiv.org/search/cs?searchtype=author&query=Krahmer%2C+E)

*(Submitted on 23 Aug 2019)*

> Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.

| Comments: | Preprint version of the EMNLP 2019 article           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.09022 [cs.CL]**                         |
|           | (or **arXiv:1908.09022v1 [cs.CL]** for this version) |





<h2 id="2019-08-27-3">3. Multilingual Neural Machine Translation with Language Clustering</h2> 
Title: [Multilingual Neural Machine Translation with Language Clustering](https://arxiv.org/abs/1908.09324)

Authors: [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Jiale Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 25 Aug 2019)*

> Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods

| Comments: | Accepted by EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.09324 [cs.CL]**                                 |
|           | (or **arXiv:1908.09324v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-4">4. Efficient Bidirectional Neural Machine Translation</h2> 
Title: [Efficient Bidirectional Neural Machine Translation](https://arxiv.org/abs/1908.09329)

Authors: [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T)

*(Submitted on 25 Aug 2019)*

> The encoder-decoder based neural machine translation usually generates a target sequence token by token from left to right. Due to error propagation, the tokens in the right side of the generated sequence are usually of poorer quality than those in the left side. In this paper, we propose an efficient method to generate a sequence in both left-to-right and right-to-left manners using a single encoder and decoder, combining the advantages of both generation directions. Experiments on three translation tasks show that our method achieves significant improvements over conventional unidirectional approach. Compared with ensemble methods that train and combine two models with different generation directions, our method saves 50% model parameters and about 40% training time, and also improve inference speed.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.09329 [cs.CL]**                                 |
|           | (or **arXiv:1908.09329v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-5">5. Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation</h2> 
Title: [Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation](https://arxiv.org/abs/1908.09532)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Gideon Maillette de Buy Wenniger](https://arxiv.org/search/cs?searchtype=author&query=de+Buy+Wenniger%2C+G+M), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 26 Aug 2019)*

> Machine Translation models are trained to translate a variety of documents from one language into another. However, models specifically trained for a particular characteristics of the documents tend to perform better. Fine-tuning is a technique for adapting an NMT model to some domain. In this work, we want to use this technique to adapt the model to a given test set. In particular, we are using transductive data selection algorithms which take advantage the information of the test set to retrieve sentences from a larger parallel set. 
> In cases where the model is available at translation time (when the test set is provided), it can be adapted with a small subset of data, thereby achieving better performance than a generic model or a domain-adapted model.

| Comments:          | Proceedings of The 8th Workshop on Patent and Scientific Literature Translation, 2019, pages 13--23, Dublin |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of The 8th Workshop on Patent and Scientific Literature Translation, 2019 |
| Cite as:           | **arXiv:1908.09532 [cs.CL]**                                 |
|                    | (or **arXiv:1908.09532v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-6">6. An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation</h2> 
Title: [An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation](https://arxiv.org/abs/1908.09605)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 26 Aug 2019)*

> Domain adaptation methods have been well-studied in supervised neural machine translation (NMT). However, domain adaptation methods for unsupervised neural machine translation (UNMT) have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. Besides the inconsistent domains between training data and test data for supervised NMT, there sometimes exists an inconsistent domain between two monolingual training data for UNMT. In this work, we empirically show different scenarios for unsupervised domain-specific neural machine translation. Based on these scenarios, we propose several potential solutions to improve the performances of domain-specific UNMT systems.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.09605 [cs.CL]**                         |
|           | (or **arXiv:1908.09605v1 [cs.CL]** for this version) |





# 2019-08-26

[Return to Index](#Index)



<h2 id="2019-08-26-1">1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</h2> 
Title: [Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](https://arxiv.org/abs/1908.08597)

Authors:[Danielle Bragg](https://arxiv.org/search/cs?searchtype=author&query=Bragg%2C+D), [Oscar Koller](https://arxiv.org/search/cs?searchtype=author&query=Koller%2C+O), [Mary Bellard](https://arxiv.org/search/cs?searchtype=author&query=Bellard%2C+M), [Larwan Berke](https://arxiv.org/search/cs?searchtype=author&query=Berke%2C+L), [Patrick Boudrealt](https://arxiv.org/search/cs?searchtype=author&query=Boudrealt%2C+P), [Annelies Braffort](https://arxiv.org/search/cs?searchtype=author&query=Braffort%2C+A), [Naomi Caselli](https://arxiv.org/search/cs?searchtype=author&query=Caselli%2C+N), [Matt Huenerfauth](https://arxiv.org/search/cs?searchtype=author&query=Huenerfauth%2C+M), [Hernisa Kacorri](https://arxiv.org/search/cs?searchtype=author&query=Kacorri%2C+H), [Tessa Verhoef](https://arxiv.org/search/cs?searchtype=author&query=Verhoef%2C+T), [Christian Vogler](https://arxiv.org/search/cs?searchtype=author&query=Vogler%2C+C), [Meredith Ringel Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+M+R)

*(Submitted on 22 Aug 2019)*

> Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Computers and Society (cs.CY); Graphics (cs.GR); Human-Computer Interaction (cs.HC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.08597 [cs.CV]**                                 |
|           | (or **arXiv:1908.08597v1 [cs.CV]** for this version)         |





<h2 id="2019-08-26-2">2. A Lost Croatian Cybernetic Machine Translation Program</h2> 
Title: [A Lost Croatian Cybernetic Machine Translation Program](https://arxiv.org/abs/1908.08917)

Authors: [Sandro Skansi](https://arxiv.org/search/cs?searchtype=author&query=Skansi%2C+S), [Leo Mršić](https://arxiv.org/search/cs?searchtype=author&query=Mršić%2C+L), [Ines Skelac](https://arxiv.org/search/cs?searchtype=author&query=Skelac%2C+I)

*(Submitted on 20 Aug 2019)*

> We are exploring the historical significance of research in the field of machine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation in Yugoslavia during the 1950s. We are focused on two important seminal papers written by members of his research group from 1959 and 1962, as well as their legacy in establishing a Croatian machine translation program based around the Faculty of Humanities and Social Sciences of the University of Zagreb in the late 1950s and early 1960s. We are exploring their work in connection with the beginnings of machine translation in the USA and USSR, motivated by the Cold War and the intelligence needs of the period. We also present the approach to machine translation advocated by the Croatian group in Yugoslavia, which is different from the usual logical approaches of the period, and his advocacy of cybernetic methods, which would be adopted as a canon by the mainstream AI community only decades later.

| Comments: | To appear in "A Guide to Deep Learning Basics: Historical, Logical and Philosophical Perspectives" |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| Cite as:  | **arXiv:1908.08917 [cs.CY]**                                 |
|           | (or **arXiv:1908.08917v1 [cs.CY]** for this version)         |



<h2 id="2019-08-26-3">3. Revealing the Dark Secrets of BERT</h2> 
Title: [Revealing the Dark Secrets of BERT](https://arxiv.org/abs/1908.08593)

Authors: [Olga Kovaleva](https://arxiv.org/search/cs?searchtype=author&query=Kovaleva%2C+O), [Alexey Romanov](https://arxiv.org/search/cs?searchtype=author&query=Romanov%2C+A), [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A), [Anna Rumshisky](https://arxiv.org/search/cs?searchtype=author&query=Rumshisky%2C+A)

*(Submitted on 21 Aug 2019)*

> BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.

| Comments: | Accepted to EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.08593 [cs.CL]**                                 |
|           | (or **arXiv:1908.08593v1 [cs.CL]** for this version)         |



# 2019-08-23

[Return to Index](#Index)



<h2 id="2019-08-23-1">1. Denoising based Sequence-to-Sequence Pre-training for Text Generation</h2> 
Title: [Denoising based Sequence-to-Sequence Pre-training for Text Generation](https://arxiv.org/abs/1908.08206)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ruoyu Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Sujian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

*(Submitted on 22 Aug 2019)*

> This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.

| Comments: | Accepted to EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08206 [cs.CL]**                         |
|           | (or **arXiv:1908.08206v1 [cs.CL]** for this version) |





<h2 id="2019-08-23-2">2. Dual Skew Divergence Loss for Neural Machine Translation</h2> 
Title: [Dual Skew Divergence Loss for Neural Machine Translation](https://arxiv.org/abs/1908.08399)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Yingting Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S)

*(Submitted on 22 Aug 2019)*

> For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.

| Comments: | 9pages                                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08399 [cs.CL]**                         |
|           | (or **arXiv:1908.08399v1 [cs.CL]** for this version) |








# 2019-08-22

[Return to Index](#Index)



<h2 id="2019-08-22-1">1. Improving Neural Machine Translation with Pre-trained Representation</h2> 
Title: [Improving Neural Machine Translation with Pre-trained Representation](https://arxiv.org/abs/1908.07688)

Authors: [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

*(Submitted on 21 Aug 2019)*

> Monolingual data has been demonstrated to be helpful in improving the translation quality of neural machine translation (NMT). The current methods stay at the usage of word-level knowledge, such as generating synthetic parallel data or extracting information from word embedding. In contrast, the power of sentence-level contextual knowledge which is more complex and diverse, playing an important role in natural language generation, has not been fully exploited. In this paper, we propose a novel structure which could leverage monolingual data to acquire sentence-level contextual representations. Then, we design a framework for integrating both source and target sentence-level representations into NMT model to improve the translation quality. Experimental results on Chinese-English, German-English machine translation tasks show that our proposed model achieves improvement over strong Transformer baselines, while experiments on English-Turkish further demonstrate the effectiveness of our approach in the low-resource scenario.

| Comments: | In Progress                                          |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.07688 [cs.CL]**                         |
|           | (or **arXiv:1908.07688v1 [cs.CL]** for this version) |





<h2 id="2019-08-22-2">2. On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning</h2> 
Title: [On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning](https://arxiv.org/abs/1908.07742)

Authors: [Yerai Doval](https://arxiv.org/search/cs?searchtype=author&query=Doval%2C+Y), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J), [Luis Espinosa-Anke](https://arxiv.org/search/cs?searchtype=author&query=Espinosa-Anke%2C+L), [Steven Schockaert](https://arxiv.org/search/cs?searchtype=author&query=Schockaert%2C+S)

*(Submitted on 21 Aug 2019)*

> Cross-lingual word embeddings are vector representations of words in different languages where words with similar meaning are represented by similar vectors, regardless of the language. Recent developments which construct these embeddings by aligning monolingual spaces have shown that accurate alignments can be obtained with little or no supervision. However, the focus has been on a particular controlled scenario for evaluation, and there is no strong evidence on how current state-of-the-art systems would fare with noisy text or for language pairs with major linguistic differences. In this paper we present an extensive evaluation over multiple cross-lingual embedding models, analyzing their strengths and limitations with respect to different variables such as target language, training corpora and amount of supervision. Our conclusions put in doubt the view that high-quality cross-lingual embeddings can always be learned without much supervision.

| Comments: | 13 pages, 2 figures, 7 tables                        |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.07742 [cs.CL]**                         |
|           | (or **arXiv:1908.07742v1 [cs.CL]** for this version) |





<h2 id="2019-08-22-3">3. An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing</h2> 
Title: [An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing](https://arxiv.org/abs/1908.07820)

Authors: [Jianquan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xiaokang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Min Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M), [Liqun Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L)

*(Submitted on 16 Aug 2019)*

> Multi-Task Learning (MTL) aims at boosting the overall performance of each individual task by leveraging useful information contained in multiple related tasks. It has shown great success in natural language processing (NLP). Currently, a number of MLT architectures and learning mechanisms have been proposed for various NLP tasks. However, there is no systematic exploration and comparison of different MLT architectures and learning mechanisms for their strong performance in-depth. In this paper, we conduct a thorough examination of typical MTL methods on a broad range of representative NLP tasks. Our primary goal is to understand the merits and demerits of existing MTL methods in NLP tasks, thus devising new hybrid architectures intended to combine their strengths.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.07820 [cs.CL]**                                 |
|           | (or **arXiv:1908.07820v1 [cs.CL]** for this version)         |





<h2 id="2019-08-22-4">4. A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation</h2> 
Title: [A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation](https://arxiv.org/abs/1908.07846)

Authors: [Stephen M. Petrie](https://arxiv.org/search/cs?searchtype=author&query=Petrie%2C+S+M), [T'Mir D. Julius](https://arxiv.org/search/cs?searchtype=author&query=Julius%2C+T+D)

*(Submitted on 19 Aug 2019)*

> Patent data are often used to study the process of innovation and research, but patent databases lack unique identifiers for individual inventors, making it difficult to study innovation processes at the individual level. Here we introduce an algorithm that performs highly accurate disambiguation of inventors (named entities) in US patent data (F1: 99.09%, precision: 99.41%, recall: 98.76%). The algorithm involves a novel method for converting text-based record data into abstract image representations, in which text from a given pairwise comparison between two inventor name records is converted into a 2D RGB (stacked) image representation. We train an image classification neural network to discriminate between such pairwise comparison images, and then use the trained network to label each pair of records as either matched (same inventor) or non-matched (different inventors). The resulting disambiguation algorithm produces highly accurate results, out-performing other inventor name disambiguation studies on US patent data. Our new text-to-image representation method could potentially be used more broadly for other NLP comparison problems, as it allows image-based processing techniques (e.g. image classification networks) to be applied to text-based comparison problems (such as disambiguation of academic publications, or data linkage problems).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.07846 [cs.CL]**                                 |
|           | (or **arXiv:1908.07846v1 [cs.CL]** for this version)         |





<h2 id="2019-08-22-5">5. Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples</h2> 
Title: [Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples](https://arxiv.org/abs/1908.07899)

Authors: [Marcus Soll](https://arxiv.org/search/cs?searchtype=author&query=Soll%2C+M), [Tobias Hinz](https://arxiv.org/search/cs?searchtype=author&query=Hinz%2C+T), [Sven Magg](https://arxiv.org/search/cs?searchtype=author&query=Magg%2C+S), [Stefan Wermter](https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S)

*(Submitted on 21 Aug 2019)*

> Adversarial examples are artificially modified input samples which lead to misclassifications, while not being detectable by humans. These adversarial examples are a challenge for many tasks such as image and text classification, especially as research shows that many adversarial examples are transferable between different classifiers. In this work, we evaluate the performance of a popular defensive strategy for adversarial examples called defensive distillation, which can be successful in hardening neural networks against adversarial examples in the image domain. However, instead of applying defensive distillation to networks for image classification, we examine, for the first time, its performance on text classification tasks and also evaluate its effect on the transferability of adversarial text examples. Our results indicate that defensive distillation only has a minimal impact on text classifying neural networks and does neither help with increasing their robustness against adversarial examples nor prevent the transferability of adversarial examples between neural networks.

| Comments: | Published at the International Conference on Artificial Neural Networks (ICANN) 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **arXiv:1908.07899 [cs.CL]**                                 |
|           | (or **arXiv:1908.07899v1 [cs.CL]** for this version)         |





# 2019-08-21

[Return to Index](#Index)



<h2 id="2019-08-21-1">1. Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior</h2> 
Title: [Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior](https://arxiv.org/abs/1908.07181)

Authors: [Raphael Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+R), [Jason Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Hideki Nakayama](https://arxiv.org/search/cs?searchtype=author&query=Nakayama%2C+H), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

*(Submitted on 20 Aug 2019)*

> Although neural machine translation models reached high translation quality, the autoregressive nature makes inference difficult to parallelize and leads to high translation latency. Inspired by recent refinement-based approaches, we propose a latent-variable non-autoregressive model with continuous latent variables and deterministic inference procedure. In contrast to existing approaches, we use a deterministic iterative inference algorithm to find a target sequence that maximizes the lowerbound to the log-probability. During inference, the length of translation automatically adapts itself. Our experiments show that the lowerbound can be greatly increased by running the inference algorithm for only one step, resulting in significantly improved translation quality. Our proposed model closes the gap between non-autoregressive and autoregressive approaches on ASPEC Ja-En dataset with 7.8x faster decoding. On WMT'14 En-De dataset, our model narrows the performance gap with autoregressive baseline down to 2.0 BLEU points with 12.5x speedup.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.07181 [cs.CL]**                                 |
|           | (or **arXiv:1908.07181v1 [cs.CL]** for this version)         |



<h2 id="2019-08-21-2">2. ARAML: A Stable Adversarial Training Framework for Text Generation</h2> 
Title: [ARAML: A Stable Adversarial Training Framework for Text Generation](https://arxiv.org/abs/1908.07195)

Authors: [Pei Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+P), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M), [Xiaoyan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X)

*(Submitted on 20 Aug 2019)*

> Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the discriminator assigns rewards to samples which are acquired from a stationary distribution near the data rather than the generator's distribution. The generator is optimized with maximum likelihood estimation augmented by the discriminator's rewards instead of policy gradient. Experiments show that our model can outperform state-of-the-art text GANs with a more stable training process.

| Comments:    | Accepted by EMNLP 2019                                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| Cite as:     | **arXiv:1908.07195 [cs.CL]**                                 |
|              | (or **arXiv:1908.07195v1 [cs.CL]** for this version)         |



<h2 id="2019-08-21-3">3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers</h2> 
Title: [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)

Authors: [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

*(Submitted on 20 Aug 2019)*

> Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: [this https URL](https://github.com/airsplay/lxmert)

| Comments: | EMNLP 2019 (12 pages)                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.07490 [cs.CL]**                                 |
|           | (or **arXiv:1908.07490v1 [cs.CL]** for this version)         |



# 2019-08-20

[Return to Index](#Index)



<h2 id="2019-08-20-1">1. UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task</h2> 
Title: [UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task](https://arxiv.org/abs/1908.06138)

Authors: [Santanu Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+S), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 16 Aug 2019)*

> In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our system in translating from Czech to Polish and comment on the impact of out-of-domain test data in the performance of our system. UDS-DFKI achieved competitive performance ranking second among ten teams in Czech to Polish translation.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.06138 [cs.CL]**                         |
|           | (or **arXiv:1908.06138v1 [cs.CL]** for this version) |





<h2 id="2019-08-20-2">2. Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation</h2> 
Title: [Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation](https://arxiv.org/abs/1908.06140)

Authors: [Mihaela Vela](https://arxiv.org/search/cs?searchtype=author&query=Vela%2C+M), [Santanu Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+S), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M), [Sudip Kumar Naskar](https://arxiv.org/search/cs?searchtype=author&query=Naskar%2C+S+K), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 16 Aug 2019)*

> This paper describes strategies to improve an existing web-based computer-aided translation (CAT) tool entitled CATaLog Online. CATaLog Online provides a post-editing environment with simple yet helpful project management tools. It offers translation suggestions from translation memories (TM), machine translation (MT), and automatic post-editing (APE) and records detailed logs of post-editing activities. To test the new approaches proposed in this paper, we carried out a user study on an English--German translation task using CATaLog Online. User feedback revealed that the users preferred using CATaLog Online over existing CAT tools in some respects, especially by selecting the output of the MT system and taking advantage of the color scheme for TM suggestions.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.06140 [cs.CL]**                         |
|           | (or **arXiv:1908.06140v1 [cs.CL]** for this version) |





<h2 id="2019-08-20-3">3. The Transference Architecture for Automatic Post-Editing</h2> 
Title: [The Transference Architecture for Automatic Post-Editing](https://arxiv.org/abs/1908.06151)

Authors: [Santanu Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+S), [Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Nico Herbig](https://arxiv.org/search/cs?searchtype=author&query=Herbig%2C+N), [Antonio Krueger](https://arxiv.org/search/cs?searchtype=author&query=Krueger%2C+A), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 16 Aug 2019)*

> In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-source encoder based APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt information and its integration with pe decisions. In this paper we present a new multi-source APE model, called transference. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second encoder combining src -> mt, and (iii) feeds this representation into a final decoder block generating pe. Our model outperforms the state-of-the-art by 1 BLEU point on the WMT 2016, 2017, and 2018 English--German APE shared tasks (PBSMT and NMT). We further investigate the importance of our newly introduced second encoder and find that a too small amount of layers does hurt the performance, while reducing the number of layers of the decoder does not matter much.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.06151 [cs.CL]**                         |
|           | (or **arXiv:1908.06151v1 [cs.CL]** for this version) |





<h2 id="2019-08-20-4">4. Language Graph Distillation for Low-Resource Machine Translation</h2> 
Title: [Language Graph Distillation for Low-Resource Machine Translation](https://arxiv.org/abs/1908.06258)

Authors: [Tianyu He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Jiale Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T)

*(Submitted on 17 Aug 2019)*

> Neural machine translation on low-resource language is challenging due to the lack of bilingual sentence pairs. Previous works usually solve the low-resource translation problem with knowledge transfer in a multilingual setting. In this paper, we propose the concept of Language Graph and further design a novel graph distillation algorithm that boosts the accuracy of low-resource translations in the graph with forward and backward knowledge distillation. Preliminary experiments on the TED talks multilingual dataset demonstrate the effectiveness of our proposed method. Specifically, we improve the low-resource translation pair by more than 3.13 points in terms of BLEU score.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.06258 [cs.CL]**                         |
|           | (or **arXiv:1908.06258v1 [cs.CL]** for this version) |





<h2 id="2019-08-20-5">5. Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation</h2> 
Title: [Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation](https://arxiv.org/abs/1908.06259)

Authors: [Tianyu He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T)

*(Submitted on 17 Aug 2019)*

> Neural machine translation (NMT) typically adopts the encoder-decoder framework. A good understanding of the characteristics and functionalities of the encoder and decoder can help to explain the pros and cons of the framework, and design better models for NMT. In this work, we conduct an empirical study on the encoder and the decoder in NMT, taking Transformer as an example. We find that 1) the decoder handles an easier task than the encoder in NMT, 2) the decoder is more sensitive to the input noise than the encoder, and 3) the preceding words/tokens in the decoder provide strong conditional information, which accounts for the two observations above. We hope those observations can shed light on the characteristics of the encoder and decoder and inspire future research on NMT.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.06259 [cs.CL]**                         |
|           | (or **arXiv:1908.06259v1 [cs.CL]** for this version) |





<h2 id="2019-08-20-6">6. Recurrent Graph Syntax Encoder for Neural Machine Translation</h2> 
Title: [Recurrent Graph Syntax Encoder for Neural Machine Translation](https://arxiv.org/abs/1908.06559)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

*(Submitted on 19 Aug 2019)*

> Syntax-incorporated machine translation models have been proven successful in improving the model's reasoning and meaning preservation ability. In this paper, we propose a simple yet effective graph-structured encoder, the Recurrent Graph Syntax Encoder, dubbed \textbf{RGSE}, which enhances the ability to capture useful syntactic information. The RGSE is done over a standard encoder (recurrent or self-attention encoder), regarding recurrent network units as graph nodes and injects syntactic dependencies as edges, such that RGSE models syntactic dependencies and sequential information (\textit{i.e.}, word order) simultaneously. Our approach achieves considerable improvements over several syntax-aware NMT models in English⇒German and English⇒Czech translation tasks. And RGSE-equipped big model obtains competitive result compared with the state-of-the-art model in WMT14 En-De task. Extensive analysis further verifies that RGSE could benefit long sentence modeling, and produces better translations.

| Comments: | Work in Progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.06559 [cs.CL]**                                 |
|           | (or **arXiv:1908.06559v1 [cs.CL]** for this version)         |





<h2 id="2019-08-20-7">7. Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces</h2> 
Title: [Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces](https://arxiv.org/abs/1908.06625)

Authors: [Barun Patra](https://arxiv.org/search/cs?searchtype=author&query=Patra%2C+B), [Joel Ruben Antony Moniz](https://arxiv.org/search/cs?searchtype=author&query=Moniz%2C+J+R+A), [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Matthew R. Gormley](https://arxiv.org/search/cs?searchtype=author&query=Gormley%2C+M+R), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 19 Aug 2019)*

> Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) --- a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don't appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.

| Comments:          | ACL 2019                                                     |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Journal reference: | Proceedings of the 57th Conference of the Association for Computational Linguistics (2019) 184-193 |
| Cite as:           | **arXiv:1908.06625 [cs.CL]**                                 |
|                    | (or **arXiv:1908.06625v1 [cs.CL]** for this version)         |



# 2019-08-19

[Return to Index](#Index)



<h2 id="2019-08-19-1">1. Attending to Future Tokens For Bidirectional Sequence Generation</h2> 
Title: [Attending to Future Tokens For Bidirectional Sequence Generation](https://arxiv.org/abs/1908.05915)

Authors: [Carolin Lawrence](https://arxiv.org/search/stat?searchtype=author&query=Lawrence%2C+C), [Bhushan Kotnis](https://arxiv.org/search/stat?searchtype=author&query=Kotnis%2C+B), [Mathias Niepert](https://arxiv.org/search/stat?searchtype=author&query=Niepert%2C+M)

*(Submitted on 16 Aug 2019)*

> Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.

| Comments: | Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019, Hong Kong, China |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.05915 [stat.ML]**                               |
|           | (or **arXiv:1908.05915v1 [stat.ML]** for this version)       |



<h2 id="2019-08-19-2">2. Towards Making the Most of BERT in Neural Machine Translation</h2> 
Title: [Towards Making the Most of BERT in Neural Machine Translation](https://arxiv.org/abs/1908.05672)

Authors: [Jiacheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Yong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Weinan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

*(Submitted on 15 Aug 2019)*

> GPT-2 and BERT demonstrate the effectiveness of using pre-trained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks. In this work, we introduce a concerted training framework (\method) that is the key to integrate the pre-trained LMs to neural machine translation (NMT). Our proposed Cnmt consists of three techniques: a) asymptotic distillation to ensure that the NMT model can retain the previous pre-trained knowledge; \item a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and b)a strategy to adjust the learning paces according to a scheduled policy. Our experiments in machine translation show \method gains of up to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pre-training aided NMT by 1.4 BLEU score. While for the large WMT14 English-French task with 40 millions of sentence-pairs, our base model still significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.05672 [cs.CL]**                                 |
|           | (or **arXiv:1908.05672v1 [cs.CL]** for this version)         |





<h2 id="2019-08-19-3">3. Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs</h2> 
Title: [Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs](https://arxiv.org/abs/1908.05679)

Authors: [WonKee Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+W), [Junsu Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J), [Byung-Hyun Go](https://arxiv.org/search/cs?searchtype=author&query=Go%2C+B), [Jong-Hyeok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J)

*(Submitted on 15 Aug 2019)*

> Recent approaches to the Automatic Post-Editing (APE) research have shown that better results are obtained by multi-source models, which jointly encode both source (src) and machine translation output (mt) to produce post-edited sentence (pe). Along this trend, we present a new multi-source APE model based on the Transformer. To construct effective joint representations, our model internally learns to incorporate src context into mt representation. With this approach, we achieve a significant improvement over baseline systems, as well as the state-of-the-art multi-source APE model. Moreover, to demonstrate the capability of our model to incorporate src context, we show that the word alignment of the unknown MT system is successfully captured in our encoding results.

| Comments: | 6 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.05679 [cs.CL]**                                 |
|           | (or **arXiv:1908.05679v1 [cs.CL]** for this version)         |





<h2 id="2019-08-19-4">4. Simple and Effective Noisy Channel Modeling for Neural Machine Translation</h2> 
Title: [Simple and Effective Noisy Channel Modeling for Neural Machine Translation](https://arxiv.org/abs/1908.05731)

Authors: [Kyra Yee](https://arxiv.org/search/cs?searchtype=author&query=Yee%2C+K), [Nathan Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+N), [Yann N. Dauphin](https://arxiv.org/search/cs?searchtype=author&query=Dauphin%2C+Y+N), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

*(Submitted on 15 Aug 2019)*

> Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT'17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.

| Comments: | EMNLP 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.05731 [cs.CL]**                         |
|           | (or **arXiv:1908.05731v1 [cs.CL]** for this version) |





<h2 id="2019-08-19-5">5. Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</h2> 
Title: [Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring](https://arxiv.org/abs/1908.05925)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Yan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

*(Submitted on 16 Aug 2019)*

> This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.

| Comments: | Accepted at WMT 2019. (The first and second authors contributed equally) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.05925 [cs.CL]**                                 |
|           | (or **arXiv:1908.05925v1 [cs.CL]** for this version)         |



# 2019-08-15

[Return to Index](#Index)



<h2 id="2019-08-15-1">1. On The Evaluation of Machine Translation Systems Trained With Back-Translation</h2> 
Title: [On The Evaluation of Machine Translation Systems Trained With Back-Translation](https://arxiv.org/abs/1908.05204)

Authors: [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

*(Submitted on 14 Aug 2019)*

> Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.05204 [cs.CL]**                         |
|           | (or **arXiv:1908.05204v1 [cs.CL]** for this version) |



# 2019-08-14

[Return to Index](#Index)



<h2 id="2019-08-14-1">1. Neural Text Generation with Unlikelihood Training</h2> 
Title: [Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)

Authors: [Sean Welleck](https://arxiv.org/search/cs?searchtype=author&query=Welleck%2C+S), [Ilia Kulikov](https://arxiv.org/search/cs?searchtype=author&query=Kulikov%2C+I), [Stephen Roller](https://arxiv.org/search/cs?searchtype=author&query=Roller%2C+S), [Emily Dinan](https://arxiv.org/search/cs?searchtype=author&query=Dinan%2C+E), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Jason Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+J)

*(Submitted on 12 Aug 2019)*

> Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive responses. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model itself are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences that contain repeats and frequent words unlike the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving far superior generations using standard greedy or beam search. Our approach provides a strong alternative to traditional training.

| Comments: | Sean Welleck and Ilia Kulikov contributed equally            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.04319 [cs.LG]**                                 |
|           | (or **arXiv:1908.04319v1 [cs.LG]** for this version)         |





<h2 id="2019-08-14-2">2. LSTM vs. GRU vs. Bidirectional RNN for script generation</h2> 
Title: [LSTM vs. GRU vs. Bidirectional RNN for script generation](https://arxiv.org/abs/1908.04332)

Authors: [Sanidhya Mangal](https://arxiv.org/search/cs?searchtype=author&query=Mangal%2C+S), [Poorva Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+P), [Rahul Modak](https://arxiv.org/search/cs?searchtype=author&query=Modak%2C+R)

*(Submitted on 12 Aug 2019)*

> Scripts are an important part of any TV series. They narrate movements, actions and expressions of characters. In this paper, a case study is presented on how different sequence to sequence deep learning models perform in the task of generating new conversations between characters as well as new scenarios on the basis of a script (previous conversations). A comprehensive comparison between these models, namely, LSTM, GRU and Bidirectional RNN is presented. All the models are designed to learn the sequence of recurring characters from the input sequence. Each input sequence will contain, say "n" characters, and the corresponding targets will contain the same number of characters, except, they will be shifted one character to the right. In this manner, input and output sequences are generated and used to train the models. A closer analysis of explored models performance and efficiency is delineated with the help of graph plots and generated texts by taking some input string. These graphs describe both, intraneural performance and interneural model performance for each model.

| Comments: | 7 pages, 7 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.04332 [cs.CL]**                                 |
|           | (or **arXiv:1908.04332v1 [cs.CL]** for this version)         |





<h2 id="2019-08-14-3">3. Attention is not not Explanation</h2> 
Title: [Attention is not not Explanation](https://arxiv.org/abs/1908.04626)

Authors: [Sarah Wiegreffe](https://arxiv.org/search/cs?searchtype=author&query=Wiegreffe%2C+S), [Yuval Pinter](https://arxiv.org/search/cs?searchtype=author&query=Pinter%2C+Y)

*(Submitted on 13 Aug 2019)*

> Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.

| Comments: | Accepted to EMNLP 2019; related blog post at [this https URL](https://medium.com/@yuvalpinter/attention-is-not-not-explanation-dbc25b534017) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.04626 [cs.CL]**                                 |
|           | (or **arXiv:1908.04626v1 [cs.CL]** for this version)         |





<h2 id="2019-08-14-4">4. Neural Machine Translation with Noisy Lexical Constraints</h2> 
Title: [Neural Machine Translation with Noisy Lexical Constraints](https://arxiv.org/abs/1908.04664)

Authors: [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L)

*(Submitted on 13 Aug 2019)*

> Lexically constrained decoding for machine translation has shown to be beneficial in previous studies. Unfortunately, constraints provided by users may contain mistakes in real-world situations. It is still an open question that how to manipulate these noisy constraints in such practical scenarios. We present a novel framework that treats constraints as external memories. In this soft manner, a mistaken constraint can be corrected. Experiments demonstrate that our approach can achieve substantial BLEU gains in handling noisy constraints. These results motivate us to apply the proposed approach on a new scenario where constraints are generated without the help of users. Experiments show that our approach can indeed improve the translation quality with the automatically generated constraints.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.04664 [cs.CL]**                         |
|           | (or **arXiv:1908.04664v1 [cs.CL]** for this version) |



# 2019-08-13

[Return to Index](#Index)

<h2 id="2019-08-13-1">1. On the Validity of Self-Attention as Explanation in Transformer Models</h2> 
Title: [On the Validity of Self-Attention as Explanation in Transformer Models](https://arxiv.org/abs/1908.04211)

Authors: [Gino Brunner](https://arxiv.org/search/cs?searchtype=author&query=Brunner%2C+G), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Damián Pascual](https://arxiv.org/search/cs?searchtype=author&query=Pascual%2C+D), [Oliver Richter](https://arxiv.org/search/cs?searchtype=author&query=Richter%2C+O), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

*(Submitted on 12 Aug 2019)*

> Explainability of deep learning systems is a vital requirement for many applications. However, it is still an unsolved problem. Recent self-attention based models for natural language processing, such as the Transformer or BERT, offer hope of greater explainability by providing attention maps that can be directly inspected. Nevertheless, by just looking at the attention maps one often overlooks that the attention is not over words but over hidden embeddings, which themselves can be mixed representations of multiple embeddings. We investigate to what extent the implicit assumption made in many recent papers - that hidden embeddings at all layers still correspond to the underlying words - is justified. We quantify how much embeddings are mixed based on a gradient based attribution method and find that already after the first layer less than 50% of the embedding is attributed to the underlying word, declining thereafter to a median contribution of 7.5% in the last layer. While throughout the layers the underlying word remains as the one contributing most to the embedding, we argue that attention visualizations are misleading and should be treated with care when explaining the underlying deep learning system.

| Comments:    | Preprint. Work in progress                                   |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 46-04                                                        |
| ACM classes: | I.2.7; I.7.0                                                 |
| Cite as:     | **arXiv:1908.04211 [cs.CL]**                                 |
|              | (or **arXiv:1908.04211v1 [cs.CL]** for this version)         |



# 2019-08-12

[Return to Index](#Index)

<h2 id="2019-08-12-1">1. Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling</h2> 
Title: [Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling](https://arxiv.org/abs/1908.03538)

Authors: [Siyuan Feng](https://arxiv.org/search/eess?searchtype=author&query=Feng%2C+S), [Tan Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+T)

*(Submitted on 9 Aug 2019)*

> This research addresses the problem of acoustic modeling of low-resource languages for which transcribed training data is absent. The goal is to learn robust frame-level feature representations that can be used to identify and distinguish subword-level speech units. The proposed feature representations comprise various types of multilingual bottleneck features (BNFs) that are obtained via multi-task learning of deep neural networks (MTL-DNN). One of the key problems is how to acquire high-quality frame labels for untranscribed training data to facilitate supervised DNN training. It is shown that learning of robust BNF representations can be achieved by effectively leveraging transcribed speech data and well-trained automatic speech recognition (ASR) systems from one or more out-of-domain (resource-rich) languages. Out-of-domain ASR systems can be applied to perform speaker adaptation with untranscribed training data of the target language, and to decode the training speech into frame-level labels for DNN training. It is also found that better frame labels can be generated by considering temporal dependency in speech when performing frame clustering. The proposed methods of feature learning are evaluated on the standard task of unsupervised subword modeling in Track 1 of the ZeroSpeech 2017 Challenge. The best performance achieved by our system is 9.7% in terms of across-speaker triphone minimal-pair ABX error rate, which is comparable to the best systems reported recently. Lastly, our investigation reveals that the closeness between target languages and out-of-domain languages and the amount of available training data for individual target languages could have significant impact on the goodness of learned features.

| Comments: | 12 pages, 6 figures. This manuscript has been accepted for publication as a regular paper in the IEEE Transactions on Audio, Speech and Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| Cite as:  | **arXiv:1908.03538 [eess.AS]**                               |
|           | (or **arXiv:1908.03538v1 [eess.AS]** for this version)       |







<h2 id="2019-08-12-2">2. UdS Submission for the WMT 19 Automatic Post-Editing Task</h2> 
Title: [UdS Submission for the WMT 19 Automatic Post-Editing Task](https://arxiv.org/abs/1908.03402)

Authors: [Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Qiuhui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 9 Aug 2019)*

> In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.

| Comments: | WMT 2019 Automatic Post-Editing Shared Task Paper    |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.03402 [cs.CL]**                         |
|           | (or **arXiv:1908.03402v1 [cs.CL]** for this version) |









# 2019-08-09

[Return to Index](#Index)

<h2 id="2019-08-09-1">1. A Test Suite and Manual Evaluation of Document-Level NMT at WMT19</h2> 
Title: [A Test Suite and Manual Evaluation of Document-Level NMT at WMT19](https://arxiv.org/abs/1908.03043)

Authors: [Kateřina Rysová](https://arxiv.org/search/cs?searchtype=author&query=Rysová%2C+K), [Magdaléna Rysová](https://arxiv.org/search/cs?searchtype=author&query=Rysová%2C+M), [Tomáš Musil](https://arxiv.org/search/cs?searchtype=author&query=Musil%2C+T), [Lucie Poláková](https://arxiv.org/search/cs?searchtype=author&query=Poláková%2C+L), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

*(Submitted on 8 Aug 2019)*

> As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. 
> We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.03043 [cs.CL]**                         |
|           | (or **arXiv:1908.03043v1 [cs.CL]** for this version) |



# 2019-08-07

[Return to Index](#Index)

<h2 id="2019-08-07-1">1. MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models</h2> 
Title: [MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models](https://arxiv.org/abs/1908.01816)

Authors: [Boyuan Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+B), [Yazheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Hao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Yueting Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Xiaofei He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X)

*(Submitted on 23 Jul 2019)*

> Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of the natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset.

| Comments: | Accepted In NeurIPS 2018                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.01816 [cs.CL]**                                 |
|           | (or **arXiv:1908.01816v1 [cs.CL]** for this version)         |





<h2 id="2019-08-07-2">2. A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data</h2> 
Title: [A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data](https://arxiv.org/abs/1908.01839)

Authors: [Ping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Tian Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+T), [Chandan K. Reddy](https://arxiv.org/search/cs?searchtype=author&query=Reddy%2C+C+K)

*(Submitted on 28 Jul 2019)*

> Electronic health record (EHR) data contains most of the important patient health information and is typically stored in a relational database with multiple tables. One important way for doctors to make use of EHR data is to retrieve intuitive information by posing a sequence of questions against it. However, due to a large amount of information stored in it, effectively retrieving patient information from EHR data in a short time is still a challenging issue for medical experts since it requires a good understanding of a query language to get access to the database. We tackle this challenge by developing a deep learning based approach that can translate a natural language question on multi-relational EHR data into its corresponding SQL query, which is referred to as a Question-to-SQL generation task. Most of the existing methods cannot solve this problem since they primarily focus on tackling the questions related to a single table under the table-aware assumption. While in our problem, it is possible that questions asked by clinicians are related to multiple unspecified tables. In this paper, we first create a new question to query dataset designed for healthcare to perform the Question-to-SQL generation task, named MIMICSQL, based on a publicly available electronic medical database. To address the challenge of generating queries on multi-relational databases from natural language questions, we propose a TRanslate-Edit Model for Question-to-SQL query (TREQS), which adopts the sequence-to-sequence model to directly generate SQL query for a given question, and further edits it with an attentive-copying mechanism and task-specific look-up tables. Both quantitative and qualitative experimental results indicate the flexibility and efficiency of our proposed method in tackling challenges that are unique in MIMICSQL.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.01839 [cs.CL]**                                 |
|           | (or **arXiv:1908.01839v1 [cs.CL]** for this version)         |





<h2 id="2019-08-07-3">3. Self-Knowledge Distillation in Natural Language Processing</h2> 
Title: [Self-Knowledge Distillation in Natural Language Processing](https://arxiv.org/abs/1908.01851)

Authors: [Sangchul Hahn](https://arxiv.org/search/cs?searchtype=author&query=Hahn%2C+S), [Heeyoul Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+H)

*(Submitted on 2 Aug 2019)*

> Since deep learning became a key player in natural language processing (NLP), many deep learning models have been showing remarkable performances in a variety of NLP tasks, and in some cases, they are even outperforming humans. Such high performance can be explained by efficient knowledge representation of deep learning models. While many methods have been proposed to learn more efficient representation, knowledge distillation from pretrained deep networks suggest that we can use more information from the soft target probability to train other neural networks. In this paper, we propose a new knowledge distillation method self-knowledge distillation, based on the soft target probabilities of the training model itself, where multimode information is distilled from the word embedding space right below the softmax layer. Due to the time complexity, our method approximates the soft target probabilities. In experiments, we applied the proposed method to two different and fundamental NLP tasks: language model and neural machine translation. The experiment results show that our proposed method improves performance on the tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.01851 [cs.CL]**                                 |
|           | (or **arXiv:1908.01851v1 [cs.CL]** for this version)         |



# 2019-08-06

[Return to Index](#Index)

<h2 id="2019-08-06-1">1. Invariance-based Adversarial Attack on Neural Machine Translation Systems</h2> 
Title: [Invariance-based Adversarial Attack on Neural Machine Translation Systems](https://arxiv.org/abs/1908.01165)

Authors: [Akshay Chaturvedi](https://arxiv.org/search/cs?searchtype=author&query=Chaturvedi%2C+A), [Abijith KP](https://arxiv.org/search/cs?searchtype=author&query=KP%2C+A), [Utpal Garain](https://arxiv.org/search/cs?searchtype=author&query=Garain%2C+U)

*(Submitted on 3 Aug 2019)*

> Recently, NLP models have been shown to be susceptible to adversarial attacks. In this paper, we explore adversarial attacks on neural machine translation (NMT) systems. Given a sentence in the source language, the goal of the proposed attack is to change multiple words while ensuring that the predicted translation remains unchanged. In order to choose the word from the source vocabulary, we propose a soft-attention based technique. The experiments are conducted on two language pairs: English-German (en-de) and English-French (en-fr) and two state-of-the-art NMT systems: BLSTM-based encoder-decoder with attention and Transformer. The proposed soft-attention based technique outperforms existing methods like HotFlip by a significant margin for all the conducted experiments The results demonstrate that state-of-the-art NMT systems are unable to capture the semantics of the source language.

| Comments: | Under review in IEEE/ACM TASLP                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.01165 [cs.LG]**                                 |
|           | (or **arXiv:1908.01165v1 [cs.LG]** for this version)         |



<h2 id="2019-08-06-2">2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content</h2> 
Title: [Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](https://arxiv.org/abs/1908.01587)

Authors: [Muhammad Zubair Asghar](https://arxiv.org/search/cs?searchtype=author&query=Asghar%2C+M+Z), [Fazli Subhan](https://arxiv.org/search/cs?searchtype=author&query=Subhan%2C+F), [Muhammad Imran](https://arxiv.org/search/cs?searchtype=author&query=Imran%2C+M), [Fazal Masud Kundi](https://arxiv.org/search/cs?searchtype=author&query=Kundi%2C+F+M), [Shahboddin Shamshirband](https://arxiv.org/search/cs?searchtype=author&query=Shamshirband%2C+S), [Amir Mosavi](https://arxiv.org/search/cs?searchtype=author&query=Mosavi%2C+A), [Peter Csiba](https://arxiv.org/search/cs?searchtype=author&query=Csiba%2C+P), [Annamaria R. Varkonyi-Koczy](https://arxiv.org/search/cs?searchtype=author&query=Varkonyi-Koczy%2C+A+R)

*(Submitted on 5 Aug 2019)*

> Emotion detection from the text is an important and challenging problem in text analytics. The opinion-mining experts are focusing on the development of emotion detection applications as they have received considerable attention of online community including users and business organization for collecting and interpreting public emotions. However, most of the existing works on emotion detection used less efficient machine learning classifiers with limited datasets, resulting in performance degradation. To overcome this issue, this work aims at the evaluation of the performance of different machine learning classifiers on a benchmark emotion dataset. The experimental results show the performance of different machine learning classifiers in terms of different evaluation metrics like precision, recall ad f-measure. Finally, a classifier with the best performance is recommended for the emotion classification.

| Comments:    | 30 pages, 13 tables, 1 figure                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| MSC classes: | 68T01                                                        |
| DOI:         | [10.20944/preprints201908.0019.v1](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.20944%2Fpreprints201908.0019.v1&v=7cef63df) |
| Cite as:     | **arXiv:1908.01587 [cs.IR]**                                 |
|              | (or **arXiv:1908.01587v1 [cs.IR]** for this version)         |





<h2 id="2019-08-06-3">3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation</h2> 
Title: [The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](https://arxiv.org/abs/1908.01192)

Authors: [Magdalena Biesialska](https://arxiv.org/search/cs?searchtype=author&query=Biesialska%2C+M), [Lluis Guardia](https://arxiv.org/search/cs?searchtype=author&query=Guardia%2C+L), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

*(Submitted on 3 Aug 2019)*

> Although the problem of similar language translation has been an area of research interest for many years, yet it is still far from being solved. In this paper, we study the performance of two popular approaches: statistical and neural. We conclude that both methods yield similar results; however, the performance varies depending on the language pair. While the statistical approach outperforms the neural one by a difference of 6 BLEU points for the Spanish-Portuguese language pair, the proposed neural model surpasses the statistical one by a difference of 2 BLEU points for Czech-Polish. In the former case, the language similarity (based on perplexity) is much higher than in the latter case. Additionally, we report negative results for the system combination with back-translation. Our TALP-UPC system submission won 1st place for Czech-to-Polish and 2nd place for Spanish-to-Portuguese in the official evaluation of the 1st WMT Similar Language Translation task.

| Comments: | WMT 2019 Shared Task paper                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.01192 [cs.CL]**                         |
|           | (or **arXiv:1908.01192v1 [cs.CL]** for this version) |





<h2 id="2019-08-06-4">4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English</h2> 
Title: [JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](https://arxiv.org/abs/1908.01349)

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Avishek Garain](https://arxiv.org/search/cs?searchtype=author&query=Garain%2C+A), [Adityar Rayala](https://arxiv.org/search/cs?searchtype=author&query=Rayala%2C+A), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 1 Aug 2019)*

> In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task. The system was created to translate news text from Lithuanian to English. To accomplish the given task, our system used a Word Embedding based Neural Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1908.00323](https://arxiv.org/abs/1908.00323) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.01349 [cs.CL]**                                 |
|           | (or **arXiv:1908.01349v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-5">5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian</h2> 
Title: [Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](https://arxiv.org/abs/1908.01519)

Authors: [Momchil Hardalov](https://arxiv.org/search/cs?searchtype=author&query=Hardalov%2C+M), [Ivan Koychev](https://arxiv.org/search/cs?searchtype=author&query=Koychev%2C+I), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 5 Aug 2019)*

> Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23%, which is well above the baseline of 24.89%.

| Comments: | Accepted at RANLP 2019 (13 pages, 2 figures, 6 tables)       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **arXiv:1908.01519 [cs.CL]**                                 |
|           | (or **arXiv:1908.01519v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-6">6. Predicting Actions to Help Predict Translations</h2> 
Title: [Predicting Actions to Help Predict Translations](https://arxiv.org/abs/1908.01665)

Authors: [Zixiu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Josiah Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 5 Aug 2019)*

> We address the task of text translation on the How2 dataset using a state of the art transformer-based multimodal approach. The question we ask ourselves is whether visual features can support the translation process, in particular, given that this is a dataset extracted from videos, we focus on the translation of actions, which we believe are poorly captured in current static image-text datasets currently used for multimodal translation. For that purpose, we extract different types of action features from the videos and carefully investigate how helpful this visual information is by testing whether it can increase translation quality when used in conjunction with (i) the original text and (ii) the original text where action-related words (or all verbs) are masked out. The latter is a simulation that helps us assess the utility of the image in cases where the text does not provide enough context about the action, or in the presence of noise in the input text.

| Comments: | Accepted to workshop "The How2 Challenge: New Tasks for Vision & Language" of International Conference on Machine Learning 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.01665 [cs.CL]**                                 |
|           | (or **arXiv:1908.01665v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-7">7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing</h2> 
Title: [Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](https://arxiv.org/abs/1908.01699)

Authors: [David Awad](https://arxiv.org/search/cs?searchtype=author&query=Awad%2C+D)

*(Submitted on 5 Aug 2019)*

> Thoth is a tool designed to combine many different types of speed reading technology. The largest insight is using natural language parsing for more optimal rapid serial visual presentation and more effective reading information.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **arXiv:1908.01699 [cs.CL]**                                 |
|           | (or **arXiv:1908.01699v1 [cs.CL]** for this version)         |





# 2019-08-02

[Return to Index](#Index)

<h2 id="2019-08-02-1">1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data</h2> 
Title: [Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](https://arxiv.org/abs/1908.00449)

Authors: [Jacob Harer](https://arxiv.org/search/cs?searchtype=author&query=Harer%2C+J), [Chris Reale](https://arxiv.org/search/cs?searchtype=author&query=Reale%2C+C), [Peter Chin](https://arxiv.org/search/cs?searchtype=author&query=Chin%2C+P)

*(Submitted on 1 Aug 2019)*

> Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of 25% F0.5 over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a 10% improvement in recall on the CoNLL 2014 benchmark and the highest to date F0.5 score on the AESW benchmark of 50.43.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.00449 [cs.LG]**                                 |
|           | (or **arXiv:1908.00449v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-2">2. Learning Joint Acoustic-Phonetic Word Embeddings</h2> 
Title: [Learning Joint Acoustic-Phonetic Word Embeddings](https://arxiv.org/abs/1908.00493)

Authors: [Mohamed El-Geish](https://arxiv.org/search/cs?searchtype=author&query=El-Geish%2C+M)

*(Submitted on 1 Aug 2019)*

> Most speech recognition tasks pertain to mapping words across two modalities: acoustic and orthographic. In this work, we suggest learning encoders that map variable-length, acoustic or phonetic, sequences that represent words into fixed-dimensional vectors in a shared latent space; such that the distance between two word vectors represents how closely the two words sound. Instead of directly learning the distances between word vectors, we employ weak supervision and model a binary classification task to predict whether two inputs, one of each modality, represent the same word given a distance threshold. We explore various deep-learning models, bimodal contrastive losses, and techniques for mining hard negative examples such as the semi-supervised technique of self-labeling. Our best model achieves an F1 score of 0.95 for the binary classification task.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.00493 [cs.LG]**                                 |
|           | (or **arXiv:1908.00493v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-3">3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English</h2> 
Title: [JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](https://arxiv.org/abs/1908.00323)

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 1 Aug 2019)*

> In the current work, we present a description of the system submitted to WMT 2018 News Translation Shared task. The system was created to translate news text from Finnish to English. The system used a Character Based Neural Machine Translation model to accomplish the given task. The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same. Our system garnered a BLEU score of 12.9.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.00323 [cs.CL]**                         |
|           | (or **arXiv:1908.00323v1 [cs.CL]** for this version) |


