# Daily arXiv: Machine Translation - Jan., 2020

# Index

- [2020-01-31](#2020-01-30)
  - [1. Learning Robust and Multilingual Speech Representations](#2020-01-31-1)
  - [2. Iterative Batch Back-Translation for Neural Machine Translation: A Conceptual Model](#2020-01-31-2)
  - [3. Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages](#2020-01-31-3)
- [2020-01-27](#2020-01-27)
  - [1. Exploration Based Language Learning for Text-Based Games](#2020-01-27-1)
- [2020-01-24](#2020-01-24)
  - [1. Pre-training via Leveraging Assisting Languages and Data Selection for Neural Machine Translation](#2020-01-24-1)
  - [2. Coordinated Reasoning for Cross-Lingual Knowledge Graph Alignment](#2020-01-24-2)
- [2020-01-23](#2020-01-23)
  - [1. Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP](#2020-01-23-1)
  - [2. Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation](#2020-01-23-2)
  - [3. Multilingual Denoising Pre-training for Neural Machine Translation](#2020-01-23-3)
- [2020-01-20](#2020-01-20)
  - [1. RobBERT: a Dutch RoBERTa-based Language Model](#2020-01-20-1)
- [2020-01-17](#2020-01-17)
  - [1. Insertion-Deletion Transformer](#2020-01-17-1)
- [2020-01-16](#2020-01-16)
  - [1. Parallel Machine Translation with Disentangled Context Transformer](#2020-01-16-1)
  - [2. Urdu-English Machine Transliteration using Neural Networks](#2020-01-16-2)
- [2020-01-15](#2020-01-15)
  - [1. Faster Transformer Decoding: N-gram Masked Self-Attention](#2020-01-15-1)
  - [2. Faster Transformer Decoding: N-gram Masked Self-Attention](#2020-01-15-2)
  - [3. Bi-Decoder Augmented Network for Neural Machine Translation](#2020-01-15-3)
- [2020-01-14](#2020-01-14)
  - [1. Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training](#2020-01-14-1)
  - [2. Reformer: The Efficient Transformer](#2020-01-14-2)
  - [3. PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata](#2020-01-14-3)
- [2020-01-13](#2020-01-13)
  - [1. Learning to Multi-Task Learn for Better Neural Machine Translation](#2020-01-13-1)
  - [2. Towards Minimal Supervision BERT-based Grammar Error Correction](#2020-01-13-2)
- [2020-01-07](#2020-01-07)
  - [1. A Comprehensive Survey of Multilingual Neural Machine Translation](#2020-01-07-1)
  - [2. Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation](#2020-01-07-2)
  - [3. Exploring Benefits of Transfer Learning in Neural Machine Translation](#2020-01-07-3)
- [2020-01-06](#2020-01-06)
  - [1. Learning Accurate Integer Transformer Machine-Translation Models](#2020-01-06-1)
- [2020-01-03](#2020-01-03)
  - [1. A Voice Interactive Multilingual Student Support System using IBM Watson](#2020-01-03-1)
- [2020-01-01](2020-01-01)
  - [1. TextScanner: Reading Characters in Order for Robust Scene Text Recognition](#2020-01-01-1)
  - [2. Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning](#2020-01-01-2)
  - [3. Robust Cross-lingual Embeddings from Parallel Sentences](#2020-01-01-3)
  - [4. "Hinglish" Language -- Modeling a Messy Code-Mixed Language](#2020-01-01-4)
  - [5. Amharic-Arabic Neural Machine Translation](#2020-01-01-5)
  - [6. LayoutLM: Pre-training of Text and Layout for Document Image Understanding](#2020-01-01-6)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-01-31

[Return to Index](#Index)



<h2 id="2020-01-31-1">1. Learning Robust and Multilingual Speech Representations
</h2>

Title: [Learning Robust and Multilingual Speech Representations](https://arxiv.org/abs/2001.11128)

Authors: [Kazuya Kawakami](https://arxiv.org/search/cs?searchtype=author&query=Kawakami%2C+K), [Luyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C), [Phil Blunsom](https://arxiv.org/search/cs?searchtype=author&query=Blunsom%2C+P), [Aaron van den Oord](https://arxiv.org/search/cs?searchtype=author&query=van+den+Oord%2C+A)

*(Submitted on 29 Jan 2020)*

> Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages including tonal languages and low-resource languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.11128](https://arxiv.org/abs/2001.11128) [cs.CL] |
|           | (or [arXiv:2001.11128v1](https://arxiv.org/abs/2001.11128v1) [cs.CL] for this version) |





<h2 id="2020-01-31-2">2. Iterative Batch Back-Translation for Neural Machine Translation: A Conceptual Model</h2>

Title: [Iterative Batch Back-Translation for Neural Machine Translation: A Conceptual Model]()

Authors: [Idris Abdulmumin](https://arxiv.org/search/cs?searchtype=author&query=Abdulmumin%2C+I), [Bashir Shehu Galadanci](https://arxiv.org/search/cs?searchtype=author&query=Galadanci%2C+B+S), [Abubakar Isa](https://arxiv.org/search/cs?searchtype=author&query=Isa%2C+A)

*(Submitted on 26 Nov 2019)*

> An effective method to generate a large number of parallel sentences for training improved neural machine translation (NMT) systems is the use of back-translations of the target-side monolingual data. Recently, iterative back-translation has been shown to outperform standard back-translation albeit on some language pairs. This work proposes the iterative batch back-translation that is aimed at enhancing the standard iterative back-translation and enabling the efficient utilization of more monolingual data. After each iteration, improved back-translations of new sentences are added to the parallel data that will be used to train the final forward model. The work presents a conceptual model of the proposed approach.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1912.10514](https://arxiv.org/abs/1912.10514) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.11327](https://arxiv.org/abs/2001.11327) [cs.CL] |
|           | (or [arXiv:2001.11327v1](https://arxiv.org/abs/2001.11327v1) [cs.CL] for this version) |





<h2 id="2020-01-31-3">3. Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages</h2>

Title: [Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages](https://arxiv.org/abs/2001.11453)

Authors: [Edoardo M. Ponti](https://arxiv.org/search/cs?searchtype=author&query=Ponti%2C+E+M), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Marinela Parovic](https://arxiv.org/search/cs?searchtype=author&query=Parovic%2C+M), [Roi Reichart](https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A)

*(Submitted on 30 Jan 2020)*

> Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task-language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task-language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods; it increases performance by 4.49 points for POS tagging and 7.73 points for NER on average compared to the strongest baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.11453](https://arxiv.org/abs/2001.11453) [cs.CL] |
|           | (or [arXiv:2001.11453v1](https://arxiv.org/abs/2001.11453v1) [cs.CL] for this version) |



# 2020-01-27

[Return to Index](#Index)



<h2 id="2020-01-27-1">1. Exploration Based Language Learning for Text-Based Games</h2>

Title: [Exploration Based Language Learning for Text-Based Games](https://arxiv.org/abs/2001.08868)

Authors: [Andrea Madotto](https://arxiv.org/search/cs?searchtype=author&query=Madotto%2C+A), [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [Joost Huizinga](https://arxiv.org/search/cs?searchtype=author&query=Huizinga%2C+J), [Piero Molino](https://arxiv.org/search/cs?searchtype=author&query=Molino%2C+P), [Adrien Ecoffet](https://arxiv.org/search/cs?searchtype=author&query=Ecoffet%2C+A), [Huaixiu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Alexandros Papangelis](https://arxiv.org/search/cs?searchtype=author&query=Papangelis%2C+A), [Dian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Chandra Khatri](https://arxiv.org/search/cs?searchtype=author&query=Khatri%2C+C), [Gokhan Tur](https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G)

*(Submitted on 24 Jan 2020)*

> This work presents an exploration and imitation-learning-based agent capable of state-of-the-art performance in playing text-based computer games. Text-based computer games describe their world to the player through natural language and expect the player to interact with the game using text. These games are of interest as they can be seen as a testbed for language understanding, problem-solving, and language generation by artificial agents. Moreover, they provide a learning environment in which these skills can be acquired through interactions with an environment rather than using fixed corpora. One aspect that makes these games particularly challenging for learning agents is the combinatorially large action space. Existing methods for solving text-based games are limited to games that are either very simple or have an action space restricted to a predetermined set of admissible actions. In this work, we propose to use the exploration approach of Go-Explore for solving text-based games. More specifically, in an initial exploration phase, we first extract trajectories with high rewards, after which we train a policy to solve the game by imitating these trajectories. Our experiments show that this approach outperforms existing solutions in solving text-based games, and it is more sample efficient in terms of the number of interactions with the environment. Moreover, we show that the learned policy can generalize better than existing solutions to unseen games without using any restriction on the action space.

| Comments: | Under Review                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | [arXiv:2001.08868](https://arxiv.org/abs/2001.08868) [cs.CL] |
|           | (or [arXiv:2001.08868v1](https://arxiv.org/abs/2001.08868v1) [cs.CL] for this version) |





# 2020-01-24

[Return to Index](#Index)



<h2 id="2020-01-24-1">1. Pre-training via Leveraging Assisting Languages and Data Selection for Neural Machine Translation</h2>

Title: [Pre-training via Leveraging Assisting Languages and Data Selection for Neural Machine Translation](https://arxiv.org/abs/2001.08353)

Authors: [Haiyue Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H), [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Zhuoyuan Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Fei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+F), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 23 Jan 2020)*

> Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks in low-resource settings. However, large monolingual corpora might not always be available for the languages of interest (LOI). To this end, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. A case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. We further show how to utilize script mapping (Chinese to Japanese) to increase the similarity between the two monolingual corpora leading to further improvements in translation quality. Additionally, we propose simple data-selection techniques to be used prior to pre-training that significantly impact the quality of S2S pre-training. An empirical comparison of our proposed methods reveals that leveraging assisting language monolingual corpora, data selection and script mapping are extremely important for NMT pre-training in low-resource scenarios.

| Comments: | Work in progress. Submitted to a conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.08353](https://arxiv.org/abs/2001.08353) [cs.CL] |
|           | (or [arXiv:2001.08353v1](https://arxiv.org/abs/2001.08353v1) [cs.CL] for this version) |





<h2 id="2020-01-24-2">2. Coordinated Reasoning for Cross-Lingual Knowledge Graph Alignment</h2>

Title: [Coordinated Reasoning for Cross-Lingual Knowledge Graph Alignment](https://arxiv.org/abs/2001.08728)

Authors: [Kun Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Linfeng Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L), [Yansong Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Yan Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

*(Submitted on 23 Jan 2020)*

> Existing entity alignment methods mainly vary on the choices of encoding the knowledge graph, but they typically use the same decoding method, which independently chooses the local optimal match for each source entity. This decoding method may not only cause the "many-to-one" problem but also neglect the coordinated nature of this task, that is, each alignment decision may highly correlate to the other decisions. In this paper, we introduce two coordinated reasoning methods, i.e., the Easy-to-Hard decoding strategy and joint entity alignment algorithm. Specifically, the Easy-to-Hard strategy first retrieves the model-confident alignments from the predicted results and then incorporates them as additional knowledge to resolve the remaining model-uncertain alignments. To achieve this, we further propose an enhanced alignment model that is built on the current state-of-the-art baseline. In addition, to address the many-to-one problem, we propose to jointly predict entity alignments so that the one-to-one constraint can be naturally incorporated into the alignment prediction. Experimental results show that our model achieves the state-of-the-art performance and our reasoning methods can also significantly improve existing baselines.

| Comments: | in AAAI 2020                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.08728](https://arxiv.org/abs/2001.08728) [cs.CL] |
|           | (or [arXiv:2001.08728v1](https://arxiv.org/abs/2001.08728v1) [cs.CL] for this version) |





# 2020-01-23

[Return to Index](#Index)



<h2 id="2020-01-23-1">1. Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP</h2>

Title: [Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP](https://arxiv.org/abs/2001.07820)

Authors: [Ying Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Xu Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+X), [Antonio Jose Jimeno Yepes](https://arxiv.org/search/cs?searchtype=author&query=Yepes%2C+A+J+J), [Jey Han Lau](https://arxiv.org/search/cs?searchtype=author&query=Lau%2C+J+H)

*(Submitted on 22 Jan 2020)*

> An adversarial example is an input transformed by small perturbations that machine learning models consistently misclassify. While there are a number of methods proposed to generate adversarial examples for text data, it is not trivial to assess the quality of these adversarial examples, as minor perturbations (such as changing a word in a sentence) can lead to a significant shift in their meaning, readability and classification label. In this paper, we propose an evaluation framework to assess the quality of adversarial examples based on the aforementioned properties. We experiment with five benchmark attacking methods and an alternative approach based on an auto-encoder, and found that these methods generate adversarial examples with poor readability and content preservation. We also learned that there are multiple factors that can influence the attacking performance, such as the the length of text examples and the input domain.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.07820](https://arxiv.org/abs/2001.07820) [cs.CL] |
|           | (or [arXiv:2001.07820v1](https://arxiv.org/abs/2001.07820v1) [cs.CL] for this version) |





<h2 id="2020-01-23-2">2. Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation</h2>

Title: [Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation](https://arxiv.org/abs/2001.08140)

Authors: [Di Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+D), [Zhijing Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Z), [Joey Tianyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J+T), [Peter Szolovits](https://arxiv.org/search/cs?searchtype=author&query=Szolovits%2C+P)

*(Submitted on 22 Jan 2020)*

> State-of-the-art neural machine translation (NMT) systems are data-hungry and perform poorly on domains with little supervised data. As data collection is expensive and infeasible in many cases, unsupervised domain adaptation methods are needed. We apply an Iterative Back Translation (IBT) training scheme on in-domain monolingual data, which repeatedly uses a Transformer-based NMT model to create in-domain pseudo-parallel sentence pairs in one translation direction on the fly and then use them to train the model in the other direction. Evaluated on three domains of German-to-English translation task with no supervised data, this simple technique alone (without any out-of-domain parallel data) can already surpass all previous domain adaptation methods---up to +9.48 BLEU over the strongest previous method, and up to +27.77 BLEU over the unadapted baseline. Moreover, given available supervised out-of-domain data on German-to-English and Romanian-to-English language pairs, we can further enhance the performance and obtain up to +19.31 BLEU improvement over the strongest baseline, and +47.69 BLEU increment against the unadapted model.

| Comments: | Submitted to IJCAI 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.08140](https://arxiv.org/abs/2001.08140) [cs.CL] |
|           | (or [arXiv:2001.08140v1](https://arxiv.org/abs/2001.08140v1) [cs.CL] for this version) |



<h2 id="2020-01-23-3">3. Multilingual Denoising Pre-training for Neural Machine Translation</h2>

Title: [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)

Authors: [Yinhan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 22 Jan 2020 ([v1](https://arxiv.org/abs/2001.08210v1)), last revised 23 Jan 2020 (this version, v2))*

> This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.08210](https://arxiv.org/abs/2001.08210) [cs.CL] |
|           | (or [arXiv:2001.08210v2](https://arxiv.org/abs/2001.08210v2) [cs.CL] for this version) |



# 2020-01-20

[Return to Index](#Index)



<h2 id="2020-01-20-1">1. RobBERT: a Dutch RoBERTa-based Language Model</h2>

Title: [RobBERT: a Dutch RoBERTa-based Language Model](https://arxiv.org/abs/2001.06286)

Authors: [Pieter Delobelle](https://arxiv.org/search/cs?searchtype=author&query=Delobelle%2C+P), [Thomas Winters](https://arxiv.org/search/cs?searchtype=author&query=Winters%2C+T), [Bettina Berendt](https://arxiv.org/search/cs?searchtype=author&query=Berendt%2C+B)

(Submitted on 17 Jan 2020)

> Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT (Bi-directional Encoders for Transformers), which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies showed that BERT models trained on a single language significantly outperform the multilingual results. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train their Dutch BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We show that RobBERT improves state of the art results in Dutch-specific language tasks, and also outperforms other existing Dutch BERT-based models in sentiment analysis. These results indicate that RobBERT is a powerful pre-trained model for fine-tuning for a large variety of Dutch language tasks. We publicly release this pre-trained model in hope of supporting further downstream Dutch NLP applications.

| Comments: | 7 pages, 2 tables                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.06286](https://arxiv.org/abs/2001.06286) [cs.CL] |
|           | (or [arXiv:2001.06286v1](https://arxiv.org/abs/2001.06286v1) [cs.CL] for this version) |



# 2020-01-17

[Return to Index](#Index)



<h2 id="2020-01-17-1">1. Insertion-Deletion Transformer</h2>
Title: [Insertion-Deletion Transformer](https://arxiv.org/abs/2001.05540)

Authors: [Laura Ruis](https://arxiv.org/search/cs?searchtype=author&query=Ruis%2C+L), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Julia Proskurnia](https://arxiv.org/search/cs?searchtype=author&query=Proskurnia%2C+J), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W)

*(Submitted on 15 Jan 2020)*

> We propose the Insertion-Deletion Transformer, a novel transformer-based neural architecture and training method for sequence generation. The model consists of two phases that are executed iteratively, 1) an insertion phase and 2) a deletion phase. The insertion phase parameterizes a distribution of insertions on the current output hypothesis, while the deletion phase parameterizes a distribution of deletions over the current output hypothesis. The training method is a principled and simple algorithm, where the deletion model obtains its signal directly on-policy from the insertion model output. We demonstrate the effectiveness of our Insertion-Deletion Transformer on synthetic translation tasks, obtaining significant BLEU score improvement over an insertion-only model.

| Comments: | Accepted as an Extended Abstract at the Workshop of Neural Generation and Translation (WNGT 2019) at EMNLP 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2001.05540](https://arxiv.org/abs/2001.05540) [cs.LG] |
|           | (or [arXiv:2001.05540v1](https://arxiv.org/abs/2001.05540v1) [cs.LG] for this version) |





# 2020-01-16

[Return to Index](#Index)



<h2 id="2020-01-16-1">1. Parallel Machine Translation with Disentangled Context Transformer</h2>
Title: [Parallel Machine Translation with Disentangled Context Transformer](https://arxiv.org/abs/2001.05136)

Authors: [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 15 Jan 2020)*

> State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.05136](https://arxiv.org/abs/2001.05136) [cs.CL] |
|           | (or [arXiv:2001.05136v1](https://arxiv.org/abs/2001.05136v1) [cs.CL] for this version) |





<h2 id="2020-01-16-2">2. Urdu-English Machine Transliteration using Neural Networks</h2>
Title: [Urdu-English Machine Transliteration using Neural Networks](https://arxiv.org/abs/2001.05296)

Authors: [Usman Mohy ud Din](https://arxiv.org/search/cs?searchtype=author&query=Din%2C+U+M+u)

*(Submitted on 12 Jan 2020)*

> Machine translation has gained much attention in recent years. It is a sub-field of computational linguistic which focus on translating text from one language to other language. Among different translation techniques, neural network currently leading the domain with its capabilities of providing a single large neural network with attention mechanism, sequence-to-sequence and long-short term modelling. Despite significant progress in domain of machine translation, translation of out-of-vocabulary words(OOV) which include technical terms, named-entities, foreign words are still a challenge for current state-of-art translation systems, and this situation becomes even worse while translating between low resource languages or languages having different structures. Due to morphological richness of a language, a word may have different meninges in different context. In such scenarios, translation of word is not only enough in order provide the correct/quality translation. Transliteration is a way to consider the context of word/sentence during translation. For low resource language like Urdu, it is very difficult to have/find parallel corpus for transliteration which is large enough to train the system. In this work, we presented transliteration technique based on Expectation Maximization (EM) which is un-supervised and language independent. Systems learns the pattern and out-of-vocabulary (OOV) words from parallel corpus and there is no need to train it on transliteration corpus explicitly. This approach is tested on three models of statistical machine translation (SMT) which include phrasebased, hierarchical phrase-based and factor based models and two models of neural machine translation which include LSTM and transformer model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.05296](https://arxiv.org/abs/2001.05296) [cs.CL] |
|           | (or [arXiv:2001.05296v1](https://arxiv.org/abs/2001.05296v1) [cs.CL] for this version) |



# 2020-01-15

[Return to Index](#Index)



<h2 id="2020-01-15-1">1. Faster Transformer Decoding: N-gram Masked Self-Attention</h2>
Title: [Faster Transformer Decoding: N-gram Masked Self-Attention](https://arxiv.org/abs/2001.04589)

Authors: [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Mia Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)

*(Submitted on 14 Jan 2020)*

> Motivated by the fact that most of the information relevant to the prediction of target tokens is drawn from the source sentence S=s1,…,sS, we propose truncating the target-side window used for computing self-attention by making an N-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the N-gram masked self-attention model loses very little in BLEU score for N values in the range 4,…,8, depending on the task.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04589](https://arxiv.org/abs/2001.04589) [cs.LG] |
|           | (or [arXiv:2001.04589v1](https://arxiv.org/abs/2001.04589v1) [cs.LG] for this version) |





<h2 id="2020-01-15-2">2. Improved Robust ASR for Social Robots in Public Spaces</h2>
Title: [Improved Robust ASR for Social Robots in Public Spaces](https://arxiv.org/abs/2001.04619)

Authors: [Charles Jankowski](https://arxiv.org/search/eess?searchtype=author&query=Jankowski%2C+C), [Vishwas Mruthyunjaya](https://arxiv.org/search/eess?searchtype=author&query=Mruthyunjaya%2C+V), [Ruixi Lin](https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+R)

*(Submitted on 14 Jan 2020)*

> Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04619](https://arxiv.org/abs/2001.04619) [eess.AS] |
|           | (or [arXiv:2001.04619v1](https://arxiv.org/abs/2001.04619v1) [eess.AS] for this version) |





<h2 id="2020-01-15-3">3. Bi-Decoder Augmented Network for Neural Machine Translation</h2>
Title: [Bi-Decoder Augmented Network for Neural Machine Translation](https://arxiv.org/abs/2001.04586)

Authors: [Boyuan Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+B), [Yazheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Yueting Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D)

*(Submitted on 14 Jan 2020)*

> Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It's obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04586](https://arxiv.org/abs/2001.04586) [cs.CL] |
|           | (or [arXiv:2001.04586v1](https://arxiv.org/abs/2001.04586v1) [cs.CL] for this version) |





# 2020-01-14

[Return to Index](#Index)



<h2 id="2020-01-14-1">1. Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training</h2>
Title: [Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training](https://arxiv.org/abs/2001.04260)

Authors: [Seung Hee Yang](https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+S+H), [Minhwa Chung](https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+M)

*(Submitted on 10 Jan 2020)*

> Dysarthria is a motor speech impairment affecting millions of people. Dysarthric speech can be far less intelligible than those of non-dysarthric speakers, causing significant communication difficulties. The goal of our work is to develop a model for dysarthric to healthy speech conversion using Cycle-consistent GAN. Using 18,700 dysarthric and 8,610 healthy control Korean utterances that were recorded for the purpose of automatic recognition of voice keyboard in a previous study, the generator is trained to transform dysarthric to healthy speech in the spectral domain, which is then converted back to speech. Objective evaluation using automatic speech recognition of the generated utterance on a held-out test set shows that the recognition performance is improved compared with the original dysarthic speech after performing adversarial training, as the absolute WER has been lowered by 33.4%. It demonstrates that the proposed GAN-based conversion method is useful for improving dysarthric speech intelligibility.

| Comments: | To be Published on the 24th February in BIOSIGNALS 2020. arXiv admin note: text overlap with [arXiv:1904.09407](https://arxiv.org/abs/1904.09407) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | [arXiv:2001.04260](https://arxiv.org/abs/2001.04260) [eess.AS] |
|           | (or [arXiv:2001.04260v1](https://arxiv.org/abs/2001.04260v1) [eess.AS] for this version) |





<h2 id="2020-01-14-2">2. Reformer: The Efficient Transformer</h2>
Title: [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

Authors: [Nikita Kitaev](https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N), [Łukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+Ł), [Anselm Levskaya](https://arxiv.org/search/cs?searchtype=author&query=Levskaya%2C+A)

*(Submitted on 13 Jan 2020)*

> Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.

| Comments: | ICLR 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2001.04451](https://arxiv.org/abs/2001.04451) [cs.LG] |
|           | (or [arXiv:2001.04451v1](https://arxiv.org/abs/2001.04451v1) [cs.LG] for this version) |





<h2 id="2020-01-14-3">3. PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata</h2>
Title: [PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata](https://arxiv.org/abs/2001.03708)

Authors: [Jieh-Sheng Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Jieh Hsiang](https://arxiv.org/search/cs?searchtype=author&query=Hsiang%2C+J)

*(Submitted on 11 Jan 2020)*

> PatentTransformer is our codename for patent text generation based on Transformer-based models. Our goal is "Augmented Inventing." In this second version, we leverage more of the structural metadata in patents. The structural metadata includes patent title, abstract, and dependent claim, in addition to independent claim previously. Metadata controls what kind of patent text for the model to generate. Also, we leverage the relation between metadata to build a text-to-text generation flow, for example, from a few words to a title, the title to an abstract, the abstract to an independent claim, and the independent claim to multiple dependent claims. The text flow can go backward because the relation is trained bidirectionally. We release our GPT-2 models trained from scratch and our code for inference so that readers can verify and generate patent text on their own. As for generation quality, we measure it by both ROUGE and Google Universal Sentence Encoder.

| Comments: | demo paper                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.03708](https://arxiv.org/abs/2001.03708) [cs.CL] |
|           | (or [arXiv:2001.03708v1](https://arxiv.org/abs/2001.03708v1) [cs.CL] for this version) |





# 2020-01-13

[Return to Index](#Index)



<h2 id="2020-01-13-1">1. Learning to Multi-Task Learn for Better Neural Machine Translation</h2>
Title: [Learning to Multi-Task Learn for Better Neural Machine Translation](https://arxiv.org/abs/2001.03294)

Authors: [Poorya Zaremoodi](https://arxiv.org/search/cs?searchtype=author&query=Zaremoodi%2C+P), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 10 Jan 2020)*

> Scarcity of parallel sentence pairs is a major challenge for training high quality neural machine translation (NMT) models in bilingually low-resource scenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to inject linguistic-related inductive biases into NMT, using auxiliary syntactic and semantic tasks, to improve generalisation. The challenge, however, is to devise effective training schedules, prescribing when to make use of the auxiliary tasks during the training process to fill the knowledge gaps of the main translation task, a setting referred to as biased-MTL. Current approaches for the training schedule are based on hand-engineering heuristics, whose effectiveness vary in different MTL settings. We propose a novel framework for learning the training schedule, ie learning to multi-task learn, for the MTL setting of interest. We formulate the training schedule as a Markov decision process which paves the way to employ policy learning methods to learn the scheduling policy. We effectively and efficiently learn the training schedule policy within the imitation learning framework using an oracle policy algorithm that dynamically sets the importance weights of auxiliary tasks based on their contributions to the generalisability of the main NMT task. Experiments on low-resource NMT settings show the resulting automatically learned training schedulers are competitive with the best heuristics, and lead to up to +1.1 BLEU score improvements.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.03294](https://arxiv.org/abs/2001.03294) [cs.CL] |
|           | (or [arXiv:2001.03294v1](https://arxiv.org/abs/2001.03294v1) [cs.CL] for this version) |





<h2 id="2020-01-13-2">2. Towards Minimal Supervision BERT-based Grammar Error Correction</h2>
Title: [Towards Minimal Supervision BERT-based Grammar Error Correction](https://arxiv.org/abs/2001.03521)

Authors: [Yiyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

*(Submitted on 10 Jan 2020)*

> Current grammatical error correction (GEC) models typically consider the task as sequence generation, which requires large amounts of annotated data and limit the applications in data-limited settings. We try to incorporate contextual information from pre-trained language model to leverage annotation and benefit multilingual scenarios. Results show strong potential of Bidirectional Encoder Representations from Transformers (BERT) in grammatical error correction task.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.03521](https://arxiv.org/abs/2001.03521) [cs.CL] |
|           | (or [arXiv:2001.03521v1](https://arxiv.org/abs/2001.03521v1) [cs.CL] for this version) |









# 2020-01-07

[Return to Index](#Index)



<h2 id="2020-01-07-1">1. A Comprehensive Survey of Multilingual Neural Machine Translation</h2>
Title: [A Comprehensive Survey of Multilingual Neural Machine Translation](https://arxiv.org/abs/2001.01115)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A)

*(Submitted on 4 Jan 2020)*

> We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues and challenges. Wherever possible we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions that MNMT research might take. This paper is aimed towards both, beginners and experts in NMT. We hope this paper will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.

| Comments: | This is an extended version of our survey paper on multilingual NMT. The previous version [[arXiv:1905.05395](https://arxiv.org/abs/1905.05395)] is rather condensed and is useful for speed-reading whereas this version is more beginner friendly. Under review at the computing surveys journal. We have intentionally decided to maintain both short and long versions of our survey paper for different reader groups |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.01115](https://arxiv.org/abs/2001.01115) [cs.CL] |
|           | (or [arXiv:2001.01115v1](https://arxiv.org/abs/2001.01115v1) [cs.CL] for this version) |





<h2 id="2020-01-07-2">2. Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation</h2>
Title: [Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation](https://arxiv.org/abs/2001.01589)

Authors: [Yirong Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Xiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yating Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Rui Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+R)

*(Submitted on 2 Jan 2020)*

> Neural machine translation (NMT) has achieved impressive performance on machine translation task in recent years. However, in consideration of efficiency, a limited-size vocabulary that only contains the top-N highest frequency words are employed for model training, which leads to many rare and unknown words. It is rather difficult when translating from the low-resource and morphologically-rich agglutinative languages, which have complex morphology and large vocabulary. In this paper, we propose a morphological word segmentation method on the source-side for NMT that incorporates morphology knowledge to preserve the linguistic and semantic information in the word structure while reducing the vocabulary size at training time. It can be utilized as a preprocessing tool to segment the words in agglutinative languages for other natural language processing (NLP) tasks. Experimental results show that our morphologically motivated word segmentation method is better suitable for the NMT model, which achieves significant improvements on Turkish-English and Uyghur-Chinese machine translation tasks on account of reducing data sparseness and language complexity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.01589](https://arxiv.org/abs/2001.01589) [cs.CL] |
|           | (or [arXiv:2001.01589v1](https://arxiv.org/abs/2001.01589v1) [cs.CL] for this version) |





<h2 id="2020-01-07-3">3. Exploring Benefits of Transfer Learning in Neural Machine Translation</h2>
Title: [Exploring Benefits of Transfer Learning in Neural Machine Translation](https://arxiv.org/abs/2001.01622)

Authors: [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T)

*(Submitted on 6 Jan 2020)*

> Neural machine translation is known to require large numbers of parallel training sentences, which generally prevent it from excelling on low-resource language pairs. This thesis explores the use of cross-lingual transfer learning on neural networks as a way of solving the problem with the lack of resources. We propose several transfer learning approaches to reuse a model pretrained on a high-resource language pair. We pay particular attention to the simplicity of the techniques. We study two scenarios: (a) when we reuse the high-resource model without any prior modifications to its training process and (b) when we can prepare the first-stage high-resource model for transfer learning in advance. For the former scenario, we present a proof-of-concept method by reusing a model trained by other researchers. In the latter scenario, we present a method which reaches even larger improvements in translation performance. Apart from proposed techniques, we focus on an in-depth analysis of transfer learning techniques and try to shed some light on transfer learning improvements. We show how our techniques address specific problems of low-resource languages and are suitable even in high-resource transfer learning. We evaluate the potential drawbacks and behavior by studying transfer learning in various situations, for example, under artificially damaged training corpora, or with fixed various model parts.

| Comments: | Defended PhD thesis                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | [arXiv:2001.01622](https://arxiv.org/abs/2001.01622) [cs.CL] |
|           | (or [arXiv:2001.01622v1](https://arxiv.org/abs/2001.01622v1) [cs.CL] for this version) |





# 2020-01-06

[Return to Index](#Index)



<h2 id="2020-01-06-1">1. Learning Accurate Integer Transformer Machine-Translation Models</h2>
Title: [Learning Accurate Integer Transformer Machine-Translation Models](https://arxiv.org/abs/2001.00926)

Authors: [Ephrem Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+E)

*(Submitted on 3 Jan 2020)*

> We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3% to 100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.00926](https://arxiv.org/abs/2001.00926) [cs.LG] |
|           | (or [arXiv:2001.00926v1](https://arxiv.org/abs/2001.00926v1) [cs.LG] for this version) |





# 2020-01-03

[Return to Index](#Index)




<h2 id="2020-01-03-1">1. A Voice Interactive Multilingual Student Support System using IBM Watson</h2>
Title: [A Voice Interactive Multilingual Student Support System using IBM Watson](https://arxiv.org/abs/2001.00471)

Authors: [Kennedy Ralston](https://arxiv.org/search/cs?searchtype=author&query=Ralston%2C+K), [Yuhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Haruna Isah](https://arxiv.org/search/cs?searchtype=author&query=Isah%2C+H), [Farhana Zulkernine](https://arxiv.org/search/cs?searchtype=author&query=Zulkernine%2C+F)

(Submitted on 20 Dec 2019)

> Systems powered by artificial intelligence are being developed to be more user-friendly by communicating with users in a progressively human-like conversational way. Chatbots, also known as dialogue systems, interactive conversational agents, or virtual agents are an example of such systems used in a wide variety of applications ranging from customer support in the business domain to companionship in the healthcare sector. It is becoming increasingly important to develop chatbots that can best respond to the personalized needs of their users so that they can be as helpful to the user as possible in a real human way. This paper investigates and compares three popular existing chatbots API offerings and then propose and develop a voice interactive and multilingual chatbot that can effectively respond to users mood, tone, and language using IBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was evaluated using a use case that was targeted at responding to users needs regarding exam stress based on university students survey data generated using Google Forms. The results of measuring the chatbot effectiveness at analyzing responses regarding exam stress indicate that the chatbot responding appropriately to the user queries regarding how they are feeling about exams 76.5%. The chatbot could also be adapted for use in other application areas such as student info-centers, government kiosks, and mental health support systems.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| Cite as:  | [arXiv:2001.00471](https://arxiv.org/abs/2001.00471) [cs.HC] |
|           | (or [arXiv:2001.00471v1](https://arxiv.org/abs/2001.00471v1) [cs.HC] for this version) |



# 2020-01-01

[Return to Index](#Index)



<h2 id="2020-01-01-1">1. TextScanner: Reading Characters in Order for Robust Scene Text Recognition</h2>
Title: [TextScanner: Reading Characters in Order for Robust Scene Text Recognition](https://arxiv.org/abs/1912.12422)

Authors: [Zhaoyi Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Z), [Mingling He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+M), [Haoran Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Xiang Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X), [Cong Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+C)

*(Submitted on 28 Dec 2019)*

> Driven by deep learning and the large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention based methods have dominated this field, but suffer from the problem of \textit{attention drift} in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such Chinese transcripts and aligning with target characters.

| Comments: | Accepted by AAAI-2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.12422](https://arxiv.org/abs/1912.12422) [cs.CV] |
|           | (or [arXiv:1912.12422v1](https://arxiv.org/abs/1912.12422v1) [cs.CV] for this version) |





<h2 id="2020-01-01-2">2. Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning</h2>
Title: [Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning](https://arxiv.org/abs/1912.13080)

Authors: [Sean MacAvaney](https://arxiv.org/search/cs?searchtype=author&query=MacAvaney%2C+S), [Luca Soldaini](https://arxiv.org/search/cs?searchtype=author&query=Soldaini%2C+L), [Nazli Goharian](https://arxiv.org/search/cs?searchtype=author&query=Goharian%2C+N)

*(Submitted on 30 Dec 2019)*

> While billions of non-English speaking users rely on search engines every day, the problem of ad-hoc information retrieval is rarely studied for non-English languages. This is primarily due to a lack of data set that are suitable to train ranking algorithms. In this paper, we tackle the lack of data by leveraging pre-trained multilingual language models to transfer a retrieval system trained on English collections to non-English queries and documents. Our model is evaluated in a zero-shot setting, meaning that we use them to predict relevance scores for query-document pairs in languages never seen during training. Our results show that the proposed approach can significantly outperform unsupervised retrieval techniques for Arabic, Chinese Mandarin, and Spanish. We also show that augmenting the English training collection with some examples from the target language can sometimes improve performance.

| Comments: | ECIR 2020 (short)                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.13080](https://arxiv.org/abs/1912.13080) [cs.IR] |
|           | (or [arXiv:1912.13080v1](https://arxiv.org/abs/1912.13080v1) [cs.IR] for this version) |





<h2 id="2020-01-01-3">3. Robust Cross-lingual Embeddings from Parallel Sentences</h2>
Title: [Robust Cross-lingual Embeddings from Parallel Sentences](https://arxiv.org/abs/1912.12481)

Authors: [Ali Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+A), [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Jean-Baptiste Cordonnier](https://arxiv.org/search/cs?searchtype=author&query=Cordonnier%2C+J), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R), [Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&query=Jaggi%2C+M)

*(Submitted on 28 Dec 2019)*

> Recent advances in cross-lingual word embeddings have primarily relied on mapping-based methods, which project pretrained word embeddings from different languages into a shared space through a linear transformation. However, these approaches assume word embedding spaces are isomorphic between different languages, which has been shown not to hold in practice (Søgaard et al., 2018), and fundamentally limits their performance. This motivates investigating joint learning methods which can overcome this impediment, by simultaneously learning embeddings across languages via a cross-lingual term in the training objective. Given the abundance of parallel data available (Tiedemann, 2012), we propose a bilingual extension of the CBOW method which leverages sentence-aligned corpora to obtain robust cross-lingual word and sentence representations. Our approach significantly improves cross-lingual sentence retrieval performance over all other approaches, as well as convincingly outscores mapping methods while maintaining parity with jointly trained methods on word-translation. It also achieves parity with a deep RNN method on a zero-shot cross-lingual document classification task, requiring far fewer computational resources for training and inference. As an additional advantage, our bilingual method also improves the quality of monolingual word vectors despite training on much smaller datasets. We make our code and models publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.12481](https://arxiv.org/abs/1912.12481) [cs.CL] |
|           | (or [arXiv:1912.12481v1](https://arxiv.org/abs/1912.12481v1) [cs.CL] for this version) |





<h2 id="2020-01-01-4">4. "Hinglish" Language -- Modeling a Messy Code-Mixed Language</h2>
Title: ["Hinglish" Language -- Modeling a Messy Code-Mixed Language](https://arxiv.org/abs/1912.13109)

Authors: [Vivek Kumar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+V+K)

*(Submitted on 30 Dec 2019)*

> With a sharp rise in fluency and users of "Hinglish" in linguistically diverse country, India, it has increasingly become important to analyze social content written in this language in platforms such as Twitter, Reddit, Facebook. This project focuses on using deep learning techniques to tackle a classification problem in categorizing social content written in Hindi-English into Abusive, Hate-Inducing and Not offensive categories. We utilize bi-directional sequence models with easy text augmentation techniques such as synonym replacement, random insertion, random swap, and random deletion to produce a state of the art classifier that outperforms the previous work done on analyzing this dataset.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.13109](https://arxiv.org/abs/1912.13109) [cs.CL] |
|           | (or [arXiv:1912.13109v1](https://arxiv.org/abs/1912.13109v1) [cs.CL] for this version) |





<h2 id="2020-01-01-5">5. Amharic-Arabic Neural Machine Translation</h2>
Title: [Amharic-Arabic Neural Machine Translation](https://arxiv.org/abs/1912.13161)

Authors: [Ibrahim Gashaw](https://arxiv.org/search/cs?searchtype=author&query=Gashaw%2C+I), [H L Shashirekha](https://arxiv.org/search/cs?searchtype=author&query=Shashirekha%2C+H+L)

*(Submitted on 26 Dec 2019)*

> Many automatic translation works have been addressed between major European language pairs, by taking advantage of large scale parallel corpora, but very few research works are conducted on the Amharic-Arabic language pair due to its parallel data scarcity. Two Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) based Neural Machine Translation (NMT) models are developed using Attention-based Encoder-Decoder architecture which is adapted from the open-source OpenNMT system. In order to perform the experiment, a small parallel Quranic text corpus is constructed by modifying the existing monolingual Arabic text and its equivalent translation of Amharic language text corpora available on Tanzile. LSTM and GRU based NMT models and Google Translation system are compared and found that LSTM based OpenNMT outperforms GRU based OpenNMT and Google Translation system, with a BLEU score of 12%, 11%, and 6% respectively.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.13161](https://arxiv.org/abs/1912.13161) [cs.CL] |
|           | (or [arXiv:1912.13161v1](https://arxiv.org/abs/1912.13161v1) [cs.CL] for this version) |





<h2 id="2020-01-01-6">6. LayoutLM: Pre-training of Text and Layout for Document Image Understanding</h2>
Title: [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)

Authors: [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Minghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 31 Dec 2019)*

> Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the wide spread of pre-training models for NLP applications, they almost focused on text-level manipulation, while neglecting the layout and style information that is vital for document image understanding. In this paper, we propose \textbf{LayoutLM} to jointly model the interaction between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. We also leverage the image features to incorporate the style information of words in LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training, leading to significant performance improvement in downstream tasks for document image understanding.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.13318](https://arxiv.org/abs/1912.13318) [cs.CL] |
|           | (or [arXiv:1912.13318v1](https://arxiv.org/abs/1912.13318v1) [cs.CL] for this version) |



