# Daily arXiv: Machine Translation - Mar., 2020

# Index

- [2020-03-31](#2020-03-31)
  - [1. Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin](#2020-03-31-1)
  - [2. Learning Contextualized Sentence Representations for Document-Level Neural Machine Translation](#2020-03-31-2)
  - [3. Investigating Language Impact in Bilingual Approaches for Computational Language Documentation](#2020-03-31-3)
- [2020-03-30](#2020-03-20)
  - [1. FFR V1.0: Fon-French Neural Machine Translation](#2020-03-30-1)
- [2020-03-27](#2020-03-27)
  - [1. Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks](#2020-03-27-1)
- [2020-03-26](#2020-03-26)
  - [1. XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization](#2020-03-26-1)
  - [2. Joint Multiclass Debiasing of Word Embeddings](#2020-03-26-2)
  - [3. Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response](#2020-03-26-3)
  - [4. Masakhane -- Machine Translation For Africa](#2020-03-26-4)
  - [5. Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving Adversarial Text Generation](#2020-03-26-5)
- [2020-03-25](#2020-03-25)
  - [1. Towards Neural Machine Translation for Edoid Languages](#2020-03-25-1)
  - [2. Cross-Lingual Adaptation Using Universal Dependencies](#2020-03-25-2)
- [2020-03-24](#2020-03-24)
  - [1. Analyzing Word Translation of Transformer Layers](#2020-03-24-1)
  - [2. Generating Natural Language Adversarial Examples on a Large Scale with Generative Models](#2020-03-24-2)
- [2020-03-20](#2020-03-20)
  - [1. Utilizing Language Relatedness to improve Machine Translation: A Case Study on Languages of the Indian Subcontinent](#2020-03-20-1)
- [2020-03-19](#2020-03-19)
  - [1. Calibration of Pre-trained Transformers](#2020-03-19-1)
  - [2. Pre-trained Models for Natural Language Processing: A Survey](#2020-03-19-2)
- [2020-03-18](#2020-03-18)
  - [1. Rethinking Batch Normalization in Transformers](#2020-03-18-1)
- [2020-03-17](#2020-03-17)
  - [1. A Survey on Contextual Embeddings](#2020-03-17-1)
  - [2. Synonymous Generalization in Sequence-to-Sequence Recurrent Networks](#2020-03-17-2)
  - [3. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages](#2020-03-17-3)
- [2020-03-16](#2020-03-16)
  - [1. Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks](#2020-03-16-1)
- [2020-03-12](#2020-03-12)
  - [1. Visual Grounding in Video for Unsupervised Word Translation](2020-03-12-1)
  - [2. Transformer++](2020-03-12-2)
- [2020-03-11](#2020-03-11)
  - [1. Combining Pretrained High-Resource Embeddings and Subword Representations for Low-Resource Languages](#2020-03-11-1)
- [2020-03-10](#2020-03-10)
  - [1. Discovering linguistic (ir)regularities in word embeddings through max-margin separating hyperplanes](#2020-03-10-1)
- [2020-03-09](#2020-03-09)
  - [1. Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation](#2020-03-09-1)
  - [2. What the [MASK]? Making Sense of Language-Specific BERT Models](#2020-03-09-2)
  - [3. Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach](#2020-03-09-3)
- [2020-03-06](#2020-03-06)
  - [1. Phase transitions in a decentralized graph-based approach to human language](#2020-03-06-1)
  - [2. BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward](#2020-03-06-2)
  - [3. Zero-Shot Cross-Lingual Transfer with Meta Learning](#2020-03-06-3)
  - [4. An Empirical Accuracy Law for Sequential Machine Translation: the Case of Google Translate](#2020-03-06-4)
- [2020-03-05](#2020-03-05)
  - [1. Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation](#2020-03-05-1)
- [2020-03-04](#2020-03-04)
  - [1. Understanding the Prediction Mechanism of Sentiments by XAI Visualization](#2020-03-04-1)
  - [2. XGPT: Cross-modal Generative Pre-Training for Image Captioning](#2020-03-04-2)
- [2020-03-02](#2020-03-02)
  - [1. Robust Unsupervised Neural Machine Translation with Adversarial Training](#2020-03-02-1)
  - [2. Modeling Future Cost for Neural Machine Translation](#2020-03-02-2)
  - [3. TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](#2020-03-02-3)
  - [4. Do all Roads Lead to Rome? Understanding the Role of Initialization in Iterative Back-Translation](#2020-03-02-4)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-03-31

[Return to Index](#Index)



<h2 id="2020-03-31-1">1. Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin</h2>

Title: [Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin](https://arxiv.org/abs/2003.12660)

Authors: [Orevaoghene Ahia](https://arxiv.org/search/cs?searchtype=author&query=Ahia%2C+O), [Kelechi Ogueji](https://arxiv.org/search/cs?searchtype=author&query=Ogueji%2C+K)

*(Submitted on 27 Mar 2020)*

> Nigerian Pidgin is arguably the most widely spoken language in Nigeria. Variants of this language are also spoken across West and Central Africa, making it a very important language. This work aims to establish supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. We implement and compare NMT models with different tokenization methods, creating a solid foundation for future works.

| Comments: | Accepted for the AfricaNLP Workshop, ICLR 2020               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.12660](https://arxiv.org/abs/2003.12660) [cs.CL] |
|           | (or [arXiv:2003.12660v1](https://arxiv.org/abs/2003.12660v1) [cs.CL] for this version) |







<h2 id="2020-03-31-2">2. Learning Contextualized Sentence Representations for Document-Level Neural Machine Translation</h2>

Title: [Learning Contextualized Sentence Representations for Document-Level Neural Machine Translation](https://arxiv.org/abs/2003.13205)

Authors: [Pei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Xu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Wei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Yanfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

*(Submitted on 30 Mar 2020)*

> Document-level machine translation incorporates inter-sentential dependencies into the translation of a source sentence. In this paper, we propose a new framework to model cross-sentence dependencies by training neural machine translation (NMT) to predict both the target translation and surrounding sentences of a source sentence. By enforcing the NMT model to predict source context, we want the model to learn "contextualized" source sentence representations that capture document-level dependencies on the source side. We further propose two different methods to learn and integrate such contextualized sentence embeddings into NMT: a joint training method that jointly trains an NMT model with the source context prediction model and a pre-training & fine-tuning method that pretrains the source context prediction model on a large-scale monolingual document corpus and then fine-tunes it with the NMT model. Experiments on Chinese-English and English-German translation show that both methods can substantially improve the translation quality over a strong document-level Transformer baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.13205](https://arxiv.org/abs/2003.13205) [cs.CL] |
|           | (or [arXiv:2003.13205v1](https://arxiv.org/abs/2003.13205v1) [cs.CL] for this version) |





<h2 id="2020-03-31-3">3. Investigating Language Impact in Bilingual Approaches for Computational Language Documentation</h2>

Title: [Investigating Language Impact in Bilingual Approaches for Computational Language Documentation](https://arxiv.org/abs/2003.13325)

Authors: [Marcely Zanon Boito](https://arxiv.org/search/cs?searchtype=author&query=Boito%2C+M+Z), [Aline Villavicencio](https://arxiv.org/search/cs?searchtype=author&query=Villavicencio%2C+A), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 30 Mar 2020)*

> For endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly. Therefore, it is fundamental to translate them into a widely spoken language to ensure interpretability of the recordings. In this paper we investigate how the choice of translation language affects the posterior documentation work and potential automatic approaches which will work on top of the produced bilingual corpus. For answering this question, we use the MaSS multilingual speech corpus (Boito et al., 2020) for creating 56 bilingual pairs that we apply to the task of low-resource unsupervised word segmentation and alignment. Our results highlight that the choice of language for translation influences the word segmentation performance, and that different lexicons are learned by using different aligned translations. Lastly, this paper proposes a hybrid approach for bilingual word segmentation, combining boundary clues extracted from a non-parametric Bayesian model (Goldwater et al., 2009a) with the attentional word segmentation neural model from Godard et al. (2018). Our results suggest that incorporating these clues into the neural models' input representation increases their translation and alignment quality, specially for challenging language pairs.

| Comments: | Accepted to 1st Joint SLTU and CCURL Workshop                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.13325](https://arxiv.org/abs/2003.13325) [cs.CL] |
|           | (or [arXiv:2003.13325v1](https://arxiv.org/abs/2003.13325v1) [cs.CL] for this version) |





# 2020-03-30

[Return to Index](#Index)



<h2 id="2020-03-30-1">1. FFR V1.0: Fon-French Neural Machine Translation</h2>

Title: [FFR V1.0: Fon-French Neural Machine Translation](https://arxiv.org/abs/2003.12111)

Authors: [Bonaventure F. P. Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B+F+P), [Chris C. Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C+C)

*(Submitted on 26 Mar 2020)*

> Africa has the highest linguistic diversity in the world. On account of the importance of language to communication, and the importance of reliable, powerful and accurate machine translation models in modern inter-cultural communication, there have been (and still are) efforts to create state-of-the-art translation models for the many African languages. However, the low-resources, diacritical and tonal complexities of African languages are major issues facing African NLP today. The FFR is a major step towards creating a robust translation model from Fon, a very low-resource and tonal language, to French, for research and public use. In this paper, we describe our pilot project: the creation of a large growing corpora for Fon-to-French translations and our FFR v1.0 model, trained on this dataset. The dataset and model are made publicly available.

| Comments: | Accepted for the AfricaNLP Workshop, ICLR 2020               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.12111](https://arxiv.org/abs/2003.12111) [cs.CL] |
|           | (or [arXiv:2003.12111v1](https://arxiv.org/abs/2003.12111v1) [cs.CL] for this version) |







# 2020-03-27

[Return to Index](#Index)



<h2 id="2020-03-27-1">1. Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks</h2>

Title: [Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks](https://arxiv.org/abs/2003.11645)

Authors: [Tosin P. Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T+P), [Foteini Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F), [Marcus Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M)

*(Submitted on 23 Mar 2020)*

> Word2Vec is a prominent tool for Natural Language Processing (NLP) tasks. Similar inspiration is found in distributed embeddings for state-of-the-art (sota) deep neural networks. However, wrong combination of hyper-parameters can produce poor quality vectors. The objective of this work is to show optimal combination of hyper-parameters exists and evaluate various combinations. We compare them with the original model released by Mikolov. Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data. Increasing vector dimension size after a point leads to poor quality or performance. If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases. Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to Mikolov's model, trained on 100 billion word corpus.

| Comments: | 12 pages, 7 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2003.11645](https://arxiv.org/abs/2003.11645) [cs.CL] |
|           | (or [arXiv:2003.11645v1](https://arxiv.org/abs/2003.11645v1) [cs.CL] for this version) |





# 2020-03-26

[Return to Index](#Index)



<h2 id="2020-03-26-1">1. XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</h2>

Title: [XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization](https://arxiv.org/abs/2003.11080)

Authors: [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M)

*(Submitted on 24 Mar 2020)*

> Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.11080](https://arxiv.org/abs/2003.11080) [cs.CL] |
|           | (or [arXiv:2003.11080v1](https://arxiv.org/abs/2003.11080v1) [cs.CL] for this version) |





<h2 id="2020-03-26-2">2. Joint Multiclass Debiasing of Word Embeddings</h2>

Title: [Joint Multiclass Debiasing of Word Embeddings](https://arxiv.org/abs/2003.11520)

Authors: [Radomir Popović](https://arxiv.org/search/cs?searchtype=author&query=Popović%2C+R), [Florian Lemmerich](https://arxiv.org/search/cs?searchtype=author&query=Lemmerich%2C+F), [Markus Strohmaier](https://arxiv.org/search/cs?searchtype=author&query=Strohmaier%2C+M)

*(Submitted on 9 Mar 2020)*

> Bias in Word Embeddings has been a subject of recent interest, along with efforts for its reduction. Current approaches show promising progress towards debiasing single bias dimensions such as gender or race. In this paper, we present a joint multiclass debiasing approach that is capable of debiasing multiple bias dimensions simultaneously. In that direction, we present two approaches, HardWEAT and SoftWEAT, that aim to reduce biases by minimizing the scores of the Word Embeddings Association Test (WEAT). We demonstrate the viability of our methods by debiasing Word Embeddings on three classes of biases (religion, gender and race) in three different publicly available word embeddings and show that our concepts can both reduce or even completely eliminate bias, while maintaining meaningful relationships between vectors in word embeddings. Our work strengthens the foundation for more unbiased neural representations of textual data.

| Comments: | 10 pages, 2 figures. To appear in the Proceedings of the 25th International Symposium on Intelligent Systems (ISMIS 2020), May 2020, Graz, Austria. Online appendix available at: [this https URL](https://git.io/JvL10) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2003.11520](https://arxiv.org/abs/2003.11520) [cs.CL] |
|           | (or [arXiv:2003.11520v1](https://arxiv.org/abs/2003.11520v1) [cs.CL] for this version) |





<h2 id="2020-03-26-3">3. Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response</h2>

Title: [Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response](https://arxiv.org/abs/2003.11523)

Authors: [Alp Öktem](https://arxiv.org/search/cs?searchtype=author&query=Öktem%2C+A), [Mirko Plitt](https://arxiv.org/search/cs?searchtype=author&query=Plitt%2C+M), [Grace Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+G)

*(Submitted on 9 Mar 2020)*

> We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.

| Comments: | Pre-print accepted to Africa NLP workshop organized within Eighth International Conference on Learning Representations (ICLR 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.11523](https://arxiv.org/abs/2003.11523) [cs.CL] |
|           | (or [arXiv:2003.11523v1](https://arxiv.org/abs/2003.11523v1) [cs.CL] for this version) |





<h2 id="2020-03-26-4">4. Masakhane -- Machine Translation For Africa</h2>

Title: [Masakhane -- Machine Translation For Africa](https://arxiv.org/abs/2003.11529)

Authors: [Iroro Orife](https://arxiv.org/search/cs?searchtype=author&query=Orife%2C+I), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [Blessing Sibanda](https://arxiv.org/search/cs?searchtype=author&query=Sibanda%2C+B), [Daniel Whitenack](https://arxiv.org/search/cs?searchtype=author&query=Whitenack%2C+D), [Kathleen Siminyu](https://arxiv.org/search/cs?searchtype=author&query=Siminyu%2C+K), [Laura Martinus](https://arxiv.org/search/cs?searchtype=author&query=Martinus%2C+L), [Jamiil Toure Ali](https://arxiv.org/search/cs?searchtype=author&query=Ali%2C+J+T), [Jade Abbott](https://arxiv.org/search/cs?searchtype=author&query=Abbott%2C+J), [Vukosi Marivate](https://arxiv.org/search/cs?searchtype=author&query=Marivate%2C+V), [Salomon Kabongo](https://arxiv.org/search/cs?searchtype=author&query=Kabongo%2C+S), [Musie Meressa](https://arxiv.org/search/cs?searchtype=author&query=Meressa%2C+M), [Espoir Murhabazi](https://arxiv.org/search/cs?searchtype=author&query=Murhabazi%2C+E), [Orevaoghene Ahia](https://arxiv.org/search/cs?searchtype=author&query=Ahia%2C+O), [Elan van Biljon](https://arxiv.org/search/cs?searchtype=author&query=van+Biljon%2C+E), [Arshath Ramkilowan](https://arxiv.org/search/cs?searchtype=author&query=Ramkilowan%2C+A), [Adewale Akinfaderin](https://arxiv.org/search/cs?searchtype=author&query=Akinfaderin%2C+A), [Alp Öktem](https://arxiv.org/search/cs?searchtype=author&query=Öktem%2C+A), [Wole Akin](https://arxiv.org/search/cs?searchtype=author&query=Akin%2C+W), [Ghollah Kioko](https://arxiv.org/search/cs?searchtype=author&query=Kioko%2C+G), [Kevin Degila](https://arxiv.org/search/cs?searchtype=author&query=Degila%2C+K), [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Bonaventure Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B), [Chris Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C), [Kelechi Ogueji](https://arxiv.org/search/cs?searchtype=author&query=Ogueji%2C+K), [Abdallah Bashir](https://arxiv.org/search/cs?searchtype=author&query=Bashir%2C+A)

*(Submitted on 13 Mar 2020)*

> Africa has over 2000 languages. Despite this, African languages account for a small portion of available resources and publications in Natural Language Processing (NLP). This is due to multiple factors, including: a lack of focus from government and funding, discoverability, a lack of community, sheer language complexity, difficulty in reproducing papers and no benchmarks to compare techniques. To begin to address the identified problems, MASAKHANE, an open-source, continent-wide, distributed, online research effort for machine translation for African languages, was founded. In this paper, we discuss our methodology for building the community and spurring research from the African continent, as well as outline the success of the community in terms of addressing the identified problems affecting African NLP.

| Comments: | Accepted for the AfricaNLP Workshop, ICLR 2020               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.11529](https://arxiv.org/abs/2003.11529) [cs.CL] |
|           | (or [arXiv:2003.11529v1](https://arxiv.org/abs/2003.11529v1) [cs.CL] for this version) |





<h2 id="2020-03-26-5">5. Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving Adversarial Text Generation</h2>

Title: [Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving Adversarial Text Generation](https://arxiv.org/abs/2003.11530)

Authors: [Haiyan Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+H), [Dingcheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Xu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Ping Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P)

*(Submitted on 12 Mar 2020)*

> Training generative models that can generate high-quality text with sufficient diversity is an important open problem for Natural Language Generation (NLG) community. Recently, generative adversarial models have been applied extensively on text generation tasks, where the adversarially trained generators alleviate the exposure bias experienced by conventional maximum likelihood approaches and result in promising generation quality. However, due to the notorious defect of mode collapse for adversarial training, the adversarially trained generators face a quality-diversity trade-off, i.e., the generator models tend to sacrifice generation diversity severely for increasing generation quality. In this paper, we propose a novel approach which aims to improve the performance of adversarial text generation via efficiently decelerating mode collapse of the adversarial training. To this end, we introduce a cooperative training paradigm, where a language model is cooperatively trained with the generator and we utilize the language model to efficiently shape the data distribution of the generator against mode collapse. Moreover, instead of engaging the cooperative update for the generator in a principled way, we formulate a meta learning mechanism, where the cooperative update to the generator serves as a high level meta task, with an intuition of ensuring the parameters of the generator after the adversarial update would stay resistant against mode collapse. In the experiment, we demonstrate our proposed approach can efficiently slow down the pace of mode collapse for the adversarial text generators. Overall, our proposed method is able to outperform the baseline approaches with significant margins in terms of both generation quality and diversity in the testified domains.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.11530](https://arxiv.org/abs/2003.11530) [cs.CL] |
|           | (or [arXiv:2003.11530v1](https://arxiv.org/abs/2003.11530v1) [cs.CL] for this version) |







# 2020-03-25

[Return to Index](#Index)



<h2 id="2020-03-25-1">1. Towards Neural Machine Translation for Edoid Languages</h2>

Title: [Towards Neural Machine Translation for Edoid Languages](https://arxiv.org/abs/2003.10704)

Authors: [Iroro Orife](https://arxiv.org/search/cs?searchtype=author&query=Orife%2C+I)

*(Submitted on 24 Mar 2020)*

> Many Nigerian languages have relinquished their previous prestige and purpose in modern society to English and Nigerian Pidgin. For the millions of L1 speakers of indigenous languages, there are inequalities that manifest themselves as unequal access to information, communications, health care, security as well as attenuated participation in political and civic life. To minimize exclusion and promote socio-linguistic and economic empowerment, this work explores the feasibility of Neural Machine Translation (NMT) for the Edoid language family of Southern Nigeria. Using the new JW300 public dataset, we trained and evaluated baseline translation models for four widely spoken languages in this group: Èdó, Ésán, Urhobo and Isoko. Trained models, code and datasets have been open-sourced to advance future research efforts on Edoid language technology.

| Comments: | Accepted to ICLR 2020 AfricaNLP workshop                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.10704](https://arxiv.org/abs/2003.10704) [cs.CL] |
|           | (or [arXiv:2003.10704v1](https://arxiv.org/abs/2003.10704v1) [cs.CL] for this version) |





<h2 id="2020-03-25-2">2. Cross-Lingual Adaptation Using Universal Dependencies</h2>

Title: [Cross-Lingual Adaptation Using Universal Dependencies](https://arxiv.org/abs/2003.10816)

Authors: [Nasrin Taghizadeh](https://arxiv.org/search/cs?searchtype=author&query=Taghizadeh%2C+N), [Heshaam Faili](https://arxiv.org/search/cs?searchtype=author&query=Faili%2C+H)

*(Submitted on 24 Mar 2020)*

> We describe a cross-lingual adaptation method based on syntactic parse trees obtained from the Universal Dependencies (UD), which are consistent across languages, to develop classifiers in low-resource languages. The idea of UD parsing is to capture similarities as well as idiosyncrasies among typologically different languages. In this paper, we show that models trained using UD parse trees for complex NLP tasks can characterize very different languages. We study two tasks of paraphrase identification and semantic relation extraction as case studies. Based on UD parse trees, we develop several models using tree kernels and show that these models trained on the English dataset can correctly classify data of other languages e.g. French, Farsi, and Arabic. The proposed approach opens up avenues for exploiting UD parsing in solving similar cross-lingual tasks, which is very useful for languages that no labeled data is available for them.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.10816](https://arxiv.org/abs/2003.10816) [cs.CL] |
|           | (or [arXiv:2003.10816v1](https://arxiv.org/abs/2003.10816v1) [cs.CL] for this version) |





# 2020-03-24

[Return to Index](#Index)



<h2 id="2020-03-24-1">1. Analyzing Word Translation of Transformer Layers</h2>

Title: [Analyzing Word Translation of Transformer Layers](https://arxiv.org/abs/2003.09586)

Authors: [Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D), [Qiuhui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 21 Mar 2020)*

> The Transformer translation model is popular for its effective parallelization and performance. Though a wide range of analysis about the Transformer has been conducted recently, the role of each Transformer layer in translation has not been studied to our knowledge. In this paper, we propose approaches to analyze the translation performed in encoder / decoder layers of the Transformer. Our approaches in general project the representations of an analyzed layer to the pre-trained classifier and measure the word translation accuracy. For the analysis of encoder layers, our approach additionally learns a weight vector to merge multiple attention matrices into one and transform the source encoding to the target side with the merged alignment matrix to align source tokens with target translations while bridging different input - output lengths. While analyzing decoder layers, we additionally study the effects of the source context and the decoding history in word prediction through bypassing the corresponding self-attention or cross-attention sub-layers. Our analysis reveals that the translation starts at the very beginning of the "encoding" (specifically at the source word embedding layer), and shows how translation evolves during the forward computation of layers. Based on observations gained in our analysis, we propose that increasing encoder depth while removing the same number of decoder layers can simply but significantly boost the decoding speed. Furthermore, simply inserting a linear projection layer before the decoder classifier which shares the weight matrix with the embedding layer can effectively provide small but consistent and significant improvements in our experiments on the WMT 14 English-German, English-French and WMT 15 Czech-English translation tasks (+0.42, +0.37 and +0.47 respectively).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.09586](https://arxiv.org/abs/2003.09586) [cs.CL] |
|           | (or [arXiv:2003.09586v1](https://arxiv.org/abs/2003.09586v1) [cs.CL] for this version) |



<h2 id="2020-03-24-2">2. Generating Natural Language Adversarial Examples on a Large Scale with Generative Models</h2>

Title: [Generating Natural Language Adversarial Examples on a Large Scale with Generative Models](https://arxiv.org/abs/2003.10388)

Authors: [Yankun Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y), [Jianbin Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Siliang Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+S), [Jun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Shuang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Yuan Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

*(Submitted on 10 Mar 2020)*

> Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.10388](https://arxiv.org/abs/2003.10388) [cs.CL] |
|           | (or [arXiv:2003.10388v1](https://arxiv.org/abs/2003.10388v1) [cs.CL] for this version) |







# 2020-03-20

[Return to Index](#Index)



<h2 id="2020-03-20-1">1. Utilizing Language Relatedness to improve Machine Translation: A Case Study on Languages of the Indian Subcontinent</h2>

Title: [Utilizing Language Relatedness to improve Machine Translation: A Case Study on Languages of the Indian Subcontinent](https://arxiv.org/abs/2003.08925)

Authors: [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

*(Submitted on 19 Mar 2020)*

> In this work, we present an extensive study of statistical machine translation involving languages of the Indian subcontinent. These languages are related by genetic and contact relationships. We describe the similarities between Indic languages arising from these relationships. We explore how lexical and orthographic similarity among these languages can be utilized to improve translation quality between Indic languages when limited parallel corpora is available. We also explore how the structural correspondence between Indic languages can be utilized to re-use linguistic resources for English to Indic language translation. Our observations span 90 language pairs from 9 Indic languages and English. To the best of our knowledge, this is the first large-scale study specifically devoted to utilizing language relatedness to improve translation between related languages.

| Comments: | This work was done in 2017-2018 as part of the first author's thesis |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.08925](https://arxiv.org/abs/2003.08925) [cs.CL] |
|           | (or [arXiv:2003.08925v1](https://arxiv.org/abs/2003.08925v1) [cs.CL] for this version) |





# 2020-03-19

[Return to Index](#Index)



<h2 id="2020-03-19-1">1. Calibration of Pre-trained Transformers</h2>

Title: [Calibration of Pre-trained Transformers](https://arxiv.org/abs/2003.07892)

Authors: [Shrey Desai](https://arxiv.org/search/cs?searchtype=author&query=Desai%2C+S), [Greg Durrett](https://arxiv.org/search/cs?searchtype=author&query=Durrett%2C+G)

*(Submitted on 17 Mar 2020)*

> Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.07892](https://arxiv.org/abs/2003.07892) [cs.CL] |
|           | (or [arXiv:2003.07892v1](https://arxiv.org/abs/2003.07892v1) [cs.CL] for this version) |





<h2 id="2020-03-19-2">2. Pre-trained Models for Natural Language Processing: A Survey</h2>

Title: [Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)

Authors: [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Tianxiang Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+T), [Yige Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yunfan Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Y), [Ning Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+N), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

*(Submitted on 18 Mar 2020)*

> Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.

| Comments: | Invited Review of Science China Technological Sciences       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.08271](https://arxiv.org/abs/2003.08271) [cs.CL] |
|           | (or [arXiv:2003.08271v1](https://arxiv.org/abs/2003.08271v1) [cs.CL] for this version) |











# 2020-03-18

[Return to Index](#Index)



<h2 id="2020-03-18-1">1. Rethinking Batch Normalization in Transformers</h2>

Title: [Rethinking Batch Normalization in Transformers](https://arxiv.org/abs/2003.07845)

Authors: [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Zhewei Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Amir Gholami](https://arxiv.org/search/cs?searchtype=author&query=Gholami%2C+A), [Michael Mahoney](https://arxiv.org/search/cs?searchtype=author&query=Mahoney%2C+M), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K)

*(Submitted on 17 Mar 2020)*

> The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.07845](https://arxiv.org/abs/2003.07845) [cs.CL] |
|           | (or [arXiv:2003.07845v1](https://arxiv.org/abs/2003.07845v1) [cs.CL] for this version) |







# 2020-03-17

[Return to Index](#Index)



<h2 id="2020-03-17-1">1. A Survey on Contextual Embeddings</h2>

Title: [A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)

Authors: [Qi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Matt J. Kusner](https://arxiv.org/search/cs?searchtype=author&query=Kusner%2C+M+J), [Phil Blunsom](https://arxiv.org/search/cs?searchtype=author&query=Blunsom%2C+P)

*(Submitted on 16 Mar 2020)*

> Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.07278](https://arxiv.org/abs/2003.07278) [cs.AI] |
|           | (or [arXiv:2003.07278v1](https://arxiv.org/abs/2003.07278v1) [cs.AI] for this version) |





<h2 id="2020-03-17-2">2. Synonymous Generalization in Sequence-to-Sequence Recurrent Networks</h2>

Title: [Synonymous Generalization in Sequence-to-Sequence Recurrent Networks](https://arxiv.org/abs/2003.06658)

Authors: [Ning Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+N)

*(Submitted on 14 Mar 2020)*

> When learning a language, people can quickly expand their understanding of the unknown content by using compositional skills, such as from two words "go" and "fast" to a new phrase "go fast." In recent work of Lake and Baroni (2017), modern Sequence-to-Sequence(se12seq) Recurrent Neural Networks (RNNs) can make powerful zero-shot generalizations in specifically controlled experiments. However, there is a missing regarding the property of such strong generalization and its precise requirements. This paper explores this positive result in detail and defines this pattern as the synonymous generalization, an ability to recognize an unknown sequence by decomposing the difference between it and a known sequence as corresponding existing synonyms. To better investigate it, I introduce a new environment called Colorful Extended Cleanup World (CECW), which consists of complex commands paired with logical expressions. While demonstrating that sequential RNNs can perform synonymous generalizations on foreign commands, I conclude their prerequisites for success. I also propose a data augmentation method, which is successfully verified on the Geoquery (GEO) dataset, as a novel application of synonymous generalization for real cases.

| Comments: | 7 pages, 1 figure, 3 tables                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.06658](https://arxiv.org/abs/2003.06658) [cs.CL] |
|           | (or [arXiv:2003.06658v1](https://arxiv.org/abs/2003.06658v1) [cs.CL] for this version) |





<h2 id="2020-03-17-3">3. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages</h2>

Title: [Stanza: A Python Natural Language Processing Toolkit for Many Human Languages](https://arxiv.org/abs/2003.07082)

Authors: [Peng Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+P), [Yuhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Yuhui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jason Bolton](https://arxiv.org/search/cs?searchtype=author&query=Bolton%2C+J), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

*(Submitted on 16 Mar 2020)*

> We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionalities to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at [this https URL](https://stanfordnlp.github.io/stanza).

| Comments: | First two authors contribute equally. Website: [this https URL](https://stanfordnlp.github.io/stanza) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.07082](https://arxiv.org/abs/2003.07082) [cs.CL] |
|           | (or [arXiv:2003.07082v1](https://arxiv.org/abs/2003.07082v1) [cs.CL] for this version) |







# 2020-03-16

[Return to Index](#Index)



<h2 id="2020-03-16-1">1. Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks</h2>

Title: [Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks](https://arxiv.org/abs/2003.06381)

Authors: [Yu Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+Y), [Serge Sharoff](https://arxiv.org/search/cs?searchtype=author&query=Sharoff%2C+S)

*(Submitted on 13 Mar 2020)*

> This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns of prediction of fine-grained scores for measuring different aspects of translation quality. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.06381](https://arxiv.org/abs/2003.06381) [cs.CL] |
|           | (or [arXiv:2003.06381v1](https://arxiv.org/abs/2003.06381v1) [cs.CL] for this version) |





# 2020-03-12

[Return to Index](#Index)



<h2 id="2020-03-12-1">1. Visual Grounding in Video for Unsupervised Word Translation</h2>

Title: [Visual Grounding in Video for Unsupervised Word Translation](https://arxiv.org/abs/2003.05078)

Authors: [Gunnar A. Sigurdsson](https://arxiv.org/search/cs?searchtype=author&query=Sigurdsson%2C+G+A), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac%2C+J), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh%2C+A), [Lucas Smaira](https://arxiv.org/search/cs?searchtype=author&query=Smaira%2C+L), [Mateusz Malinowski](https://arxiv.org/search/cs?searchtype=author&query=Malinowski%2C+M), [João Carreira](https://arxiv.org/search/cs?searchtype=author&query=Carreira%2C+J), [Phil Blunsom](https://arxiv.org/search/cs?searchtype=author&query=Blunsom%2C+P), [Andrew Zisserman](https://arxiv.org/search/cs?searchtype=author&query=Zisserman%2C+A)

*(Submitted on 11 Mar 2020)*

> There are thousands of actively spoken languages on Earth, but a single visual world. Grounding in this visual world has the potential to bridge the gap between all these languages. Our goal is to use visual grounding to improve unsupervised word mapping between languages. The key idea is to establish a common visual representation between two languages by learning embeddings from unpaired instructional videos narrated in the native language. Given this shared embedding we demonstrate that (i) we can map words between the languages, particularly the 'visual' words; (ii) that the shared embedding provides a good initialization for existing unsupervised text-based word translation techniques, forming the basis for our proposed hybrid visual-text mapping algorithm, MUVE; and (iii) our approach achieves superior performance by addressing the shortcomings of text-based methods -- it is more robust, handles datasets with less commonality, and is applicable to low-resource languages. We apply these methods to translate words from English to French, Korean, and Japanese -- all without any parallel corpora and simply by watching many videos of people speaking while doing things.





<h2 id="2020-03-12-2">2. Transformer++</h2>

Title: [Transformer++](https://arxiv.org/abs/2003.04974)

Authors: [Prakhar Thapak](https://arxiv.org/search/cs?searchtype=author&query=Thapak%2C+P), [Prodip Hore](https://arxiv.org/search/cs?searchtype=author&query=Hore%2C+P)

*(Submitted on 2 Mar 2020)*

> Recent advancements in attention mechanisms have replaced recurrent neural networks and its variants for machine translation tasks. Transformer using attention mechanism solely achieved state-of-the-art results in sequence modeling. Neural machine translation based on the attention mechanism is parallelizable and addresses the problem of handling long-range dependencies among words in sentences more effectively than recurrent neural networks. One of the key concepts in attention is to learn three matrices, query, key, and value, where global dependencies among words are learned through linearly projecting word embeddings through these matrices. Multiple query, key, value matrices can be learned simultaneously focusing on a different subspace of the embedded dimension, which is called multi-head in Transformer. We argue that certain dependencies among words could be learned better through an intermediate context than directly modeling word-word dependencies. This could happen due to the nature of certain dependencies or lack of patterns that lend them difficult to be modeled globally using multi-head self-attention. In this work, we propose a new way of learning dependencies through a context in multi-head using convolution. This new form of multi-head attention along with the traditional form achieves better results than Transformer on the WMT 2014 English-to-German and English-to-French translation tasks. We also introduce a framework to learn POS tagging and NER information during the training of encoder which further improves results achieving a new state-of-the-art of 32.1 BLEU, better than existing best by 1.4 BLEU, on the WMT 2014 English-to-German and 44.6 BLEU, better than existing best by 1.1 BLEU, on the WMT 2014 English-to-French translation tasks. We call this Transformer++.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.04974](https://arxiv.org/abs/2003.04974) [cs.CL] |
|           | (or [arXiv:2003.04974v1](https://arxiv.org/abs/2003.04974v1) [cs.CL] for this version) |



# 2020-03-11

[Return to Index](#Index)



<h2 id="2020-03-11-1">1. Combining Pretrained High-Resource Embeddings and Subword Representations for Low-Resource Languages</h2>

Title: [Combining Pretrained High-Resource Embeddings and Subword Representations for Low-Resource Languages](https://arxiv.org/abs/2003.04419)

Authors: [Machel Reid](https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M), [Edison Marrese-Taylor](https://arxiv.org/search/cs?searchtype=author&query=Marrese-Taylor%2C+E), [Yutaka Matsuo](https://arxiv.org/search/cs?searchtype=author&query=Matsuo%2C+Y)

*(Submitted on 9 Mar 2020)*

> The contrast between the need for large amounts of data for current Natural Language Processing (NLP) techniques, and the lack thereof, is accentuated in the case of African languages, most of which are considered low-resource. To help circumvent this issue, we explore techniques exploiting the qualities of morphologically rich languages (MRLs), while leveraging pretrained word vectors in well-resourced languages. In our exploration, we show that a meta-embedding approach combining both pretrained and morphologically-informed word embeddings performs best in the downstream task of Xhosa-English translation.

| Comments: | To appear at ICLR (International Conference of Learning Representations) 2020 Africa NLP Workshop |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.04419](https://arxiv.org/abs/2003.04419) [cs.CL] |
|           | (or [arXiv:2003.04419v1](https://arxiv.org/abs/2003.04419v1) [cs.CL] for this version) |







# 2020-03-10

[Return to Index](#Index)



<h2 id="2020-03-10-1">1. Discovering linguistic (ir)regularities in word embeddings through max-margin separating hyperplanes</h2>

Title: [Discovering linguistic (ir)regularities in word embeddings through max-margin separating hyperplanes](https://arxiv.org/abs/2003.03654)

Authors: [Noel Kennedy](https://arxiv.org/search/cs?searchtype=author&query=Kennedy%2C+N), [Imogen Schofield](https://arxiv.org/search/cs?searchtype=author&query=Schofield%2C+I), [Dave C. Brodbelt](https://arxiv.org/search/cs?searchtype=author&query=Brodbelt%2C+D+C), [David B. Church](https://arxiv.org/search/cs?searchtype=author&query=Church%2C+D+B), [Dan G. O'Neill](https://arxiv.org/search/cs?searchtype=author&query=O'Neill%2C+D+G)

*(Submitted on 7 Mar 2020)*

> We experiment with new methods for learning how related words are positioned relative to each other in word embedding spaces. Previous approaches learned constant vector offsets: vectors that point from source tokens to target tokens with an assumption that these offsets were parallel to each other. We show that the offsets between related tokens are closer to orthogonal than parallel, and that they have low cosine similarities. We proceed by making a different assumption; target tokens are linearly separable from source and un-labeled tokens. We show that a max-margin hyperplane can separate target tokens and that vectors orthogonal to this hyperplane represent the relationship between source and targets. We find that this representation of the relationship obtains the best results in dis-covering linguistic regularities. We experiment with vector space models trained by a variety of algorithms (Word2vec: CBOW/skip-gram, fastText, or GloVe), and various word context choices such as linear word-order, syntax dependency grammars, and with and without knowledge of word position. These experiments show that our model, SVMCos, is robust to a range of experimental choices when training word embeddings.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.03654](https://arxiv.org/abs/2003.03654) [cs.CL] |
|           | (or [arXiv:2003.03654v1](https://arxiv.org/abs/2003.03654v1) [cs.CL] for this version) |



# 2020-03-09

[Return to Index](#Index)



<h2 id="2020-03-09-1">1. Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation</h2>

Title: [Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation](https://arxiv.org/abs/2003.02877)

Authors:[Mitchell A. Gordon](https://arxiv.org/search/cs?searchtype=author&query=Gordon%2C+M+A), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K)

*(Submitted on 5 Mar 2020)*

> Abstract: We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.

| Subjects: | Computation and Language (cs.CL)                             |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.02877](https://arxiv.org/abs/2003.02877) [cs.CL] |
|           | (or [arXiv:2003.02877v1](https://arxiv.org/abs/2003.02877v1) [cs.CL] for this version) |



<h2 id="2020-03-09-2">2. What the [MASK]? Making Sense of Language-Specific BERT Models</h2>

Title: [What the [MASK]? Making Sense of Language-Specific BERT Models](https://arxiv.org/abs/2003.02912)

Authors: Authors:[Debora Nozza](https://arxiv.org/search/cs?searchtype=author&query=Nozza%2C+D), [Federico Bianchi](https://arxiv.org/search/cs?searchtype=author&query=Bianchi%2C+F), [Dirk Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+D)

*(Submitted on 5 Mar 2020)*

> Abstract: Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at [this https URL](https://bertlang.unibocconi.it/).

| Subjects: | Computation and Language (cs.CL)                             |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.02912](https://arxiv.org/abs/2003.02912) [cs.CL] |
|           | (or [arXiv:2003.02912v1](https://arxiv.org/abs/2003.02912v1) [cs.CL] for this version) |



<h2 id="2020-03-09-3">3. Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach</h2>

Title: [Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach](https://arxiv.org/abs/2003.03350)

Authors: Authors:[Oleksandr Palagin](https://arxiv.org/search/cs?searchtype=author&query=Palagin%2C+O), [Vitalii Velychko](https://arxiv.org/search/cs?searchtype=author&query=Velychko%2C+V), [Kyrylo Malakhov](https://arxiv.org/search/cs?searchtype=author&query=Malakhov%2C+K), [Oleksandr Shchurov](https://arxiv.org/search/cs?searchtype=author&query=Shchurov%2C+O)

*(Submitted on 6 Mar 2020)*

> Abstract: We design a new technique for the distributional semantic modeling with a neural network-based approach to learn distributed term representations (or term embeddings) - term vector space models as a result, inspired by the recent ontology-related approach (using different types of contextual knowledge such as syntactic knowledge, terminological knowledge, semantic knowledge, etc.) to the identification of terms (term extraction) and relations between them (relation extraction) called semantic pre-processing technology - SPT. Our method relies on automatic term extraction from the natural language texts and subsequent formation of the problem-oriented or application-oriented (also deeply annotated) text corpora where the fundamental entity is the term (includes non-compositional and compositional terms). This gives us an opportunity to changeover from distributed word representations (or word embeddings) to distributed term representations (or term embeddings). This transition will allow to generate more accurate semantic maps of different subject domains (also, of relations between input terms - it is useful to explore clusters and oppositions, or to test your hypotheses about them). The semantic map can be represented as a graph using Vec2graph - a Python library for visualizing word embeddings (term embeddings in our case) as dynamic and interactive graphs. The Vec2graph library coupled with term embeddings will not only improve accuracy in solving standard NLP tasks, but also update the conventional concept of automated ontology development. The main practical result of our work is the development kit (set of toolkits represented as web service APIs and web application), which provides all necessary routines for the basic linguistic pre-processing and the semantic pre-processing of the natural language texts in Ukrainian for future training of term vector space models.

| Comments: | In English, 9 pages, 2 figures. Not published yet. Prepared for special issue (UkrPROG 2020 conference) of the scientific journal "Problems in programming" (Founder: National Academy of Sciences of Ukraine, Institute of Software Systems of NAS Ukraine) |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI) |
| Cite as:  | [arXiv:2003.03350](https://arxiv.org/abs/2003.03350) [cs.CL] |
|           | (or [arXiv:2003.03350v1](https://arxiv.org/abs/2003.03350v1) [cs.CL] for this version) |





# 2020-03-06

[Return to Index](#Index)



<h2 id="2020-03-06-1">1. Phase transitions in a decentralized graph-based approach to human language</h2>

Title: [Phase transitions in a decentralized graph-based approach to human language](https://arxiv.org/abs/2003.02639)

Authors: [Javier Vera](https://arxiv.org/search/physics?searchtype=author&query=Vera%2C+J), [Felipe Urbina](https://arxiv.org/search/physics?searchtype=author&query=Urbina%2C+F), [Wenceslao Palma](https://arxiv.org/search/physics?searchtype=author&query=Palma%2C+W)

*(Submitted on 4 Mar 2020)*

> Zipf's law establishes a scaling behavior for word-frequencies in large text corpora. The appearance of Zipfian properties in human language has been previously explained as an optimization problem for the interests of speakers and hearers. On the other hand, human-like vocabularies can be viewed as bipartite graphs. The aim here is double: within a bipartite-graph approach to human vocabularies, to propose a decentralized language game model for the formation of Zipfian properties. To do this, we define a language game, in which a population of artificial agents is involved in idealized linguistic interactions. Numerical simulations show the appearance of a phase transition from an initially disordered state to three possible phases for language formation. Our results suggest that Zipfian properties in language seem to arise partly from decentralized linguistic interactions between agents endowed with bipartite word-meaning mappings.

| Subjects: | **Physics and Society (physics.soc-ph)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.02639](https://arxiv.org/abs/2003.02639) [physics.soc-ph] |
|           | (or [arXiv:2003.02639v1](https://arxiv.org/abs/2003.02639v1) [physics.soc-ph] for this version) |





<h2 id="2020-03-06-2">2. BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward</h2>

Title: [BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward](https://arxiv.org/abs/2003.02738)

Authors: [Florian Schmidt](https://arxiv.org/search/cs?searchtype=author&query=Schmidt%2C+F), [Thomas Hofmann](https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+T)

*(Submitted on 5 Mar 2020)*

> Measuring the quality of a generated sequence against a set of references is a central problem in many learning frameworks, be it to compute a score, to assign a reward, or to perform discrimination. Despite great advances in model architectures, metrics that scale independently of the number of references are still based on n-gram estimates. We show that the underlying operations, counting words and comparing counts, can be lifted to embedding words and comparing embeddings. An in-depth analysis of BERT embeddings shows empirically that contextual embeddings can be employed to capture the required dependencies while maintaining the necessary scalability through appropriate pruning and smoothing techniques. We cast unconditional generation as a reinforcement learning problem and show that our reward function indeed provides a more effective learning signal than n-gram reward in this challenging setting.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.02738](https://arxiv.org/abs/2003.02738) [cs.LG] |
|           | (or [arXiv:2003.02738v1](https://arxiv.org/abs/2003.02738v1) [cs.LG] for this version) |





<h2 id="2020-03-06-3">3. Zero-Shot Cross-Lingual Transfer with Meta Learning</h2>

Title: [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://arxiv.org/abs/2003.02739)

Authors: [Farhad Nooralahzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nooralahzadeh%2C+F), [Giannis Bekoulis](https://arxiv.org/search/cs?searchtype=author&query=Bekoulis%2C+G), [Johannes Bjerva](https://arxiv.org/search/cs?searchtype=author&query=Bjerva%2C+J), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

*(Submitted on 5 Mar 2020)*

> Learning what to share between tasks has been a topic of high importance recently, as strategic sharing of knowledge has been shown to improve the performance of downstream tasks. The same applies to sharing between languages, and is especially important when considering the fact that most languages in the world suffer from being under-resourced. In this paper, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning, where, in addition to training a source language model, another model learns to select which training instances are the most beneficial. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning, on a total 16 languages. We improve upon state-of-the-art on zero-shot and few-shot NLI and QA tasks on the XNLI and X-WikiRe datasets, respectively. We further conduct a comprehensive analysis which indicates that correlation of typological features between languages can further explain when parameter sharing learned via meta learning is beneficial.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.02739](https://arxiv.org/abs/2003.02739) [cs.CL] |
|           | (or [arXiv:2003.02739v1](https://arxiv.org/abs/2003.02739v1) [cs.CL] for this version) |





<h2 id="2020-03-06-4">4. An Empirical Accuracy Law for Sequential Machine Translation: the Case of Google Translate</h2>

Title: [An Empirical Accuracy Law for Sequential Machine Translation: the Case of Google Translate](https://arxiv.org/abs/2003.02817)

Authors: [Lucas Nunes Sequeira](https://arxiv.org/search/cs?searchtype=author&query=Sequeira%2C+L+N), [Bruno Moreschi](https://arxiv.org/search/cs?searchtype=author&query=Moreschi%2C+B), [Fabio Gagliardi Cozman](https://arxiv.org/search/cs?searchtype=author&query=Cozman%2C+F+G), [Bernardo Fontes](https://arxiv.org/search/cs?searchtype=author&query=Fontes%2C+B)

*(Submitted on 5 Mar 2020)*

> We have established, through empirical testing, a law that relates the number of translating hops to translation accuracy in sequential machine translation in Google Translate. Both accuracy and size decrease with the number of hops; the former displays a decrease closely following a power law. Such a law allows one to predict the behavior of translation chains that may be built as society increasingly depends on automated devices.

| Comments: | 11 pages, 8 figures (mostly graphs), a few mathematical functions and samples of the experiment |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2003.02817](https://arxiv.org/abs/2003.02817) [cs.CL] |
|           | (or [arXiv:2003.02817v1](https://arxiv.org/abs/2003.02817v1) [cs.CL] for this version) |





# 2020-03-05

[Return to Index](#Index)



<h2 id="2020-03-05-1">1. Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation</h2>

Title: [Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation](https://arxiv.org/abs/2003.02197)

Authors: [Hongzheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H)

*(Submitted on 4 Mar 2020)*

> Back translation (BT) has been widely used and become one of standard techniques for data augmentation in Neural Machine Translation (NMT), BT has proven to be helpful for improving the performance of translation effectively, especially for low-resource scenarios. While most works related to BT mainly focus on European languages, few of them study languages in other areas around the world. In this paper, we investigate the impacts of BT on Asia language translations between the extremely low-resource Chinese and Vietnamese language pair. We evaluate and compare the effects of different sizes of synthetic data on both NMT and Statistical Machine Translation (SMT) models for Chinese to Vietnamese and Vietnamese to Chinese, with character-based and word-based settings. Some conclusions from previous works are partially confirmed and we also draw some other interesting findings and conclusions, which are beneficial to understand BT further.

| Comments: | 10 pages, 5 tables and 4 figures                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.02197](https://arxiv.org/abs/2003.02197) [cs.CL] |
|           | (or [arXiv:2003.02197v1](https://arxiv.org/abs/2003.02197v1) [cs.CL] for this version) |





# 2020-03-04

[Return to Index](#Index)



<h2 id="2020-03-04-1">1. Understanding the Prediction Mechanism of Sentiments by XAI Visualization</h2>

Title: [Understanding the Prediction Mechanism of Sentiments by XAI Visualization](https://arxiv.org/abs/2003.01425)

Authors: [Chaehan So](https://arxiv.org/search/cs?searchtype=author&query=So%2C+C)

*(Submitted on 3 Mar 2020)*

> People often rely on online reviews to make purchase decisions. The present work aimed to gain an understanding of a machine learning model's prediction mechanism by visualizing the effect of sentiments extracted from online hotel reviews with explainable AI (XAI) methodology. Study 1 used the extracted sentiments as features to predict the review ratings by five machine learning algorithms (knn, CART decision trees, support vector machines, random forests, gradient boosting machines) and identified random forests as best algorithm. Study 2 analyzed the random forests model by feature importance and revealed the sentiments joy, disgust, positive and negative as the most predictive features. Furthermore, the visualization of additive variable attributions and their prediction distribution showed correct prediction in direction and effect size for the 5-star rating but partially wrong direction and insufficient effect size for the 1-star rating. These prediction details were corroborated by a what-if analysis for the four top features. In conclusion, the prediction mechanism of a machine learning model can be uncovered by visualization of particular observations. Comparing instances of contrasting ground truth values can draw a differential picture of the prediction mechanism and inform decisions for model improvement.

| Comments: | This is the author's prefinal version be published in conference proceedings: 4th International Conference on Natural Language Processing and Information Retrieval, Sejong, South Korea, 26-28 June, 2020, ACM |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.01425](https://arxiv.org/abs/2003.01425) [cs.HC] |
|           | (or [arXiv:2003.01425v1](https://arxiv.org/abs/2003.01425v1) [cs.HC] for this version) |



<h2 id="2020-03-04-2">2. XGPT: Cross-modal Generative Pre-Training for Image Captioning</h2>

Title: [XGPT: Cross-modal Generative Pre-Training for Image Captioning](https://arxiv.org/abs/2003.01473)

Authors: [Qiaolin Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Q), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Lei Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 3 Mar 2020)*

> While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.01473](https://arxiv.org/abs/2003.01473) [cs.CL] |
|           | (or [arXiv:2003.01473v1](https://arxiv.org/abs/2003.01473v1) [cs.CL] for this version) |





# 2020-03-02

[Return to Index](#Index)



<h2 id="2020-03-02-1">1. Robust Unsupervised Neural Machine Translation with Adversarial Training</h2>

Title: [Robust Unsupervised Neural Machine Translation with Adversarial Training](https://arxiv.org/abs/2002.12549)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 28 Feb 2020)*

> Unsupervised neural machine translation (UNMT) has recently attracted great interest in the machine translation community, achieving only slightly worse results than supervised neural machine translation. However, in real-world scenarios, there usually exists minor noise in the input sentence and the neural translation system is sensitive to the small perturbations in the input, leading to poor performance. In this paper, we first define two types of noises and empirically show the effect of these noisy data on UNMT performance. Moreover, we propose adversarial training methods to improve the robustness of UNMT in the noisy scenario. To the best of our knowledge, this paper is the first work to explore the robustness of UNMT. Experimental results on several language pairs show that our proposed methods substantially outperform conventional UNMT systems in the noisy scenario.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.12549](https://arxiv.org/abs/2002.12549) [cs.CL] |
|           | (or [arXiv:2002.12549v1](https://arxiv.org/abs/2002.12549v1) [cs.CL] for this version) |





<h2 id="2020-03-02-2">2. Modeling Future Cost for Neural Machine Translation</h2>

Title: [Modeling Future Cost for Neural Machine Translation](https://arxiv.org/abs/2002.12558)

Authors: [Chaoqun Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+C), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Conghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 28 Feb 2020)*

> Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a time-dependent future cost is estimated based on the current generated target word and its contextual information to boost the training of the NMT model. Furthermore, the learned future context representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 German-to-English, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.12558](https://arxiv.org/abs/2002.12558) [cs.CL] |
|           | (or [arXiv:2002.12558v1](https://arxiv.org/abs/2002.12558v1) [cs.CL] for this version) |





<h2 id="2020-03-02-3">3. TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing</h2>

Title: [TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://arxiv.org/abs/2002.12620)

Authors: [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Zhipeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Guoping Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+G)

*(Submitted on 28 Feb 2020)*

> In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setup of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configuration, we achieve results that are comparable with or even higher than the state-of-the-art performance. Our toolkit is available through: [this http URL](http://textbrewer.hfl-rc.com/)

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | [arXiv:2002.12620](https://arxiv.org/abs/2002.12620) [cs.CL] |
|           | (or [arXiv:2002.12620v1](https://arxiv.org/abs/2002.12620v1) [cs.CL] for this version) |





<h2 id="2020-03-02-4">4. Do all Roads Lead to Rome? Understanding the Role of Initialization in Iterative Back-Translation</h2>

Title: [Do all Roads Lead to Rome? Understanding the Role of Initialization in Iterative Back-Translation](https://arxiv.org/abs/2002.12867)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Noe Casas](https://arxiv.org/search/cs?searchtype=author&query=Casas%2C+N), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

*(Submitted on 28 Feb 2020)*

> Back-translation provides a simple yet effective approach to exploit monolingual corpora in Neural Machine Translation (NMT). Its iterative variant, where two opposite NMT models are jointly trained by alternately using a synthetic parallel corpus generated by the reverse model, plays a central role in unsupervised machine translation. In order to start producing sound translations and provide a meaningful training signal to each other, existing approaches rely on either a separate machine translation system to warm up the iterative procedure, or some form of pre-training to initialize the weights of the model. In this paper, we analyze the role that such initialization plays in iterative back-translation. Is the behavior of the final system heavily dependent on it? Or does iterative back-translation converge to a similar solution given any reasonable initialization? Through a series of empirical experiments over a diverse set of warmup systems, we show that, although the quality of the initial system does affect final performance, its effect is relatively small, as iterative back-translation has a strong tendency to convergence to a similar solution. As such, the margin of improvement left for the initialization method is narrow, suggesting that future research should focus more on improving the iterative mechanism itself.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.12867](https://arxiv.org/abs/2002.12867) [cs.CL] |
|           | (or [arXiv:2002.12867v1](https://arxiv.org/abs/2002.12867v1) [cs.CL] for this version) |



