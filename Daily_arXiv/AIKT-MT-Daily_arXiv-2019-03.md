# Daily arXiv: Machine Translation - Mar., 2019

### Index

- [2019-03-29](#2019-03-29)
  - [1. Mining Discourse Markers for Unsupervised Sentence Representation Learning](#2019-03-29-1)
  - [2. Train, Sort, Explain: Learning to Diagnose Translation Models](#2019-03-29-2)
- [2019-03-28](#2019-03-28)
  - [1. Using Monolingual Data in Neural Machine Translation: a Systematic Study](#2019-03-28-1)
  - [2. Multilevel Text Normalization with Sequence-to-Sequence Networks and Multisource Learning](#2019-03-28-2)
  - [3. Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation](#2019-03-28-3)
- [2019-03-27](#2019-03-27)
  - [1. Interoperability and machine-to-machine translation model with mappings to machine learning tasks](#2019-03-27-1)
  - [2. Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus](#2019-03-27-2)
- [2019-03-26](#2019-03-26)
  - [1. Competence-based Curriculum Learning for Neural Machine Translation](#2019-03-26-1)
  - [2. Aligning Vector-spaces with Noisy Supervised Lexicons](#2019-03-26-2)
  - [3. Pre-trained Language Model Representations for Language Generation](#2019-03-26-3)
- [2019-03-22](#2019-03-22)
  - [1. Probing the Need for Visual Context in Multimodal Machine Translation](#2019-03-22-1)
  - [2. Selective Attention for Context-aware Neural Machine Translation](#2019-03-22-2)
  - [3. Linguistic Knowledge and Transferability of Contextual Representations](#2019-03-22-3)
- [2019-03-21](#2019-03-21)
  - [1. Aligning Biomedical Metadata with Ontologies Using Clustering and Embeddings](#2019-03-21-1)
- [2019-03-20](#2019-03-20)
  - [1. Cloze-driven Pretraining of Self-attention Networks](#2019-03-20-1)
  - [2. CVIT-MT Systems for WAT-2018](#2019-03-20-2)
  - [3. compare-mt: A Tool for Holistic Comparison of Language Generation Systems](#2019-03-20-3)
- [2019-03-19](#2019-03-19)
  - [1. The Missing Ingredient in Zero-Shot Neural Machine Translation](#2019-03-19-1)
  - [2. Topic-Guided Variational Autoencoders for Text Generation](#2019-03-19-2)
- [2019-03-18](#2019-03-18)
  - [1. On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models](#2019-03-18-1)
- [2019-03-15](#2019-03-15)
  - [1. Low-Resource Syntactic Transfer with Unsupervised Source Reordering](#2019-03-15-1)
  - [2. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](#2019-03-15-2)
- [2019-03-13](#2019-03-13)
  - [1. Context-Aware Learning for Neural Machine Translation](#2019-03-13-1)
- [2019-03-12](#2019-03-12)
  - [1. Partially Shuffling the Training Data to Improve Language Models](#2019-03-12-1)
  - [2. ETNLP: A Toolkit for Extraction, Evaluation and Visualization of Pre-trained Word Embeddings](#2019-03-12-2)
- [2019-03-11](#2019-03-11)
  - [1. Context-Aware Crosslingual Mapping](#2019-03-11-1)
  - [2. Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection](#2019-03-11-2)
- [2019-03-08](#2019-03-08)
  - [1. Integrating Artificial and Human Intelligence for Efficient Translation](#2019-03-08-1)
- [2019-03-04](#2019-03-04)
  - [1. Chinese-Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information](#2019-03-04-1)
  - [2. Massively Multilingual Neural Machine Translation](#2019-03-04-2)
  - [3. Non-Parametric Adaptation for Neural Machine Translation](#2019-03-04-3)
  - [4. Reinforcement Learning based Curriculum Optimization for Neural Machine Translation](#2019-03-04-4)
- [2019-03-01](#2019-03-01)
  - [1. Efficient Contextual Representation Learning Without Softmax Layer](#2019-03-01-1)

* [2019-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-02.md)



# 2019-03-29

[Return to Index](#Index)

<h2 id="2019-03-29-1">1. Mining Discourse Markers for Unsupervised Sentence Representation Learning</h2> 

Title: [Mining Discourse Markers for Unsupervised Sentence Representation Learning](<https://arxiv.org/abs/1903.11850>)

Authors: [Damien Sileo](https://arxiv.org/search/cs?searchtype=author&query=Sileo%2C+D), [Tim Van-De-Cruys](https://arxiv.org/search/cs?searchtype=author&query=Van-De-Cruys%2C+T), [Camille Pradel](https://arxiv.org/search/cs?searchtype=author&query=Pradel%2C+C), [Philippe Muller](https://arxiv.org/search/cs?searchtype=author&query=Muller%2C+P)

*(Submitted on 28 Mar 2019)*

> Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data -- such as discourse markers between sentences -- mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as coincidentally or amazingly We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse markers yields state of the art results across different transfer tasks, it is not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements. Our datasets are publicly available ([this https URL](https://github.com/synapse-developpement/Discovery))

| Comments: | Camera-ready for NAACL HLT 2019                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.11850](https://arxiv.org/abs/1903.11850) [cs.CL] |
|           | (or **arXiv:1903.11850v1 [cs.CL]** for this version)         |



<h2 id="2019-03-29-2">2. Train, Sort, Explain: Learning to Diagnose Translation Models</h2> 

Title: [Train, Sort, Explain: Learning to Diagnose Translation Models](<https://arxiv.org/abs/1903.12017>)

*Authors: (Submitted on 28 Mar 2019)*

> Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.

| Comments: | NAACL-HLT 2019: Demonstrations                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1903.12017](https://arxiv.org/abs/1903.12017) [cs.CL] |
|           | (or **arXiv:1903.12017v1 [cs.CL]** for this version)         |





# 2019-03-28

[Return to Index](#Index)

<h2 id="2019-03-28-1">1. 
Using Monolingual Data in Neural Machine Translation: a Systematic Study</h2> 

Title: [Using Monolingual Data in Neural Machine Translation: a Systematic Study](<https://arxiv.org/abs/1903.11437>)

Authors: [Franck Burlot](https://arxiv.org/search/cs?searchtype=author&query=Burlot%2C+F), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

*(Submitted on 27 Mar 2019)*

> Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through \textsl{back-translation} - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.

| Comments: | Published in the Proceedings of the Third Conference on Machine Translation (Research Papers), 2018 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.11437](https://arxiv.org/abs/1903.11437) [cs.CL] |
|           | (or **arXiv:1903.11437v1 [cs.CL]** for this version)         |



<h2 id="2019-03-28-2">2. 
Multilevel Text Normalization with Sequence-to-Sequence Networks and Multisource Learning</h2> 

Title: [Multilevel Text Normalization with Sequence-to-Sequence Networks and Multisource Learning](<https://arxiv.org/abs/1903.11340>)

Authors: [Tatyana Ruzsics](https://arxiv.org/search/cs?searchtype=author&query=Ruzsics%2C+T), [Tanja Samardžić](https://arxiv.org/search/cs?searchtype=author&query=Samard%C5%BEi%C4%87%2C+T)

*(Submitted on 27 Mar 2019)*

> We define multilevel text normalization as sequence-to-sequence processing that transforms naturally noisy text into a sequence of normalized units of meaning (morphemes) in three steps: 1) writing normalization, 2) lemmatization, 3) canonical segmentation. These steps are traditionally considered separate NLP tasks, with diverse solutions, evaluation schemes and data sources. We exploit the fact that all these tasks involve sub-word sequence-to-sequence transformation to propose a systematic solution for all of them using neural encoder-decoder technology. The specific challenge that we tackle in this paper is integrating the traditional know-how on separate tasks into the neural sequence-to-sequence framework to improve the state of the art. We address this challenge by enriching the general framework with mechanisms that allow processing the information on multiple levels of text organization (characters, morphemes, words, sentences) in combination with structural information (multilevel language model, part-of-speech) and heterogeneous sources (text, dictionaries). We show that our solution consistently improves on the current methods in all three steps. In addition, we analyze the performance of our system to show the specific contribution of the integrating components to the overall improvement.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.11340](https://arxiv.org/abs/1903.11340) [cs.CL] |
|           | (or **arXiv:1903.11340v1 [cs.CL]** for this version)         |



<h2 id="2019-03-28-3">3. 
Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation</h2> 

Title: [Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation](<https://arxiv.org/abs/1903.11283>)

Authors: [Elizaveta Korotkova](https://arxiv.org/search/cs?searchtype=author&query=Korotkova%2C+E), [Agnes Luhtaru](https://arxiv.org/search/cs?searchtype=author&query=Luhtaru%2C+A), [Maksym Del](https://arxiv.org/search/cs?searchtype=author&query=Del%2C+M), [Krista Liin](https://arxiv.org/search/cs?searchtype=author&query=Liin%2C+K), [Daiga Deksne](https://arxiv.org/search/cs?searchtype=author&query=Deksne%2C+D), [Mark Fishel](https://arxiv.org/search/cs?searchtype=author&query=Fishel%2C+M)

*(Submitted on 27 Mar 2019)*

> Both grammatical error correction and text style transfer can be viewed as monolingual sequence-to-sequence transformation tasks, but the scarcity of directly annotated data for either task makes them unfeasible for most languages. We present an approach that does both tasks within the same trained model, and only uses regular language parallel data, without requiring error-corrected or style-adapted texts. We apply our model to three languages and present a thorough evaluation on both tasks, showing that the model is reliable for a number of error types and style transfer aspects.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.11283](https://arxiv.org/abs/1903.11283) [cs.CL] |
|           | (or **arXiv:1903.11283v1 [cs.CL]** for this version)         |



# 2019-03-27

[Return to Index](#Index)

<h2 id="2019-03-27-1">1. Interoperability and machine-to-machine translation model with mappings to machine learning tasks</h2> 

Title: [Interoperability and machine-to-machine translation model with mappings to machine learning tasks](https://arxiv.org/abs/1903.10735)

Authors: [Jacob Nilsson](https://arxiv.org/search/cs?searchtype=author&query=Nilsson%2C+J), [Fredrik Sandin](https://arxiv.org/search/cs?searchtype=author&query=Sandin%2C+F), [Jerker Delsing](https://arxiv.org/search/cs?searchtype=author&query=Delsing%2C+J)

*(Submitted on 26 Mar 2019)*

> Modern large-scale automation systems integrate thousands to hundreds of thousands of physical sensors and actuators. Demands for more flexible reconfiguration of production systems and optimization across different information models, standards and legacy systems challenge current system interoperability concepts. Automatic semantic translation across information models and standards is an increasingly important problem that needs to be addressed to fulfill these demands in a cost-efficient manner under constraints of human capacity and resources in relation to timing requirements and system complexity. Here we define a translator-based operational interoperability model for interacting cyber-physical systems in mathematical terms, which includes system identification and ontology-based translation as special cases. We present alternative mathematical definitions of the translator learning task and mappings to similar machine learning tasks and solutions based on recent developments in machine learning. Possibilities to learn translators between artefacts without a common physical context, for example in simulations of digital twins and across layers of the automation pyramid are briefly discussed.

| Comments: | 7 pages, 2 figures, 1 table, 1 listing. Submitted to the IEEE International Conference on Industrial Informatics 2019, INDIN'19 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | [arXiv:1903.10735](https://arxiv.org/abs/1903.10735) [cs.LG] |
|           | (or **arXiv:1903.10735v1 [cs.LG]** for this version)         |



<h2 id="2019-03-27-2">2. Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus</h2> 

Title: [Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus](https://arxiv.org/abs/1903.10671)

Authors: [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Suma Bhat](https://arxiv.org/search/cs?searchtype=author&query=Bhat%2C+S), [Lingfei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Jinjun Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+J), [Wen-mei Hwu](https://arxiv.org/search/cs?searchtype=author&query=Hwu%2C+W)

*(Submitted on 26 Mar 2019)*

> Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks (sentiment transfer and formality transfer) show that our model outperforms state-of-the-art approaches. Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.10671](https://arxiv.org/abs/1903.10671) [cs.CL] |
|           | (or **arXiv:1903.10671v1 [cs.CL]** for this version)         |



# 2019-03-26

[Return to Index](#Index)

<h2 id="2019-03-26-1">1. Competence-based Curriculum Learning for Neural Machine Translation</h2> 

Title: [Competence-based Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/1903.09848)

Auhtors: [Emmanouil Antonios Platanios](https://arxiv.org/search/cs?searchtype=author&query=Platanios%2C+E+A), [Otilia Stretcu](https://arxiv.org/search/cs?searchtype=author&query=Stretcu%2C+O), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Barnabas Poczos](https://arxiv.org/search/cs?searchtype=author&query=Poczos%2C+B), [Tom M. Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell%2C+T+M)

*(Submitted on 23 Mar 2019)*

> Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | NAACL 2019                                                   |
| Cite as:           | [arXiv:1903.09848](https://arxiv.org/abs/1903.09848) [cs.CL] |
|                    | (or **arXiv:1903.09848v1 [cs.CL]** for this version)         |





<h2 id="2019-03-26-2">2. Aligning Vector-spaces with Noisy Supervised Lexicons</h2> 

Title: [Aligning Vector-spaces with Noisy Supervised Lexicons](https://arxiv.org/abs/1903.10238)

Authors: [Noa Yehezkel Lubin](https://arxiv.org/search/cs?searchtype=author&query=Lubin%2C+N+Y), [Jacob Goldberger](https://arxiv.org/search/cs?searchtype=author&query=Goldberger%2C+J), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

*(Submitted on 25 Mar 2019)*

> The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process. We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods. We propose a model that accounts for noisy pairs. This is achieved by introducing a generative model with a compatible iterative EM algorithm. The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness of our proposed algorithm on two alignment problems: bilingual word embedding translation, and mapping between diachronic embedding spaces for recovering the semantic shifts of words across time periods.

| Comments: | Accepted as a short paper in NAACL 2019                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.10238](https://arxiv.org/abs/1903.10238) [cs.CL] |
|           | (or **arXiv:1903.10238v1 [cs.CL]** for this version)         |



<h2 id="2019-03-26-3">3. Pre-trained Language Model Representations for Language Generation</h2> 

Title: [Pre-trained Language Model Representations for Language Generation](https://arxiv.org/abs/1903.09722)

Authors:[Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

*(Submitted on 22 Mar 2019)*

> Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14%. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.

| Comments: | NAACL 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.09722](https://arxiv.org/abs/1903.09722) [cs.CL] |
|           | (or **arXiv:1903.09722v1 [cs.CL]** for this version)         |



# 2019-03-22

[Return to Index](#Index)

<h2 id="2019-03-22-1">1. Probing the Need for Visual Context in Multimodal Machine Translation</h2> 

Title: [Probing the Need for Visual Context in Multimodal Machine Translation](https://arxiv.org/abs/1903.08678)

Authors: [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L), [Loïc Barrault](https://arxiv.org/search/cs?searchtype=author&query=Barrault%2C+L)

*(Submitted on 20 Mar 2019)*

> Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.

| Comments: | Accepted to NAACL-HLT 2019, reviewer comments addressed. Appendix included for the arXiv version |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.08678](https://arxiv.org/abs/1903.08678) [cs.CL] |
|           | (or **arXiv:1903.08678v1 [cs.CL]** for this version)         |



<h2 id="2019-03-22-2">2. Selective Attention for Context-aware Neural Machine Translation</h2> 

Title: [Selective Attention for Context-aware Neural Machine Translation](https://arxiv.org/abs/1903.08788)

Authors: [Sameen Maruf](https://arxiv.org/search/cs?searchtype=author&query=Maruf%2C+S), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 21 Mar 2019)*

> Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.

| Comments: | Accepted at NAACL-HLT 2019                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.08788](https://arxiv.org/abs/1903.08788) [cs.CL] |
|           | (or **arXiv:1903.08788v1 [cs.CL]** for this version)         |



<h2 id="2019-03-22-3">3. Linguistic Knowledge and Transferability of Contextual Representations</h2> 

Title: [Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855)

Authors: [Nelson F. Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+N+F), [Matt Gardner](https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+M), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y), [Matthew Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 21 Mar 2019)*

> Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer LM, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between RNNs and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.

| Comments: | 22 pages, 4 figures; accepted to NAACL 2019                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.08855](https://arxiv.org/abs/1903.08855) [cs.CL] |
|           | (or **arXiv:1903.08855v1 [cs.CL]** for this version)         |



# 2019-03-21

[Return to Index](#Index)

<h2 id="2019-03-21-1">1. Aligning Biomedical Metadata with Ontologies Using Clustering and Embeddings</h2> 

Title: [Aligning Biomedical Metadata with Ontologies Using Clustering and Embeddings](https://arxiv.org/abs/1903.08206)

Authors: [Rafael S. Gonçalves](https://arxiv.org/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+R+S), [Maulik R. Kamdar](https://arxiv.org/search/cs?searchtype=author&query=Kamdar%2C+M+R), [Mark A. Musen](https://arxiv.org/search/cs?searchtype=author&query=Musen%2C+M+A)

*(Submitted on 19 Mar 2019)*

> The metadata about scientific experiments published in online repositories have been shown to suffer from a high degree of representational heterogeneity---there are often many ways to represent the same type of information, such as a geographical location via its latitude and longitude. To harness the potential that metadata have for discovering scientific data, it is crucial that they be represented in a uniform way that can be queried effectively. One step toward uniformly-represented metadata is to normalize the multiple, distinct field names used in metadata (e.g., lat lon, lat and long) to describe the same type of value. To that end, we present a new method based on clustering and embeddings (i.e., vector representations of words) to align metadata field names with ontology terms. We apply our method to biomedical metadata by generating embeddings for terms in biomedical ontologies from the BioPortal repository. We carried out a comparative study between our method and the NCBO Annotator, which revealed that our method yields more and substantially better alignments between metadata and ontology terms.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.08206](https://arxiv.org/abs/1903.08206) [cs.CL] |
|           | (or **arXiv:1903.08206v1 [cs.CL]** for this version)         |



# 2019-03-20

[Return to Index](#Index)

<h2 id="2019-03-20-1">1. Cloze-driven Pretraining of Self-attention Networks</h2> 

Title: [Cloze-driven Pretraining of Self-attention Networks](https://arxiv.org/abs/1903.07785)

Authors: [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Yinhan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

*(Submitted on 19 Mar 2019)*

> We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.07785](https://arxiv.org/abs/1903.07785) [cs.CL] |
|           | (or **arXiv:1903.07785v1 [cs.CL]** for this version)         |



<h2 id="2019-03-20-2">2. CVIT-MT Systems for WAT-2018</h2> 

Title: [CVIT-MT Systems for WAT-2018](https://arxiv.org/abs/1903.07917)

Authors: [Jerin Philip](https://arxiv.org/search/cs?searchtype=author&query=Philip%2C+J), [Vinay P. Namboodiri](https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P), [C.V. Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C)

*(Submitted on 19 Mar 2019)*

> This document describes the machine translation system used in the submissions of IIIT-Hyderabad CVIT-MT for the WAT-2018 English-Hindi translation task. Performance is evaluated on the associated corpus provided by the organizers. We experimented with convolutional sequence to sequence architectures. We also train with additional data obtained through backtranslation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.07917](https://arxiv.org/abs/1903.07917) [cs.CL] |
|           | (or **arXiv:1903.07917v1 [cs.CL]** for this version)         |



<h2 id="2019-03-20-3">3. compare-mt: A Tool for Holistic Comparison of Language Generation Systems</h2> 

Title: [compare-mt: A Tool for Holistic Comparison of Language Generation Systems](https://arxiv.org/abs/1903.07926)

Authors: [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Paul Michel](https://arxiv.org/search/cs?searchtype=author&query=Michel%2C+P), [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

*(Submitted on 19 Mar 2019)*

> In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. The code is available at [this https URL](https://github.com/neulab/compare-mt)

| Comments: | NAACL 2019 Demo Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.07926](https://arxiv.org/abs/1903.07926) [cs.CL] |
|           | (or **arXiv:1903.07926v1 [cs.CL]** for this version)         |



# 2019-03-19

[Return to Index](#Index)

<h2 id="2019-03-19-1">1. The Missing Ingredient in Zero-Shot Neural Machine Translation</h2> 

Title: [The Missing Ingredient in Zero-Shot Neural Machine Translation](https://arxiv.org/abs/1903.07091)

Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Roee Aharoni](https://arxiv.org/search/cs?searchtype=author&query=Aharoni%2C+R), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

*(Submitted on 17 Mar 2019)*

> Multilingual Neural Machine Translation (NMT) models are capable of translating between multiple source and target languages. Despite various approaches to train such models, they have difficulty with zero-shot translation: translating between language pairs that were not together seen during training. In this paper we first diagnose why state-of-the-art multilingual NMT models that rely purely on parameter sharing, fail to generalize to unseen language pairs. We then propose auxiliary losses on the NMT encoder that impose representational invariance across languages. Our simple approach vastly improves zero-shot translation quality without regressing on supervised directions. For the first time, on WMT14 English-FrenchGerman, we achieve zero-shot performance that is on par with pivoting. We also demonstrate the easy scalability of our approach to multiple languages on the IWSLT 2017 shared task.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.07091](https://arxiv.org/abs/1903.07091) [cs.CL] |
|           | (or **arXiv:1903.07091v1 [cs.CL]** for this version)         |



<h2 id="2019-03-19-2">2. Topic-Guided Variational Autoencoders for Text Generation</h2> 

Title: [Topic-Guided Variational Autoencoders for Text Generation](https://arxiv.org/abs/1903.07137)

Authors: [Wenlin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Hongteng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Ruiyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Dinghan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Lawrence Carin](https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L)

*(Submitted on 17 Mar 2019)*

> We propose a topic-guided variational autoencoder (TGVAE) model for text generation. Distinct from existing variational autoencoder (VAE) based approaches, which assume a simple Gaussian prior for the latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during model inference. Experimental results show that our TGVAE outperforms alternative approaches on both unconditional and conditional text generation, which can generate semantically-meaningful sentences with various topics.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.07137](https://arxiv.org/abs/1903.07137) [cs.CL] |
|           | (or **arXiv:1903.07137v1 [cs.CL]** for this version)         |



# 2019-03-18

[Return to Index](#Index)

<h2 id="2019-03-18-1">1. On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models</h2> 

Title: [On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models](https://arxiv.org/abs/1903.06620)

Authors: [Paul Michel](https://arxiv.org/search/cs?searchtype=author&query=Michel%2C+P), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Juan Miguel Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J+M)

*(Submitted on 15 Mar 2019)*

> Adversarial examples --- perturbations to the input of a model that elicit large changes in the output --- have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at [this https URL](https://github.com/pmichel31415/teapot-nlp).

| Comments: | NAACL-HLT 2019 long paper                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.06620](https://arxiv.org/abs/1903.06620) [cs.CL] |
|           | (or **arXiv:1903.06620v1 [cs.CL]** for this version)         |



# 2019-03-15

[Return to Index](#Index)

<h2 id="2019-03-15-1">1. Low-Resource Syntactic Transfer with Unsupervised Source Reordering</h2> 

Title: [Low-Resource Syntactic Transfer with Unsupervised Source Reordering](https://arxiv.org/abs/1903.05683)

Authors: [Mohammad Sadegh Rasooli](https://arxiv.org/search/cs?searchtype=author&query=Rasooli%2C+M+S), [Michael Collins](https://arxiv.org/search/cs?searchtype=author&query=Collins%2C+M)

*(Submitted on 13 Mar 2019)*

> We describe a cross-lingual transfer method for dependency parsing that takes into account the problem of word order differences between source and target languages. Our model only relies on the Bible, a considerably smaller parallel data than the commonly used parallel data in transfer methods. We use the concatenation of projected trees from the Bible corpus, and the gold-standard treebanks in multiple source languages along with cross-lingual word representations. We demonstrate that reordering the source treebanks before training on them for a target language improves the accuracy of languages outside the European language family. Our experiments on 68 treebanks (38 languages) in the Universal Dependencies corpus achieve a high accuracy for all languages. Among them, our experiments on 16 treebanks of 12 non-European languages achieve an average UAS absolute improvement of 3.3% over a state-of-the-art method.

| Comments: | Accepted in NAACL 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.05683](https://arxiv.org/abs/1903.05683) [cs.CL] |
|           | (or **arXiv:1903.05683v1 [cs.CL]** for this version)         |



<h2 id="2019-03-15-2">2. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</h2> 

Title: [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/abs/1903.05987)

Authors: [Matthew Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 14 Mar 2019)*

> While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.05987](https://arxiv.org/abs/1903.05987) [cs.CL] |
|           | (or **arXiv:1903.05987v1 [cs.CL]** for this version)         |



# 2019-03-13

[Return to Index](#Index)

<h2 id="2019-03-13-1">1. Context-Aware Learning for Neural Machine Translation</h2> 

Title: [Context-Aware Learning for Neural Machine Translation](https://arxiv.org/abs/1903.04715)

Authors: [Sébastien Jean](https://arxiv.org/search/cs?searchtype=author&query=Jean%2C+S), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

*(Submitted on 12 Mar 2019)*

> Interest in larger-context neural machine translation, including document-level and multi-modal translation, has been growing. Multiple works have proposed new network architectures or evaluation schemes, but potentially helpful context is still sometimes ignored by larger-context translation models. In this paper, we propose a novel learning algorithm that explicitly encourages a neural translation model to take into account additional context using a multilevel pair-wise ranking loss. We evaluate the proposed learning algorithm with a transformer-based larger-context translation system on document-level translation. By comparing performance using actual and random contexts, we show that a model trained with the proposed algorithm is more sensitive to the additional context.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.04715](https://arxiv.org/abs/1903.04715) [cs.CL] |
|           | (or **arXiv:1903.04715v1 [cs.CL]** for this version)         |



# 2019-03-12

[Return to Index](#Index)

<h2 id="2019-03-12-1">1. Partially Shuffling the Training Data to Improve Language Models</h2> 

Title: [Partially Shuffling the Training Data to Improve Language Models](https://arxiv.org/abs/1903.04167)

Authors: [Ofir Press](https://arxiv.org/search/cs?searchtype=author&query=Press%2C+O)

*(Submitted on 11 Mar 2019)*

> Although SGD requires shuffling the training data between epochs, currently none of the word-level language modeling systems do this. Naively shuffling all sentences in the training data would not permit the model to learn inter-sentence dependencies. Here we present a method that partially shuffles the training data between epochs. This method makes each batch random, while keeping most sentence ordering intact. It achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.04167](https://arxiv.org/abs/1903.04167) [cs.CL] |
|           | (or **arXiv:1903.04167v1 [cs.CL]** for this version)         |



<h2 id="2019-03-12-2">2. ETNLP: A Toolkit for Extraction, Evaluation and Visualization of Pre-trained Word Embeddings</h2> 

Title: [ETNLP: A Toolkit for Extraction, Evaluation and Visualization of Pre-trained Word Embeddings](https://arxiv.org/abs/1903.04433)

Authors: [Xuan-Son Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+X), [Thanh Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Son N. Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+S+N), [Lili Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L)

*(Submitted on 11 Mar 2019)*

> In this paper, we introduce a comprehensive toolkit, ETNLP, which can evaluate, extract, and visualize multiple sets of pre-trained word embeddings. First, for evaluation, ETNLP analyses the quality of pre-trained embeddings based on an input word analogy list. Second, for extraction ETNLP provides a subset of the embeddings to be used in the downstream NLP tasks. Finally, ETNLP has a visualization module which is for exploring the embedded words interactively. We demonstrate the effectiveness of ETNLP on our pre-trained word embeddings in Vietnamese. Specifically, we create a large Vietnamese word analogy list to evaluate the embeddings. We then utilize the pre-trained embeddings for the name entity recognition (NER) task in Vietnamese and achieve the new state-of-the-art results on a benchmark dataset for the NER task. A video demonstration of ETNLP is available at [this https URL](https://vimeo.com/317599106). The source code and data are available at https: //github.com/vietnlp/etnlp.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.04433](https://arxiv.org/abs/1903.04433) [cs.CL] |
|           | (or **arXiv:1903.04433v1 [cs.CL]** for this version)         |



# 2019-03-11

[Return to Index](#Index)

<h2 id="2019-03-11-1">1. Context-Aware Crosslingual Mapping</h2> 

Title: [Context-Aware Crosslingual Mapping](https://arxiv.org/abs/1903.03243)

Authors:[Hanan Aldarmaki](https://arxiv.org/search/cs?searchtype=author&query=Aldarmaki%2C+H), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

*(Submitted on 8 Mar 2019)*

> Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.

| Comments: | NAACL-HLT 2019 (short paper). 5 pages, 1 figure              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.03243](https://arxiv.org/abs/1903.03243) [cs.CL] |
|           | (or **arXiv:1903.03243v1 [cs.CL]** for this version)         |







<h2 id="2019-03-11-2">2. Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection</h2> 

Title: [Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection](https://arxiv.org/abs/1903.03467)

Authors: [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Roee Aharoni](https://arxiv.org/search/cs?searchtype=author&query=Aharoni%2C+R), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

*(Submitted on 8 Mar 2019)*

> When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must "guess" this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.03467](https://arxiv.org/abs/1903.03467) [cs.CL] |
|           | (or **arXiv:1903.03467v1 [cs.CL]** for this version)         |



# 2019-03-08

[Return to Index](#Index)

<h2 id="2019-03-08-1">1. Integrating Artificial and Human Intelligence for Efficient Translation</h2> 

Title: [Integrating Artificial and Human Intelligence for Efficient Translation](https://arxiv.org/abs/1903.02978)

Authors: [Nico Herbig](https://arxiv.org/search/cs?searchtype=author&query=Herbig%2C+N), [Santanu Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+S), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Antonio Krüger](https://arxiv.org/search/cs?searchtype=author&query=Kr%C3%BCger%2C+A)

*(Submitted on 7 Mar 2019)*

> Current advances in machine translation increase the need for translators to switch from traditional translation to post-editing of machine-translated text, a process that saves time and improves quality. Human and artificial intelligence need to be integrated in an efficient way to leverage the advantages of both for the translation task. This paper outlines approaches at this boundary of AI and HCI and discusses open research questions to further advance the field.

| Subjects: | **Human-Computer Interaction (cs.HC)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1903.02978](https://arxiv.org/abs/1903.02978) [cs.HC] |
|           | (or **arXiv:1903.02978v1 [cs.HC]** for this version)         |





# 2019-03-04

[Return to Index](#Index)

<h2 id="2019-03-04-1">1. Chinese-Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information</h2> 

Title: [Chinese-Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information](https://arxiv.org/abs/1903.00149)

Authors: [Longtu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

*(Submitted on 1 Mar 2019)*

> Unsupervised neural machine translation (UNMT) requires only monolingual data of similar language pairs during training and can produce bi-directional translation models with relatively good performance on alphabetic languages (Lample et al., 2018). However, no research has been done to logographic language pairs. This study focuses on Chinese-Japanese UNMT trained by data containing sub-character (ideograph or stroke) level information which is decomposed from character level data. BLEU scores of both character and sub-character level systems were compared against each other and the results showed that despite the effectiveness of UNMT on character level data, sub-character level data could further enhance the performance, in which the stroke level system outperformed the ideograph level system.

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.00149](https://arxiv.org/abs/1903.00149) [cs.CL] |
|           | (or **arXiv:1903.00149v1 [cs.CL]** for this version)         |



<h2 id="2019-03-04-2">2. Massively Multilingual Neural Machine Translation</h2> 

Title: [Massively Multilingual Neural Machine Translation](https://arxiv.org/abs/1903.00089)

Authors: [Roee Aharoni](https://arxiv.org/search/cs?searchtype=author&query=Aharoni%2C+R), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 28 Feb 2019)*

> Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.

| Comments: | Accepted as a long paper in NAACL 2019                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.00089](https://arxiv.org/abs/1903.00089) [cs.CL] |
|           | (or **arXiv:1903.00089v1 [cs.CL]** for this version)         |



<h2 id="2019-03-04-3">3. Non-Parametric Adaptation for Neural Machine Translation</h2> 

Title: [Non-Parametric Adaptation for Neural Machine Translation](https://arxiv.org/abs/1903.00058)

Authors: [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 28 Feb 2019)*

> Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.

| Comments: | To appear at NAACL 2019                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1903.00058](https://arxiv.org/abs/1903.00058) [cs.CL] |
|           | (or **arXiv:1903.00058v1 [cs.CL]** for this version)         |



<h2 id="2019-03-04-4">4. Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</h2> 

Title: [Reinforcement Learning based Curriculum Optimization for Neural Machine Translation](https://arxiv.org/abs/1903.00041)

Authors: [Gaurav Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+G), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M)

*(Submitted on 28 Feb 2019)*

> We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform and filtering baselines on Paracrawl and WMT English-to-French datasets by up to +3.4 BLEU, and match the performance of a hand-designed, state-of-the-art curriculum.

| Comments: | NAACL 2019 short paper. Reviewer comments not yet addressed  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1903.00041](https://arxiv.org/abs/1903.00041) [cs.CL] |
|           | (or **arXiv:1903.00041v1 [cs.CL]** for this version)         |



# 2019-03-01

[Return to Index](#Index)

<h2 id="2019-03-01-1">1. Efficient Contextual Representation Learning Without Softmax Layer</h2> 

Title: [Efficient Contextual Representation Learning Without Softmax Layer](https://arxiv.org/pdf/1902.11269)

Authors: [Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Patrick H. Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P+H), [Cho-Jui Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+C), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)

Note: Contextual Representaion

*(Submitted on 28 Feb 2019)*

> Contextual representation models have achieved great success in improving various downstream tasks. However, these language-model-based encoders are difficult to train due to the large parameter sizes and high computational complexity. By carefully examining the training procedure, we find that the softmax layer (the output layer) causes significant inefficiency due to the large vocabulary size. Therefore, we redesign the learning objective and propose an efficient framework for training contextual representation models. Specifically, the proposed approach bypasses the softmax layer by performing language modeling with dimension reduction, and allows the models to leverage pre-trained word embeddings. Our framework reduces the time spent on the output layer to a negligible level, eliminates almost all the trainable parameters of the softmax layer and performs language modeling without truncating the vocabulary. When applied to ELMo, our method achieves a 4 times speedup and eliminates 80% trainable parameters while achieving competitive performance on downstream tasks.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1902.11269](https://arxiv.org/abs/1902.11269) [cs.CL] |
|           | (or **arXiv:1902.11269v1 [cs.CL]** for this version)         |



