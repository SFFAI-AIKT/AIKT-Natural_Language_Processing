# Daily arXiv: Machine Translation - April, 2021

# Index

- [2021-04-19](#2021-04-19)
  - [1. Improving Gender Translation Accuracy with Filtered Self-Training](#2021-04-19-1)
  - [2. Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation](#2021-04-19-2)
  - [3. Investigating Failures of Automatic Translation in the Case of Unambiguous Gender](#2021-04-19-3)
  - [4. Comparison of Grammatical Error Correction Using Back-Translation Models](#2021-04-19-4)
  - [5. Segmenting Subtitles for Correcting ASR Segmentation Errors](#2021-04-19-5)
  - [6. Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research](#2021-04-19-6)
  - [7. Generating Bug-Fixes Using Pretrained Transformers](#2021-04-19-7)
  - [8. MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning](#2021-04-19-8)
  - [9. Language Models are Few-Shot Butlers](#2021-04-19-9)
  - [10. Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders](#2021-04-19-10)
  - [11. Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models](#2021-04-19-11)
  - [12. Towards Variable-Length Textual Adversarial Attacks](#2021-04-19-12)
  - [13. Serial or Parallel? Plug-able Adapter for multilingual machine translation](#2021-04-19-13)
  - [14. Robust Open-Vocabulary Translation from Visual Text Representations](#2021-04-19-14)
  - [15. Is Your Language Model Ready for Dense Representation Fine-tuning?](#2021-04-19-15)
  - [16. Context-Adaptive Document-Level Neural Machine Translation](#2021-04-19-16)
- [2021-04-16](#2021-04-16)
  - [1. What Makes a Scientific Paper be Accepted for Publication?](#2021-04-16-1)
  - [2. An Interpretability Illusion for BERT](#2021-04-16-2)
  - [3. An Alignment-Agnostic Model for Chinese Text Error Correction](#2021-04-16-3)
  - [4. Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](#2021-04-16-4)
  - [5. Sentence-Permuted Paragraph Generation](#2021-04-16-5)
  - [6. Adaptive Sparse Transformer for Multilingual Translation](#2021-04-16-6)
  - [7. Simultaneous Multi-Pivot Neural Machine Translation](#2021-04-16-7)
  - [8. First the worst: Finding better gender translations during beam search](#2021-04-16-8)
  - [9. Effect of Post-processing on Contextualized Word Representations](#2021-04-16-9)
  - [10. IndT5: A Text-to-Text Transformer for 10 Indigenous Languages](#2021-04-16-10)
  - [11. Generating Datasets with Pretrained Language Models](#2021-04-16-11)
  - [12. Reward Optimization for Neural Machine Translation with Learned Metrics](#2021-04-16-12)
  - [13. Hierarchical Learning for Generation with Long Source Sequences](#2021-04-16-13)
  - [14. Sometimes We Want Translationese](#2021-04-16-14)
  - [15. Demystify Optimization Challenges in Multilingual Transformers](#2021-04-16-15)
  - [16. Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](#2021-04-16-16)
- [2021-04-15](#2021-04-15)
  - [1. Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation](#2021-04-15-1)
  - [2. Large-Scale Self- and Semi-Supervised Learning for Speech Translation](#2021-04-15-2)
  - [3. The Curious Case of Hallucinations in Neural Machine Translation](#2021-04-15-3)
  - [4. Sentence Embeddings by Ensemble Distillation](#2021-04-15-4)
  - [5. Distributed Word Representation in Tsetlin Machine](#2021-04-15-5)
  - [6. Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey](#2021-04-15-6)
  - [7. TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](#2021-04-15-7)
  - [8. Sparse Attention with Linear Units](#2021-04-15-8 )
- [2021-04-14](#2021-04-14)
  - [1. Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique](#2021-04-14-1)
  - [2. Targeted Adversarial Training for Natural Language Understanding](#2021-04-14-2)
  - [3. Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation](#2021-04-14-3)
  - [4. Discourse Probing of Pretrained Language Models](#2021-04-14-4)
  - [5. Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation](#2021-04-14-5)
  - [6. Gender Bias in Machine Translation](#2021-04-14-6)
  - [7. Lessons on Parameter Sharing across Layers in Transformers](#2021-04-14-7)
  - [8. What's in your Head? Emergent Behaviour in Multi-Task Transformer Models](#2021-04-14-8)
  - [9. Understanding Hard Negatives in Noise Contrastive Estimation](#2021-04-14-9)
  - [10. Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling](#2021-04-14-10)
  - [11. EXPLAINABOARD: An Explainable Leaderboard for NLP](#2021-04-14-11)
  - [12. Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding](#2021-04-14-12)
- [2021-04-13](#2021-04-13)
  - [1. Achieving Model Robustness through Discrete Adversarial Training](#2021-04-13-1)
  - [2. TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation](#2021-04-13-2)
  - [3. Not All Attention Is All You Need](#2021-04-13-3)
  - [4. Sentiment-based Candidate Selection for NMT](#2021-04-13-4)
  - [5. Disentangled Contrastive Learning for Learning Robust Textual Representations](#2021-04-13-5)
  - [6. Assessing Reference-Free Peer Evaluation for Machine Translation](#2021-04-13-6)
  - [7. FUDGE: Controlled Text Generation With Future Discriminators](#2021-04-13-7)
  - [8. Machine Translation Decoding beyond Beam Search](#2021-04-13-8)
  - [9. Self-Training with Weak Supervision](#2021-04-13-9)
  - [10. Survey on reinforcement learning for language processing](#2021-04-13-10)
  - [11. Backtranslation Feedback Improves User Confidence in MT, Not Quality](#2021-04-13-11)
- [2021-04-12](#2021-04-12)
  - [1. Video-aided Unsupervised Grammar Induction](#2021-04-12-1)
  - [2. Design and Implementation of English To Yoruba Verb Phrase Machine Translation System](#2021-04-12-2)
  - [3. Efficient Large-Scale Language Model Training on GPU Clusters](#2021-04-12-3)
  - [4. Chinese Character Decomposition for Neural MT with Multi-Word Expressions](#2021-04-12-4)
- [2021-04-09](#2021-04-09)
  - [1. Extended Parallel Corpus for Amharic-English Machine Translation](#2021-04-09-1)
  - [2. BSTC: A Large-Scale Chinese-English Speech Translation Dataset](#2021-04-09-2)
  - [3. A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](#2021-04-09-3)
  - [4. Probing BERT in Hyperbolic Spaces](#2021-04-09-4)
- [2021-04-08](#2021-04-08)
  - [1. VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations](#2021-04-08-1)
  - [2. Better Neural Machine Translation by Extracting Linguistic Information from BERT](#2021-04-08-2)
  - [3. GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education](#2021-04-08-3)
- [2021-04-07](#2021-04-07)
  - [1. Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding](#2021-04-07-1)
  - [2. ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation](#2021-04-07-2)
- [2021-04-06](#2021-04-06)
  - [1. TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition](#2021-04-06-1)
  - [2. Attention Forcing for Machine Translation](#2021-04-06-2)
  - [3. WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](#2021-04-06-3)
  - [4. Rethinking Perturbations in Encoder-Decoders for Fast Training](#2021-04-06-4)
- [2021-04-02](#2021-04-02)
  - [1. Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](#2021-04-02-1)
	- [2. Domain-specific MT for Low-resource Languages: The case of Bambara-French](#2021-04-02-2)
  - [3. Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation](#2021-04-02-3)
  - [4. Detecting over/under-translation errors for determining adequacy in human translations](#2021-04-02-4)
  - [5. Many-to-English Machine Translation Tools, Data, and Pretrained Models](#2021-04-02-5)
  - [6. Low-Resource Neural Machine Translation for South-Eastern African Languages](#2021-04-02-6)
  - [7. WakaVT: A Sequential Variational Transformer for Waka Generation](#2021-04-02-7)
  - [8. Sampling and Filtering of Neural Machine Translation Distillation Data](#2021-04-02-8)
- [2021-04-01](#2021-04-01)
  - [1. An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation](#2021-04-01-1)
	- [2. Few-shot learning through contextual data augmentation](#2021-04-01-2)
  - [3. UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language](#2021-04-01-3)
  - [4. Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources](#2021-04-01-4)
  - [5. Leveraging Neural Machine Translation for Word Alignment](#2021-04-01-5)
- [2021-03-31](#2021-03-31)	
  - [1. Diagnosing Vision-and-Language Navigation: What Really Matters](#2021-03-31-1)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-04-19

[Return to Index](#Index)



<h2 id="2021-04-19-1">1. Improving Gender Translation Accuracy with Filtered Self-Training
</h2>

Title: [Improving Gender Translation Accuracy with Filtered Self-Training](https://arxiv.org/abs/2104.07695)

Authors: [Prafulla Kumar Choubey](https://arxiv.org/search/cs?searchtype=author&query=Choubey%2C+P+K), [Anna Currey](https://arxiv.org/search/cs?searchtype=author&query=Currey%2C+A), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G)

> Targeted evaluations have found that machine translation systems often output incorrect gender, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose a gender-filtered self-training technique to improve gender translation accuracy on unambiguously gendered inputs. This approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then added to the training data. We filter the gender-specific corpora on the source and target sides to ensure that sentence pairs contain and correctly translate the specified gender. We evaluate our approach on translation from English into five languages, finding that our models improve gender translation accuracy without any cost to generic translation quality. In addition, we show the viability of our approach on several settings, including re-training from scratch, fine-tuning, controlling the balance of the training data, forward translation, and back-translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07695](https://arxiv.org/abs/2104.07695) [cs.CL]** |
|           | (or **[arXiv:2104.07695v1](https://arxiv.org/abs/2104.07695v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-2">2. Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation
</h2>

Title: [Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation](https://arxiv.org/abs/2104.07837)

Authors: [Gong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+G), [Yang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Sixing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Zeru Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Dejing Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+D)

> Cross-lingual entity alignment, which aims to precisely connect the same entities in different monolingual knowledge bases (KBs) together, often suffers challenges from feature inconsistency to sequence context unawareness. This paper presents a dual adversarial learning framework for cross-lingual entity alignment, DAEA, with two original contributions. First, in order to address the structural and attribute feature inconsistency between entities in two knowledge graphs (KGs), an adversarial kernel embedding technique is proposed to extract graph-invariant information in an unsupervised manner, and project two KGs into the common embedding space. Second, in order to further improve successful rate of entity alignment, we propose to produce multiple random walks through each entity to be aligned and mask these entities in random walks. With the guidance of known aligned entities in the context of multiple random walks, an adversarial knowledge translation model is developed to fill and translate masked entities in pairwise random walks from two KGs. Extensive experiments performed on real-world datasets show that DAEA can well solve the feature inconsistency and sequence context unawareness issues and significantly outperforms thirteen state-of-the-art entity alignment methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07837](https://arxiv.org/abs/2104.07837) [cs.CL]** |
|           | (or **[arXiv:2104.07837v1](https://arxiv.org/abs/2104.07837v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-3">3. Investigating Failures of Automatic Translation in the Case of Unambiguous Gender
</h2>

Title: [Investigating Failures of Automatic Translation in the Case of Unambiguous Gender](https://arxiv.org/abs/2104.07838)

Authors: [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> Transformer based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by transformer based models with regards to translating from a language that doesn't mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no transformer based model we tested was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of transformer based NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.

| Comments: | 10 pages, 2 figures, 4 tables, submitting to EMNLP 2021      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07838](https://arxiv.org/abs/2104.07838) [cs.CL]** |
|           | (or **[arXiv:2104.07838v1](https://arxiv.org/abs/2104.07838v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-4">4. Comparison of Grammatical Error Correction Using Back-Translation Models
</h2>

Title: [Comparison of Grammatical Error Correction Using Back-Translation Models](https://arxiv.org/abs/2104.07848)

Authors: [Aomi Koyama](https://arxiv.org/search/cs?searchtype=author&query=Koyama%2C+A), [Kengo Hotate](https://arxiv.org/search/cs?searchtype=author&query=Hotate%2C+K), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

> Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Therefore, GEC studies have developed various methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous GEC studies using BT have employed the same architecture for both GEC and BT models. However, GEC models have different correction tendencies depending on their architectures. Thus, in this study, we compare the correction tendencies of the GEC models trained on pseudo data generated by different BT models, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. Additionally, we examine the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the F_0.5 scores of each error type compared with that of single BT models with different seeds.

| Comments: | 10 pages; camera-ready for NAACL Student Research Workshop 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07848](https://arxiv.org/abs/2104.07848) [cs.CL]** |
|           | (or **[arXiv:2104.07848v1](https://arxiv.org/abs/2104.07848v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-5">5. Segmenting Subtitles for Correcting ASR Segmentation Errors
</h2>

Title: [Segmenting Subtitles for Correcting ASR Segmentation Errors](https://arxiv.org/abs/2104.07868)

Authors: [David Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+D), [Chris Kedzie](https://arxiv.org/search/cs?searchtype=author&query=Kedzie%2C+C), [Faisal Ladhak](https://arxiv.org/search/cs?searchtype=author&query=Ladhak%2C+F), [Elsbeth Turcan](https://arxiv.org/search/cs?searchtype=author&query=Turcan%2C+E), [Petra Galuščáková](https://arxiv.org/search/cs?searchtype=author&query=Galuščáková%2C+P), [Elena Zotkina](https://arxiv.org/search/cs?searchtype=author&query=Zotkina%2C+E), [Zhengping Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Peter Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P), [Kathleen McKeown](https://arxiv.org/search/cs?searchtype=author&query=McKeown%2C+K)

> Typical ASR systems segment the input audio into utterances using purely acoustic information, which may not resemble the sentence-like units that are expected by conventional machine translation (MT) systems for Spoken Language Translation. In this work, we propose a model for correcting the acoustic segmentation of ASR models for low-resource languages to improve performance on downstream tasks. We propose the use of subtitles as a proxy dataset for correcting ASR acoustic segmentation, creating synthetic acoustic utterances by modeling common error modes. We train a neural tagging model for correcting ASR acoustic segmentation and show that it improves downstream performance on MT and audio-document cross-language information retrieval (CLIR).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07868](https://arxiv.org/abs/2104.07868) [cs.CL]** |
|           | (or **[arXiv:2104.07868v1](https://arxiv.org/abs/2104.07868v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-6">6. Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research
</h2>

Title: [Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research](https://arxiv.org/abs/2104.07874)

Authors: [Denis Newman-Griffis](https://arxiv.org/search/cs?searchtype=author&query=Newman-Griffis%2C+D), [Jill Fain Lehman](https://arxiv.org/search/cs?searchtype=author&query=Lehman%2C+J+F), [Carolyn Rosé](https://arxiv.org/search/cs?searchtype=author&query=Rosé%2C+C), [Harry Hochheiser](https://arxiv.org/search/cs?searchtype=author&query=Hochheiser%2C+H)

> Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.

| Comments: | Accepted to NAACL-HLT 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.07874](https://arxiv.org/abs/2104.07874) [cs.CL]** |
|           | (or **[arXiv:2104.07874v1](https://arxiv.org/abs/2104.07874v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-7">7. Generating Bug-Fixes Using Pretrained Transformers
</h2>

Title: [Generating Bug-Fixes Using Pretrained Transformers](https://arxiv.org/abs/2104.07896)

Authors: [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Chen Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N)

> Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.

| Subjects: | **Computation and Language (cs.CL)**; Programming Languages (cs.PL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07896](https://arxiv.org/abs/2104.07896) [cs.CL]** |
|           | (or **[arXiv:2104.07896v1](https://arxiv.org/abs/2104.07896v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-8">8. MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning
</h2>

Title: [MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning](https://arxiv.org/abs/2104.07908)

Authors: [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Milad Shokouhi](https://arxiv.org/search/cs?searchtype=author&query=Shokouhi%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages - without access to large-scale monolingual corpora or large amounts of labeled data - for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at [this http URL](http://github.com/microsoft/MetaXL).

| Comments: | 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.07908](https://arxiv.org/abs/2104.07908) [cs.CL]** |
|           | (or **[arXiv:2104.07908v1](https://arxiv.org/abs/2104.07908v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-9">9. Language Models are Few-Shot Butlers
</h2>

Title: [Language Models are Few-Shot Butlers](https://arxiv.org/abs/2104.07972)

Authors: [Vincent Micheli](https://arxiv.org/search/cs?searchtype=author&query=Micheli%2C+V), [François Fleuret](https://arxiv.org/search/cs?searchtype=author&query=Fleuret%2C+F)

> Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine-tuned with only 1.2% of the expert demonstrations and a simple reinforcement learning algorithm achieve a 51% absolute improvement in success rate over existing methods in the ALFWorld environment.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07972](https://arxiv.org/abs/2104.07972) [cs.CL]** |
|           | (or **[arXiv:2104.07972v1](https://arxiv.org/abs/2104.07972v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-10">10. Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders
</h2>

Title: [Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders](https://arxiv.org/abs/2104.08027)

Authors: [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in less than a minute without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective univeral lexical and sentence encoders.

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.08027](https://arxiv.org/abs/2104.08027) [cs.CL]** |
|           | (or **[arXiv:2104.08027v1](https://arxiv.org/abs/2104.08027v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-11">11. Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models
</h2>

Title: [Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models](https://arxiv.org/abs/2104.08066)

Authors: [Taichi Iki](https://arxiv.org/search/cs?searchtype=author&query=Iki%2C+T), [Akiko Aizawa](https://arxiv.org/search/cs?searchtype=author&query=Aizawa%2C+A)

> Extending language models with structural modifications and vision-and-language (V&L) pretraining are successful ways of making V&L models that can ground vision and language. Potential applications of these advanced models include multi-modal machine reading comprehension models and multi-modal dialogue models, which require language ability upon grounding. Although language capability is crucial for such applications, the impact of extending their visual capabilities on their language capabilities is not fully understood. This paper investigates how visual extension affects the language capability of V&L models using the GLUE benchmark. We found that visual extension causes some decreases in language capability and that V&L pretraining has a greater impact than structural modifications on the decreases. Our results suggest the need for further study on pretraining that can maintain or, if possible, improve a model's language capability.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08066](https://arxiv.org/abs/2104.08066) [cs.CL]** |
|           | (or **[arXiv:2104.08066v1](https://arxiv.org/abs/2104.08066v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-12">12. Towards Variable-Length Textual Adversarial Attacks
</h2>

Title: [Towards Variable-Length Textual Adversarial Attacks](https://arxiv.org/abs/2104.08139)

Authors: [Junliang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Linlin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Linli Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Enhong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+E), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Adversarial attacks have shown the vulnerability of machine learning models, however, it is non-trivial to conduct textual adversarial attacks on natural language processing tasks due to the discreteness of data. Most previous approaches conduct attacks with the atomic \textit{replacement} operation, which usually leads to fixed-length adversarial examples and therefore limits the exploration on the decision space. In this paper, we propose variable-length textual adversarial attacks~(VL-Attack) and integrate three atomic operations, namely \textit{insertion}, \textit{deletion} and \textit{replacement}, into a unified framework, by introducing and manipulating a special \textit{blank} token while attacking. In this way, our approach is able to more comprehensively find adversarial examples around the decision boundary and effectively conduct adversarial attacks. Specifically, our method drops the accuracy of IMDB classification by 96% with only editing 1.3% tokens while attacking a pre-trained BERT model. In addition, fine-tuning the victim model with generated adversarial samples can improve the robustness of the model without hurting the performance, especially for length-sensitive models. On the task of non-autoregressive machine translation, our method can achieve 33.18 BLEU score on IWSLT14 German-English translation, achieving an improvement of 1.47 over the baseline model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08139](https://arxiv.org/abs/2104.08139) [cs.CL]** |
|           | (or **[arXiv:2104.08139v1](https://arxiv.org/abs/2104.08139v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-13">13. Serial or Parallel? Plug-able Adapter for multilingual machine translation
</h2>

Title: [Serial or Parallel? Plug-able Adapter for multilingual machine translation](https://arxiv.org/abs/2104.08154)

Authors: [Yaoming Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Developing a unified multilingual translation model is a key topic in machine translation research. However, existing approaches suffer from performance degradation: multilingual models yield inferior performance compared to the ones trained separately on rich bilingual data. We attribute the performance degradation to two issues: multilingual embedding conflation and multilingual fusion effects. To address the two issues, we propose PAM, a Transformer model augmented with defusion adaptation for multilingual machine translation. Specifically, PAM consists of embedding and layer adapters to shift the word and intermediate representations towards language-specific ones. Extensive experiment results on IWSLT, OPUS-100, and WMT benchmarks show that \method outperforms several strong competitors, including series adapter and multilingual knowledge distillation.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.08154](https://arxiv.org/abs/2104.08154) [cs.CL]** |
|           | (or **[arXiv:2104.08154v1](https://arxiv.org/abs/2104.08154v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-14">14. Robust Open-Vocabulary Translation from Visual Text Representations
</h2>

Title: [Robust Open-Vocabulary Translation from Visual Text Representations](https://arxiv.org/abs/2104.08211)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [David Etter](https://arxiv.org/search/cs?searchtype=author&query=Etter%2C+D), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an 'open-vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text. We show that models using visual text representations approach or match performance of text baselines on clean TED datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German--English task where subword models degrade to 1.9.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08211](https://arxiv.org/abs/2104.08211) [cs.CL]** |
|           | (or **[arXiv:2104.08211v1](https://arxiv.org/abs/2104.08211v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-15">15. Is Your Language Model Ready for Dense Representation Fine-tuning?
</h2>

Title: [Is Your Language Model Ready for Dense Representation Fine-tuning?](https://arxiv.org/abs/2104.08253)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Jamie Callan](https://arxiv.org/search/cs?searchtype=author&query=Callan%2C+J)

> Pre-trained language models (LM) have become go-to text representation encoders. Prior research used deep LMs to encode text sequences such as sentences and passages into single dense vector representations. These dense representations have been used in efficient text comparison and embedding-based retrieval. However, dense encoders suffer in low resource situations. Many techniques have been developed to solve this problem. Despite their success, not much is known about why this happens. This paper shows that one cause lies in the readiness of the LM to expose its knowledge through dense representation in fine-tuning, which we term Optimization Readiness. To validate the theory, we present Condenser, a general pre-training architecture based on Transformer LMs, to improve dense optimization readiness. We show that fine-tuning from Condenser significantly improves performance for small and/or noisy training sets.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08253](https://arxiv.org/abs/2104.08253) [cs.CL]** |
|           | (or **[arXiv:2104.08253v1](https://arxiv.org/abs/2104.08253v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-16">16. Context-Adaptive Document-Level Neural Machine Translation
</h2>

Title: [Context-Adaptive Document-Level Neural Machine Translation](https://arxiv.org/abs/2104.08259)

Authors: [Linlin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

> Most existing document-level neural machine translation (NMT) models leverage a fixed number of the previous or all global source sentences to handle the context-independent problem in standard NMT. However, the translating of each source sentence benefits from various sizes of context, and inappropriate context may harm the translation performance. In this work, we introduce a data-adaptive method that enables the model to adopt the necessary and useful context. Specifically, we introduce a light predictor into two document-level translation models to select the explicit context. Experiments demonstrate the proposed approach can significantly improve the performance over the previous methods with a gain up to 1.99 BLEU points.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08259](https://arxiv.org/abs/2104.08259) [cs.CL]** |
|           | (or **[arXiv:2104.08259v1](https://arxiv.org/abs/2104.08259v1) [cs.CL]** for this version) |






# 2021-04-16

[Return to Index](#Index)



<h2 id="2021-04-16-1">1. What Makes a Scientific Paper be Accepted for Publication?
</h2>

Title: [What Makes a Scientific Paper be Accepted for Publication?](https://arxiv.org/abs/2104.07112)

Authors: [Panagiotis Fytas](https://arxiv.org/search/cs?searchtype=author&query=Fytas%2C+P), [Georgios Rizos](https://arxiv.org/search/cs?searchtype=author&query=Rizos%2C+G), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer review process. We start by extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peer-review dataset. Second, since such global explanations do not justify causal interpretations, we provide a methodology for detecting confounding effects in natural language in order to generate causal explanations, under assumptions, in the form of lexicons. Our proposed linguistic explanation methodology indicates the following on a case dataset of ICLR submissions: a) the organising committee follows, for the most part, the recommendations of reviewers, and, b) the paper's main characteristics that led to reviewers recommending acceptance for publication are originality, clarity and substance.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2104.07112](https://arxiv.org/abs/2104.07112) [cs.CL]** |
|              | (or **[arXiv:2104.07112v1](https://arxiv.org/abs/2104.07112v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-2">2. An Interpretability Illusion for BERT
</h2>

Title: [An Interpretability Illusion for BERT](https://arxiv.org/abs/2104.07143)

Authors: [Tolga Bolukbasi](https://arxiv.org/search/cs?searchtype=author&query=Bolukbasi%2C+T), [Adam Pearce](https://arxiv.org/search/cs?searchtype=author&query=Pearce%2C+A), [Ann Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A), [Andy Coenen](https://arxiv.org/search/cs?searchtype=author&query=Coenen%2C+A), [Emily Reif](https://arxiv.org/search/cs?searchtype=author&query=Reif%2C+E), [Fernanda Viégas](https://arxiv.org/search/cs?searchtype=author&query=Viégas%2C+F), [Martin Wattenberg](https://arxiv.org/search/cs?searchtype=author&query=Wattenberg%2C+M)

> We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07143](https://arxiv.org/abs/2104.07143) [cs.CL]** |
|           | (or **[arXiv:2104.07143v1](https://arxiv.org/abs/2104.07143v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-3">3. An Alignment-Agnostic Model for Chinese Text Error Correction
</h2>

Title: [An Alignment-Agnostic Model for Chinese Text Error Correction](https://arxiv.org/abs/2104.07190)

Authors: [Liying Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+L), [Yue Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Weishun Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+W), [Liang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Jing Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J)

> This paper investigates how to correct Chinese text errors with types of mistaken, missing and redundant characters, which is common for Chinese native speakers. Most existing models based on detect-correct framework can correct mistaken characters errors, but they cannot deal with missing or redundant characters. The reason is that lengths of sentences before and after correction are not the same, leading to the inconsistence between model inputs and outputs. Although the Seq2Seq-based or sequence tagging methods provide solutions to the problem and achieved relatively good results on English context, but they do not perform well in Chinese context according to our experimental results. In our work, we propose a novel detect-correct framework which is alignment-agnostic, meaning that it can handle both text aligned and non-aligned occasions, and it can also serve as a cold start model when there are no annotated data provided. Experimental results on three datasets demonstrate that our method is effective and achieves the best performance among existing published models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07190](https://arxiv.org/abs/2104.07190) [cs.CL]** |
|           | (or **[arXiv:2104.07190v1](https://arxiv.org/abs/2104.07190v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-4">4. Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models
</h2>

Title: [Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](https://arxiv.org/abs/2104.07204)

Authors: [Yuxuan Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Y), [Yijia Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yansong Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Dongyan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+D)

> Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at [this https URL](https://github.com/alibaba/pretrained-language-models/LatticeBERT).

| Comments: | Accepted at NAACL 2021, 16 pages                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07204](https://arxiv.org/abs/2104.07204) [cs.CL]** |
|           | (or **[arXiv:2104.07204v1](https://arxiv.org/abs/2104.07204v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-5">5. Sentence-Permuted Paragraph Generation
</h2>

Title: [Sentence-Permuted Paragraph Generation](https://arxiv.org/abs/2104.07228)

Authors: [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Tong Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Zhichun Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M)

> Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for training, decoding, and candidate ranking in the sentence-permuted generation. Experiments on three paragraph generation benchmarks demonstrate PermGen generates more diverse outputs with a higher quality than existing models.

| Comments: | 19 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.07228](https://arxiv.org/abs/2104.07228) [cs.CL]** |
|           | (or **[arXiv:2104.07228v1](https://arxiv.org/abs/2104.07228v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-6">6. Adaptive Sparse Transformer for Multilingual Translation
</h2>

Title: [Adaptive Sparse Transformer for Multilingual Translation](https://arxiv.org/abs/2104.07358)

Authors: [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Dmitriy Genzel](https://arxiv.org/search/cs?searchtype=author&query=Genzel%2C+D)

> Multilingual machine translation has attracted much attention recently due to its support of knowledge transfer among languages and the low cost of training and deployment compared with numerous bilingual models. A known challenge of multilingual models is the negative language interference. In order to enhance the translation quality, deeper and wider architectures are applied to multilingual modeling for larger model capacity, which suffers from the increased inference cost at the same time. It has been pointed out in recent studies that parameters shared among languages are the cause of interference while they may also enable positive transfer. Based on these insights, we propose an adaptive and sparse architecture for multilingual modeling, and train the model to learn shared and language-specific parameters to improve the positive transfer and mitigate the interference. The sparse architecture only activates a subnetwork which preserves inference efficiency, and the adaptive design selects different subnetworks based on the input languages. Evaluated on multilingual translation across multiple public datasets, our model outperforms strong baselines in terms of translation quality without increasing the inference cost.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07358](https://arxiv.org/abs/2104.07358) [cs.CL]** |
|           | (or **[arXiv:2104.07358v1](https://arxiv.org/abs/2104.07358v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-7">7. Simultaneous Multi-Pivot Neural Machine Translation
</h2>

Title: [Simultaneous Multi-Pivot Neural Machine Translation](https://arxiv.org/abs/2104.07410)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Abhisek Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+A)

> Parallel corpora are indispensable for training neural machine translation (NMT) models, and parallel corpora for most language pairs do not exist or are scarce. In such cases, pivot language NMT can be helpful where a pivot language is used such that there exist parallel corpora between the source and pivot and pivot and target languages. Naturally, the quality of pivot language translation is more inferior to what could be achieved with a direct parallel corpus of a reasonable size for that pair. In a real-time simultaneous translation setting, the quality of pivot language translation deteriorates even further given that the model has to output translations the moment a few source words become available. To solve this issue, we propose multi-pivot translation and apply it to a simultaneous translation setting involving pivot languages. Our approach involves simultaneously translating a source language into multiple pivots, which are then simultaneously translated together into the target language by leveraging multi-source NMT. Our experiments in a low-resource setting using the N-way parallel UN corpus for Arabic to English NMT via French and Spanish as pivots reveals that in a simultaneous pivot NMT setting, using two pivot languages can lead to an improvement of up to 5.8 BLEU.

| Comments: | preliminary work. pardon the messy writing and mistakes. will be submitted to emnlp after major overhaul |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07410](https://arxiv.org/abs/2104.07410) [cs.CL]** |
|           | (or **[arXiv:2104.07410v1](https://arxiv.org/abs/2104.07410v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-8">8. First the worst: Finding better gender translations during beam search
</h2>

Title: [First the worst: Finding better gender translations during beam search](https://arxiv.org/abs/2104.07429)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Rosie Sallis](https://arxiv.org/search/cs?searchtype=author&query=Sallis%2C+R), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> Neural machine translation inference procedures like beam search generate the most likely output under the model. This can exacerbate any demographic biases exhibited by the model. We focus on gender bias resulting from systematic errors in grammatical gender translation, which can lead to human referents being misrepresented or misgendered.
> Most approaches to this problem adjust the training data or the model. By contrast, we experiment with simply adjusting the inference procedure. We experiment with reranking nbest lists using gender features obtained automatically from the source sentence, and applying gender constraints while decoding to improve nbest list gender diversity. We find that a combination of these techniques allows large gains in WinoMT accuracy without requiring additional bilingual data or an additional NMT model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07429](https://arxiv.org/abs/2104.07429) [cs.CL]** |
|           | (or **[arXiv:2104.07429v1](https://arxiv.org/abs/2104.07429v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-9">9. Effect of Post-processing on Contextualized Word Representations
</h2>

Title: [Effect of Post-processing on Contextualized Word Representations](https://arxiv.org/abs/2104.07456)

Authors: [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Firoj Alam](https://arxiv.org/search/cs?searchtype=author&query=Alam%2C+F), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N)

> Post-processing of static embedding has beenshown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principle components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy)and sequence classification tasks. Our findings raise interesting points in relation to theresearch studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07456](https://arxiv.org/abs/2104.07456) [cs.CL]** |
|           | (or **[arXiv:2104.07456v1](https://arxiv.org/abs/2104.07456v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-10">10. IndT5: A Text-to-Text Transformer for 10 Indigenous Languages
</h2>

Title: [IndT5: A Text-to-Text Transformer for 10 Indigenous Languages](https://arxiv.org/abs/2104.07483)

Authors: [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Wei-Rui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Hasan Cavusogl](https://arxiv.org/search/cs?searchtype=author&query=Cavusogl%2C+H)

> Transformer language models have become fundamental components of NLP based pipelines. Although several Transformer have been introduced to serve many languages, there is a shortage of models pre-trained for low-resource and Indigenous languages in particular. In this work, we introduce IndT5, the first Transformer language model for Indigenous languages. To train IndT5, we build IndCorpus, a new corpus for 10 Indigenous languages and Spanish. We also present the application of IndT5 to machine translation by investigating different approaches to translate between Spanish and the Indigenous languages as part of our contribution to the AmericasNLP 2021 Shared Task on Open Machine Translation. IndT5 and IndCorpus are publicly available for research.

| Comments: | AmericasNLP 2021                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07483](https://arxiv.org/abs/2104.07483) [cs.CL]** |
|           | (or **[arXiv:2104.07483v1](https://arxiv.org/abs/2104.07483v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-11">11. Generating Datasets with Pretrained Language Models
</h2>

Title: [Generating Datasets with Pretrained Language Models](https://arxiv.org/abs/2104.07540)

Authors: [Timo Schick](https://arxiv.org/search/cs?searchtype=author&query=Schick%2C+T), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> To obtain high-quality sentence embeddings from pretrained language models, they must either be augmented with additional pretraining objectives or finetuned on large amounts of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how large pretrained language models can be leveraged to obtain high-quality embeddings without requiring any labeled data, finetuning or modifications to their pretraining objective: We utilize their generative abilities to generate entire datasets of labeled text pairs from scratch, which can then be used for regular finetuning of much smaller models. Our fully unsupervised approach outperforms strong baselines on several English semantic textual similarity datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07540](https://arxiv.org/abs/2104.07540) [cs.CL]** |
|           | (or **[arXiv:2104.07540v1](https://arxiv.org/abs/2104.07540v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-12">12. Reward Optimization for Neural Machine Translation with Learned Metrics
</h2>

Title: [Reward Optimization for Neural Machine Translation with Learned Metrics](https://arxiv.org/abs/2104.07541)

Authors: [Raphael Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+R), [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Jung-Woo Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+J)

> Neural machine translation (NMT) models are conventionally trained with token-level negative log-likelihood (NLL), which does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. Multiple approaches are proposed to train NMT with BLEU as the reward, in order to directly improve the metric. However, it was reported that the gain in BLEU does not translate to real quality improvement, limiting the application in industry. Recently, it became clear to the community that BLEU has a low correlation with human judgment when dealing with state-of-the-art models. This leads to the emerging of model-based evaluation metrics. These new metrics are shown to have a much higher human correlation. In this paper, we investigate whether it is beneficial to optimize NMT models with the state-of-the-art model-based metric, BLEURT. We propose a contrastive-margin loss for fast and stable reward optimization suitable for large NMT models. In experiments, we perform automatic and human evaluations to compare models trained with smoothed BLEU and BLEURT to the baseline models. Results show that the reward optimization with BLEURT is able to increase the metric scores by a large margin, in contrast to limited gain when training with smoothed BLEU. The human evaluation shows that models trained with BLEURT improve adequacy and coverage of translations. Code is available via [this https URL](https://github.com/naver-ai/MetricMT).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07541](https://arxiv.org/abs/2104.07541) [cs.CL]** |
|           | (or **[arXiv:2104.07541v1](https://arxiv.org/abs/2104.07541v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-13">13. Hierarchical Learning for Generation with Long Source Sequences
</h2>

Title: [Hierarchical Learning for Generation with Long Source Sequences](https://arxiv.org/abs/2104.07545)

Authors: [Tobias Rohde](https://arxiv.org/search/cs?searchtype=author&query=Rohde%2C+T), [Xiaoxia Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yinhan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> One of the challenges for current sequence to sequence (seq2seq) models is processing long sequences, such as those in summarization and document level machine translation tasks. These tasks require the model to reason at the token level as well as the sentence and paragraph level. We design and study a new Hierarchical Attention Transformer-based architecture (HAT) that outperforms standard Transformers on several sequence to sequence tasks. In particular, our model achieves stateof-the-art results on four summarization tasks, including ArXiv, CNN/DM, SAMSum, and AMI, and we push PubMed R1 & R2 SOTA further. Our model significantly outperforms our document-level machine translation baseline by 28 BLEU on the WMT19 EN-DE document translation task. We also investigate what the hierarchical layers learn by visualizing the hierarchical encoder-decoder attention. Finally, we study hierarchical learning on encoder-only pre-training and analyze its performance on classification downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07545](https://arxiv.org/abs/2104.07545) [cs.CL]** |
|           | (or **[arXiv:2104.07545v1](https://arxiv.org/abs/2104.07545v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-14">14. Sometimes We Want Translationese
</h2>

Title: [Sometimes We Want Translationese](https://arxiv.org/abs/2104.07623)

Authors: [Prasanna Parthasarathi](https://arxiv.org/search/cs?searchtype=author&query=Parthasarathi%2C+P), [Koustuv Sinha](https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+K), [Joelle Pineau](https://arxiv.org/search/cs?searchtype=author&query=Pineau%2C+J), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case.

| Comments: | 16 pages, 11 figures and 3 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07623](https://arxiv.org/abs/2104.07623) [cs.CL]** |
|           | (or **[arXiv:2104.07623v1](https://arxiv.org/abs/2104.07623v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-15">15. Demystify Optimization Challenges in Multilingual Transformers
</h2>

Title: [Demystify Optimization Challenges in Multilingual Transformers](https://arxiv.org/abs/2104.07639)

Authors: [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H)

> Multilingual Transformer improves parameter efficiency and crosslingual transfer. How to effectively train multilingual models has not been well studied. Using multilingual machine translation as a testbed, we study optimization challenges from loss landscape and parameter plasticity perspectives. We found that imbalanced training data poses task interference between high and low resource languages, characterized by nearly orthogonal gradients for major parameters and the optimization trajectory being mostly dominated by high resource. We show that local curvature of the loss surface affects the degree of interference, and existing heuristics of data subsampling implicitly reduces the sharpness, although still face a trade-off between high and low resource languages. We propose a principled multi-objective optimization algorithm, Curvature Aware Task Scaling (CATS), which improves both optimization and generalization especially for low resource. Experiments on TED, WMT and OPUS-100 benchmarks demonstrate that CATS advances the Pareto front of accuracy while being efficient to apply to massive multilingual settings at the scale of 100 languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07639](https://arxiv.org/abs/2104.07639) [cs.CL]** |
|           | (or **[arXiv:2104.07639v1](https://arxiv.org/abs/2104.07639v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-16">16. Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining
</h2>

Title: [Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](https://arxiv.org/abs/2104.07642)

Authors: [Chih-chan Tien](https://arxiv.org/search/cs?searchtype=author&query=Tien%2C+C), [Shane Steinert-Threlkeld](https://arxiv.org/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S)

> This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. And such transfer from bilingual alignment to multilingual alignment is a dual-pivot transfer from two pivot languages to other language pairs. To study this theory, we train an unsupervised model with unpaired sentences and another single-pair supervised model with bitexts, both based on the unsupervised language model XLM-R. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with higher multilingual alignment.

| Comments: | 10 pages, 2 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07642](https://arxiv.org/abs/2104.07642) [cs.CL]** |
|           | (or **[arXiv:2104.07642v1](https://arxiv.org/abs/2104.07642v1) [cs.CL]** for this version) |





# 2021-04-15

[Return to Index](#Index)



<h2 id="2021-04-15-1">1. Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation
</h2>

Title: [Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation](https://arxiv.org/abs/2104.06457)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.

| Comments: | Accepted at NAACL-HLT 2021 (short paper)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.06457](https://arxiv.org/abs/2104.06457) [cs.CL]** |
|           | (or **[arXiv:2104.06457v1](https://arxiv.org/abs/2104.06457v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-2">2. Large-Scale Self- and Semi-Supervised Learning for Speech Translation
</h2>

Title: [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06678](https://arxiv.org/abs/2104.06678) [cs.CL]** |
|           | (or **[arXiv:2104.06678v1](https://arxiv.org/abs/2104.06678v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-3">3. The Curious Case of Hallucinations in Neural Machine Translation
</h2>

Title: [The Curious Case of Hallucinations in Neural Machine Translation](https://arxiv.org/abs/2104.06683)

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Arul Menezes](https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+A), [Marcin Junczys-Dowmunt](https://arxiv.org/search/cs?searchtype=author&query=Junczys-Dowmunt%2C+M)

> In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman (2020), and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06683](https://arxiv.org/abs/2104.06683) [cs.CL]** |
|           | (or **[arXiv:2104.06683v1](https://arxiv.org/abs/2104.06683v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-4">4. Sentence Embeddings by Ensemble Distillation
</h2>

Title: [Sentence Embeddings by Ensemble Distillation](https://arxiv.org/abs/2104.06719)

Authors: [Fredrik Carlsson Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+F+C+M)

> This paper contributes a new State Of The Art (SOTA) for Semantic Textual Similarity (STS). We compare and combine a number of recently proposed sentence embedding methods for STS, and propose a novel and simple ensemble knowledge distillation scheme that improves on previous approaches. Our experiments demonstrate that a model trained to learn the average embedding space from multiple ensemble students outperforms all the other individual models with high robustness. Utilizing our distillation method in combination with previous methods, we significantly improve on the SOTA unsupervised STS, and by proper hyperparameter tuning of previous methods we improve the supervised SOTA scores.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06719](https://arxiv.org/abs/2104.06719) [cs.CL]** |
|           | (or **[arXiv:2104.06719v1](https://arxiv.org/abs/2104.06719v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-5">5. Distributed Word Representation in Tsetlin Machine
</h2>

Title: [Distributed Word Representation in Tsetlin Machine](https://arxiv.org/abs/2104.06901)

Authors: [Rohan Kumar Yadav](https://arxiv.org/search/cs?searchtype=author&query=Yadav%2C+R+K), [Lei Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+L), [Ole-Christoffer Granmo](https://arxiv.org/search/cs?searchtype=author&query=Granmo%2C+O), [Morten Goodwin](https://arxiv.org/search/cs?searchtype=author&query=Goodwin%2C+M)

> Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based on propositional logic. The algorithm has demonstrated competitive performance in many Natural Language Processing (NLP) tasks, including sentiment analysis, text classification, and Word Sense Disambiguation (WSD). To obtain human-level interpretability, legacy TM employs Boolean input features such as bag-of-words (BOW). However, the BOW representation makes it difficult to use any pre-trained information, for instance, word2vec and GloVe word representations. This restriction has constrained the performance of TM compared to deep neural networks (DNNs) in NLP. To reduce the performance gap, in this paper, we propose a novel way of using pre-trained word representations for TM. The approach significantly enhances the TM performance and maintains interpretability at the same time. We achieve this by extracting semantically related words from pre-trained word representations as input features to the TM. Our experiments show that the accuracy of the proposed approach is significantly higher than the previous BOW-based TM, reaching the level of DNN-based models.

| Comments: | 9 pages, 13 figures, and 4 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06901](https://arxiv.org/abs/2104.06901) [cs.CL]** |
|           | (or **[arXiv:2104.06901v1](https://arxiv.org/abs/2104.06901v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-6">6. Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
</h2>

Title: [Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey](https://arxiv.org/abs/2104.06951)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D)

> The development of deep learning techniques has allowed Neural Machine Translation (NMT) models to become extremely powerful, given sufficient training data and training time. However, systems struggle when translating text from a new domain with a distinct style or vocabulary. Tuning on a representative training corpus allows good in-domain translation, but such data-centric approaches can cause over-fitting to new data and `catastrophic forgetting' of previously learned behaviour.
> We concentrate on more robust approaches to domain adaptation for NMT, particularly the case where a system may need to translate sentences from multiple domains. We divide techniques into those relating to data selection, model architecture, parameter adaptation procedure, and inference procedure. We finally highlight the benefits of domain adaptation and multi-domain adaptation techniques to other lines of NMT research.

| Comments: | 39 pages + references                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.06951](https://arxiv.org/abs/2104.06951) [cs.CL]** |
|           | (or **[arXiv:2104.06951v1](https://arxiv.org/abs/2104.06951v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-7">7. TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning
</h2>

Title: [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](https://arxiv.org/abs/2104.06979)

Authors: [Kexin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Nils Reimers](https://arxiv.org/search/cs?searchtype=author&query=Reimers%2C+N), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Learning sentence embeddings often requires large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong pre-training method for learning sentence embeddings, significantly outperforming other approaches like Masked Language Model.
> A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06979](https://arxiv.org/abs/2104.06979) [cs.CL]** |
|           | (or **[arXiv:2104.06979v1](https://arxiv.org/abs/2104.06979v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-8">8. Sparse Attention with Linear Units
</h2>

Title: [Sparse Attention with Linear Units](https://arxiv.org/abs/2104.07012)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07012](https://arxiv.org/abs/2104.07012) [cs.CL]** |
|           | (or **[arXiv:2104.07012v1](https://arxiv.org/abs/2104.07012v1) [cs.CL]** for this version) |







# 2021-04-14

[Return to Index](#Index)



<h2 id="2021-04-14-1">1. Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique
</h2>

Title: [Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique](https://arxiv.org/abs/2104.05753)

Authors: [Felermino D. M. A. Ali](https://arxiv.org/search/cs?searchtype=author&query=Ali%2C+F+D+M+A), [Andrew Caines](https://arxiv.org/search/cs?searchtype=author&query=Caines%2C+A), [Jaimito L. A. Malavi](https://arxiv.org/search/cs?searchtype=author&query=Malavi%2C+J+L+A)

> Major advancement in the performance of machine translation models has been made possible in part thanks to the availability of large-scale parallel corpora. But for most languages in the world, the existence of such corpora is rare. Emakhuwa, a language spoken in Mozambique, is like most African languages low-resource in NLP terms. It lacks both computational and linguistic resources and, to the best of our knowledge, few parallel corpora including Emakhuwa already exist. In this paper we describe the creation of the Emakhuwa-Portuguese parallel corpus, which is a collection of texts from the Jehovah's Witness website and a variety of other sources including the African Story Book website, the Universal Declaration of Human Rights and Mozambican legal documents. The dataset contains 47,415 sentence pairs, amounting to 699,976 word tokens of Emakhuwa and 877,595 word tokens in Portuguese. After normalization processes which remain to be completed, the corpus will be made freely available for research use.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05753](https://arxiv.org/abs/2104.05753) [cs.CL]** |
|           | (or **[arXiv:2104.05753v1](https://arxiv.org/abs/2104.05753v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-2">2. Targeted Adversarial Training for Natural Language Understanding
</h2>

Title: [Targeted Adversarial Training for Natural Language Understanding](https://arxiv.org/abs/2104.05847)

Authors: [Lis Pereira](https://arxiv.org/search/cs?searchtype=author&query=Pereira%2C+L), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Hoifung Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Ichiro Kobayashi](https://arxiv.org/search/cs?searchtype=author&query=Kobayashi%2C+I)

> We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: [this https URL](https://github.com/namisan/mt-dnn).

| Comments: | 9 pages, 4 tables, 3 figurers, NAACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05847](https://arxiv.org/abs/2104.05847) [cs.CL]** |
|           | (or **[arXiv:2104.05847v1](https://arxiv.org/abs/2104.05847v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-3">3. Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation
</h2>

Title: [Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation](https://arxiv.org/abs/2104.05848)

Authors: [Zhong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

> We translate a closed text that is known in advance into a severely low resource language by leveraging massive source parallelism. Our contribution is four-fold. Firstly, we rank 124 source languages empirically to determine their closeness to the low resource language and select the top few. We call the linguistic definition of language family Family of Origin (FAMO), and we call the empirical definition of higher-ranked languages using our metrics Family of Choice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual Order-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines (~3.5\%) of low resource data from the Bible dataset and the medical EMEA dataset. Using English as a hypothetical low resource language to translate from Spanish, we obtain a +24.7 BLEU increase over a multilingual baseline, and a +10.2 BLEU increase over our asymmetric baseline. Thirdly, we also use a real severely low resource Mayan language, Eastern Pokomchi. Finally, we add an order-preserving lexiconized component to translate named entities accurately. We build a massive lexicon table for 2,939 Bible named entities in 124 source languages, and include many that occur once and covers more than 66 severely low resource languages. Training on randomly sampled 1,093 lines of low resource data, we reach a 30.3 BLEU score for Spanish-English translation testing on 30,022 lines of Bible, and a 42.8 BLEU score for Portuguese-English translation on the medical EMEA dataset.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05848](https://arxiv.org/abs/2104.05848) [cs.CL]** |
|           | (or **[arXiv:2104.05848v1](https://arxiv.org/abs/2104.05848v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-4">4. Discourse Probing of Pretrained Language Models
</h2>

Title: [Discourse Probing of Pretrained Language Models](https://arxiv.org/abs/2104.05882)

Authors: [Fajri Koto](https://arxiv.org/search/cs?searchtype=author&query=Koto%2C+F), [Jey Han Lau](https://arxiv.org/search/cs?searchtype=author&query=Lau%2C+J+H), [Timothy Baldwin](https://arxiv.org/search/cs?searchtype=author&query=Baldwin%2C+T)

> Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -- but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.

| Comments: | Accepted at NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05882](https://arxiv.org/abs/2104.05882) [cs.CL]** |
|           | (or **[arXiv:2104.05882v1](https://arxiv.org/abs/2104.05882v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-5">5. Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation
</h2>

Title: [Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation](https://arxiv.org/abs/2104.05964)

Authors: [Kyeongpil Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+K), [Kyohoon Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+K), [Soyoung Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Sujin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+S), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J), [Yougbin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y)

> Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a self-attention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.05964](https://arxiv.org/abs/2104.05964) [cs.CL]** |
|           | (or **[arXiv:2104.05964v1](https://arxiv.org/abs/2104.05964v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-6">6. Gender Bias in Machine Translation
</h2>

Title: [Gender Bias in Machine Translation](https://arxiv.org/abs/2104.06001)

Authors: [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i)critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii)discuss the mitigating strategies proposed so far, and iv)point toward potential directions for future work.

| Comments: | Accepted for publication in Transaction of the Association for Computational Linguistics (TACL), 2021. Pre-MIT Press publication version |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.06001](https://arxiv.org/abs/2104.06001) [cs.CL]** |
|           | (or **[arXiv:2104.06001v1](https://arxiv.org/abs/2104.06001v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-7">7. Lessons on Parameter Sharing across Layers in Transformers
</h2>

Title: [Lessons on Parameter Sharing across Layers in Transformers](https://arxiv.org/abs/2104.06022)

Authors: [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S)

> We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06022](https://arxiv.org/abs/2104.06022) [cs.CL]** |
|           | (or **[arXiv:2104.06022v1](https://arxiv.org/abs/2104.06022v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-8">8. What's in your Head? Emergent Behaviour in Multi-Task Transformer Models
</h2>

Title: [What's in your Head? Emergent Behaviour in Multi-Task Transformer Models](https://arxiv.org/abs/2104.06129)

Authors: [Mor Geva](https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M), [Uri Katz](https://arxiv.org/search/cs?searchtype=author&query=Katz%2C+U), [Aviv Ben-Arie](https://arxiv.org/search/cs?searchtype=author&query=Ben-Arie%2C+A), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06129](https://arxiv.org/abs/2104.06129) [cs.CL]** |
|           | (or **[arXiv:2104.06129v1](https://arxiv.org/abs/2104.06129v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-9">9. Understanding Hard Negatives in Noise Contrastive Estimation
</h2>

Title: [Understanding Hard Negatives in Noise Contrastive Estimation](https://arxiv.org/abs/2104.06245)

Authors: [Wenzheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Karl Stratos](https://arxiv.org/search/cs?searchtype=author&query=Stratos%2C+K)

> The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives -- highest-scoring incorrect examples under the model -- are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06245](https://arxiv.org/abs/2104.06245) [cs.CL]** |
|           | (or **[arXiv:2104.06245v1](https://arxiv.org/abs/2104.06245v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-10">10. Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling
</h2>

Title: [Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling](https://arxiv.org/abs/2104.06268)

Authors: [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I)

> In this thesis, we address the data scarcity and limitations of linguistic theory by proposing language-agnostic multi-task training methods. First, we introduce a meta-learning-based approach, meta-transfer learning, in which information is judiciously extracted from high-resource monolingual speech data to the code-switching domain. The meta-transfer learning quickly adapts the model to the code-switching task from a number of monolingual tasks by learning to learn in a multi-task learning fashion. Second, we propose a novel multilingual meta-embeddings approach to effectively represent code-switching data by acquiring useful knowledge learned in other languages, learning the commonalities of closely related languages and leveraging lexical composition. The method is far more efficient compared to contextualized pre-trained multilingual models. Third, we introduce multi-task learning to integrate syntactic information as a transfer learning strategy to a language model and learn where to code-switch. To further alleviate the aforementioned issues, we propose a data augmentation method using Pointer-Gen, a neural network using a copy mechanism to teach the model the code-switch points from monolingual parallel sentences. We disentangle the need for linguistic theory, and the model captures code-switching points by attending to input words and aligning the parallel words, without requiring any word alignments or constituency parsers. More importantly, the model can be effectively used for languages that are syntactically different, and it outperforms the linguistic theory-based models.

| Comments: | HKUST PhD Thesis. 120 pages                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.06268](https://arxiv.org/abs/2104.06268) [cs.CL]** |
|           | (or **[arXiv:2104.06268v1](https://arxiv.org/abs/2104.06268v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-11">11. EXPLAINABOARD: An Explainable Leaderboard for NLP
</h2>

Title: [EXPLAINABOARD: An Explainable Leaderboard for NLP](https://arxiv.org/abs/2104.06387)

Authors: [Pengfei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Jinlan Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Yang Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [Weizhe Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+W), [Shuaicheng Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Junqi Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Yixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Zihuiwen Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B, C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or and in what contexts do particular errors occur?). ExplainaBoard has been deployed at \url{[this http URL](http://explainaboard.nlpedia.ai/)}, and we have additionally released our interpretable evaluation code at \url{[this https URL](https://github.com/neulab/ExplainaBoard)} and output files from more than 300 systems, 40 datasets, and 9 tasks to motivate the "output-driven" research in the future.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06387](https://arxiv.org/abs/2104.06387) [cs.CL]** |
|           | (or **[arXiv:2104.06387v1](https://arxiv.org/abs/2104.06387v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-12">12. Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding
</h2>

Title: [Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding](https://arxiv.org/abs/2104.06393)

Authors: [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Yiren Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Spoken language understanding (SLU) system usually consists of various pipeline components, where each component heavily relies on the results of its upstream ones. For example, Intent detection (ID), and slot filling (SF) require its upstream automatic speech recognition (ASR) to transform the voice into text. In this case, the upstream perturbations, e.g. ASR errors, environmental noise and careless user speaking, will propagate to the ID and SF models, thus deteriorating the system performance. Therefore, the well-performing SF and ID models are expected to be noise resistant to some extent. However, existing models are trained on clean data, which causes a \textit{gap between clean data training and real-world inference.} To bridge the gap, we propose a method from the perspective of domain adaptation, by which both high- and low-quality samples are embedding into similar vector space. Meanwhile, we design a denoising generation model to reduce the impact of the low-quality samples. Experiments on the widely-used dataset, i.e. Snips, and large scale in-house dataset (10 million training examples) demonstrate that this method not only outperforms the baseline models on real-world (noisy) corpus but also enhances the robustness, that is, it produces high-quality results under a noisy environment. The source code will be released.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.06393](https://arxiv.org/abs/2104.06393) [cs.CL]** |
|           | (or **[arXiv:2104.06393v1](https://arxiv.org/abs/2104.06393v1) [cs.CL]** for this version) |





# 2021-04-13

[Return to Index](#Index)



<h2 id="2021-04-13-1">1. Achieving Model Robustness through Discrete Adversarial Training
</h2>

Title: [Achieving Model Robustness through Discrete Adversarial Training](https://arxiv.org/abs/2104.05062)

Authors: [Maor Ivgi](https://arxiv.org/search/cs?searchtype=author&query=Ivgi%2C+M), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only, i.e., given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every step, adapting to the changing nature of the model. We also consider efficient attacks based on random sampling, that unlike prior work are not based on expensive search-based procedures. As a second contribution, we provide a general formulation for multiple search-based attacks from past work, and propose a new attack based on best-first search. Surprisingly, we find that random sampling leads to impressive gains in robustness, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher training cost, significantly improving robustness on three datasets. Last, we show that our proposed algorithm substantially improves robustness compared to prior methods.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05062](https://arxiv.org/abs/2104.05062) [cs.LG]** |
|           | (or **[arXiv:2104.05062v1](https://arxiv.org/abs/2104.05062v1) [cs.LG]** for this version) |





<h2 id="2021-04-13-2">2. TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation
</h2>

Title: [TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation](https://arxiv.org/abs/2104.04632)

Authors: [Hansi Hettiarachchi](https://arxiv.org/search/cs?searchtype=author&query=Hettiarachchi%2C+H), [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T)

> Identifying whether a word carries the same meaning or different meaning in two contexts is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. Most of the previous work in this area rely on language-specific resources making it difficult to generalise across languages. Considering this limitation, our approach to SemEval-2021 Task 2 is based only on pretrained transformer models and does not use any language-specific processing and resources. Despite that, our best model achieves 0.90 accuracy for English-English subtask which is very compatible compared to the best result of the subtask; 0.93 accuracy. Our approach also achieves satisfactory results in other monolingual and cross-lingual language pairs as well.

| Comments: | Accepted to SemEval-2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.04632](https://arxiv.org/abs/2104.04632) [cs.CL]** |
|           | (or **[arXiv:2104.04632v1](https://arxiv.org/abs/2104.04632v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-3">3. Not All Attention Is All You Need
</h2>

Title: [Not All Attention Is All You Need](https://arxiv.org/abs/2104.04692)

Authors: [Hongqiu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M)

> Self-attention based models have achieved remarkable success in natural language processing. However, the self-attention network design is questioned as suboptimal in recent studies, due to its veiled validity and high redundancy. In this paper, we focus on pre-trained language models with self-pruning training design on task-specific tuning. We demonstrate that the lighter state-of-the-art models with nearly 80% of self-attention layers pruned, may achieve even better results on multiple tasks, including natural language understanding, document classification, named entity recognition and POS tagging, with nearly twice faster inference.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.04692](https://arxiv.org/abs/2104.04692) [cs.CL]** |
|           | (or **[arXiv:2104.04692v1](https://arxiv.org/abs/2104.04692v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-4">4. Sentiment-based Candidate Selection for NMT
</h2>

Title: [Sentiment-based Candidate Selection for NMT](https://arxiv.org/abs/2104.04840)

Authors: [Alex Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+A), [Derry Tanti Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D+T)

> The explosion of user-generated content (UGC)--e.g. social media posts, comments, and reviews--has motivated the development of NLP applications tailored to these types of informal texts. Prevalent among these applications have been sentiment analysis and machine translation (MT). Grounded in the observation that UGC features highly idiomatic, sentiment-charged language, we propose a decoder-side approach that incorporates automatic sentiment scoring into the MT candidate selection process. We train separate English and Spanish sentiment classifiers, then, using n-best candidates generated by a baseline MT model with beam search, select the candidate that minimizes the absolute difference between the sentiment score of the source sentence and that of the translation, and perform a human evaluation to assess the produced translations. Unlike previous work, we select this minimally divergent translation by considering the sentiment scores of the source sentence and translation on a continuous interval, rather than using e.g. binary classification, allowing for more fine-grained selection of translation candidates. The results of human evaluations show that, in comparison to the open-source MT baseline model on top of which our sentiment-based pipeline is built, our pipeline produces more accurate translations of colloquial, sentiment-heavy source texts.

| Comments:    | 14 pages, 1 figure                                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2104.04840](https://arxiv.org/abs/2104.04840) [cs.CL]** |
|              | (or **[arXiv:2104.04840v1](https://arxiv.org/abs/2104.04840v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-5">5. Disentangled Contrastive Learning for Learning Robust Textual Representations
</h2>

Title: [Disentangled Contrastive Learning for Learning Robust Textual Representations](https://arxiv.org/abs/2104.04907)

Authors: [Xiang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Xin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+X), [Zhen Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+Z), [Hongbin Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Shumin Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Although the self-supervised pre-training of transformer models has resulted in the revolutionizing of natural language processing (NLP) applications and the achievement of state-of-the-art results with regard to various benchmarks, this process is still vulnerable to small and imperceptible permutations originating from legitimate inputs. Intuitively, the representations should be similar in the feature space with subtle input permutations, while large variations occur with different meanings. This motivates us to investigate the learning of robust textual representation in a contrastive manner. However, it is non-trivial to obtain opposing semantic instances for textual samples. In this study, we propose a disentangled contrastive learning method that separately optimizes the uniformity and alignment of representations without negative sampling. Specifically, we introduce the concept of momentum representation consistency to align features and leverage power normalization while conforming the uniformity. Our experimental results for the NLP benchmarks demonstrate that our approach can obtain better results compared with the baselines, as well as achieve promising improvements with invariance tests and adversarial attacks. The code is available in [this https URL](https://github.com/zjunlp/DCL).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.04907](https://arxiv.org/abs/2104.04907) [cs.CL]** |
|           | (or **[arXiv:2104.04907v1](https://arxiv.org/abs/2104.04907v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-6">6. Assessing Reference-Free Peer Evaluation for Machine Translation
</h2>

Title: [Assessing Reference-Free Peer Evaluation for Machine Translation](https://arxiv.org/abs/2104.05146)

Authors: [Sweta Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+S), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05146](https://arxiv.org/abs/2104.05146) [cs.CL]** |
|           | (or **[arXiv:2104.05146v1](https://arxiv.org/abs/2104.05146v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-7">7. FUDGE: Controlled Text Generation With Future Discriminators
</h2>

Title: [FUDGE: Controlled Text Generation With Future Discriminators](https://arxiv.org/abs/2104.05218)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in language generation, and formality change in machine translation -- and observe gains in all three tasks.

| Comments: | To appear at NAACL 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.05218](https://arxiv.org/abs/2104.05218) [cs.CL]** |
|           | (or **[arXiv:2104.05218v1](https://arxiv.org/abs/2104.05218v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-8">8. Machine Translation Decoding beyond Beam Search
</h2>

Title: [Machine Translation Decoding beyond Beam Search](https://arxiv.org/abs/2104.05336)

Authors: [Rémi Leblond](https://arxiv.org/search/cs?searchtype=author&query=Leblond%2C+R), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac%2C+J), [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L), [Miruna Pislar](https://arxiv.org/search/cs?searchtype=author&query=Pislar%2C+M), [Jean-Baptiste Lespiau](https://arxiv.org/search/cs?searchtype=author&query=Lespiau%2C+J), [Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&query=Antonoglou%2C+I), [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals%2C+O)

> Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.

| Comments: | 23 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.05336](https://arxiv.org/abs/2104.05336) [cs.CL]** |
|           | (or **[arXiv:2104.05336v1](https://arxiv.org/abs/2104.05336v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-9">9. Self-Training with Weak Supervision
</h2>

Title: [Self-Training with Weak Supervision](https://arxiv.org/abs/2104.05514)

Authors: [Giannis Karamanolakis](https://arxiv.org/search/cs?searchtype=author&query=Karamanolakis%2C+G), [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind.
> In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.

| Comments: | Accepted to NAACL 2021 (Long Paper)                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2104.05514](https://arxiv.org/abs/2104.05514) [cs.CL]** |
|           | (or **[arXiv:2104.05514v1](https://arxiv.org/abs/2104.05514v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-10">10. Survey on reinforcement learning for language processing
</h2>

Title: [Survey on reinforcement learning for language processing](https://arxiv.org/abs/2104.05565)

Authors: [Victor Uc-Cetina](https://arxiv.org/search/cs?searchtype=author&query=Uc-Cetina%2C+V), [Nicolas Navarro-Guerrero](https://arxiv.org/search/cs?searchtype=author&query=Navarro-Guerrero%2C+N), [Anabel Martin-Gonzalez](https://arxiv.org/search/cs?searchtype=author&query=Martin-Gonzalez%2C+A), [Cornelius Weber](https://arxiv.org/search/cs?searchtype=author&query=Weber%2C+C), [Stefan Wermter](https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S)

> In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of natural language processing, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in natural language processing that might benefit from reinforcement learning.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05565](https://arxiv.org/abs/2104.05565) [cs.CL]** |
|           | (or **[arXiv:2104.05565v1](https://arxiv.org/abs/2104.05565v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-11">11. Backtranslation Feedback Improves User Confidence in MT, Not Quality
</h2>

Title: [Backtranslation Feedback Improves User Confidence in MT, Not Quality](https://arxiv.org/abs/2104.05688)

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Michal Novák](https://arxiv.org/search/cs?searchtype=author&query=Novák%2C+M), [Matúš Žilinec](https://arxiv.org/search/cs?searchtype=author&query=Žilinec%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O), [Mateo Obregón](https://arxiv.org/search/cs?searchtype=author&query=Obregón%2C+M), [Robin L. Hill](https://arxiv.org/search/cs?searchtype=author&query=Hill%2C+R+L), [Frédéric Blain](https://arxiv.org/search/cs?searchtype=author&query=Blain%2C+F), [Marina Fomicheva](https://arxiv.org/search/cs?searchtype=author&query=Fomicheva%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L), [Lisa Yankovskaya](https://arxiv.org/search/cs?searchtype=author&query=Yankovskaya%2C+L)

> Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.

| Comments: | 9 pages (excluding references); to appear at NAACL-HWT 2021  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2104.05688](https://arxiv.org/abs/2104.05688) [cs.CL]** |
|           | (or **[arXiv:2104.05688v1](https://arxiv.org/abs/2104.05688v1) [cs.CL]** for this version) |













# 2021-04-12

[Return to Index](#Index)



<h2 id="2021-04-12-1">1. Video-aided Unsupervised Grammar Induction
</h2>

Title: [Video-aided Unsupervised Grammar Induction](https://arxiv.org/abs/2104.04369)

Authors: [Songyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Linfeng Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L), [Lifeng Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+L), [Kun Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on learning syntactic grammars from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.

| Comments: | This paper is accepted by NAACL'21                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.04369](https://arxiv.org/abs/2104.04369) [cs.CV]** |
|           | (or **[arXiv:2104.04369v1](https://arxiv.org/abs/2104.04369v1) [cs.CV]** for this version) |





<h2 id="2021-04-12-2">2. Design and Implementation of English To Yoruba Verb Phrase Machine Translation System
</h2>

Title: [Design and Implementation of English To Yoruba Verb Phrase Machine Translation System](https://arxiv.org/abs/2104.04125)

Authors: [Safiriyu Eludiora](https://arxiv.org/search/cs?searchtype=author&query=Eludiora%2C+S), [Benjamin Ajibade](https://arxiv.org/search/cs?searchtype=author&query=Ajibade%2C+B)

> We aim to develop an English to Yoruba machine translation system which can translate English verb phrase text to its Yoruba equivalent.Words from both languages Source Language and Target Language were collected for the verb phrase group in the home domain.The lexical translation is done by assigning values of the matching word in the dictionary.The syntax of the two languages was realized using Context-Free Grammar,we validated the rewrite rules with finite state automata.The human evaluation method was used and expert fluency scored.The evaluation shows the system performed better than that of sampled Google translation with over 70 percent of the response matching that of the system's output.

| Comments: | 9 pages, 9 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.04125](https://arxiv.org/abs/2104.04125) [cs.CL]** |
|           | (or **[arXiv:2104.04125v1](https://arxiv.org/abs/2104.04125v1) [cs.CL]** for this version) |





<h2 id="2021-04-12-3">3. Efficient Large-Scale Language Model Training on GPU Clusters
</h2>

Title: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473)

Authors: [Deepak Narayanan](https://arxiv.org/search/cs?searchtype=author&query=Narayanan%2C+D), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Jared Casper](https://arxiv.org/search/cs?searchtype=author&query=Casper%2C+J), [Patrick LeGresley](https://arxiv.org/search/cs?searchtype=author&query=LeGresley%2C+P), [Mostofa Patwary](https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M), [Vijay Korthikanti](https://arxiv.org/search/cs?searchtype=author&query=Korthikanti%2C+V), [Dmitri Vainbrand](https://arxiv.org/search/cs?searchtype=author&query=Vainbrand%2C+D), [Prethvi Kashinkunti](https://arxiv.org/search/cs?searchtype=author&query=Kashinkunti%2C+P), [Julie Bernauer](https://arxiv.org/search/cs?searchtype=author&query=Bernauer%2C+J), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B), [Amar Phanishayee](https://arxiv.org/search/cs?searchtype=author&query=Phanishayee%2C+A), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia%2C+M)

> Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these large models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on a single GPU or even on a multi-GPU server; and b) the number of compute operations required to train these models can result in unrealistically long training times. New methods of model parallelism such as tensor and pipeline parallelism have been proposed to address these challenges; unfortunately, naive usage leads to fundamental scaling issues at thousands of GPUs due to various reasons, e.g., expensive cross-node communication or idle periods waiting on other devices.
> In this work, we show how to compose different types of parallelism methods (tensor, pipeline, and data paralleism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models we can efficiently train compared to existing systems. We discuss various implementations of pipeline parallelism and propose a novel schedule that can improve throughput by more than 10% with comparable memory footprint compared to previously-proposed approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. The composition of these techniques allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of peak; previous efforts to train similar-sized models achieve much lower throughput (36% of theoretical peak). Our code has been open-sourced at [this https URL](https://github.com/nvidia/megatron-lm).

| Subjects: | **Computation and Language (cs.CL)**; Distributed, Parallel, and Cluster Computing (cs.DC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.04473](https://arxiv.org/abs/2104.04473) [cs.CL]** |
|           | (or **[arXiv:2104.04473v1](https://arxiv.org/abs/2104.04473v1) [cs.CL]** for this version) |





<h2 id="2021-04-12-4">4. Chinese Character Decomposition for Neural MT with Multi-Word Expressions
</h2>

Title: [Chinese Character Decomposition for Neural MT with Multi-Word Expressions](https://arxiv.org/abs/2104.04497)

Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Gareth J. F. Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+G+J+F), [Alan F. Smeaton](https://arxiv.org/search/cs?searchtype=author&query=Smeaton%2C+A+F), [Paolo Bolzoni](https://arxiv.org/search/cs?searchtype=author&query=Bolzoni%2C+P)

> Chinese character decomposition has been used as a feature to enhance Machine Translation (MT) models, combining radicals into character and word level models. Recent work has investigated ideograph or stroke level embedding. However, questions remain about different decomposition levels of Chinese character representations, radical and strokes, best suited for MT. To investigate the impact of Chinese decomposition embedding in detail, i.e., radical, stroke, and intermediate levels, and how well these decompositions represent the meaning of the original character sequences, we carry out analysis with both automated and human evaluation of MT. Furthermore, we investigate if the combination of decomposed Multiword Expressions (MWEs) can enhance the model learning. MWE integration into MT has seen more than a decade of exploration. However, decomposed MWEs has not previously been explored.

| Comments: | Accepted to publish in NoDaLiDa2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.04497](https://arxiv.org/abs/2104.04497) [cs.CL]** |
|           | (or **[arXiv:2104.04497v1](https://arxiv.org/abs/2104.04497v1) [cs.CL]** for this version) |










# 2021-04-09

[Return to Index](#Index)



<h2 id="2021-04-09-1">1. Extended Parallel Corpus for Amharic-English Machine Translation
</h2>

Title: [Extended Parallel Corpus for Amharic-English Machine Translation](https://arxiv.org/abs/2104.03543)

Authors: [Andargachew Mekonnen Gezmu](https://arxiv.org/search/cs?searchtype=author&query=Gezmu%2C+A+M), [Andreas Nürnberger](https://arxiv.org/search/cs?searchtype=author&query=Nürnberger%2C+A), [Tesfaye Bayu Bati](https://arxiv.org/search/cs?searchtype=author&query=Bati%2C+T+B)

> This paper describes the acquisition, preprocessing, segmentation, and alignment of an Amharic-English parallel corpus. It will be useful for machine translation of an under-resourced language, Amharic. The corpus is larger than previously compiled corpora; it is released for research purposes. We trained neural machine translation and phrase-based statistical machine translation models using the corpus. In the automatic evaluation, neural machine translation models outperform phrase-based statistical machine translation models.

| Comments: | Accepted to AfricanNLP workshop under EACL 2021              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.03543](https://arxiv.org/abs/2104.03543) [cs.CL]** |
|           | (or **[arXiv:2104.03543v1](https://arxiv.org/abs/2104.03543v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-2">2. BSTC: A Large-Scale Chinese-English Speech Translation Dataset
</h2>

Title: [BSTC: A Large-Scale Chinese-English Speech Translation Dataset](https://arxiv.org/abs/2104.03575)

Authors: [Ruiqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Xiyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Chuanqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Zhongjun HeHua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z+H), [Zhi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Ying Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Qinfei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q)

> This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale Chinese-English speech translation dataset. This dataset is constructed based on a collection of licensed videos of talks or lectures, including about 68 hours of Mandarin data, their manual transcripts and translations into English, as well as automated transcripts by an automatic speech recognition (ASR) model. We have further asked three experienced interpreters to simultaneously interpret the testing talks in a mock conference setting. This corpus is expected to promote the research of automatic simultaneous translation as well as the development of practical systems. We have organized simultaneous translation tasks and used this corpus to evaluate automatic simultaneous translation systems.

| Comments: | 8 pages, 6 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.03575](https://arxiv.org/abs/2104.03575) [cs.CL]** |
|           | (or **[arXiv:2104.03575v1](https://arxiv.org/abs/2104.03575v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-3">3. A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders
</h2>

Title: [A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](https://arxiv.org/abs/2104.03630)

Authors: [Maarten De Raedt](https://arxiv.org/search/cs?searchtype=author&query=De+Raedt%2C+M), [Fréderic Godin](https://arxiv.org/search/cs?searchtype=author&query=Godin%2C+F), [Pieter Buteneers](https://arxiv.org/search/cs?searchtype=author&query=Buteneers%2C+P), [Chris Develder](https://arxiv.org/search/cs?searchtype=author&query=Develder%2C+C), [Thomas Demeester](https://arxiv.org/search/cs?searchtype=author&query=Demeester%2C+T)

> Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. We investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and cross-lingual settings.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.03630](https://arxiv.org/abs/2104.03630) [cs.CL]** |
|           | (or **[arXiv:2104.03630v1](https://arxiv.org/abs/2104.03630v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-4">4. Probing BERT in Hyperbolic Spaces
</h2>

Title: [Probing BERT in Hyperbolic Spaces](https://arxiv.org/abs/2104.03869)

Authors: [Boli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Yao Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Guangwei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G), [Pengjun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+P), [Chuanqi Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C), [Mosha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Liping Jing](https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+L)

> Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a Poincare probe, a structural probe projecting these embeddings into a Poincare subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincare probe via extensive experiments and visualization. Our results can be reproduced at [this https URL](https://github.com/FranxYao/PoincareProbe).

| Comments: | ICLR 2021 Camera ready                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.03869](https://arxiv.org/abs/2104.03869) [cs.CL]** |
|           | (or **[arXiv:2104.03869v1](https://arxiv.org/abs/2104.03869v1) [cs.CL]** for this version) |





# 2021-04-08

[Return to Index](#Index)



<h2 id="2021-04-08-1">1. VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations
</h2>

Title: [VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations](https://arxiv.org/abs/2104.02797)

Authors: [Archit Rathore](https://arxiv.org/search/cs?searchtype=author&query=Rathore%2C+A), [Sunipa Dev](https://arxiv.org/search/cs?searchtype=author&query=Dev%2C+S), [Jeff M. Phillips](https://arxiv.org/search/cs?searchtype=author&query=Phillips%2C+J+M), [Vivek Srikumar](https://arxiv.org/search/cs?searchtype=author&query=Srikumar%2C+V), [Yan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Chin-Chia Michael Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+C+M), [Junpeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Bei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B)

> Word vector embeddings have been shown to contain and amplify biases in data they are extracted from. Consequently, many techniques have been proposed to identify, mitigate, and attenuate these biases in word representations. In this paper, we utilize interactive visualization to increase the interpretability and accessibility of a collection of state-of-the-art debiasing techniques. To aid this, we present Visualization of Embedding Representations for deBiasing system ("VERB"), an open-source web-based visualization tool that helps the users gain a technical understanding and visual intuition of the inner workings of debiasing techniques, with a focus on their geometric properties. In particular, VERB offers easy-to-follow use cases in exploring the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, VERB decomposes each technique into interpretable sequences of primitive transformations and highlights their effect on the word vectors using dimensionality reduction and interactive visual exploration. VERB is designed to target natural language processing (NLP) practitioners who are designing decision-making systems on top of word embeddings, and also researchers working with fairness and ethics of machine learning systems in NLP. It can also serve as a visual medium for education, which helps an NLP novice to understand and mitigate biases in word embeddings.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2104.02797](https://arxiv.org/abs/2104.02797) [cs.CL]** |
|           | (or **[arXiv:2104.02797v1](https://arxiv.org/abs/2104.02797v1) [cs.CL]** for this version) |





<h2 id="2021-04-08-2">2. Better Neural Machine Translation by Extracting Linguistic Information from BERT
</h2>

Title: [Better Neural Machine Translation by Extracting Linguistic Information from BERT](https://arxiv.org/abs/2104.02831)

Authors: [Hassan S. Shavarani](https://arxiv.org/search/cs?searchtype=author&query=Shavarani%2C+H+S), [Anoop Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+A)

> Adding linguistic information (syntax or semantics) to neural machine translation (NMT) has mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT (Devlin et al., 2019) has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer-based NMT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.02831](https://arxiv.org/abs/2104.02831) [cs.CL]** |
|           | (or **[arXiv:2104.02831v1](https://arxiv.org/abs/2104.02831v1) [cs.CL]** for this version) |



<h2 id="2021-04-08-3">3. GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education
</h2>

Title: [GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education](https://arxiv.org/abs/2104.03190)

Authors: [Masato Hagiwara](https://arxiv.org/search/cs?searchtype=author&query=Hagiwara%2C+M), [Joshua Tanner](https://arxiv.org/search/cs?searchtype=author&query=Tanner%2C+J), [Keisuke Sakaguchi](https://arxiv.org/search/cs?searchtype=author&query=Sakaguchi%2C+K)

> We present GrammarTagger, an open-source grammar profiler which, given an input text, identifies grammatical features useful for language education. The model architecture enables it to learn from a small amount of texts annotated with spans and their labels, which 1) enables easier and more intuitive annotation, 2) supports overlapping spans, and 3) is less prone to error propagation, compared to complex hand-crafted rules defined on constituency/dependency parses. We show that we can bootstrap a grammar profiler model with F1≈0.6 from only a couple hundred sentences both in English and Chinese, which can be further boosted via learning a multilingual model. With GrammarTagger, we also build Octanove Learn, a search engine of language learning materials indexed by their reading difficulty and grammatical features. The code and pretrained models are publicly available at \url{[this https URL](https://github.com/octanove/grammartagger)}.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.03190](https://arxiv.org/abs/2104.03190) [cs.CL]** |
|           | (or **[arXiv:2104.03190v1](https://arxiv.org/abs/2104.03190v1) [cs.CL]** for this version) |









# 2021-04-07

[Return to Index](#Index)



<h2 id="2021-04-07-1">1. Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding
</h2>

Title: [Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding](https://arxiv.org/abs/2104.02138)

Authors: [Suyoun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Abhinav Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+A), [Duc Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+D), [Ching-Feng Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+C), [Christian Fuegen](https://arxiv.org/search/cs?searchtype=author&query=Fuegen%2C+C), [Ozlem Kalinli](https://arxiv.org/search/cs?searchtype=author&query=Kalinli%2C+O), [Michael L. Seltzer](https://arxiv.org/search/cs?searchtype=author&query=Seltzer%2C+M+L)

> Word Error Rate (WER) has been the predominant metric used to evaluate the performance of automatic speech recognition (ASR) systems. However, WER is sometimes not a good indicator for downstream Natural Language Understanding (NLU) tasks, such as intent recognition, slot filling, and semantic parsing in task-oriented dialog systems. This is because WER takes into consideration only literal correctness instead of semantic correctness, the latter of which is typically more important for these downstream tasks. In this study, we propose a novel Semantic Distance (SemDist) measure as an alternative evaluation metric for ASR systems to address this issue. We define SemDist as the distance between a reference and hypothesis pair in a sentence-level embedding space. To represent the reference and hypothesis as a sentence embedding, we exploit RoBERTa, a state-of-the-art pre-trained deep contextualized language model based on the transformer architecture. We demonstrate the effectiveness of our proposed metric on various downstream tasks, including intent recognition, semantic parsing, and named entity recognition.

| Comments: | submitted to Interspeech 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.02138](https://arxiv.org/abs/2104.02138) [cs.CL]** |
|           | (or **[arXiv:2104.02138v1](https://arxiv.org/abs/2104.02138v1) [cs.CL]** for this version) |





<h2 id="2021-04-07-2">2. ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation
</h2>

Title: [ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation](https://arxiv.org/abs/2104.02308)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T), [Shuhan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S), [Xin Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> It has been found that residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODEs). In this paper, we explore a deeper relationship between Transformer and numerical methods of ODEs. We show that a residual block of layers in Transformer can be described as a higher-order solution to ODEs. This leads us to design a new architecture (call it ODE Transformer) analogous to the Runge-Kutta method that is well motivated in ODEs. As a natural extension to Transformer, ODE Transformer is easy to implement and parameter efficient. Our experiments on three WMT tasks demonstrate the genericity of this model, and large improvements in performance over several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the WMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the WMT'14 En-Fr task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.02308](https://arxiv.org/abs/2104.02308) [cs.CL]** |
|           | (or **[arXiv:2104.02308v1](https://arxiv.org/abs/2104.02308v1) [cs.CL]** for this version) |







# 2021-04-06

[Return to Index](#Index)



<h2 id="2021-04-06-1">1. TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition
</h2>

Title: [TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition](https://arxiv.org/abs/2104.01522)

Authors: [Zhengkun Tian](https://arxiv.org/search/eess?searchtype=author&query=Tian%2C+Z), [Jiangyan Yi](https://arxiv.org/search/eess?searchtype=author&query=Yi%2C+J), [Jianhua Tao](https://arxiv.org/search/eess?searchtype=author&query=Tao%2C+J), [Ye Bai](https://arxiv.org/search/eess?searchtype=author&query=Bai%2C+Y), [Shuai Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+S), [Zhengqi Wen](https://arxiv.org/search/eess?searchtype=author&query=Wen%2C+Z), [Xuefei Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X)

> The autoregressive (AR) models, such as attention-based encoder-decoder models and RNN-Transducer, have achieved great success in speech recognition. They predict the output sequence conditioned on the previous tokens and acoustic encoded states, which is inefficient on GPUs. The non-autoregressive (NAR) models can get rid of the temporal dependency between the output tokens and predict the entire output tokens in at least one step. However, the NAR model still faces two major problems. On the one hand, there is still a great gap in performance between the NAR models and the advanced AR models. On the other hand, it's difficult for most of the NAR models to train and converge. To address these two problems, we propose a new model named the two-step non-autoregressive transformer(TSNAT), which improves the performance and accelerating the convergence of the NAR model by learning prior knowledge from a parameters-sharing AR model. Furthermore, we introduce the two-stage method into the inference process, which improves the model performance greatly. All the experiments are conducted on a public Chinese mandarin dataset ASIEHLL-1. The results show that the TSNAT can achieve a competitive performance with the AR model and outperform many complicated NAR models.

| Comments: | Submitted to Interspeech2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.01522](https://arxiv.org/abs/2104.01522) [eess.AS]** |
|           | (or **[arXiv:2104.01522v1](https://arxiv.org/abs/2104.01522v1) [eess.AS]** for this version) |





<h2 id="2021-04-06-2">2. Attention Forcing for Machine Translation
</h2>

Title: [Attention Forcing for Machine Translation](https://arxiv.org/abs/2104.01264)

Authors: [Qingyun Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Q), [Yiting Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Potsawee Manakul](https://arxiv.org/search/cs?searchtype=author&query=Manakul%2C+P), [Xixin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Mark J. F. Gales](https://arxiv.org/search/cs?searchtype=author&query=Gales%2C+M+J+F)

> Auto-regressive sequence-to-sequence models with attention mechanisms have achieved state-of-the-art performance in various tasks including Text-To-Speech (TTS) and Neural Machine Translation (NMT). The standard training approach, teacher forcing, guides a model with the reference output history. At inference stage, the generated output history must be used. This mismatch can impact performance. However, it is highly challenging to train the model using the generated output. Several approaches have been proposed to address this problem, normally by selectively using the generated output history. To make training stable, these approaches often require a heuristic schedule or an auxiliary classifier. This paper introduces attention forcing for NMT. This approach guides the model with the generated output history and reference attention, and can reduce the training-inference mismatch without a schedule or a classifier. Attention forcing has been successful in TTS, but its application to NMT is more challenging, due to the discrete and multi-modal nature of the output space. To tackle this problem, this paper adds a selection scheme to vanilla attention forcing, which automatically selects a suitable training approach for each pair of training data. Experiments show that attention forcing can improve the overall translation quality and the diversity of the translations.

| Comments: | arXiv admin note: text overlap with [arXiv:1909.12289](https://arxiv.org/abs/1909.12289) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.01264](https://arxiv.org/abs/2104.01264) [cs.CL]** |
|           | (or **[arXiv:2104.01264v1](https://arxiv.org/abs/2104.01264v1) [cs.CL]** for this version) |







<h2 id="2021-04-06-3">3. WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach
</h2>

Title: [WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](https://arxiv.org/abs/2104.01767)

Authors: [Junjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Wanjun Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+W), [Shuai Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N)

> Producing the embedding of a sentence in an unsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have there main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top andbottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.01767](https://arxiv.org/abs/2104.01767) [cs.CL]** |
|           | (or **[arXiv:2104.01767v1](https://arxiv.org/abs/2104.01767v1) [cs.CL]** for this version) |







<h2 id="2021-04-06-4">4. Rethinking Perturbations in Encoder-Decoders for Fast Training
</h2>

Title: [Rethinking Perturbations in Encoder-Decoders for Fast Training](https://arxiv.org/abs/2104.01853)

Authors: [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S)

> We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster. Our code is publicly available at [this https URL](https://github.com/takase/rethink_perturbations).

| Comments: | Accepted at NAACL-HLT 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.01853](https://arxiv.org/abs/2104.01853) [cs.CL]** |
|           | (or **[arXiv:2104.01853v1](https://arxiv.org/abs/2104.01853v1) [cs.CL]** for this version) |







# 2021-04-02

[Return to Index](#Index)



<h2 id="2021-04-02-1">1. Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study
</h2>

Title: [Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://arxiv.org/abs/2104.00676)

Authors: [Zhiqiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Zechun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Dejia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+D), [Zitian Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Kwang-Ting Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+K), [Marios Savvides](https://arxiv.org/search/cs?searchtype=author&query=Savvides%2C+M)

> This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation. We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness. Project page: [this http URL](http://zhiqiangshen.com/projects/LS_and_KD/index.html).

| Comments: | ICLR 2021. Project page: [this http URL](http://zhiqiangshen.com/projects/LS_and_KD/index.html) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2104.00676](https://arxiv.org/abs/2104.00676) [cs.LG]** |
|           | (or **[arXiv:2104.00676v1](https://arxiv.org/abs/2104.00676v1) [cs.LG]** for this version) |





<h2 id="2021-04-02-2">2. Domain-specific MT for Low-resource Languages: The case of Bambara-French
</h2>

Title: [Domain-specific MT for Low-resource Languages: The case of Bambara-French](https://arxiv.org/abs/2104.00041)

Authors: [Allahsera Auguste Tapo](https://arxiv.org/search/cs?searchtype=author&query=Tapo%2C+A+A), [Michael Leventhal](https://arxiv.org/search/cs?searchtype=author&query=Leventhal%2C+M), [Sarah Luger](https://arxiv.org/search/cs?searchtype=author&query=Luger%2C+S), [Christopher M. Homan](https://arxiv.org/search/cs?searchtype=author&query=Homan%2C+C+M), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M)

> Translating to and from low-resource languages is a challenge for machine translation (MT) systems due to a lack of parallel data. In this paper we address the issue of domain-specific MT for Bambara, an under-resourced Mande language spoken in Mali. We present the first domain-specific parallel dataset for MT of Bambara into and from French. We discuss challenges in working with small quantities of domain-specific data for a low-resource language and we present the results of machine learning experiments on this data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00041](https://arxiv.org/abs/2104.00041) [cs.CL]** |
|           | (or **[arXiv:2104.00041v1](https://arxiv.org/abs/2104.00041v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-3">3. Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation
</h2>

Title: [Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation](https://arxiv.org/abs/2104.00106)

Authors: [Aviral Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+A), [Chengzhi Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Har Simrat Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+H+S)

> This work focuses on comparing different solutions for machine translation on low resource language pairs, namely, with zero-shot transfer learning and unsupervised machine translation. We discuss how the data size affects the performance of both unsupervised MT and transfer learning. Additionally we also look at how the domain of the data affects the result of unsupervised MT. The code to all the experiments performed in this project are accessible on Github.

| Comments: | 7 pages, 2 figures, 4 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00106](https://arxiv.org/abs/2104.00106) [cs.CL]** |
|           | (or **[arXiv:2104.00106v1](https://arxiv.org/abs/2104.00106v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-4">4. Detecting over/under-translation errors for determining adequacy in human translations
</h2>

Title: [Detecting over/under-translation errors for determining adequacy in human translations](https://arxiv.org/abs/2104.00267)

Authors: [Prabhakar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Ridha Juneja](https://arxiv.org/search/cs?searchtype=author&query=Juneja%2C+R), [Anil Nelakanti](https://arxiv.org/search/cs?searchtype=author&query=Nelakanti%2C+A), [Tamojit Chatterjee](https://arxiv.org/search/cs?searchtype=author&query=Chatterjee%2C+T)

> We present a novel approach to detecting over and under translations (OT/UT) as part of adequacy error checks in translation evaluation. We do not restrict ourselves to machine translation (MT) outputs and specifically target applications with human generated translation pipeline. The goal of our system is to identify OT/UT errors from human translated video subtitles with high error recall. We achieve this without reference translations by learning a model on synthesized training data. We compare various classification networks that we trained on embeddings from pre-trained language model with our best hybrid network of GRU + CNN achieving 89.3% accuracy on high-quality human-annotated evaluation data in 8 languages.

| Comments: | 6 pages, 5 tables                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00267](https://arxiv.org/abs/2104.00267) [cs.CL]** |
|           | (or **[arXiv:2104.00267v1](https://arxiv.org/abs/2104.00267v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-5">5. Many-to-English Machine Translation Tools, Data, and Pretrained Models
</h2>

Title: [Many-to-English Machine Translation Tools, Data, and Pretrained Models](https://arxiv.org/abs/2104.00290)

Authors: [Thamme Gowda](https://arxiv.org/search/cs?searchtype=author&query=Gowda%2C+T), [Zhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Chris A Mattmann](https://arxiv.org/search/cs?searchtype=author&query=Mattmann%2C+C+A), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

> While there are more than 7000 languages in the world, most translation research efforts have targeted a few high-resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00290](https://arxiv.org/abs/2104.00290) [cs.CL]** |
|           | (or **[arXiv:2104.00290v1](https://arxiv.org/abs/2104.00290v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-6">6. Low-Resource Neural Machine Translation for South-Eastern African Languages
</h2>

Title: [Low-Resource Neural Machine Translation for South-Eastern African Languages](https://arxiv.org/abs/2104.00366)

Authors: [Evander Nyoni](https://arxiv.org/search/cs?searchtype=author&query=Nyoni%2C+E), [Bruce A. Bassett](https://arxiv.org/search/cs?searchtype=author&query=Bassett%2C+B+A)

> Low-resource African languages have not fully benefited from the progress in neural machine translation because of a lack of data. Motivated by this challenge we compare zero-shot learning, transfer learning and multilingual learning on three Bantu languages (Shona, isiXhosa and isiZulu) and English. Our main target is English-to-isiZulu translation for which we have just 30,000 sentence pairs, 28% of the average size of our other corpora. We show the importance of language similarity on the performance of English-to-isiZulu transfer learning based on English-to-isiXhosa and English-to-Shona parent models whose BLEU scores differ by 5.2. We then demonstrate that multilingual learning surpasses both transfer learning and zero-shot learning on our dataset, with BLEU score improvements relative to the baseline English-to-isiZulu model of 9.9, 6.1 and 2.0 respectively. Our best model also improves the previous SOTA BLEU score by more than 10.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00366](https://arxiv.org/abs/2104.00366) [cs.CL]** |
|           | (or **[arXiv:2104.00366v1](https://arxiv.org/abs/2104.00366v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-7">7. WakaVT: A Sequential Variational Transformer for Waka Generation
</h2>

Title: [WakaVT: A Sequential Variational Transformer for Waka Generation](https://arxiv.org/abs/2104.00426)

Authors: [Yuka Takeishi](https://arxiv.org/search/cs?searchtype=author&query=Takeishi%2C+Y), [Mingxuan Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+M), [Jing Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Zhong Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Z), [Xinyu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X)

> Poetry generation has long been a challenge for artificial intelligence. In the scope of Japanese poetry generation, many researchers have paid attention to Haiku generation, but few have focused on Waka generation. To further explore the creative potential of natural language generation systems in Japanese poetry creation, we propose a novel Waka generation model, WakaVT, which automatically produces Waka poems given user-specified keywords. Firstly, an additive mask-based approach is presented to satisfy the form constraint. Secondly, the structures of Transformer and variational autoencoder are integrated to enhance the quality of generated content. Specifically, to obtain novelty and diversity, WakaVT employs a sequence of latent variables, which effectively captures word-level variability in Waka data. To improve linguistic quality in terms of fluency, coherence, and meaningfulness, we further propose the fused multilevel self-attention mechanism, which properly models the hierarchical linguistic structure of Waka. To the best of our knowledge, we are the first to investigate Waka generation with models based on Transformer and/or variational autoencoder. Both objective and subjective evaluation results demonstrate that our model outperforms baselines significantly.

| Comments: | This paper has been submitted to Neural Processing Letters   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.00426](https://arxiv.org/abs/2104.00426) [cs.CL]** |
|           | (or **[arXiv:2104.00426v1](https://arxiv.org/abs/2104.00426v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-8">8. Sampling and Filtering of Neural Machine Translation Distillation Data
</h2>

Title: [Sampling and Filtering of Neural Machine Translation Distillation Data](https://arxiv.org/abs/2104.00664)

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V)

> In most of neural machine translation distillation or stealing scenarios, the goal is to preserve the performance of the target model (teacher). The highest-scoring hypothesis of the teacher model is commonly used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be upsampled and poor hypotheses either removed or undersampled.
> This paper explores the importance sampling method landscape (pruning, hypothesis upsampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful upsampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.

| Comments: | 6 pages (without references); to be published in NAACL-SRW   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00664](https://arxiv.org/abs/2104.00664) [cs.CL]** |
|           | (or **[arXiv:2104.00664v1](https://arxiv.org/abs/2104.00664v1) [cs.CL]** for this version) |







# 2021-04-01

[Return to Index](#Index)



<h2 id="2021-04-01-1">1. An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation
</h2>

Title: [An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation](https://arxiv.org/abs/2103.16789)

Authors:[Lidia Kidane](https://arxiv.org/search/cs?searchtype=author&query=Kidane%2C+L), [Sachin Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y)

> It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, often requiring large amounts of auxiliary data to achieve competitive results. An effective method of generating auxiliary data is back-translation of target language sentences. In this work, we present a case study of Tigrinya where we investigate several back-translation methods to generate synthetic source sentences. We find that in low-resource conditions, back-translation by pivoting through a higher-resource language related to the target language proves most effective resulting in substantial improvements over baselines.

| Comments: | Accepted at AfricaNLP Workshop, EACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16789](https://arxiv.org/abs/2103.16789) [cs.CL]** |
|           | (or **[arXiv:2103.16789v1](https://arxiv.org/abs/2103.16789v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-2">2. Few-shot learning through contextual data augmentation
</h2>

Title: [Few-shot learning through contextual data augmentation](https://arxiv.org/abs/2103.16911)

Authors:[Farid Arthaud](https://arxiv.org/search/cs?searchtype=author&query=Arthaud%2C+F), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.

| Comments: | 14 pages includince 3 of appendices                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16911](https://arxiv.org/abs/2103.16911) [cs.CL]** |
|           | (or **[arXiv:2103.16911v1](https://arxiv.org/abs/2103.16911v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-3">3. UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language
</h2>

Title: [UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language](https://arxiv.org/abs/2103.16997)

Authors:[Oleksiy Syvokon](https://arxiv.org/search/cs?searchtype=author&query=Syvokon%2C+O), [Olena Nahorna](https://arxiv.org/search/cs?searchtype=author&query=Nahorna%2C+O)

> We present a corpus professionally annotated for grammatical error correction (GEC) and fluency edits in the Ukrainian language. To the best of our knowledge, this is the first GEC corpus for the Ukrainian language. We collected texts with errors (20,715 sentences) from a diverse pool of contributors, including both native and non-native speakers. The data cover a wide variety of writing domains, from text chats and essays to formal writing. Professional proofreaders corrected and annotated the corpus for errors relating to fluency, grammar, punctuation, and spelling. This corpus can be used for developing and evaluating GEC systems in Ukrainian. More generally, it can be used for researching multilingual and low-resource NLP, morphologically rich languages, document-level GEC, and fluency correction. The corpus is publicly available at [this https URL](https://github.com/grammarly/ua-gec)

| Comments: | See [this https URL](https://github.com/grammarly/ua-gec) for the dataset. Version 2 of the data is in progress |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16997](https://arxiv.org/abs/2103.16997) [cs.CL]** |
|           | (or **[arXiv:2103.16997v1](https://arxiv.org/abs/2103.16997v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-4">4. Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources
</h2>

Title: [Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources](https://arxiv.org/abs/2103.17151)

Authors:[Lorenzo Lupo](https://arxiv.org/search/cs?searchtype=author&query=Lupo%2C+L), [Marco Dinarelli](https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Multi-encoder models are a broad family of context-aware Neural Machine Translation (NMT) systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we show that training these parameters takes large amount of data, since the contextual training signal is sparse. We propose an efficient alternative, based on splitting sentence pairs, that allows to enrich the training signal of a set of parallel sentences by breaking intra-sentential syntactic links, and thus frequently pushing the model to search the context for disambiguating clues. We evaluate our approach with BLEU and contrastive test sets, showing that it allows multi-encoder models to achieve comparable performances to a setting where they are trained with ×10 document-level data. We also show that our approach is a viable option to context-aware NMT for language pairs with zero document-level parallel data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.17151](https://arxiv.org/abs/2103.17151) [cs.CL]** |
|           | (or **[arXiv:2103.17151v1](https://arxiv.org/abs/2103.17151v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-5">5. Leveraging Neural Machine Translation for Word Alignment
</h2>

Title: [Leveraging Neural Machine Translation for Word Alignment](https://arxiv.org/abs/2103.17250)

Authors:[Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Daria Pylypenko](https://arxiv.org/search/cs?searchtype=author&query=Pylypenko%2C+D)

> The most common tools for word-alignment rely on a large amount of parallel sentences, which are then usually processed according to one of the IBM model algorithms. The training data is, however, the same as for machine translation (MT) systems, especially for neural MT (NMT), which itself is able to produce word-alignments using the trained attention heads. This is convenient because word-alignment is theoretically a viable byproduct of any attention-based NMT, which is also able to provide decoder scores for a translated sentence pair.
> We summarize different approaches on how word-alignment can be extracted from alignment scores and then explore ways in which scores can be extracted from NMT, focusing on inferring the word-alignment scores based on output sentence and token probabilities. We compare this to the extraction of alignment scores from attention. We conclude with aggregating all of the sources of alignment scores into a simple feed-forward network which achieves the best results when combined alignment extractors are used.

| Comments: | 16 pages (without references). To be published in PBML 116   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.17250](https://arxiv.org/abs/2103.17250) [cs.CL]** |
|           | (or **[arXiv:2103.17250v1](https://arxiv.org/abs/2103.17250v1) [cs.CL]** for this version) |






