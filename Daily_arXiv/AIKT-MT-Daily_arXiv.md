# Daily arXiv: Machine Translation - June, 2020

# Index


- [2020-06-09](#2020-06-09)

  - [1. Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation](#2020-06-09-1)
  - [2. Modeling Discourse Structure for Document-level Neural Machine Translation](#2020-06-09-2)
  - [3. What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT](#2020-06-09-3)
  - [4. Filtered Inner Product Projection for Multilingual Embedding Alignment](#2020-06-09-4)
  - [5. DeBERTa: Decoding-enhanced BERT with Disentangled Attention](#2020-06-09-5)
- [2020-06-08](#2020-06-08)

  - [1. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](#2020-06-08-1)
  - [2. GMAT: Global Memory Augmentation for Transformers](#2020-06-08-2)
  - [3. ELITR Non-Native Speech Translation at IWSLT 2020](#2020-06-08-3)
  - [4. Unsupervised Translation of Programming Languages](#2020-06-08-4)
- [2020-06-05](#2020-06-05)
- [1. CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning](#2020-06-05-1)
  - [2. Self-Training for End-to-End Speech Translation](#2020-06-05-2)
  - [3. M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training](#2020-06-05-3)
  - [4. Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation](#2020-06-05-4)
  - [5. Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1](#2020-06-05-5)
  - [6. End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020](#2020-06-05-6)
- [2020-06-04](#2020-06-04)

  - [1. The Typology of Polysemy: A Multilingual Distributional Framework](#2020-06-01-1)
  - [2. Norm-Based Curriculum Learning for Neural Machine Translation](#2020-06-01-2)
  - [3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](#2020-06-01-3)
  - [4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer](#2020-06-01-4)
- [2020-06-03](#2020-06-03)

  - [1. WikiBERT models: deep transfer learning for many languages](#2020-06-03-1)
  - [2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](#2020-06-03-2)
- [2020-06-02](#2020-06-02)
- [1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](#2020-06-02-1)
  - [2. Dynamic Masking for Improved Stability in Spoken Language Translation](#2020-06-02-2)
  - [3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](#2020-06-02-3)
  - [4. Neural Unsupervised Domain Adaptation in NLP---A Survey](#2020-06-02-4)
  - [5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](#2020-06-02-5)
  - [6. Attention Word Embedding](#2020-06-02-6)
  - [7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](#2020-06-02-7)
  - [8. Cascaded Text Generation with Markov Transformers](#2020-06-02-8)
- [2020-06-01](#2020-06-01)

  - [1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP](#2020-06-01-1)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-06-09

[Return to Index](#Index)



<h2 id="2020-06-09-1">1. Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation</h2>

Title: [Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation](https://arxiv.org/abs/2006.04050)

Authors: [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Hasan Cavusoglu](https://arxiv.org/search/cs?searchtype=author&query=Cavusoglu%2C+H)

> We describe our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE) (Mayhew et al., 2020). We view MT models at various training stages (i.e., checkpoints) as human learners at different levels. Hence, we employ an ensemble of multi-checkpoints from the same model to generate translation sequences with various levels of fluency. From each checkpoint, for our best model, we sample n-Best sequences (n=10) with a beam width =100. We achieve 37.57 macro F1 with a 6 checkpoint model ensemble on the official English to Portuguese shared task test data, outperforming a baseline Amazon translation system of 21.30 macro F1 and ultimately demonstrating the utility of our intuitive method.

| Comments: | Accepted to the 4th Workshop on Neural Generation and Translation (Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education Mayhew et al., 2020) collocated with ACL 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2006.04050](https://arxiv.org/abs/2006.04050) [cs.CL]** |
|           | (or **[arXiv:2006.04050v1](https://arxiv.org/abs/2006.04050v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-2">2. Modeling Discourse Structure for Document-level Neural Machine Translation</h2>

Title: [Modeling Discourse Structure for Document-level Neural Machine Translation](https://arxiv.org/abs/2006.04721)

Authors: [Junxuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Xiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Jiarui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Jianwei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+J), [Bin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.04721](https://arxiv.org/abs/2006.04721) [cs.CL]** |
|           | (or **[arXiv:2006.04721v1](https://arxiv.org/abs/2006.04721v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-3">3. What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT</h2>

Title: [What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT](https://arxiv.org/abs/2006.04781)

Authors: [Lukas Fischer](https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+L), [Samuel Läubli](https://arxiv.org/search/cs?searchtype=author&query=Läubli%2C+S)

> Machine translation (MT) has been shown to produce a number of errors that require human post-editing, but the extent to which professional human translation (HT) contains such errors has not yet been compared to MT. We compile pre-translated documents in which MT and HT are interleaved, and ask professional translators to flag errors and post-edit these documents in a blind evaluation. We find that the post-editing effort for MT segments is only higher in two out of three language pairs, and that the number of segments with wrong terminology, omissions, and typographical problems is similar in HT.

| Comments: | EAMT 2020 (Research Track)                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.04781](https://arxiv.org/abs/2006.04781) [cs.CL]** |
|           | (or **[arXiv:2006.04781v1](https://arxiv.org/abs/2006.04781v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-4">4. Filtered Inner Product Projection for Multilingual Embedding Alignment</h2>

Title: [Filtered Inner Product Projection for Multilingual Embedding Alignment](https://arxiv.org/abs/2006.03652)

Authors: [Vin Sachidananda](https://arxiv.org/search/cs?searchtype=author&query=Sachidananda%2C+V), [Ziyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C)

> Due to widespread interest in machine translation and transfer learning, there are numerous algorithms for mapping multiple embeddings to a shared representation space. Recently, these algorithms have been studied in the setting of bilingual dictionary induction where one seeks to align the embeddings of a source and a target language such that translated word pairs lie close to one another in a common representation space. In this paper, we propose a method, Filtered Inner Product Projection (FIPP), for mapping embeddings to a common representation space and evaluate FIPP in the context of bilingual dictionary induction. As semantic shifts are pervasive across languages and domains, FIPP first identifies the common geometric structure in both embeddings and then, only on the common structure, aligns the Gram matrices of these embeddings. Unlike previous approaches, FIPP is applicable even when the source and target embeddings are of differing dimensionalities. We show that our approach outperforms existing methods on the MUSE dataset for various language pairs. Furthermore, FIPP provides computational benefits both in ease of implementation and scalability.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03652](https://arxiv.org/abs/2006.03652) [cs.CL]** |
|           | (or **[arXiv:2006.03652v1](https://arxiv.org/abs/2006.03652v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-5">5. DeBERTa: Decoding-enhanced BERT with Disentangled Attention</h2>

Title: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)

Authors: [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at [this https URL](https://github.com/microsoft/DeBERTa).

| Comments:    | 17 pages,4 figures, 8 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2; I.7                                                     |
| Cite as:     | **[arXiv:2006.03654](https://arxiv.org/abs/2006.03654) [cs.CL]** |
|              | (or **[arXiv:2006.03654v1](https://arxiv.org/abs/2006.03654v1) [cs.CL]** for this version) |













# 2020-06-08

[Return to Index](#Index)



<h2 id="2020-06-08-1">1. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing</h2>

Title: [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)

Authors: [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [Guokun Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+G), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at [this https URL](https://github.com/laiguokun/Funnel-Transformer).

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03236](https://arxiv.org/abs/2006.03236) [cs.LG]** |
|           | (or **[arXiv:2006.03236v1](https://arxiv.org/abs/2006.03236v1) [cs.LG]** for this version) |





<h2 id="2020-06-08-2">2. GMAT: Global Memory Augmentation for Transformers</h2>

Title: [GMAT: Global Memory Augmentation for Transformers](https://arxiv.org/abs/2006.03274)

Authors:[Ankit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the pairwise dot-product attention that has a large Ω(L2) memory requirement for length L sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based global memory of length M (≪L) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable O(M⋅(L+M)) memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03274](https://arxiv.org/abs/2006.03274) [cs.LG]** |
|           | (or **[arXiv:2006.03274v1](https://arxiv.org/abs/2006.03274v1) [cs.LG]** for this version) |





<h2 id="2020-06-08-3">3. ELITR Non-Native Speech Translation at IWSLT 2020</h2>

Title: [ELITR Non-Native Speech Translation at IWSLT 2020](https://arxiv.org/abs/2006.03331)

Authors: [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Jonáš Kratochvíl](https://arxiv.org/search/cs?searchtype=author&query=Kratochvíl%2C+J), [Sangeet Sagar](https://arxiv.org/search/cs?searchtype=author&query=Sagar%2C+S), [Matúš Žilinec](https://arxiv.org/search/cs?searchtype=author&query=Žilinec%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O), [Thai-Son Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T), [Felix Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+F), [Philip Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+P), [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y)

> This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.

| Comments: | IWSLT 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.03331](https://arxiv.org/abs/2006.03331) [cs.CL]** |
|           | (or **[arXiv:2006.03331v1](https://arxiv.org/abs/2006.03331v1) [cs.CL]** for this version) |





<h2 id="2020-06-08-4">4. Unsupervised Translation of Programming Languages</h2>

Title: [Unsupervised Translation of Programming Languages](https://arxiv.org/abs/2006.03511)

Authors: [Marie-Anne Lachaux](https://arxiv.org/search/cs?searchtype=author&query=Lachaux%2C+M), [Baptiste Roziere](https://arxiv.org/search/cs?searchtype=author&query=Roziere%2C+B), [Lowik Chanussot](https://arxiv.org/search/cs?searchtype=author&query=Chanussot%2C+L), [Guillaume Lample](https://arxiv.org/search/cs?searchtype=author&query=Lample%2C+G)

> A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.

| Subjects: | **Computation and Language (cs.CL)**; Programming Languages (cs.PL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03511](https://arxiv.org/abs/2006.03511) [cs.CL]** |
|           | (or **[arXiv:2006.03511v1](https://arxiv.org/abs/2006.03511v1) [cs.CL]** for this version) |







# 2020-06-05

[Return to Index](#Index)



<h2 id="2020-06-05-1">1. CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning</h2>

Title: [CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning](https://arxiv.org/abs/2006.02814)

Authors: [Sameer Khurana](https://arxiv.org/search/eess?searchtype=author&query=Khurana%2C+S), [Antoine Laurent](https://arxiv.org/search/eess?searchtype=author&query=Laurent%2C+A), [James Glass](https://arxiv.org/search/eess?searchtype=author&query=Glass%2C+J)

> More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.

| Comments: | submitted to INTERSPEECH                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| Cite as:  | **[arXiv:2006.02814](https://arxiv.org/abs/2006.02814) [eess.AS]** |
|           | (or **[arXiv:2006.02814v1](https://arxiv.org/abs/2006.02814v1) [eess.AS]** for this version) |





<h2 id="2020-06-05-2">2. Self-Training for End-to-End Speech Translation</h2>

Title: [Self-Training for End-to-End Speech Translation](https://arxiv.org/abs/2006.02490)

Authors: [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Qiantong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y)

> One of the main challenges for end-to-end speech translation is data scarcity. We leverage pseudo-labels generated from unlabeled audio by a cascade and an end-to-end speech translation model. This provides 8.3 and 5.7 BLEU gains over a strong semi-supervised baseline on the MuST-C English-French and English-German datasets, reaching state-of-the art performance. The effect of the quality of the pseudo-labels is investigated. Our approach is shown to be more effective than simply pre-training the encoder on the speech recognition task. Finally, we demonstrate the effectiveness of self-training by directly generating pseudo-labels with an end-to-end model instead of a cascade model.

| Comments: | Submitted to INTERSPEECH 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02490](https://arxiv.org/abs/2006.02490) [cs.CL]** |
|           | (or **[arXiv:2006.02490v1](https://arxiv.org/abs/2006.02490v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-3">3. M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training</h2>

Title: [M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training](https://arxiv.org/abs/2006.02635)

Authors: [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Lin Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+L), [Di Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+D), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Bei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Jianlong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Xin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> This paper presents a Multitask Multilingual Multimodal Pre-trained model (M3P) that combines multilingual-monomodal pre-training and monolingual-multimodal pre-training into a unified framework via multitask learning and weight sharing. The model learns universal representations that can map objects that occurred in different modalities or expressed in different languages to vectors in a common semantic space. To verify the generalization capability of M3P, we fine-tune the pre-trained model for different types of downstream tasks: multilingual image-text retrieval, multilingual image captioning, multimodal machine translation, multilingual natural language inference and multilingual text generation. Evaluation shows that M3P can (i) achieve comparable results on multilingual tasks and English multimodal tasks, compared to the state-of-the-art models pre-trained for these two types of tasks separately, and (ii) obtain new state-of-the-art results on non-English multimodal tasks in the zero-shot or few-shot setting. We also build a new Multilingual Image-Language Dataset (MILD) by collecting large amounts of (text-query, image, context) triplets in 8 languages from the logs of a commercial search engine

| Comments: | 10 pages,2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2006.02635](https://arxiv.org/abs/2006.02635) [cs.CL]** |
|           | (or **[arXiv:2006.02635v1](https://arxiv.org/abs/2006.02635v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-4">4. Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation</h2>

Title: [Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation](https://arxiv.org/abs/2006.02876)

Authors: [Idris Abdulmumin](https://arxiv.org/search/cs?searchtype=author&query=Abdulmumin%2C+I), [Bashir Shehu Galadanci](https://arxiv.org/search/cs?searchtype=author&query=Galadanci%2C+B+S), [Abubakar Isa](https://arxiv.org/search/cs?searchtype=author&query=Isa%2C+A)

> Improving neural machine translation (NMT) models using the back-translations of the monolingual target data (synthetic parallel data) is currently the state-of-the-art approach for training improved translation systems. The quality of the backward system - which is trained on the available parallel data and used for the back-translation - has been shown in many studies to affect the performance of the final NMT model. In low resource conditions, the available parallel data is usually not enough to train a backward model that can produce the qualitative synthetic data needed to train a standard translation model. This work proposes a self-training strategy where the output of the backward model is used to improve the model itself through the forward translation technique. The technique was shown to improve baseline low resource IWSLT'14 English-German and IWSLT'15 English-Vietnamese backward translation models by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the improved English-German backward model was used to train a forward model which out-performed another forward model trained using standard back-translation by 2.7 BLEU.

| Comments: | 8 pages, 5 figures, 4 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.02876](https://arxiv.org/abs/2006.02876) [cs.CL]** |
|           | (or **[arXiv:2006.02876v1](https://arxiv.org/abs/2006.02876v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-5">5. Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1</h2>

Title: [Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1](https://arxiv.org/abs/2006.02964)

Authors: [Maria Nadejde](https://arxiv.org/search/cs?searchtype=author&query=Nadejde%2C+M), [Joel Tetreault](https://arxiv.org/search/cs?searchtype=author&query=Tetreault%2C+J)

> Grammar error correction (GEC) systems have become ubiquitous in a variety of software applications, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these systems to the user's characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general-purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences. Our study is the broadest of its kind, covering five proficiency levels and twelve different languages, and comparing three different adaptation scenarios: adapting to the proficiency level only, to the first language only, or to both aspects simultaneously. We show that tailoring to both scenarios achieves the largest performance improvement (3.6 F0.5) relative to a strong baseline.

| Comments:          | Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 27-33, Hong Kong, Nov 4, 2019 |
| Cite as:           | **[arXiv:2006.02964](https://arxiv.org/abs/2006.02964) [cs.CL]** |
|                    | (or **[arXiv:2006.02964v1](https://arxiv.org/abs/2006.02964v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-6">6. End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020</h2>

Title: [End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020](https://arxiv.org/abs/2006.02965)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> This paper describes FBK's participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems' ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whether to work on custom segmentation or not. We used the provided segmentation. Our system is an end-to-end model based on an adaptation of the Transformer for speech data. Its training process is the main focus of this paper and it is based on: i) transfer learning (ASR pretraining and knowledge distillation), ii) data augmentation (SpecAugment, time stretch and synthetic data), iii) combining synthetic and real data marked as different domains, and iv) multi-task learning using the CTC loss. Finally, after the training with word-level knowledge distillation is complete, our ST models are fine-tuned using label smoothed cross entropy. Our best model scored 29 BLEU on the MuST-C En-De test set, which is an excellent result compared to recent papers, and 23.7 BLEU on the same data segmented with VAD, showing the need for researching solutions addressing this specific data condition.

| Comments: | Accepted at IWSLT2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02965](https://arxiv.org/abs/2006.02965) [cs.CL]** |
|           | (or **[arXiv:2006.02965v1](https://arxiv.org/abs/2006.02965v1) [cs.CL]** for this version) |









# 2020-06-04

[Return to Index](#Index)



<h2 id="2020-06-04-1">1. The Typology of Polysemy: A Multilingual Distributional Framework</h2>

Title: [The Typology of Polysemy: A Multilingual Distributional Framework](https://arxiv.org/abs/2006.01966)

Authors: [Ella Rabinovich](https://arxiv.org/search/cs?searchtype=author&query=Rabinovich%2C+E), [Yang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Suzanne Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+S)

> Lexical semantic typology has identified important cross-linguistic generalizations about the variation and commonalities in polysemy patterns---how languages package up meanings into words. Recent computational research has enabled investigation of lexical semantics at a much larger scale, but little work has explored lexical typology across semantic domains, nor the factors that influence cross-linguistic similarities. We present a novel computational framework that quantifies semantic affinity, the cross-linguistic similarity of lexical semantics for a concept. Our approach defines a common multilingual semantic space that enables a direct comparison of the lexical expression of concepts across languages. We validate our framework against empirical findings on lexical semantic typology at both the concept and domain levels. Our results reveal an intricate interaction between semantic domains and extra-linguistic factors, beyond language phylogeny, that co-shape the typology of polysemy across languages.

| Comments: | CogSci 2020 (Annual Meeting of the Cognitive Science Society) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01966](https://arxiv.org/abs/2006.01966) [cs.CL]** |
|           | (or **[arXiv:2006.01966v1](https://arxiv.org/abs/2006.01966v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-2">2. Norm-Based Curriculum Learning for Neural Machine Translation</h2>

Title: [Norm-Based Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2006.02014)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Houtim Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+H), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).

| Comments: | Accepted to ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.02014](https://arxiv.org/abs/2006.02014) [cs.CL]** |
|           | (or **[arXiv:2006.02014v1](https://arxiv.org/abs/2006.02014v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-3">3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation</h2>

Title: [Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](https://arxiv.org/abs/2006.02163)

Authors: [Xuan-Phi Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Wu Kui](https://arxiv.org/search/cs?searchtype=author&query=Kui%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T)

> Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply these principles differently. This work introduces another component to this framework: Multi-Agent Cross-translated Diversification (MACD). The method trains multiple UMT agents and then translates monolingual data back and forth using non-duplicative agents to acquire synthetic parallel data for supervised MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation tasks. Through extensive experimental analyses, we show that MACD is effective because it embraces data diversity while other similar variants do not.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.02163](https://arxiv.org/abs/2006.02163) [cs.CL]** |
|           | (or **[arXiv:2006.02163v1](https://arxiv.org/abs/2006.02163v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-4">4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer</h2>

Title: [Improved acoustic word embeddings for zero-resource languages using multilingual transfer](https://arxiv.org/abs/2006.02295)

Authors: [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Yevgen Matusevych](https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

> Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. Such embeddings can form the basis for speech search, indexing and discovery systems when conventional speech recognition is not possible. In zero-resource settings where unlabelled speech is the only available resource, we need a method that gives robust embeddings on an arbitrary language. Here we explore multilingual transfer: we train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. We consider three multilingual recurrent neural network (RNN) models: a classifier trained on the joint vocabularies of all training languages; a Siamese RNN trained to discriminate between same and different words from multiple languages; and a correspondence autoencoder (CAE) RNN trained to reconstruct word pairs. In a word discrimination task on six target languages, all of these models outperform state-of-the-art unsupervised models trained on the zero-resource languages themselves, giving relative improvements of more than 30% in average precision. When using only a few training languages, the multilingual CAE performs better, but with more training languages the other multilingual models perform similarly. Using more training languages is generally beneficial, but improvements are marginal on some languages. We present probing experiments which show that the CAE encodes more phonetic, word duration, language identity and speaker information than the other multilingual models.

| Comments: | 11 pages, 7 figures, 8 tables. arXiv admin note: text overlap with [arXiv:2002.02109](https://arxiv.org/abs/2002.02109) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02295](https://arxiv.org/abs/2006.02295) [cs.CL]** |
|           | (or **[arXiv:2006.02295v1](https://arxiv.org/abs/2006.02295v1) [cs.CL]** for this version) |







# 2020-06-03

[Return to Index](#Index)



<h2 id="2020-06-03-1">1. WikiBERT models: deep transfer learning for many languages</h2>

Title: [WikiBERT models: deep transfer learning for many languages](https://arxiv.org/abs/2006.01538)

Authors: [Sampo Pyysalo](https://arxiv.org/search/cs?searchtype=author&query=Pyysalo%2C+S), [Jenna Kanerva](https://arxiv.org/search/cs?searchtype=author&query=Kanerva%2C+J), [Antti Virtanen](https://arxiv.org/search/cs?searchtype=author&query=Virtanen%2C+A), [Filip Ginter](https://arxiv.org/search/cs?searchtype=author&query=Ginter%2C+F)

> Deep neural language models such as BERT have enabled substantial recent advances in many natural language processing tasks. Due to the effort and computational cost involved in their pre-training, language-specific models are typically introduced only for a small number of high-resource languages such as English. While multilingual models covering large numbers of languages are available, recent work suggests monolingual training can produce better models, and our understanding of the tradeoffs between mono- and multilingual training is incomplete. In this paper, we introduce a simple, fully automated pipeline for creating language-specific BERT models from Wikipedia data and introduce 42 new such models, most for languages up to now lacking dedicated deep neural language models. We assess the merits of these models using the state-of-the-art UDify parser on Universal Dependencies data, contrasting performance with results using the multilingual BERT model. We find that UDify using WikiBERT models outperforms the parser using mBERT on average, with the language-specific models showing substantially improved performance for some languages, yet limited improvement or a decrease in performance for others. We also present preliminary results as first steps toward an understanding of the conditions under which language-specific models are most beneficial. All of the methods and models introduced in this work are available under open licenses from [this https URL](https://github.com/turkunlp/wikibert).

| Comments: | 7 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.01538](https://arxiv.org/abs/2006.01538) [cs.CL]** |
|           | (or **[arXiv:2006.01538v1](https://arxiv.org/abs/2006.01538v1) [cs.CL]** for this version) |





<h2 id="2020-06-03-2">2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders</h2>

Title: [Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](https://arxiv.org/abs/2006.01594)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

> We propose a modular architecture of language-specific encoder-decoders that constitutes a multilingual machine translation system that can be incrementally extended to new languages without the need for retraining the existing system when adding new languages. Differently from previous works, we simultaneously train N languages in all translation directions by alternately freezing encoder or decoder modules, which indirectly forces the system to train in a common intermediate representation for all languages. Experimental results from multilingual machine translation show that we can successfully train this modular architecture improving on the initial languages while falling slightly behind when adding new languages or doing zero-shot translation. Additional comparison of the quality of sentence representation in the task of natural language inference shows that the alternately freezing training is also beneficial in this direction.

| Comments:    | arXiv admin note: text overlap with [arXiv:2004.06575](https://arxiv.org/abs/2004.06575) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2006.01594](https://arxiv.org/abs/2006.01594) [cs.CL]** |
|              | (or **[arXiv:2006.01594v1](https://arxiv.org/abs/2006.01594v1) [cs.CL]** for this version) |





# 2020-06-02

[Return to Index](#Index)



<h2 id="2020-06-02-1">1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models</h2>

Title: [A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](https://arxiv.org/abs/2006.00031)

Authors: [Nikolay Arefyev](https://arxiv.org/search/cs?searchtype=author&query=Arefyev%2C+N), [Boris Sheludko](https://arxiv.org/search/cs?searchtype=author&query=Sheludko%2C+B), [Alexander Podolskiy](https://arxiv.org/search/cs?searchtype=author&query=Podolskiy%2C+A), [Alexander Panchenko](https://arxiv.org/search/cs?searchtype=author&query=Panchenko%2C+A)

> Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00031](https://arxiv.org/abs/2006.00031) [cs.CL]** |
|           | (or **[arXiv:2006.00031v1](https://arxiv.org/abs/2006.00031v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-2">2. Dynamic Masking for Improved Stability in Spoken Language Translation</h2>

Title: [Dynamic Masking for Improved Stability in Spoken Language Translation](https://arxiv.org/abs/2006.00249)

Authors: [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> For spoken language translation (SLT) in live scenarios such as conferences, lectures and meetings, it is desirable to show the translation to the user as quickly as possible, avoiding an annoying lag between speaker and translated captions. In other words, we would like low-latency, online SLT. If we assume a pipeline of automatic speech recognition (ASR) and machine translation (MT) then a viable approach to online SLT is to pair an online ASR system, with a a retranslation strategy, where the MT system re-translates every update received from ASR. However this can result in annoying "flicker" as the MT system updates its translation. A possible solution is to add a fixed delay, or "mask" to the the output of the MT system, but a fixed global mask introduces undesirable latency to the output. We show how this mask can be set dynamically, improving the latency-flicker trade-off without sacrificing translation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00249](https://arxiv.org/abs/2006.00249) [cs.CL]** |
|           | (or **[arXiv:2006.00249v1](https://arxiv.org/abs/2006.00249v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-3">3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation</h2>

Title: [Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](https://arxiv.org/abs/2006.00262)

Authors: [Sosuke Nishikawa](https://arxiv.org/search/cs?searchtype=author&query=Nishikawa%2C+S), [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Unsupervised bilingual word embedding (BWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method assumes that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we propose using a pseudo-parallel corpus generated by an unsupervised machine translation model to facilitate structural similarity of the two embedding spaces and improve the quality of BWEs in the mapping method. We show that our approach substantially outperforms baselines and other alternative approaches given the same amount of data, and, through detailed analysis, we argue that data augmentation with the pseudo data from unsupervised machine translation is especially effective for BWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data reflects some nature of the original language that helps learning similar embedding spaces between the source and target languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00262](https://arxiv.org/abs/2006.00262) [cs.CL]** |
|           | (or **[arXiv:2006.00262v1](https://arxiv.org/abs/2006.00262v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-4">4. Neural Unsupervised Domain Adaptation in NLP---A Survey</h2>

Title: [Neural Unsupervised Domain Adaptation in NLP---A Survey](https://arxiv.org/abs/2006.00632)

Authors: [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00632](https://arxiv.org/abs/2006.00632) [cs.CL]** |
|           | (or **[arXiv:2006.00632v1](https://arxiv.org/abs/2006.00632v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-5">5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English</h2>

Title: [Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](https://arxiv.org/abs/2006.00814)

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Michael Ustaszewski](https://arxiv.org/search/cs?searchtype=author&query=Ustaszewski%2C+M), [Emmanuelle Esperança-Rodier](https://arxiv.org/search/cs?searchtype=author&query=Esperança-Rodier%2C+E), [Francis Brunet Manquat](https://arxiv.org/search/cs?searchtype=author&query=Manquat%2C+F+B), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> We conduct in this work an evaluation study comparing offline and online neural machine translation architectures. Two sequence-to-sequence models: convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based Transformer (Vaswani et al. 2017) are considered. We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English-German and German-English language pairs, the latter being particularly sensitive to latency constraints. The evaluation results allow us to identify the strengths and shortcomings of each model when we shift to the online setup.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00814](https://arxiv.org/abs/2006.00814) [cs.CL]** |
|           | (or **[arXiv:2006.00814v1](https://arxiv.org/abs/2006.00814v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-6">6. Attention Word Embedding</h2>

Title: [Attention Word Embedding](https://arxiv.org/abs/2006.00988)

Authors: [Shashank Sonkar](https://arxiv.org/search/cs?searchtype=author&query=Sonkar%2C+S), [Andrew E. Waters](https://arxiv.org/search/cs?searchtype=author&query=Waters%2C+A+E), [Richard G. Baraniuk](https://arxiv.org/search/cs?searchtype=author&query=Baraniuk%2C+R+G)

> Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00988](https://arxiv.org/abs/2006.00988) [cs.CL]** |
|           | (or **[arXiv:2006.00988v1](https://arxiv.org/abs/2006.00988v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-7">7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</h2>

Title: [Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](https://arxiv.org/abs/2006.01080)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.

| Comments: | Accepted at IWSLT 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01080](https://arxiv.org/abs/2006.01080) [cs.CL]** |
|           | (or **[arXiv:2006.01080v1](https://arxiv.org/abs/2006.01080v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-8">8. Cascaded Text Generation with Markov Transformers</h2>

Title: [Cascaded Text Generation with Markov Transformers](https://arxiv.org/abs/2006.01112)

Authors: [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.01112](https://arxiv.org/abs/2006.01112) [cs.CL]** |
|           | (or **[arXiv:2006.01112v1](https://arxiv.org/abs/2006.01112v1) [cs.CL]** for this version) |







# 2020-06-01

[Return to Index](#Index)



<h2 id="2020-06-01-1">1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP</h2>

Title: [Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP]()

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy use of fine-tuning BERT-like models in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of NLP tasks in a uniform toolkit, from text classification to sequence labeling and dependency parsing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.14672](https://arxiv.org/abs/2005.14672) [cs.CL]** |
|           | (or **[arXiv:2005.14672v1](https://arxiv.org/abs/2005.14672v1) [cs.CL]** for this version) |