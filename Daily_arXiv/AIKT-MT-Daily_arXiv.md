# Daily arXiv: Machine Translation - July, 2020

# Index


- [2020-07-28](#2020-07-28)

  - [1. Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling](#2020-07-28-1)
  - [2. Consistent Transcription and Translation of Speech](#2020-07-28-2)
- [2020-07-24](#2020-07-24)
- [1. Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models](#2020-07-24-1)
- [2020-07-23](#2020-07-23)

  - [1. Exploratory Search with Sentence Embeddings](#2020-07-23-1)
- [2020-07-22](#2020-07-22)

  - [1. Neural Machine Translation with Error Correction](#2020-07-22-1)
- [2020-07-21](#2020-07-21)

  - [1. WordCraft: An Environment for Benchmarking Commonsense Agents](#2020-07-21-1)
  - [2. On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Learning](#2020-07-21-2)
  - [3. Meta-learning for Few-shot Natural Language Processing: A Survey](#2020-07-21-3)
  - [4. One-Shot Learning for Language Modelling](#2020-07-21-4)
  - [5. Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks](#2020-07-21-5)
  - [6. Morphological Skip-Gram: Using morphological knowledge to improve word representation](#2020-07-21-6)
  - [7. CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus](#2020-07-21-7)
- [2020-07-20](#2020-07-20)

  - [1. Unsupervised Text Generation by Learning from Search](#2020-07-20-1)
  - [2. A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation](#2020-07-20-2)
  - [3. Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation](#2020-07-20-3)
  - [4. Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures](#2020-07-20-4)
  - [5. Constructing a Family Tree of Ten Indo-European Languages with Delexicalized Cross-linguistic Transfer Patterns](#2020-07-20-5)
  - [6. Neural Architecture Search for Speech Recognition](#2020-07-20-6)
- [2020-07-17](#2020-07-17)
- [1. Translate Reverberated Speech to Anechoic Ones: Speech Dereverberation with BERT](#2020-07-17-1)
  - [2. Towards Debiasing Sentence Representations](#2020-07-17-2)
  - [3. Investigating Pretrained Language Models for Graph-to-Text Generation](#2020-07-17-3)
- [2020-07-16](#2020-07-16)

  - [1. Deep learning models for representing out-of-vocabulary words](#2020-07-16-1)
  - [2. A Multilingual Parallel Corpora Collection Effort for Indian Languages](#2020-07-16-2)
  - [3. Dual Past and Future for Neural Machine Translation](#2020-07-16-3)
  - [4. Multimodal Word Sense Disambiguation in Creative Practice](#2020-07-16-4)
  - [5. AdapterHub: A Framework for Adapting Transformers](#2020-07-16-5)
  - [6. InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training](#2020-07-16-6)
- [2020-07-15](#2020-07-15)

  - [1. Modeling Voting for System Combination in Machine Translation](#2020-07-15-1)
- [2020-07-14](#2020-07-14)

  - [1. TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech](#2020-07-14-1)
  - [2. Fine-grained Language Identification with Multilingual CapsNet Model](#2020-07-14-2)
  - [3. Is Machine Learning Speaking my Language? A Critical Look at the NLP-Pipeline Across 8 Human Languages](#2020-07-14-3)
  - [4. Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods](#2020-07-14-4)
  - [5. Generating Fluent Adversarial Examples for Natural Languages](#2020-07-14-5)
  - [6. Transformer with Depth-Wise LSTM](#2020-07-14-6)
- [2020-07-13](#2020-07-13)

  - [1. Pragmatic information in translation: a corpus-based study of tense and mood in English and German](#2020-07-13-1)
  - [2. Learn to Use Future Information in Simultaneous Translation](#2020-07-13-2)
- [2020-07-10](#2020-07-10)

  - [1. Principal Word Vectors](#2020-07-10-1)
  - [2. Targeting the Benchmark: On Methodology in Current Natural Language Processing Research](#2020-07-10-2)
- [2020-07-09](#2020-07-09)

  - [1. Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision](2020-07-09-1)
  - [2. Best-First Beam Search](2020-07-09-2)
  - [3. A Survey on Transfer Learning in Natural Language Processing](2020-07-09-3)
- [2020-07-08](#2020-07-08-1)

  - [1. Do Transformers Need Deep Long-Range Memory](#2020-07-08-1)
  - [2. The Go Transformer: Natural Language Modeling for Game Play](#2020-07-08-2)
  - [3. scb-mt-en-th-2020: A Large English-Thai Parallel Corpus](#2020-07-08-3)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-07-28

[Return to Index](#Index)



<h2 id="2020-07-28-1">1. Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling</h2>

Title: [Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling](https://arxiv.org/abs/2007.13002)

Authors: [Siyuan Feng](https://arxiv.org/search/eess?searchtype=author&query=Feng%2C+S), [Odette Scharenborg](https://arxiv.org/search/eess?searchtype=author&query=Scharenborg%2C+O)

> This study addresses unsupervised subword modeling, i.e., learning feature representations that can distinguish subword units of a language. The proposed approach adopts a two-stage bottleneck feature (BNF) learning framework, consisting of autoregressive predictive coding (APC) as a front-end and a DNN-BNF model as a back-end. APC pretrained features are set as input features to a DNN-BNF model. A language-mismatched ASR system is used to provide cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are extracted as the subword-discriminative feature representation. A second aim of this work is to investigate the robustness of our approach's effectiveness to different amounts of training data. The results on Libri-light and the ZeroSpeech 2017 databases show that APC is effective in front-end feature pretraining. Our whole system outperforms the state of the art on both databases. Cross-lingual phone labels for English data by a Dutch ASR outperform those by a Mandarin ASR, possibly linked to the larger similarity of Dutch compared to Mandarin with English. Our system is less sensitive to training data amount when the training data is over 50 hours. APC pretraining leads to a reduction of needed training material from over 5,000 hours to around 200 hours with little performance degradation.

| Comments: | 5 pages, 3 figures. Accepted for publication in INTERSPEECH 2020, Shanghai, China |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2007.13002](https://arxiv.org/abs/2007.13002) [eess.AS]** |
|           | (or **[arXiv:2007.13002v1](https://arxiv.org/abs/2007.13002v1) [eess.AS]** for this version) |



<h2 id="2020-07-28-2">2. Consistent Transcription and Translation of Speech</h2>

Title: [Consistent Transcription and Translation of Speech](https://arxiv.org/abs/2007.12741)

Authors: [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Hendra Setiawan](https://arxiv.org/search/cs?searchtype=author&query=Setiawan%2C+H), [Christian Gollan](https://arxiv.org/search/cs?searchtype=author&query=Gollan%2C+C), [Udhyakumar Nallasamy](https://arxiv.org/search/cs?searchtype=author&query=Nallasamy%2C+U), [Matthias Paulik](https://arxiv.org/search/cs?searchtype=author&query=Paulik%2C+M)

> The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explores end-to-end trainable direct models that translate without transcribing. However, transcripts can be an indispensable output in practical applications, which often display transcripts alongside the translations to users.
> We make this common requirement explicit and explore the task of jointly transcribing and translating speech. While high accuracy of transcript and translation are crucial, even highly accurate systems can suffer from inconsistencies between both outputs that degrade the user experience. We introduce a methodology to evaluate consistency and compare several modeling approaches, including the traditional cascaded approach and end-to-end models. We find that direct models are poorly suited to the joint transcription/translation task, but that end-to-end models that feature a coupled inference procedure are able to achieve strong consistency. We further introduce simple techniques for directly optimizing for consistency, and analyze the resulting trade-offs between consistency, transcription accuracy, and translation accuracy.

| Comments: | Accepted at TACL (pre-MIT Press publication version)         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.12741](https://arxiv.org/abs/2007.12741) [cs.CL]** |
|           | (or **[arXiv:2007.12741v1](https://arxiv.org/abs/2007.12741v1) [cs.CL]** for this version) |







# 2020-07-24

[Return to Index](#Index)



<h2 id="2020-07-24-1">1. Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models</h2>

Title: [Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models](https://arxiv.org/abs/2007.11648)

Authors: [Mittul Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M), [Peter Smit](https://arxiv.org/search/cs?searchtype=author&query=Smit%2C+P), [Sami Virpioja](https://arxiv.org/search/cs?searchtype=author&query=Virpioja%2C+S), [Mikko Kurimo](https://arxiv.org/search/cs?searchtype=author&query=Kurimo%2C+M)

> Character-based Neural Network Language Models (NNLM) have the advantage of smaller vocabulary and thus faster training times in comparison to NNLMs based on multi-character units. However, in low-resource scenarios, both the character and multi-character NNLMs suffer from data sparsity. In such scenarios, cross-lingual transfer has improved multi-character NNLM performance by allowing information transfer from a source to the target language. In the same vein, we propose to use cross-lingual transfer for character NNLMs applied to low-resource Automatic Speech Recognition (ASR). However, applying cross-lingual transfer to character NNLMs is not as straightforward. We observe that relatedness of the source language plays an important role in cross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR tasks for two target languages: Finnish (with English and Estonian as source) and Swedish (with Danish, Norwegian, and English as source). Prior work has observed no difference between using the related or unrelated language for multi-character NNLMs. We, however, show that for character-based NNLMs, only pretraining with a related language improves the ASR performance, and using an unrelated language may deteriorate it. We also observe that the benefits are larger when there is much lesser target data than source data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.11648](https://arxiv.org/abs/2007.11648) [cs.CL]** |
|           | (or **[arXiv:2007.11648v1](https://arxiv.org/abs/2007.11648v1) [cs.CL]** for this version) |







# 2020-07-23

[Return to Index](#Index)



<h2 id="2020-07-23-1">1. Exploratory Search with Sentence Embeddings</h2>

Title: [Exploratory Search with Sentence Embeddings](https://arxiv.org/abs/2007.11198)

Authors:[Austin Silveria](https://arxiv.org/search/cs?searchtype=author&query=Silveria%2C+A)

> Exploratory search aims to guide users through a corpus rather than pinpointing exact information. We propose an exploratory search system based on hierarchical clusters and document summaries using sentence embeddings. With sentence embeddings, we represent documents as the mean of their embedded sentences, extract summaries containing sentences close to this document representation and extract keyphrases close to the document representation. To evaluate our search system, we scrape our personal search history over the past year and report our experience with the system. We then discuss motivating use cases of an exploratory search system of this nature and conclude with possible directions of future work.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.11198](https://arxiv.org/abs/2007.11198) [cs.CL]** |
|           | (or **[arXiv:2007.11198v1](https://arxiv.org/abs/2007.11198v1) [cs.CL]** for this version) |







# 2020-07-22

[Return to Index](#Index)



<h2 id="2020-07-22-1">1. Neural Machine Translation with Error Correction</h2>

Title: [Neural Machine Translation with Error Correction](https://arxiv.org/abs/2007.10681)

Authors: [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Jianfeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J)

> Neural machine translation (NMT) generates the next target token given as input the previous ground truth target tokens during training while the previous generated target tokens during inference, which causes discrepancy between training and inference as well as error propagation, and affects the translation accuracy. In this paper, we introduce an error correction mechanism into NMT, which corrects the error information in the previous generated tokens to better predict the next token. Specifically, we introduce two-stream self-attention from XLNet into NMT decoder, where the query stream is used to predict the next token, and meanwhile the content stream is used to correct the error information from the previous predicted tokens. We leverage scheduled sampling to simulate the prediction errors during training. Experiments on three IWSLT translation datasets and two WMT translation datasets demonstrate that our method achieves improvements over Transformer baseline and scheduled sampling. Further experimental analyses also verify the effectiveness of our proposed error correction mechanism to improve the translation quality.

| Comments: | Accepted by IJCAI 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.10681](https://arxiv.org/abs/2007.10681) [cs.CL]** |
|           | (or **[arXiv:2007.10681v1](https://arxiv.org/abs/2007.10681v1) [cs.CL]** for this version) |





# 2020-07-21

[Return to Index](#Index)



<h2 id="2020-07-21-1">1. WordCraft: An Environment for Benchmarking Commonsense Agents</h2>

Title: [WordCraft: An Environment for Benchmarking Commonsense Agents](https://arxiv.org/abs/2007.09185)

Authors: [Minqi Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M), [Jelena Luketina](https://arxiv.org/search/cs?searchtype=author&query=Luketina%2C+J), [Nantas Nardelli](https://arxiv.org/search/cs?searchtype=author&query=Nardelli%2C+N), [Pasquale Minervini](https://arxiv.org/search/cs?searchtype=author&query=Minervini%2C+P), [Philip H. S. Torr](https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S), [Shimon Whiteson](https://arxiv.org/search/cs?searchtype=author&query=Whiteson%2C+S), [Tim Rocktäschel](https://arxiv.org/search/cs?searchtype=author&query=Rocktäschel%2C+T)

> The ability to quickly solve a wide range of real-world tasks requires a commonsense understanding of the world. Yet, how to best extract such knowledge from natural language corpora and integrate it with reinforcement learning (RL) agents remains an open challenge. This is partly due to the lack of lightweight simulation environments that sufficiently reflect the semantics of the real world and provide knowledge sources grounded with respect to observations in an RL environment. To better enable research on agents making use of commonsense knowledge, we propose WordCraft, an RL environment based on Little Alchemy 2. This lightweight environment is fast to run and built upon entities and relations inspired by real-world semantics. We evaluate several representation learning methods on this new benchmark and propose a new method for integrating knowledge graphs with an RL agent.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.09185](https://arxiv.org/abs/2007.09185) [cs.AI]** |
|           | (or **[arXiv:2007.09185v1](https://arxiv.org/abs/2007.09185v1) [cs.AI]** for this version) |





<h2 id="2020-07-21-2">2. On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Learning</h2>

Title: [On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Learning](https://arxiv.org/abs/2007.09456)

Authors: [Guillem Ramírez](https://arxiv.org/search/cs?searchtype=author&query=Ramírez%2C+G), [Rumen Dangovski](https://arxiv.org/search/cs?searchtype=author&query=Dangovski%2C+R), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P), [Marin Soljačić](https://arxiv.org/search/cs?searchtype=author&query=Soljačić%2C+M)

> The emergence of unsupervised word embeddings, pre-trained on very large monolingual text corpora, is at the core of the ongoing neural revolution in Natural Language Processing (NLP). Initially introduced for English, such pre-trained word embeddings quickly emerged for a number of other languages. Subsequently, there have been a number of attempts to align the embedding spaces across languages, which could enable a number of cross-language NLP applications. Performing the alignment using unsupervised cross-lingual learning (UCL) is especially attractive as it requires little data and often rivals supervised and semi-supervised approaches. Here, we analyze popular methods for UCL and we find that often their objectives are, intrinsically, versions of the Wasserstein-Procrustes problem. Hence, we devise an approach to solve Wasserstein-Procrustes in a direct way, which can be used to refine and to improve popular UCL methods such as iterative closest point (ICP), multilingual unsupervised and supervised embeddings (MUSE) and supervised Procrustes methods. Our evaluation experiments on standard datasets show sizable improvements over these approaches. We believe that our rethinking of the Wasserstein-Procrustes problem could enable further research, thus helping to develop better algorithms for aligning word embeddings across languages. Our code and instructions to reproduce the experiments are available at [this https URL](https://github.com/guillemram97/wp-hungarian).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.09456](https://arxiv.org/abs/2007.09456) [cs.CL]** |
|           | (or **[arXiv:2007.09456v1](https://arxiv.org/abs/2007.09456v1) [cs.CL]** for this version) |





<h2 id="2020-07-21-3">3. Meta-learning for Few-shot Natural Language Processing: A Survey</h2>

Title: [Meta-learning for Few-shot Natural Language Processing: A Survey](https://arxiv.org/abs/2007.09604)

Authors: [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W)

> Few-shot natural language processing (NLP) refers to NLP tasks that are accompanied with merely a handful of labeled examples. This is a real-world challenge that an AI system must learn to handle. Usually we rely on collecting more auxiliary information or developing a more efficient learning algorithm. However, the general gradient-based optimization in high capacity models, if training from scratch, requires many parameter-updating steps over a large number of labeled examples to perform well (Snell et al., 2017).
> If the target task itself cannot provide more information, how about collecting more tasks equipped with rich annotations to help the model learning? The goal of meta-learning is to train a model on a variety of tasks with rich annotations, such that it can solve a new task using only a few labeled samples. The key idea is to train the model's initial parameters such that the model has maximal performance on a new task after the parameters have been updated through zero or a couple of gradient steps.
> There are already some surveys for meta-learning, such as (Vilalta and Drissi, 2002; Vanschoren, 2018; Hospedales et al., 2020). Nevertheless, this paper focuses on NLP domain, especially few-shot applications. We try to provide clearer definitions, progress summary and some common datasets of applying meta-learning to few-shot NLP.

| Comments: | no submission intent                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.09604](https://arxiv.org/abs/2007.09604) [cs.CL]** |
|           | (or **[arXiv:2007.09604v1](https://arxiv.org/abs/2007.09604v1) [cs.CL]** for this version) |





<h2 id="2020-07-21-4">4. One-Shot Learning for Language Modelling</h2>

Title: [One-Shot Learning for Language Modelling](https://arxiv.org/abs/2007.09679)

Authors: [Talip Ucar](https://arxiv.org/search/cs?searchtype=author&query=Ucar%2C+T), [Adrian Gonzalez-Martin](https://arxiv.org/search/cs?searchtype=author&query=Gonzalez-Martin%2C+A), [Matthew Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+M), [Adrian Daniel Szwarc](https://arxiv.org/search/cs?searchtype=author&query=Szwarc%2C+A+D)

> Humans can infer a great deal about the meaning of a word, using the syntax and semantics of surrounding words even if it is their first time reading or hearing it. We can also generalise the learned concept of the word to new tasks. Despite great progress in achieving human-level performance in certain tasks (Silver et al., 2016), learning from one or few examples remains a key challenge in machine learning, and has not thoroughly been explored in Natural Language Processing (NLP).
> In this work we tackle the problem of oneshot learning for an NLP task by employing ideas from recent developments in machine learning: embeddings, attention mechanisms (softmax) and similarity measures (cosine, Euclidean, Poincare, and Minkowski). We adapt the framework suggested in matching networks (Vinyals et al., 2016), and explore the effectiveness of the aforementioned methods in one, two and three-shot learning problems on the task of predicting missing word explored in (Vinyals et al., 2016) by using the WikiText-2 dataset. Our work contributes in two ways: Our first contribution is that we explore the effectiveness of different distance metrics on k-shot learning, and show that there is no single best distance metric for k-shot learning, which challenges common belief. We found that the performance of a distance metric depends on the number of shots used during training. The second contribution of our work is that we establish a benchmark for one, two, and three-shot learning on a language task with a publicly available dataset that can be used to benchmark against in future research.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.09679](https://arxiv.org/abs/2007.09679) [cs.CL]** |
|           | (or **[arXiv:2007.09679v1](https://arxiv.org/abs/2007.09679v1) [cs.CL]** for this version) |





<h2 id="2020-07-21-5">5. Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks</h2>

Title: [Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks](https://arxiv.org/abs/2007.09757)

Authors: [Diego de Vargas Feijo](https://arxiv.org/search/cs?searchtype=author&query=de+Vargas+Feijo%2C+D), [Viviane Pereira Moreira](https://arxiv.org/search/cs?searchtype=author&query=Moreira%2C+V+P)

> BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.09757](https://arxiv.org/abs/2007.09757) [cs.CL]** |
|           | (or **[arXiv:2007.09757v1](https://arxiv.org/abs/2007.09757v1) [cs.CL]** for this version) |





<h2 id="2020-07-21-6">6. Morphological Skip-Gram: Using morphological knowledge to improve word representation</h2>

Title: [Morphological Skip-Gram: Using morphological knowledge to improve word representation](https://arxiv.org/abs/2007.10055)

Authors: [Flávio Santos](https://arxiv.org/search/cs?searchtype=author&query=Santos%2C+F), [Hendrik Macedo](https://arxiv.org/search/cs?searchtype=author&query=Macedo%2C+H), [Thiago Bispo](https://arxiv.org/search/cs?searchtype=author&query=Bispo%2C+T), [Cleber Zanchetting](https://arxiv.org/search/cs?searchtype=author&query=Zanchetting%2C+C)

> Natural language processing models have attracted much interest in the deep learning community. This branch of study is composed of some applications such as machine translation, sentiment analysis, named entity recognition, question and answer, and others. Word embeddings are continuous word representations, they are an essential module for those applications and are generally used as input word representation to the deep learning models. Word2Vec and GloVe are two popular methods to learn word embeddings. They achieve good word representations, however, they learn representations with limited information because they ignore the morphological information of the words and consider only one representation vector for each word. This approach implies that Word2Vec and GloVe are unaware of the word inner structure. To mitigate this problem, the FastText model represents each word as a bag of characters n-grams. Hence, each n-gram has a continuous vector representation, and the final word representation is the sum of its characters n-grams vectors. Nevertheless, the use of all n-grams character of a word is a poor approach since some n-grams have no semantic relation with their words and increase the amount of potentially useless information. This approach also increases the training phase time. In this work, we propose a new method for training word embeddings, and its goal is to replace the FastText bag of character n-grams for a bag of word morphemes through the morphological analysis of the word. Thus, words with similar context and morphemes are represented by vectors close to each other. To evaluate our new approach, we performed intrinsic evaluations considering 15 different tasks, and the results show a competitive performance compared to FastText.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.10055](https://arxiv.org/abs/2007.10055) [cs.CL]** |
|           | (or **[arXiv:2007.10055v1](https://arxiv.org/abs/2007.10055v1) [cs.CL]** for this version) |





<h2 id="2020-07-21-7">7. CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus</h2>

Title: [CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus](https://arxiv.org/abs/2007.10310)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.10310](https://arxiv.org/abs/2007.10310) [cs.CL]** |
|           | (or **[arXiv:2007.10310v1](https://arxiv.org/abs/2007.10310v1) [cs.CL]** for this version) |



# 2020-07-20

[Return to Index](#Index)



<h2 id="2020-07-20-1">1. Unsupervised Text Generation by Learning from Search</h2>

Title: [Unsupervised Text Generation by Learning from Search](https://arxiv.org/abs/2007.08557)

Authors: [Jingjing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Zichao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Lili Mou](https://arxiv.org/search/cs?searchtype=author&query=Mou%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Irwin King](https://arxiv.org/search/cs?searchtype=author&query=King%2C+I)

> In this work, we present TGLS, a novel framework to unsupervised Text Generation by Learning from Search. We start by applying a strong search algorithm (in particular, simulated annealing) towards a heuristically defined objective that (roughly) estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping. We demonstrate the effectiveness of TGLS on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08557](https://arxiv.org/abs/2007.08557) [cs.CL]** |
|           | (or **[arXiv:2007.08557v1](https://arxiv.org/abs/2007.08557v1) [cs.CL]** for this version) |





<h2 id="2020-07-20-2">2. A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</h2>

Title: [A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation](https://arxiv.org/abs/2007.08742)

Authors: [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Zhengyuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08742](https://arxiv.org/abs/2007.08742) [cs.CL]** |
|           | (or **[arXiv:2007.08742v1](https://arxiv.org/abs/2007.08742v1) [cs.CL]** for this version) |







<h2 id="2020-07-20-3">3. Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation</h2>

Title: [Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2007.08772)

Authors:[Jinglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Yi Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Chen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Non-autoregressive translation (NAT) achieves faster inference speed but at the cost of worse accuracy compared with autoregressive translation (AT). Since AT and NAT can share model structure and AT is an easier task than NAT due to the explicit dependency on previous target-side tokens, a natural idea is to gradually shift the model training from the easier AT task to the harder NAT task. To smooth the shift from AT training to NAT training, in this paper, we introduce semi-autoregressive translation (SAT) as intermediate tasks. SAT contains a hyperparameter k, and each k value defines a SAT task with different degrees of parallelism. Specially, SAT covers AT and NAT as its special cases: it reduces to AT when k = 1 and to NAT when k = N (N is the length of target sentence). We design curriculum schedules to gradually shift k from 1 to N, with different pacing functions and number of tasks trained at the same time. We called our method as task-level curriculum learning for NAT (TCL-NAT). Experiments on IWSLT14 De-En, IWSLT16 En-De, WMT14 En-De and De-En datasets show that TCL-NAT achieves significant accuracy improvements over previous NAT baselines and reduces the performance gap between NAT and AT models to 1-2 BLEU points, demonstrating the effectiveness of our proposed method.

| Comments: | Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.08772](https://arxiv.org/abs/2007.08772) [cs.CL]** |
|           | (or **[arXiv:2007.08772v1](https://arxiv.org/abs/2007.08772v1) [cs.CL]** for this version) |







<h2 id="2020-07-20-4">4. Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures</h2>

Title: [Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures](https://arxiv.org/abs/2007.08970)

Authors: [Daniel Furrer](https://arxiv.org/search/cs?searchtype=author&query=Furrer%2C+D), [Marc van Zee](https://arxiv.org/search/cs?searchtype=author&query=van+Zee%2C+M), [Nathan Scales](https://arxiv.org/search/cs?searchtype=author&query=Scales%2C+N), [Nathanael Schärli](https://arxiv.org/search/cs?searchtype=author&query=Schärli%2C+N)

> While mainstream machine learning methods are known to have limited ability to compositionally generalize, new architectures and techniques continue to be proposed to address this limitation. We investigate state-of-the-art techniques and architectures in order to assess their effectiveness in improving compositional generalization in semantic parsing tasks based on the SCAN and CFQ datasets. We show that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits. On a more complex compositional task, we show that pre-training leads to significant improvements in performance vs. comparable non-pre-trained models, whereas architectures proposed to encourage compositional generalization on SCAN or in the area of algorithm learning fail to lead to significant improvements. We establish a new state of the art on the CFQ compositional generalization benchmark using MLM pre-training together with an intermediate representation.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08970](https://arxiv.org/abs/2007.08970) [cs.CL]** |
|           | (or **[arXiv:2007.08970v1](https://arxiv.org/abs/2007.08970v1) [cs.CL]** for this version) |







<h2 id="2020-07-20-5">5. Constructing a Family Tree of Ten Indo-European Languages with Delexicalized Cross-linguistic Transfer Patterns</h2>

Title: [Constructing a Family Tree of Ten Indo-European Languages with Delexicalized Cross-linguistic Transfer Patterns](https://arxiv.org/abs/2007.09076)

Authors: [Yuanyuan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Weiwei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+W), [Xiaojun Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+X)

> It is reasonable to hypothesize that the divergence patterns formulated by historical linguists and typologists reflect constraints on human languages, and are thus consistent with Second Language Acquisition (SLA) in a certain way. In this paper, we validate this hypothesis on ten Indo-European languages. We formalize the delexicalized transfer as interpretable tree-to-string and tree-to-tree patterns which can be automatically induced from web data by applying neural syntactic parsing and grammar induction technologies. This allows us to quantitatively probe cross-linguistic transfer and extend inquiries of SLA. We extend existing works which utilize mixed features and support the agreement between delexicalized cross-linguistic transfer and the phylogenetic structure resulting from the historical-comparative paradigm.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.09076](https://arxiv.org/abs/2007.09076) [cs.CL]** |
|           | (or **[arXiv:2007.09076v1](https://arxiv.org/abs/2007.09076v1) [cs.CL]** for this version) |







<h2 id="2020-07-20-6">6. Neural Architecture Search for Speech Recognition</h2>

Title: [Neural Architecture Search for Speech Recognition](https://arxiv.org/abs/2007.08818)

Authors: [Shoukang Hu](https://arxiv.org/search/eess?searchtype=author&query=Hu%2C+S), [Xurong Xie](https://arxiv.org/search/eess?searchtype=author&query=Xie%2C+X), [Shansong Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+S), [Mengzhe Geng](https://arxiv.org/search/eess?searchtype=author&query=Geng%2C+M), [Xunying Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X), [Helen Meng](https://arxiv.org/search/eess?searchtype=author&query=Meng%2C+H)

> Deep neural networks (DNNs) based automatic speech recognition (ASR) systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network (TDNN-F) acoustic models: i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free MMI (LF-MMI) training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; Pipelined DARTS that circumvents the overfitting of architecture weights using held-out data; and Penalized DARTS that further incorporates resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over up to 728 different TDNN systems. Experiments conducted on a 300-hour Switchboard conversational telephone speech recognition task suggest the NAS auto-configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manual expert configurations. Absolute word error rate reductions up to 1.0% and relative model size reduction of 28% were obtained.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08818](https://arxiv.org/abs/2007.08818) [eess.AS]** |
|           | (or **[arXiv:2007.08818v1](https://arxiv.org/abs/2007.08818v1) [eess.AS]** for this version) |





# 2020-07-17

[Return to Index](#Index)



<h2 id="2020-07-17-1">1. Translate Reverberated Speech to Anechoic Ones: Speech Dereverberation with BERT</h2>

Title: [Translate Reverberated Speech to Anechoic Ones: Speech Dereverberation with BERT](https://arxiv.org/abs/2007.08052)

Authors: [Yang Jiao](https://arxiv.org/search/eess?searchtype=author&query=Jiao%2C+Y)

> Single channel speech dereverberation is considered in this work. Inspired by the recent success of Bidirectional Encoder Representations from Transformers (BERT) model in the domain of Natural Language Processing (NLP), we investigate its applicability as backbone sequence model to enhance reverberated speech signal. We present a variation of the basic BERT model: a pre-sequence network, which extracts local spectral-temporal information and/or provides order information, before the backbone sequence model. In addition, we use pre-trained neural vocoder for implicit phase reconstruction. To evaluate our method, we used the data from the 3rd CHiME challenge, and compare our results with other methods. Experiments show that the proposed method outperforms traditional method WPE, and achieve comparable performance with state-of-the-art BLSTM-based sequence models.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08052](https://arxiv.org/abs/2007.08052) [eess.AS]** |
|           | (or **[arXiv:2007.08052v1](https://arxiv.org/abs/2007.08052v1) [eess.AS]** for this version) |





<h2 id="2020-07-17-2">2. Towards Debiasing Sentence Representations</h2>

Title: [Towards Debiasing Sentence Representations](https://arxiv.org/abs/2007.08100)

Authors: [Paul Pu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P), [Irene Mengze Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+I+M), [Emily Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+E), [Yao Chong Lim](https://arxiv.org/search/cs?searchtype=author&query=Lim%2C+Y+C), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R), [Louis-Philippe Morency](https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L)

> As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.

| Comments: | ACL 2020, code available at [this https URL](https://github.com/pliang279/sent_debias) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.08100](https://arxiv.org/abs/2007.08100) [cs.CL]** |
|           | (or **[arXiv:2007.08100v1](https://arxiv.org/abs/2007.08100v1) [cs.CL]** for this version) |





<h2 id="2020-07-17-3">3. Investigating Pretrained Language Models for Graph-to-Text Generation</h2>

Title: [Investigating Pretrained Language Models for Graph-to-Text Generation](https://arxiv.org/abs/2007.08426)

Authors: [Leonardo F. R. Ribeiro](https://arxiv.org/search/cs?searchtype=author&query=Ribeiro%2C+L+F+R), [Martin Schmitt](https://arxiv.org/search/cs?searchtype=author&query=Schmitt%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Graph-to-text generation, a subtask of data-to-text generation, aims to generate fluent texts from graph-based data. Many graph-to-text models have shown strong performance in this task employing specialized graph encoders. However, recent approaches employ large pretrained language models (PLMs) achieving state-of-the-art results in data-to-text generation. In this paper, we aim to investigate the impact of large PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. Our analysis shows that PLMs such as BART and T5 achieve state-of-the-art results in graph-to-text benchmarks without explicitly encoding the graph structure. We also demonstrate that task-adaptive pretraining strategies are beneficial to the target task, improving even further the state of the art in two benchmarks for graph-to-text generation. In a final analysis, we investigate possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about the world gives them a big advantage, especially when generating texts from KGs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.08426](https://arxiv.org/abs/2007.08426) [cs.CL]** |
|           | (or **[arXiv:2007.08426v1](https://arxiv.org/abs/2007.08426v1) [cs.CL]** for this version) |







# 2020-07-16

[Return to Index](#Index)



<h2 id="2020-07-16-1">1. Deep learning models for representing out-of-vocabulary words</h2>

Title: [Deep learning models for representing out-of-vocabulary words](https://arxiv.org/abs/2007.07318)

Authors: [Johannes V. Lochter](https://arxiv.org/search/cs?searchtype=author&query=Lochter%2C+J+V), [Renato M. Silva](https://arxiv.org/search/cs?searchtype=author&query=Silva%2C+R+M), [Tiago A. Almeida](https://arxiv.org/search/cs?searchtype=author&query=Almeida%2C+T+A)

> Communication has become increasingly dynamic with the popularization of social networks and applications that allow people to express themselves and communicate instantly. In this scenario, distributed representation models have their quality impacted by new words that appear frequently or that are derived from spelling errors. These words that are unknown by the models, known as out-of-vocabulary (OOV) words, need to be properly handled to not degrade the quality of the natural language processing (NLP) applications, which depend on the appropriate vector representation of the texts. To better understand this problem and finding the best techniques to handle OOV words, in this study, we present a comprehensive performance evaluation of deep learning models for representing OOV words. We performed an intrinsic evaluation using a benchmark dataset and an extrinsic evaluation using different NLP tasks: text categorization, named entity recognition, and part-of-speech tagging. The results indicated that the best technique for handling OOV words can be different for each task. But, in general, deep learning models that infer the embedding based on the context and the morphological structure of the OOV word obtained promising results.

| Comments: | Preprint of the paper accepted to presentation at the 9th Brazilian Conference on Intelligent Systems (BRACIS 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.07318](https://arxiv.org/abs/2007.07318) [cs.CL]** |
|           | (or **[arXiv:2007.07318v1](https://arxiv.org/abs/2007.07318v1) [cs.CL]** for this version) |





<h2 id="2020-07-16-2">2. A Multilingual Parallel Corpora Collection Effort for Indian Languages</h2>

Title: [A Multilingual Parallel Corpora Collection Effort for Indian Languages](https://arxiv.org/abs/2007.07691)

Authors: [Shashank Siripragada](https://arxiv.org/search/cs?searchtype=author&query=Siripragada%2C+S), [Jerin Philip](https://arxiv.org/search/cs?searchtype=author&query=Philip%2C+J), [Vinay P. Namboodiri](https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P), [C V Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C+V)

> We present sentence aligned parallel corpora across 10 Indian Languages - Hindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi, Punjabi, and English - many of which are categorized as low resource. The corpora are compiled from online sources which have content shared across languages. The corpora presented significantly extends present resources that are either not large enough or are restricted to a specific domain (such as health). We also provide a separate test corpus compiled from an independent online source that can be independently used for validating the performance in 10 Indian languages. Alongside, we report on the methods of constructing such corpora using tools enabled by recent advances in machine translation and cross-lingual retrieval using deep neural network based methods.

| Comments: | 9 pages. Accepted in LREC 2020                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.07691](https://arxiv.org/abs/2007.07691) [cs.CL]** |
|           | (or **[arXiv:2007.07691v1](https://arxiv.org/abs/2007.07691v1) [cs.CL]** for this version) |





<h2 id="2020-07-16-3">3. Dual Past and Future for Neural Machine Translation</h2>

Title: [Dual Past and Future for Neural Machine Translation](https://arxiv.org/abs/2007.07728)

Authors: [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Though remarkable successes have been achieved by Neural Machine Translation (NMT) in recent years, it still suffers from the inadequate-translation problem. Previous studies show that explicitly modeling the Past and Future contents of the source sentence is beneficial for translation performance. However, it is not clear whether the commonly used heuristic objective is good enough to guide the Past and Future. In this paper, we present a novel dual framework that leverages both source-to-target and target-to-source NMT models to provide a more direct and accurate supervision signal for the Past and Future modules. Experimental results demonstrate that our proposed method significantly improves the adequacy of NMT predictions and surpasses previous methods in two well-studied translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.07728](https://arxiv.org/abs/2007.07728) [cs.CL]** |
|           | (or **[arXiv:2007.07728v1](https://arxiv.org/abs/2007.07728v1) [cs.CL]** for this version) |





<h2 id="2020-07-16-4">4. Multimodal Word Sense Disambiguation in Creative Practice</h2>

Title: [Multimodal Word Sense Disambiguation in Creative Practice](https://arxiv.org/abs/2007.07758)

Authors: [Manuel Ladron de Guevara](https://arxiv.org/search/cs?searchtype=author&query=de+Guevara%2C+M+L), [Christopher George](https://arxiv.org/search/cs?searchtype=author&query=George%2C+C), [Akshat Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Daragh Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+D), [Ramesh Krishnamurti](https://arxiv.org/search/cs?searchtype=author&query=Krishnamurti%2C+R)

> Language is ambiguous; many terms and expressions can convey the same idea. This is especially true in creative practice, where ideas and design intents are highly subjective. We present a dataset, Ambiguous Descriptions of Art Images (ADARI), of contemporary workpieces, which aims to provide a foundational resource for subjective image description and multimodal word disambiguation in the context of creative practice. The dataset contains a total of 240k images labeled with 260k descriptive sentences. It is additionally organized into sub-domains of architecture, art, design, fashion, furniture, product design and technology. In subjective image description, labels are not deterministic: for example, the ambiguous label dynamic might correspond to hundreds of different images. To understand this complexity, we analyze the ambiguity and relevance of text with respect to images using the state-of-the-art pre-trained BERT model for sentence classification. We provide a baseline for multi-label classification tasks and demonstrate the potential of multimodal approaches for understanding ambiguity in design intentions. We hope that ADARI dataset and baselines constitute a first step towards subjective label classification.

| Comments: | 9 pages, 5 figures, 2 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.07758](https://arxiv.org/abs/2007.07758) [cs.CL]** |
|           | (or **[arXiv:2007.07758v1](https://arxiv.org/abs/2007.07758v1) [cs.CL]** for this version) |





<h2 id="2020-07-16-5">5. AdapterHub: A Framework for Adapting Transformers</h2>

Title: [AdapterHub: A Framework for Adapting Transformers](https://arxiv.org/abs/2007.07779)

Authors: [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Andreas Rücklé](https://arxiv.org/search/cs?searchtype=author&query=Rücklé%2C+A), [Clifton Poth](https://arxiv.org/search/cs?searchtype=author&query=Poth%2C+C), [Aishwarya Kamath](https://arxiv.org/search/cs?searchtype=author&query=Kamath%2C+A), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic "stitching-in" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at [this https URL](https://adapterhub.ml/).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.07779](https://arxiv.org/abs/2007.07779) [cs.CL]** |
|           | (or **[arXiv:2007.07779v1](https://arxiv.org/abs/2007.07779v1) [cs.CL]** for this version) |





<h2 id="2020-07-16-6">6. InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</h2>

Title: [InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training](https://arxiv.org/abs/2007.07834)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Nan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+N), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> In this work, we formulate cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, the information-theoretic framework inspires us to propose a pre-training task based on contrastive learning. Given a bilingual sentence pair, we regard them as two views of the same meaning, and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at [this http URL](http://aka.ms/infoxlm).

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.07834](https://arxiv.org/abs/2007.07834) [cs.CL]** |
|           | (or **[arXiv:2007.07834v1](https://arxiv.org/abs/2007.07834v1) [cs.CL]** for this version) |





# 2020-07-15

[Return to Index](#Index)



<h2 id="2020-07-15-1">1. Modeling Voting for System Combination in Machine Translation</h2>

Title: [Modeling Voting for System Combination in Machine Translation](https://arxiv.org/abs/2007.06943)

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jiacheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Huanbo Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+H), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> System combination is an important technique for combining the hypotheses of different machine translation systems to improve translation performance. Although early statistical approaches to system combination have been proven effective in analyzing the consensus between hypotheses, they suffer from the error propagation problem due to the use of pipelines. While this problem has been alleviated by end-to-end training of multi-source sequence-to-sequence models recently, these neural models do not explicitly analyze the relations between hypotheses and fail to capture their agreement because the attention to a word in a hypothesis is calculated independently, ignoring the fact that the word might occur in multiple hypotheses. In this work, we propose an approach to modeling voting for system combination in machine translation. The basic idea is to enable words in hypotheses from different systems to vote on words that are representative and should get involved in the generation process. This can be done by quantifying the influence of each voter and its preference for each candidate. Our approach combines the advantages of statistical and neural methods since it can not only analyze the relations between hypotheses but also allow for end-to-end training. Experiments show that our approach is capable of better taking advantage of the consensus between hypotheses and achieves significant improvements over state-of-the-art baselines on Chinese-English and English-German machine translation tasks.

| Comments: | Accepted by main track of IJCAI2020;SOLE copyright holder is IJCAI (international Joint Conferences on Artificial Intelligence), all rights reserved. [this https URL](https://www.ijcai.org/Proceedings/2020/511) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| DOI:      | [10.24963/ijcai.2020/511](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.24963%2Fijcai.2020%2F511&v=3ced08f2) |
| Cite as:  | **[arXiv:2007.06943](https://arxiv.org/abs/2007.06943) [cs.CL]** |
|           | (or **[arXiv:2007.06943v1](https://arxiv.org/abs/2007.06943v1) [cs.CL]** for this version) |







# 2020-07-14

[Return to Index](#Index)



<h2 id="2020-07-14-1">1. TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech</h2>

Title: [TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech](https://arxiv.org/abs/2007.06028)

Authors: [Andy T. Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+A+T), [Shang-Wen Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+S), [Hung-yi Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+H)

> We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn through the formulation of a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous approaches, we use a multi-target auxiliary task to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from its altered counterpart, where we use a stochastic policy to alter along three dimensions: temporal, channel, and magnitude. TERA can be used to extract speech representations or fine-tune with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, speaker recognition, and speech recognition. TERA achieved strong performance on these tasks by improving upon surface features and outperforming previous methods. In our experiments, we show that through alteration along different dimensions, the model learns to encode distinct aspects of speech. We explore different knowledge transfer methods to incorporate the pre-trained model with downstream models. Furthermore, we show that the proposed method can be easily transferred to another dataset not used in pre-training.

| Comments: | Submitted to IEEE TASLP, currently under review              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.06028](https://arxiv.org/abs/2007.06028) [eess.AS]** |
|           | (or **[arXiv:2007.06028v1](https://arxiv.org/abs/2007.06028v1) [eess.AS]** for this version) |





<h2 id="2020-07-14-2">2. Fine-grained Language Identification with Multilingual CapsNet Model</h2>

Title: [Fine-grained Language Identification with Multilingual CapsNet Model](https://arxiv.org/abs/2007.06078)

Authors: [Mudit Verma](https://arxiv.org/search/eess?searchtype=author&query=Verma%2C+M), [Arun Balaji Buduru](https://arxiv.org/search/eess?searchtype=author&query=Buduru%2C+A+B)

> Due to a drastic improvement in the quality of internet services worldwide, there is an explosion of multilingual content generation and consumption. This is especially prevalent in countries with large multilingual audience, who are increasingly consuming media outside their linguistic familiarity/preference. Hence, there is an increasing need for real-time and fine-grained content analysis services, including language identification, content transcription, and analysis. Accurate and fine-grained spoken language detection is an essential first step for all the subsequent content analysis algorithms. Current techniques in spoken language detection may lack on one of these fronts: accuracy, fine-grained detection, data requirements, manual effort in data collection \& pre-processing. Hence in this work, a real-time language detection approach to detect spoken language from 5 seconds' audio clips with an accuracy of 91.8\% is presented with exiguous data requirements and minimal pre-processing. Novel architectures for Capsule Networks is proposed which operates on spectrogram images of the provided audio snippets. We use previous approaches based on Recurrent Neural Networks and iVectors to present the results. Finally we show a ``Non-Class'' analysis to further stress on why CapsNet architecture works for LID task.

| Comments: | 5 pages, 6 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2007.06078](https://arxiv.org/abs/2007.06078) [eess.AS]** |
|           | (or **[arXiv:2007.06078v1](https://arxiv.org/abs/2007.06078v1) [eess.AS]** for this version) |





<h2 id="2020-07-14-3">3. Is Machine Learning Speaking my Language? A Critical Look at the NLP-Pipeline Across 8 Human Languages</h2>

Title: [Is Machine Learning Speaking my Language? A Critical Look at the NLP-Pipeline Across 8 Human Languages](https://arxiv.org/abs/2007.05872)

Authors: [Esma Wali](https://arxiv.org/search/cs?searchtype=author&query=Wali%2C+E), [Yan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Christopher Mahoney](https://arxiv.org/search/cs?searchtype=author&query=Mahoney%2C+C), [Thomas Middleton](https://arxiv.org/search/cs?searchtype=author&query=Middleton%2C+T), [Marzieh Babaeianjelodar](https://arxiv.org/search/cs?searchtype=author&query=Babaeianjelodar%2C+M), [Mariama Njie](https://arxiv.org/search/cs?searchtype=author&query=Njie%2C+M), [Jeanna Neefe Matthews](https://arxiv.org/search/cs?searchtype=author&query=Matthews%2C+J+N)

> Natural Language Processing (NLP) is increasingly used as a key ingredient in critical decision-making systems such as resume parsers used in sorting a list of job candidates. NLP systems often ingest large corpora of human text, attempting to learn from past human behavior and decisions in order to produce systems that will make recommendations about our future world. Over 7000 human languages are being spoken today and the typical NLP pipeline underrepresents speakers of most of them while amplifying the voices of speakers of other languages. In this paper, a team including speakers of 8 languages - English, Chinese, Urdu, Farsi, Arabic, French, Spanish, and Wolof - takes a critical look at the typical NLP pipeline and how even when a language is technically supported, substantial caveats remain to prevent full participation. Despite huge and admirable investments in multilingual support in many tools and resources, we are still making NLP-guided decisions that systematically and dramatically underrepresent the voices of much of the world.

| Comments:    | Participatory Approaches to Machine Learning Workshop, 37th International Conference on Machine Learning |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2007.05872](https://arxiv.org/abs/2007.05872) [cs.CL]** |
|              | (or **[arXiv:2007.05872v1](https://arxiv.org/abs/2007.05872v1) [cs.CL]** for this version) |





<h2 id="2020-07-14-4">4. Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods</h2>

Title: [Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods](https://arxiv.org/abs/2007.06162)

Authors: [Ning Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+N), [Yuxuan Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimation problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach. Our code is available at this url.

| Comments: | Accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.06162](https://arxiv.org/abs/2007.06162) [cs.CL]** |
|           | (or **[arXiv:2007.06162v1](https://arxiv.org/abs/2007.06162v1) [cs.CL]** for this version) |





<h2 id="2020-07-14-5">5. Generating Fluent Adversarial Examples for Natural Languages</h2>

Title: [Generating Fluent Adversarial Examples for Natural Languages](https://arxiv.org/abs/2007.06174)

Authors: [Huangzhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Ning Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+N), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms the baseline model on attacking capability. Adversarial training with MAH also leads to better robustness and performance.

| Comments: | Accepted by ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.06174](https://arxiv.org/abs/2007.06174) [cs.CL]** |
|           | (or **[arXiv:2007.06174v1](https://arxiv.org/abs/2007.06174v1) [cs.CL]** for this version) |





<h2 id="2020-07-14-6">6. Transformer with Depth-Wise LSTM</h2>

Title: [Transformer with Depth-Wise LSTM](https://arxiv.org/abs/2007.06257)

Authors: [Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Qiuhui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

> Increasing the depth of models allows neural models to model complicated functions but may also lead to optimization issues. The Transformer translation model employs the residual connection to ensure its convergence. In this paper, we suggest that the residual connection has its drawbacks, and propose to train Transformers with the depth-wise LSTM which regards outputs of layers as steps in time series instead of residual connections, under the motivation that the vanishing gradient problem suffered by deep networks is the same as recurrent networks applied to long sequences, while LSTM (Hochreiter and Schmidhuber, 1997) has been proven of good capability in capturing long-distance relationship, and its design may alleviate some drawbacks of residual connections while ensuring the convergence. We integrate the computation of multi-head attention networks and feed-forward networks with the depth-wise LSTM for the Transformer, which shows how to utilize the depth-wise LSTM like the residual connection. Our experiment with the 6-layer Transformer shows that our approach can bring about significant BLEU improvements in both WMT 14 English-German and English-French tasks, and our deep Transformer experiment demonstrates the effectiveness of the depth-wise LSTM on the convergence of deep Transformers. Additionally, we propose to measure the impacts of the layer's non-linearity on the performance by distilling the analyzing layer of the trained model into a linear transformation and observing the performance degradation with the replacement. Our analysis results support the more efficient use of per-layer non-linearity with depth-wise LSTM than with residual connections.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.06257](https://arxiv.org/abs/2007.06257) [cs.CL]** |
|           | (or **[arXiv:2007.06257v1](https://arxiv.org/abs/2007.06257v1) [cs.CL]** for this version) |





# 2020-07-13

[Return to Index](#Index)



<h2 id="2020-07-13-1">1. Pragmatic information in translation: a corpus-based study of tense and mood in English and German
</h2>

Title: [Pragmatic information in translation: a corpus-based study of tense and mood in English and German](https://arxiv.org/abs/2007.05234)

Authors: [Anita Ramm](https://arxiv.org/search/cs?searchtype=author&query=Ramm%2C+A), [Ekaterina Lapshinova-Koltunski](https://arxiv.org/search/cs?searchtype=author&query=Lapshinova-Koltunski%2C+E), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Grammatical tense and mood are important linguistic phenomena to consider in natural language processing (NLP) research. We consider the correspondence between English and German tense and mood in translation. Human translators do not find this correspondence easy, and as we will show through careful analysis, there are no simplistic ways to map tense and mood from one language to another. Our observations about the challenges of human translation of tense and mood have important implications for multilingual NLP. Of particular importance is the challenge of modeling tense and mood in rule-based, phrase-based statistical and neural machine translation.

| Comments:    | Technical Report of CIS, LMU Munich. September 19th, 2019    |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.1.2                                                        |
| Cite as:     | **[arXiv:2007.05234](https://arxiv.org/abs/2007.05234) [cs.CL]** |
|              | (or **[arXiv:2007.05234v1](https://arxiv.org/abs/2007.05234v1) [cs.CL]** for this version) |





<h2 id="2020-07-13-2">2. Learn to Use Future Information in Simultaneous Translation</h2>

Title: [Learn to Use Future Information in Simultaneous Translation](https://arxiv.org/abs/2007.05290)

Authors: [Xueqing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Shufang Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+S), [Weiqing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Jiang Bian](https://arxiv.org/search/cs?searchtype=author&query=Bian%2C+J), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can utilize the full input sentence, simultaneous NMT is formulated as a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and more uncertainty is introduced to decoding. Wait-k is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence k words behind the input words. We observed that training simultaneous NMT systems with future information (i.e., trained with a larger k) generally outperforms the standard ones (i.e., trained with the given k). Based on this observation, we propose a framework that automatically learns how much future information to use in training for simultaneous NMT. We first build a series of tasks where each one is associated with a different k, and then learn a model on these tasks guided by a controller. The controller is jointly trained with the translation model through bi-level optimization. We conduct experiments on four datasets to demonstrate the effectiveness of our method.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.05290](https://arxiv.org/abs/2007.05290) [cs.CL]** |
|           | (or **[arXiv:2007.05290v1](https://arxiv.org/abs/2007.05290v1) [cs.CL]** for this version) |



# 2020-07-10

[Return to Index](#Index)



<h2 id="2020-07-10-1">1. Principal Word Vectors</h2>

Title: [Principal Word Vectors](https://arxiv.org/abs/2007.04629)

Authors: [Ali Basirat](https://arxiv.org/search/cs?searchtype=author&query=Basirat%2C+A), [Christian Hardmeier](https://arxiv.org/search/cs?searchtype=author&query=Hardmeier%2C+C), [Joakim Nivre](https://arxiv.org/search/cs?searchtype=author&query=Nivre%2C+J)

> We generalize principal component analysis for embedding words into a vector space. The generalization is made in two major levels. The first is to generalize the concept of the corpus as a counting process which is defined by three key elements vocabulary set, feature (annotation) set, and context. This generalization enables the principal word embedding method to generate word vectors with regard to different types of contexts and different types of annotations provided for a corpus. The second is to generalize the transformation step used in most of the word embedding methods. To this end, we define two levels of transformations. The first is a quadratic transformation, which accounts for different types of weighting over the vocabulary units and contextual features. Second is an adaptive non-linear transformation, which reshapes the data distribution to be meaningful to principal component analysis. The effect of these generalizations on the word vectors is intrinsically studied with regard to the spread and the discriminability of the word vectors. We also provide an extrinsic evaluation of the contribution of the principal word vectors on a word similarity benchmark and the task of dependency parsing. Our experiments are finalized by a comparison between the principal word vectors and other sets of word vectors generated with popular word embedding methods. The results obtained from our intrinsic evaluation metrics show that the spread and the discriminability of the principal word vectors are higher than that of other word embedding methods. The results obtained from the extrinsic evaluation metrics show that the principal word vectors are better than some of the word embedding methods and on par with popular methods of word embedding.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.04629](https://arxiv.org/abs/2007.04629) [cs.CL]** |
|           | (or **[arXiv:2007.04629v1](https://arxiv.org/abs/2007.04629v1) [cs.CL]** for this version) |





<h2 id="2020-07-10-2">2. Targeting the Benchmark: On Methodology in Current Natural Language Processing Research</h2>

Title: [Targeting the Benchmark: On Methodology in Current Natural Language Processing Research](https://arxiv.org/abs/2007.04792)

Authors: [David Schlangen](https://arxiv.org/search/cs?searchtype=author&query=Schlangen%2C+D)

> It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.

| Comments: | arXiv admin note: text overlap with [arXiv:1908.10747](https://arxiv.org/abs/1908.10747) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.04792](https://arxiv.org/abs/2007.04792) [cs.CL]** |
|           | (or **[arXiv:2007.04792v1](https://arxiv.org/abs/2007.04792v1) [cs.CL]** for this version) |



# 2020-07-09

[Return to Index](#Index)



<h2 id="2020-07-09-1">1. Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision</h2>

Title: [Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision](https://arxiv.org/abs/2007.04134)

Authors: [Abhinav Shukla](https://arxiv.org/search/eess?searchtype=author&query=Shukla%2C+A), [Stavros Petridis](https://arxiv.org/search/eess?searchtype=author&query=Petridis%2C+S), [Maja Pantic](https://arxiv.org/search/eess?searchtype=author&query=Pantic%2C+M)

> The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision (by predicting informative audio attributes) with visual self-supervision (by generating talking faces from audio). The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations.

| Comments: | Accepted at the Workshop on Self-supervision in Audio and Speech at ICML 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Sound (cs.SD) |
| Cite as:  | **[arXiv:2007.04134](https://arxiv.org/abs/2007.04134) [eess.AS]** |
|           | (or **[arXiv:2007.04134v1](https://arxiv.org/abs/2007.04134v1) [eess.AS]** for this version) |





<h2 id="2020-07-09-2">2. Best-First Beam Search</h2>

Title: [Best-First Beam Search](https://arxiv.org/abs/2007.03909)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Tim Vieira](https://arxiv.org/search/cs?searchtype=author&query=Vieira%2C+T), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search--a pruned version of breadth-first search--which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size). We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.

| Comments: | TACL 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Data Structures and Algorithms (cs.DS) |
| Cite as:  | **[arXiv:2007.03909](https://arxiv.org/abs/2007.03909) [cs.CL]** |
|           | (or **[arXiv:2007.03909v2](https://arxiv.org/abs/2007.03909v2) [cs.CL]** for this version) |





<h2 id="2020-07-09-3">3. A Survey on Transfer Learning in Natural Language Processing</h2>

Title: [A Survey on Transfer Learning in Natural Language Processing](https://arxiv.org/abs/2007.04239)

Authors: [Zaid Alyafeai](https://arxiv.org/search/cs?searchtype=author&query=Alyafeai%2C+Z), [Maged Saeed AlShaibani](https://arxiv.org/search/cs?searchtype=author&query=AlShaibani%2C+M+S), [Irfan Ahmad](https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+I)

> Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.04239](https://arxiv.org/abs/2007.04239) [cs.CL]** |
|           | (or **[arXiv:2007.04239v1](https://arxiv.org/abs/2007.04239v1) [cs.CL]** for this version) |



# 2020-07-08

[Return to Index](#Index)



<h2 id="2020-07-08-1">1. Do Transformers Need Deep Long-Range Memory</h2>

Title: [Do Transformers Need Deep Long-Range Memory](https://arxiv.org/abs/2007.03356)

Authors: [Jack W. Rae](https://arxiv.org/search/cs?searchtype=author&query=Rae%2C+J+W), [Ali Razavi](https://arxiv.org/search/cs?searchtype=author&query=Razavi%2C+A)

> Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL -- a Transformer augmented with a long-range memory of past activations -- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.

| Comments: | published at 58th Annual Meeting of the Association for Computational Linguistics. 6 pages, 4 figures, 1 table |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2007.03356](https://arxiv.org/abs/2007.03356) [cs.LG]** |
|           | (or **[arXiv:2007.03356v1](https://arxiv.org/abs/2007.03356v1) [cs.LG]** for this version) |





<h2 id="2020-07-08-2">2. The Go Transformer: Natural Language Modeling for Game Play</h2>

Title: [The Go Transformer: Natural Language Modeling for Game Play](https://arxiv.org/abs/2007.03500)

Authors: [David Noever](https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D), [Matthew Ciolino](https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M), [Josh Kalin](https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J)

> This work applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project's (2020) game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons & Connect 4/6).

| Comments: | 8 Pages, 5 Figures, 1 Table                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.03500](https://arxiv.org/abs/2007.03500) [cs.CL]** |
|           | (or **[arXiv:2007.03500v1](https://arxiv.org/abs/2007.03500v1) [cs.CL]** for this version) |







<h2 id="2020-07-08-3">3. scb-mt-en-th-2020: A Large English-Thai Parallel Corpus</h2>

Title: [scb-mt-en-th-2020: A Large English-Thai Parallel Corpus](https://arxiv.org/abs/2007.03541)

Authors: [Lalita Lowphansirikul](https://arxiv.org/search/cs?searchtype=author&query=Lowphansirikul%2C+L), [Charin Polpanumas](https://arxiv.org/search/cs?searchtype=author&query=Polpanumas%2C+C), [Attapol T. Rutherford](https://arxiv.org/search/cs?searchtype=author&query=Rutherford%2C+A+T), [Sarana Nutanong](https://arxiv.org/search/cs?searchtype=author&query=Nutanong%2C+S)

> The primary objective of our work is to build a large-scale English-Thai dataset for machine translation. We construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents. Methodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner. We train machine translation models based on this dataset. Our models' performance are comparable to that of Google Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is included in the training data for both Thai-English and English-Thai translation. The dataset, pre-trained models, and source code to reproduce our work are available for public use.

| Comments: | 35 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2007.03541](https://arxiv.org/abs/2007.03541) [cs.CL]** |
|           | (or **[arXiv:2007.03541v1](https://arxiv.org/abs/2007.03541v1) [cs.CL]** for this version) |



