# Daily arXiv: Machine Translation - February, 2021

# Index

- [2021-02-19](#2021-02-19)
  - [1. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](#2021-02-19-1)
  - [2. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](#2021-02-19-2)
- [2021-02-18](#2021-02-18)
  - [1. COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](#2021-02-18-1)
  - [2. Sparsely Factored Neural Machine Translation](#2021-02-18-2)
- [2021-02-17](#2021-02-17)
  - [1. Meta Back-translation](#2021-02-17-1)
  - [2. Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet](#2021-02-17-2)
  - [3. Non-Autoregressive Text Generation with Pre-trained Language Models](#2021-02-17-3)
  - [4. Revisiting Language Encoding in Learning Multilingual Representations](#2021-02-17-4)
- [2021-02-16](#2021-02-16)
  - [1. MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training](#2021-02-16-1)
- [2021-02-15](#2021-02-15)
  - [1. Continuous Learning in Neural Machine Translation using Bilingual Dictionaries](#2021-02-15-1)
  - [2. Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders](#2021-02-15-2)
- [2021-02-12](#2021-02-12)
  - [1. Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation](#2021-02-12-1)
  - [2. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](#2021-02-12-2)
- [2021-02-11](#2021-02-11)
  - [1. Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models](#2021-02-11-1)
- [2021-02-10](#2021-02-10)
  - [1. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](#2021-02-10-1)
- [2021-02-09](#2021-02-09)
  - [1. Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning](#2021-02-09-1)
  - [2. Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models](#2021-02-09-2)
  - [3. Representation Learning for Natural Language Processing](#2021-02-09-3)
  - [4. CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models](#2021-02-09-4)
  - [5. SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning](#2021-02-09-5)
  - [6. Quality Estimation without Human-labeled Data](#2021-02-09-6)
- [2021-02-08](#2021-02-08)
  - [1. Understanding Pre-Editing for Black-Box Neural Machine Translation](#2021-02-08-1)
  - [2. Spell Correction for Azerbaijani Language using Deep Neural Networks](#2021-02-08-2)


- [2021-02-05](#2021-02-05)
- [1. Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models](#2021-02-05-1)
  - [2. Unifying Vision-and-Language Tasks via Text Generation](#2021-02-05-2)
- [2021-02-04](#2021-02-04)

  - [1. The Multilingual TEDx Corpus for Speech Recognition and Translation](#2021-02-04-1)
  - [2. Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation](#2021-02-04-2)
  - [3. When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data](#2021-02-04-3)
- [2021-02-03](#2021-02-03)

  - [1. Two Demonstrations of the Machine Translation Applications to Historical Documents](#2021-02-03-1)
  - [2. CTC-based Compression for Direct Speech Translation](#2021-02-03-2)
  - [3. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics](#2021-02-03-3)
- [2021-02-02](#2021-02-02)

  - [1. Speech Recognition by Simply Fine-tuning BERT](#2021-02-02-1)
  - [2. Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript](#2021-02-02-2)
  - [3. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation](#2021-02-02-3)
  - [4. Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](#2021-02-02-4)
  - [5. Neural OCR Post-Hoc Correction of Historical Corpora](#2021-02-02-5)
  - [6. GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer](#2021-02-02-6)
  - [7. Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](#2021-02-02-7)
  - [8. End2End Acoustic to Semantic Transduction](#2021-02-02-8)
  - [9. Measuring and Improving Consistency in Pretrained Language Models](#2021-02-02-9)
- [2021-02-01](#2021-02-01)

  - [1. Combining pre-trained language models and structured knowledge](#2021-02-01-1)
  - [2. Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning](#2021-02-01-2)
  - [3. Synthesizing Monolingual Data for Neural Machine Translation](#2021-02-01-3)
  - [4. Transition based Graph Decoder for Neural Machine Translation](#2021-02-01-4)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-02-19

[Return to Index](#Index)



<h2 id="2021-02-19-1">1. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</h2>

Title: [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)

Authors: [Soravit Changpinyo](https://arxiv.org/search/cs?searchtype=author&query=Changpinyo%2C+S), [Piyush Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+P), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements, inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset, as well as benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. The quantitative and qualitative results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08981](https://arxiv.org/abs/2102.08981) [cs.CV]** |
|           | (or **[arXiv:2102.08981v1](https://arxiv.org/abs/2102.08981v1) [cs.CV]** for this version) |





<h2 id="2021-02-19-2">2. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer</h2>

Title: [Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](https://arxiv.org/abs/2102.09550)

Authors: [Rafał Powalski](https://arxiv.org/search/cs?searchtype=author&query=Powalski%2C+R), [Łukasz Borchmann](https://arxiv.org/search/cs?searchtype=author&query=Borchmann%2C+Ł), [Dawid Jurkiewicz](https://arxiv.org/search/cs?searchtype=author&query=Jurkiewicz%2C+D), [Tomasz Dwojak](https://arxiv.org/search/cs?searchtype=author&query=Dwojak%2C+T), [Michał Pietruszka](https://arxiv.org/search/cs?searchtype=author&query=Pietruszka%2C+M), [Gabriela Pałka](https://arxiv.org/search/cs?searchtype=author&query=Pałka%2C+G)

> We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of solving all problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. We trained our network on real-world documents with different layouts, such as tables, figures, and forms. Our novel approach achieves state-of-the-art in extracting information from documents and answering questions, demanding layout understanding (DocVQA, CORD, WikiOps, SROIE). At the same time, we simplify the process by employing an end-to-end model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.09550](https://arxiv.org/abs/2102.09550) [cs.CL]** |
|           | (or **[arXiv:2102.09550v1](https://arxiv.org/abs/2102.09550v1) [cs.CL]** for this version) |





# 2021-02-18

[Return to Index](#Index)



<h2 id="2021-02-18-1">1. COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</h2>

Title: [COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](https://arxiv.org/abs/2102.08473)

Authors: [Yu Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Chenyan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Saurabh Tiwary](https://arxiv.org/search/cs?searchtype=author&query=Tiwary%2C+S), [Paul Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+P), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X)

> We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: The first task, corrective language modeling, learns to correct the auxiliary model's corruptions by recovering the original tokens. The second task, sequence contrastive learning, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM's advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08473](https://arxiv.org/abs/2102.08473) [cs.CL]** |
|           | (or **[arXiv:2102.08473v1](https://arxiv.org/abs/2102.08473v1) [cs.CL]** for this version) |





<h2 id="2021-02-18-2">2. Sparsely Factored Neural Machine Translation</h2>

Title: [Sparsely Factored Neural Machine Translation](https://arxiv.org/abs/2102.08934)

Authors: [Noe Casas](https://arxiv.org/search/cs?searchtype=author&query=Casas%2C+N), [Jose A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> The standard approach to incorporate linguistic information to neural machine translation systems consists in maintaining separate vocabularies for each of the annotated features to be incorporated (e.g. POS tags, dependency relation label), embed them, and then aggregate them with each subword in the word they belong to. This approach, however, cannot easily accommodate annotation schemes that are not dense for every word.
> We propose a method suited for such a case, showing large improvements in out-of-domain data, and comparable quality for the in-domain data. Experiments are performed in morphologically-rich languages like Basque and German, for the case of low-resource scenarios.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08934](https://arxiv.org/abs/2102.08934) [cs.CL]** |
|           | (or **[arXiv:2102.08934v1](https://arxiv.org/abs/2102.08934v1) [cs.CL]** for this version) |



# 2021-02-17

[Return to Index](#Index)



<h2 id="2021-02-17-1">1. Meta Back-translation</h2>

Title: [Meta Back-translation](https://arxiv.org/abs/2102.07847)

Authors: [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results. In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudo-parallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. Our code will be made available.

| Comments: | Accepted to ICLR 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.07847](https://arxiv.org/abs/2102.07847) [cs.CL]** |
|           | (or **[arXiv:2102.07847v1](https://arxiv.org/abs/2102.07847v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-2">2. Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet</h2>

Title: [Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet](https://arxiv.org/abs/2102.08036)

Authors: [M. Onat Topal](https://arxiv.org/search/cs?searchtype=author&query=Topal%2C+M+O), [Anil Bas](https://arxiv.org/search/cs?searchtype=author&query=Bas%2C+A), [Imke van Heerden](https://arxiv.org/search/cs?searchtype=author&query=van+Heerden%2C+I)

> Recent years have seen a proliferation of attention mechanisms and the rise of Transformers in Natural Language Generation (NLG). Previously, state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing gradient problems; as sentences grew larger, distance between positions remained linear, and sequential computation hindered parallelization since sentences were processed word by word. Transformers usher in a new era. In this paper, we explore three major Transformer-based models, namely GPT, BERT, and XLNet, that carry significant implications for the field. NLG is a burgeoning area that is now bolstered with rapid developments in attention mechanisms. From poetry generation to summarization, text generation derives benefit as Transformer-based language models achieve groundbreaking results.

| Comments: | Accepted as oral presentation to ICIDAAI 2021 - Short Paper  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.08036](https://arxiv.org/abs/2102.08036) [cs.CL]** |
|           | (or **[arXiv:2102.08036v1](https://arxiv.org/abs/2102.08036v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-3">3. Non-Autoregressive Text Generation with Pre-trained Language Models</h2>

Title: [Non-Autoregressive Text Generation with Pre-trained Language Models](https://arxiv.org/abs/2102.08220)

Authors: [Yixuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [David Vandyke](https://arxiv.org/search/cs?searchtype=author&query=Vandyke%2C+D), [Simon Baker](https://arxiv.org/search/cs?searchtype=author&query=Baker%2C+S), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.08220](https://arxiv.org/abs/2102.08220) [cs.CL]** |
|           | (or **[arXiv:2102.08220v1](https://arxiv.org/abs/2102.08220v1) [cs.CL]** for this version) |





<h2 id="2021-02-17-4">4. Revisiting Language Encoding in Learning Multilingual Representations</h2>

Title: [Revisiting Language Encoding in Learning Multilingual Representations](https://arxiv.org/abs/2102.08357)

Authors: [Shengjie Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Kaiyuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+K), [Shuxin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S), [Guolin Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+G), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Transformer has demonstrated its great power to learn contextual word representations for multiple languages in a single model. To process multilingual sentences in the model, a learnable vector is usually assigned to each language, which is called "language embedding". The language embedding can be either added to the word embedding or attached at the beginning of the sentence. It serves as a language-specific signal for the Transformer to capture contextual representations across languages. In this paper, we revisit the use of language embedding and identify several problems in the existing formulations. By investigating the interaction between language embedding and word embedding in the self-attention module, we find that the current methods cannot reflect the language-specific word correlation well. Given these findings, we propose a new approach called Cross-lingual Language Projection (XLP) to replace language embedding. For a sentence, XLP projects the word embeddings into language-specific semantic space, and then the projected embeddings will be fed into the Transformer model to process with their language-specific meanings. In such a way, XLP achieves the purpose of appropriately encoding "language" in a multilingual Transformer model. Experimental results show that XLP can freely and significantly boost the model performance on extensive multilingual benchmark datasets. Codes and models will be released at [this https URL](https://github.com/lsj2408/XLP).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.08357](https://arxiv.org/abs/2102.08357) [cs.CL]** |
|           | (or **[arXiv:2102.08357v1](https://arxiv.org/abs/2102.08357v1) [cs.CL]** for this version) |







# 2021-02-16

[Return to Index](#Index)



<h2 id="2021-02-16-1">1. MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training</h2>

Title: [MAPGN: MAsked Pointer-Generator network for sequence-to-sequence pre-training](https://arxiv.org/abs/2102.07380)

Authors: [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N), [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R)

> This paper presents a self-supervised learning method for pointer-generator networks to improve spoken-text normalization. Spoken-text normalization that converts spoken-style text into style normalized text is becoming an important technology for improving subsequent processing such as machine translation and summarization. The most successful spoken-text normalization method to date is sequence-to-sequence (seq2seq) mapping using pointer-generator networks that possess a copy mechanism from an input sequence. However, these models require a large amount of paired data of spoken-style text and style normalized text, and it is difficult to prepare such a volume of data. In order to construct spoken-text normalization model from the limited paired data, we focus on self-supervised learning which can utilize unpaired text data to improve seq2seq models. Unfortunately, conventional self-supervised learning methods do not assume that pointer-generator networks are utilized. Therefore, we propose a novel self-supervised learning method, MAsked Pointer-Generator Network (MAPGN). The proposed method can effectively pre-train the pointer-generator network by learning to fill masked tokens using the copy mechanism. Our experiments demonstrate that MAPGN is more effective for pointer-generator networks than the conventional self-supervised learning methods in two spoken-text normalization tasks.

| Comments: | Accepted at ICASSP 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.07380](https://arxiv.org/abs/2102.07380) [cs.CL]** |
|           | (or **[arXiv:2102.07380v1](https://arxiv.org/abs/2102.07380v1) [cs.CL]** for this version) |







# 2021-02-15

[Return to Index](#Index)



<h2 id="2021-02-15-1">1. Continuous Learning in Neural Machine Translation using Bilingual Dictionaries</h2>

Title: [Continuous Learning in Neural Machine Translation using Bilingual Dictionaries](https://arxiv.org/abs/2102.06558)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

> While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language.
> In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.

| Comments: | 9 pages, EACL 2021                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.06558](https://arxiv.org/abs/2102.06558) [cs.CL]** |
|           | (or **[arXiv:2102.06558v1](https://arxiv.org/abs/2102.06558v1) [cs.CL]** for this version) |





<h2 id="2021-02-15-2">2. Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders</h2>

Title: [Improving Zero-shot Neural Machine Translation on Language-specific Encoders-Decoders](https://arxiv.org/abs/2102.06578)

Authors: [Junwei Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+J), [Yu Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Hong Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Recently, universal neural machine translation (NMT) with shared encoder-decoder gained good performance on zero-shot translation. Unlike universal NMT, jointly trained language-specific encoders-decoders aim to achieve universal representation across non-shared modules, each of which is for a language or language family. The non-shared architecture has the advantage of mitigating internal language competition, especially when the shared vocabulary and model parameters are restricted in their size. However, the performance of using multiple encoders and decoders on zero-shot translation still lags behind universal NMT. In this work, we study zero-shot translation using language-specific encoders-decoders. We propose to generalize the non-shared architecture and universal NMT by differentiating the Transformer layers between language-specific and interlingua. By selectively sharing parameters and applying cross-attentions, we explore maximizing the representation universality and realizing the best alignment of language-agnostic information. We also introduce a denoising auto-encoding (DAE) objective to jointly train the model with the translation task in a multi-task manner. Experiments on two public multilingual parallel datasets show that our proposed model achieves a competitive or better results than universal NMT and strong pivot baseline. Moreover, we experiment incrementally adding new language to the trained model by only updating the new model parameters. With this little effort, the zero-shot translation between this newly added language and existing languages achieves a comparable result with the model trained jointly from scratch on all languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.06578](https://arxiv.org/abs/2102.06578) [cs.CL]** |
|           | (or **[arXiv:2102.06578v1](https://arxiv.org/abs/2102.06578v1) [cs.CL]** for this version) |





# 2021-02-12

[Return to Index](#Index)



<h2 id="2021-02-12-1">1. Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation</h2>

Title: [Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation](https://arxiv.org/abs/2102.05766)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Recently text and speech representation learning has successfully improved many language related tasks. However, all existing methods only learn from one input modality, while a unified acoustic and text representation is desired by many speech-related tasks such as speech translation. We propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly learns a unified representation for both acoustic and text in-put. Within this cross modal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that our proposed speech translation models fine-tuned from FAT-MLM substantially improve translation quality (+5.90 BLEU).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05766](https://arxiv.org/abs/2102.05766) [cs.CL]** |
|           | (or **[arXiv:2102.05766v1](https://arxiv.org/abs/2102.05766v1) [cs.CL]** for this version) |





<h2 id="2021-02-12-2">2. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</h2>

Title: [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)

Authors: [Chao Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+C), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Ye Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Yi-Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Zarana Parekh](https://arxiv.org/search/cs?searchtype=author&query=Parekh%2C+Z), [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V), [Yunhsuan Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+Y), [Zhen Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Tom Duerig](https://arxiv.org/search/cs?searchtype=author&query=Duerig%2C+T)

> Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations also set new state-of-the-art results on Flickr30K and MSCOCO benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05918](https://arxiv.org/abs/2102.05918) [cs.CV]** |
|           | (or **[arXiv:2102.05918v1](https://arxiv.org/abs/2102.05918v1) [cs.CV]** for this version) |





# 2021-02-11

[Return to Index](#Index)



<h2 id="2021-02-11-1">1. Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models</h2>

Title: [Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models](https://arxiv.org/abs/2102.05379)

Authors: [Emiel Hoogeboom](https://arxiv.org/search/stat?searchtype=author&query=Hoogeboom%2C+E), [Didrik Nielsen](https://arxiv.org/search/stat?searchtype=author&query=Nielsen%2C+D), [Priyank Jaini](https://arxiv.org/search/stat?searchtype=author&query=Jaini%2C+P), [Patrick Forré](https://arxiv.org/search/stat?searchtype=author&query=Forré%2C+P), [Max Welling](https://arxiv.org/search/stat?searchtype=author&query=Welling%2C+M)

> The field of language modelling has been largely dominated by autoregressive models, for which sampling is inherently difficult to parallelize. This paper introduces two new classes of generative models for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our models perform competitively on language modelling and modelling of image segmentation maps.

| Subjects: | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.05379](https://arxiv.org/abs/2102.05379) [stat.ML]** |
|           | (or **[arXiv:2102.05379v1](https://arxiv.org/abs/2102.05379v1) [stat.ML]** for this version) |













# 2021-02-10

[Return to Index](#Index)



<h2 id="2021-02-10-1">1. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</h2>

Title: [CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](https://arxiv.org/abs/2102.04664)

Authors: [Shuai Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Shuo Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+S), [Junjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Ambrosio Blanco](https://arxiv.org/search/cs?searchtype=author&query=Blanco%2C+A), [Colin Clement](https://arxiv.org/search/cs?searchtype=author&query=Clement%2C+C), [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Ge Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lidong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Michele Tufano](https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N), [Shao Kun Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S+K), [Shengyu Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+S), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S)

> Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.









# 2021-02-09

[Return to Index](#Index)



<h2 id="2021-02-09-1">1. Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning</h2>

Title: [Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning](https://arxiv.org/abs/2102.03554)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> Recent advancements in data-to-text generation largely take on the form of neural end-to-end systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. Our benchmarks show faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.03554](https://arxiv.org/abs/2102.03554) [cs.CL]** |
|           | (or **[arXiv:2102.03554v1](https://arxiv.org/abs/2102.03554v1) [cs.CL]** for this version) |





<h2 id="2021-02-09-2">2. Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models</h2>

Title: [Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models](https://arxiv.org/abs/2102.03596)

Authors: [Lutfi Kerem Senel](https://arxiv.org/search/cs?searchtype=author&query=Senel%2C+L+K), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Recent progress in pretraining language models on large corpora has resulted in large performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with `fill in the blank' style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definition Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.

| Comments: | 5 pages, to appear in EACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.03596](https://arxiv.org/abs/2102.03596) [cs.CL]** |
|           | (or **[arXiv:2102.03596v1](https://arxiv.org/abs/2102.03596v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-3">3. Representation Learning for Natural Language Processing</h2>

Title: [Representation Learning for Natural Language Processing](https://arxiv.org/abs/2102.03732)

Authors: [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> This book aims to review and present the recent advances of distributed representation learning for NLP, including why representation learning can improve NLP, how representation learning takes part in various important topics of NLP, and what challenges are still not well addressed by distributed representation.

| Comments: | Published in Springer                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1007/978-981-15-5573-2](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-981-15-5573-2&v=bf73f6ca) |
| Cite as:  | **[arXiv:2102.03732](https://arxiv.org/abs/2102.03732) [cs.CL]** |
|           | (or **[arXiv:2102.03732v1](https://arxiv.org/abs/2102.03732v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-4">4. CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models</h2>

Title: [CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models](https://arxiv.org/abs/2102.03752)

Authors: [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many low-resource scenarios, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named "CSS-LM") to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings, and outperforms the latest supervised contrastive fine-tuning strategies. Our datasets and source code will be available to provide more details.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.03752](https://arxiv.org/abs/2102.03752) [cs.CL]** |
|           | (or **[arXiv:2102.03752v1](https://arxiv.org/abs/2102.03752v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-5">5. SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning</h2>

Title: [SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning](https://arxiv.org/abs/2102.04009)

Authors: [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Shuo Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Word alignment is essential for the down-streaming cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a super lightweight unsupervised word alignment (SLUA) model, in which bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental results on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while significantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed SLUA. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. Encouragingly, our approach achieves 16.4x speedup against GIZA++, and 50x parameter compression} compared with the Transformer-based alignment methods. We will release our code to facilitate the community.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2102.04009](https://arxiv.org/abs/2102.04009) [cs.CL]** |
|           | (or **[arXiv:2102.04009v1](https://arxiv.org/abs/2102.04009v1) [cs.CL]** for this version) |







<h2 id="2021-02-09-6">6. Quality Estimation without Human-labeled Data</h2>

Title: [Quality Estimation without Human-labeled Data](https://arxiv.org/abs/2102.04020)

Authors: [Yi-Lin Tuan](https://arxiv.org/search/cs?searchtype=author&query=Tuan%2C+Y), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.

| Comments: | Accepted by EACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.04020](https://arxiv.org/abs/2102.04020) [cs.CL]** |
|           | (or **[arXiv:2102.04020v1](https://arxiv.org/abs/2102.04020v1) [cs.CL]** for this version) |









# 2021-02-08

[Return to Index](#Index)



<h2 id="2021-02-08-1">1. Understanding Pre-Editing for Black-Box Neural Machine Translation</h2>

Title: [Understanding Pre-Editing for Black-Box Neural Machine Translation](https://arxiv.org/abs/2102.02955)

Auhors: [Rei Miyata](https://arxiv.org/search/cs?searchtype=author&query=Miyata%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of pre-editing across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.02955](https://arxiv.org/abs/2102.02955) [cs.CL]** |
|           | (or **[arXiv:2102.02955v1](https://arxiv.org/abs/2102.02955v1) [cs.CL]** for this version) |





<h2 id="2021-02-08-2">2. Spell Correction for Azerbaijani Language using Deep Neural Networks</h2>

Title: [Spell Correction for Azerbaijani Language using Deep Neural Networks](https://arxiv.org/abs/2102.03218)

Auhors: [Ahmad Ahmadzade](https://arxiv.org/search/cs?searchtype=author&query=Ahmadzade%2C+A), [Saber Malekzadeh](https://arxiv.org/search/cs?searchtype=author&query=Malekzadeh%2C+S)

> Spell correction is used to detect and correct orthographic mistakes in texts. Most of the time, traditional dictionary lookup with string similarity methods is suitable for the languages that have a less complex structure such as the English language. However, the Azerbaijani language has a more complex structure and due to its morphological structure, the derivation of words is plenty that several words are derived from adding suffices, affixes to the words. Therefore, in this paper sequence to sequence model with an attention mechanism is used to develop spelling correction for Azerbaijani. Total 12000 wrong and correct sentence pairs used for training, and the model is tested on 1000 real-world misspelled words and F1-score results are 75% for distance 0, 90% for distance 1, and 96% for distance 2.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.03218](https://arxiv.org/abs/2102.03218) [cs.CL]** |
|           | (or **[arXiv:2102.03218v1](https://arxiv.org/abs/2102.03218v1) [cs.CL]** for this version) |







# 2021-02-05

[Return to Index](#Index)



<h2 id="2021-02-05-1">1. Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</h2>

Title: [Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models](https://arxiv.org/abs/2102.02503)

Authors: [Alex Tamkin](https://arxiv.org/search/cs?searchtype=author&query=Tamkin%2C+A), [Miles Brundage](https://arxiv.org/search/cs?searchtype=author&query=Brundage%2C+M), [Jack Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+J), [Deep Ganguli](https://arxiv.org/search/cs?searchtype=author&query=Ganguli%2C+D)

> On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.02503](https://arxiv.org/abs/2102.02503) [cs.CL]** |
|           | (or **[arXiv:2102.02503v1](https://arxiv.org/abs/2102.02503v1) [cs.CL]** for this version) |





<h2 id="2021-02-05-2">2. Unifying Vision-and-Language Tasks via Text Generation</h2>

Title: [Unifying Vision-and-Language Tasks via Text Generation](https://arxiv.org/abs/2102.02779)

Authors: [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on answering questions that have rare answers. In addition, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, which achieves similar performance to separately optimized single-task models. Our code will be publicly available at: [this https URL](https://github.com/j-min/VL-T5)

| Comments: | 16 pages, 4 figures, 13 tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.02779](https://arxiv.org/abs/2102.02779) [cs.CL]** |
|           | (or **[arXiv:2102.02779v1](https://arxiv.org/abs/2102.02779v1) [cs.CL]** for this version) |





# 2021-02-04

[Return to Index](#Index)



<h2 id="2021-02-04-1">1. The Multilingual TEDx Corpus for Speech Recognition and Translation</h2>

Title: [The Multilingual TEDx Corpus for Speech Recognition and Translation](https://arxiv.org/abs/2102.01757)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthew Wiesner](https://arxiv.org/search/cs?searchtype=author&query=Wiesner%2C+M), [Jacob Bremerman](https://arxiv.org/search/cs?searchtype=author&query=Bremerman%2C+J), [Roldano Cattoni](https://arxiv.org/search/cs?searchtype=author&query=Cattoni%2C+R), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M), [Douglas W. Oard](https://arxiv.org/search/cs?searchtype=author&query=Oard%2C+D+W), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> We present the Multilingual TEDx corpus, built to support speech recognition (ASR) and speech translation (ST) research across many non-English source languages. The corpus is a collection of audio recordings from TEDx talks in 8 source languages. We segment transcripts into sentences and align them to the source-language audio and target-language translations. The corpus is released along with open-sourced code enabling extension to new talks and languages as they become available. Our corpus creation methodology can be applied to more languages than previous work, and creates multi-way parallel evaluation sets. We provide baselines in multiple ASR and ST settings, including multilingual models to improve translation performance for low-resource language pairs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01757](https://arxiv.org/abs/2102.01757) [cs.CL]** |
|           | (or **[arXiv:2102.01757v1](https://arxiv.org/abs/2102.01757v1) [cs.CL]** for this version) |



<h2 id="2021-02-04-2">2. Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation</h2>

Title: [Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation](https://arxiv.org/abs/2102.01818)

Authors: [Aparna Elangovan](https://arxiv.org/search/cs?searchtype=author&query=Elangovan%2C+A), [Jiayuan He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Karin Verspoor](https://arxiv.org/search/cs?searchtype=author&query=Verspoor%2C+K)

> Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize.

| Comments: | To appear EACL 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.01818](https://arxiv.org/abs/2102.01818) [cs.CL]** |
|           | (or **[arXiv:2102.01818v1](https://arxiv.org/abs/2102.01818v1) [cs.CL]** for this version) |





<h2 id="2021-02-04-3">3. When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data</h2>

Title: [When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data](https://arxiv.org/abs/2102.02201)

Authors: [Peter Hase](https://arxiv.org/search/cs?searchtype=author&query=Hase%2C+P), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Many methods now exist for conditioning model outputs on task instructions, retrieved documents, and user-provided explanations and feedback. Rather than relying solely on examples of task inputs and outputs, these approaches allow for valuable additional data to be used in modeling with the purpose of improving model correctness and aligning learned models with human priors. Meanwhile, a growing body of evidence suggests that some language models can (1) store a large amount of knowledge in their parameters, and (2) perform inference over tasks in unstructured text to solve new tasks at test time. These results raise the possibility that, for some tasks, humans cannot explain to a model any more about the task than it already knows or could infer on its own. In this paper, we study the circumstances under which explanations of individual data points can (or cannot) improve modeling performance. In order to carefully control important properties of the data and explanations, we introduce a synthetic dataset for experiments, and we also make use of three existing datasets with explanations: e-SNLI, TACRED, SemEval. We first give a formal framework for the available modeling approaches, in which explanation data can be used as model inputs, as labels, or as a prior. After arguing that the most promising role for explanation data is as model inputs, we propose to use a retrieval-based method and show that it solves our synthetic task with accuracies upwards of 95%, while baselines without explanation data achieve below 65% accuracy. We then identify properties of datasets for which retrieval-based modeling fails. With the three existing datasets, we find no improvements from explanation retrieval. Drawing on our findings from our synthetic task, we suggest that at least one of six preconditions for successful modeling fails to hold with these datasets.

| Comments: | 25 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.02201](https://arxiv.org/abs/2102.02201) [cs.CL]** |
|           | (or **[arXiv:2102.02201v1](https://arxiv.org/abs/2102.02201v1) [cs.CL]** for this version) |





# 2021-02-03

[Return to Index](#Index)



<h2 id="2021-02-03-1">1. Two Demonstrations of the Machine Translation Applications to Historical Documents</h2>

Title: [Two Demonstrations of the Machine Translation Applications to Historical Documents](https://arxiv.org/abs/2102.01417)

Authors:[Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F)

> We present our demonstration of two machine translation applications to historical documents. The first task consists in generating a new version of a historical document, written in the modern version of its original language. The second application is limited to a document's orthography. It adapts the document's spelling to modern standards in order to achieve an orthography consistency and accounting for the lack of spelling conventions. We followed an interactive, adaptive framework that allows the user to introduce corrections to the system's hypothesis. The system reacts to these corrections by generating a new hypothesis that takes them into account. Once the user is satisfied with the system's hypothesis and validates it, the system adapts its model following an online learning strategy. This system is implemented following a client-server architecture. We developed a website which communicates with the neural models. All code is open-source and publicly available. The demonstration is hosted at [this http URL](http://demosmt.prhlt.upv.es/mthd/).

| Comments: | Presented at the Demos session of ICPR 2020: [this https URL](https://www.micc.unifi.it/icpr2020/index.php/demos/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.01417](https://arxiv.org/abs/2102.01417) [cs.CL]** |
|           | (or **[arXiv:2102.01417v1](https://arxiv.org/abs/2102.01417v1) [cs.CL]** for this version) |





<h2 id="2021-02-03-2">2. CTC-based Compression for Direct Speech Translation</h2>

Title: [CTC-based Compression for Direct Speech Translation](https://arxiv.org/abs/2102.01578)

Authors:[Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input indirect ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%.

| Comments: | Accepted at EACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.01578](https://arxiv.org/abs/2102.01578) [cs.CL]** |
|           | (or **[arXiv:2102.01578v1](https://arxiv.org/abs/2102.01578v1) [cs.CL]** for this version) |





<h2 id="2021-02-03-3">3. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</h2>

Title: [The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics](https://arxiv.org/abs/2102.01672)

Authors:[Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Tosin Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T), [Karmanya Aggarwal](https://arxiv.org/search/cs?searchtype=author&query=Aggarwal%2C+K), [Pawan Sasanka Ammanamanchi](https://arxiv.org/search/cs?searchtype=author&query=Ammanamanchi%2C+P+S), [Aremu Anuoluwapo](https://arxiv.org/search/cs?searchtype=author&query=Anuoluwapo%2C+A), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A), [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Miruna Clinciu](https://arxiv.org/search/cs?searchtype=author&query=Clinciu%2C+M), [Dipanjan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Kaustubh D. Dhole](https://arxiv.org/search/cs?searchtype=author&query=Dhole%2C+K+D), [Wanyu Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+W), [Esin Durmus](https://arxiv.org/search/cs?searchtype=author&query=Durmus%2C+E), [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Chris Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Cristina Garbacea](https://arxiv.org/search/cs?searchtype=author&query=Garbacea%2C+C), [Tatsunori Hashimoto](https://arxiv.org/search/cs?searchtype=author&query=Hashimoto%2C+T), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Yacine Jernite](https://arxiv.org/search/cs?searchtype=author&query=Jernite%2C+Y), [Harsh Jhamtani](https://arxiv.org/search/cs?searchtype=author&query=Jhamtani%2C+H), [Yangfeng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+Y), [Shailza Jolly](https://arxiv.org/search/cs?searchtype=author&query=Jolly%2C+S), [Dhruv Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+D), [Faisal Ladhak](https://arxiv.org/search/cs?searchtype=author&query=Ladhak%2C+F), [Aman Madaan](https://arxiv.org/search/cs?searchtype=author&query=Madaan%2C+A), [Mounica Maddela](https://arxiv.org/search/cs?searchtype=author&query=Maddela%2C+M), [Khyati Mahajan](https://arxiv.org/search/cs?searchtype=author&query=Mahajan%2C+K), [Saad Mahamood](https://arxiv.org/search/cs?searchtype=author&query=Mahamood%2C+S), [Bodhisattwa Prasad Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+B+P), [Pedro Henrique Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H), [Angelina McMillan-Major](https://arxiv.org/search/cs?searchtype=author&query=McMillan-Major%2C+A), [Simon Mille](https://arxiv.org/search/cs?searchtype=author&query=Mille%2C+S), [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Moin Nadeem](https://arxiv.org/search/cs?searchtype=author&query=Nadeem%2C+M), [Shashi Narayan](https://arxiv.org/search/cs?searchtype=author&query=Narayan%2C+S), [Vitaly Nikolaev](https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+V), [Rubungo Andre Niyongabo](https://arxiv.org/search/cs?searchtype=author&query=Niyongabo%2C+R+A), [Salomey Osei](https://arxiv.org/search/cs?searchtype=author&query=Osei%2C+S), [Ankur Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A), [Laura Perez-Beltrachini](https://arxiv.org/search/cs?searchtype=author&query=Perez-Beltrachini%2C+L), [Niranjan Ramesh Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+N+R), [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Juan Diego Rodriguez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez%2C+J+D), [Sashank Santhanam](https://arxiv.org/search/cs?searchtype=author&query=Santhanam%2C+S), [João Sedoc](https://arxiv.org/search/cs?searchtype=author&query=Sedoc%2C+J), [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Samira Shaikh](https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+S), [Anastasia Shimorina](https://arxiv.org/search/cs?searchtype=author&query=Shimorina%2C+A), [Marco Antonio Sobrevilla Cabezudo](https://arxiv.org/search/cs?searchtype=author&query=Cabezudo%2C+M+A+S), [Hendrik Strobelt](https://arxiv.org/search/cs?searchtype=author&query=Strobelt%2C+H), [Nishant Subramani](https://arxiv.org/search/cs?searchtype=author&query=Subramani%2C+N), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Akhila Yerukola](https://arxiv.org/search/cs?searchtype=author&query=Yerukola%2C+A), [Jiawei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. However, due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of corpora and evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models.
> This paper serves as the description of the initial release for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01672](https://arxiv.org/abs/2102.01672) [cs.CL]** |
|           | (or **[arXiv:2102.01672v2](https://arxiv.org/abs/2102.01672v2) [cs.CL]** for this version) |





# 2021-02-02

[Return to Index](#Index)



<h2 id="2021-02-02-1">1. Speech Recognition by Simply Fine-tuning BERT</h2>

Title: [Speech Recognition by Simply Fine-tuning BERT](https://arxiv.org/abs/2102.00291)

Authors: [Wen-Chin Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W), [Chia-Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Shang-Bao Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Kuan-Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Hsin-Min Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Tomoki Toda](https://arxiv.org/search/cs?searchtype=author&query=Toda%2C+T)

> We propose a simple method for automatic speech recognition (ASR) by fine-tuning BERT, which is a language model (LM) trained on large-scale unlabeled text data and can generate rich contextual representations. Our assumption is that given a history context sequence, a powerful LM can narrow the range of possible choices and the speech signal can be used as a simple clue. Hence, comparing to conventional ASR systems that train a powerful acoustic model (AM) from scratch, we believe that speech recognition is possible by simply fine-tuning a BERT model. As an initial study, we demonstrate the effectiveness of the proposed idea on the AISHELL dataset and show that stacking a very simple AM on top of BERT can yield reasonable performance.

| Comments: | Accepted to ICASSP 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2102.00291](https://arxiv.org/abs/2102.00291) [cs.SD]** |
|           | (or **[arXiv:2102.00291v1](https://arxiv.org/abs/2102.00291v1) [cs.SD]** for this version) |





<h2 id="2021-02-02-2">2. Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript</h2>

Title: [Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript](https://arxiv.org/abs/2102.00804)

Authors: [Mukuntha Narayanan Sundararaman](https://arxiv.org/search/eess?searchtype=author&query=Sundararaman%2C+M+N), [Ayush Kumar](https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+A), [Jithendra Vepa](https://arxiv.org/search/eess?searchtype=author&query=Vepa%2C+J)

> Recent years have witnessed significant improvement in ASR systems to recognize spoken utterances. However, it is still a challenging task for noisy and out-of-domain data, where substitution and deletion errors are prevalent in the transcribed text. These errors significantly degrade the performance of downstream tasks. In this work, we propose a BERT-style language model, referred to as PhonemeBERT, that learns a joint language model with phoneme sequence and ASR transcript to learn phonetic-aware representations that are robust to ASR errors. We show that PhonemeBERT can be used on downstream tasks using phoneme sequences as additional features, and also in low-resource setup where we only have ASR-transcripts for the downstream tasks with no phoneme information available. We evaluate our approach extensively by generating noisy data for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS for sentiment, question and intent classification tasks respectively. The results of the proposed approach beats the state-of-the-art baselines comprehensively on each dataset.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.00804](https://arxiv.org/abs/2102.00804) [eess.AS]** |
|           | (or **[arXiv:2102.00804v1](https://arxiv.org/abs/2102.00804v1) [eess.AS]** for this version) |





<h2 id="2021-02-02-3">3. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation</h2>

Title: [Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation](https://arxiv.org/abs/2102.00287)

Authors: [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Matthew Gwilliam](https://arxiv.org/search/cs?searchtype=author&query=Gwilliam%2C+M)

> Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the 'algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: 'machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms - phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and morphological richness in the translations produced by all investigated MT paradigms for two language pairs (EN<=>FR and EN<=>ES).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.00287](https://arxiv.org/abs/2102.00287) [cs.CL]** |
|           | (or **[arXiv:2102.00287v1](https://arxiv.org/abs/2102.00287v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-4">4. Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers</h2>

Title: [Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](https://arxiv.org/abs/2102.00529)

Authors: [Lisa Anne Hendricks](https://arxiv.org/search/cs?searchtype=author&query=Hendricks%2C+L+A), [John Mellor](https://arxiv.org/search/cs?searchtype=author&query=Mellor%2C+J), [Rosalia Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+R), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac%2C+J), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh%2C+A)

> Recently multimodal transformer models have gained popularity because their performance on language and vision tasks suggest they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors which can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers

| Comments: | pre-print of MIT Press Publication version                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2102.00529](https://arxiv.org/abs/2102.00529) [cs.CL]** |
|           | (or **[arXiv:2102.00529v1](https://arxiv.org/abs/2102.00529v1) [cs.CL]** for this version) |







<h2 id="2021-02-02-5">5. Neural OCR Post-Hoc Correction of Historical Corpora</h2>

Title: [Neural OCR Post-Hoc Correction of Historical Corpora](https://arxiv.org/abs/2102.00583)

Authors: [Lijun Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+L), [Maria Koutraki](https://arxiv.org/search/cs?searchtype=author&query=Koutraki%2C+M), [Martin Krickl](https://arxiv.org/search/cs?searchtype=author&query=Krickl%2C+M), [Besnik Fetahu](https://arxiv.org/search/cs?searchtype=author&query=Fetahu%2C+B)

> Optical character recognition (OCR) is crucial for a deeper access to historical collections. OCR needs to account for orthographic variations, typefaces, or language evolution (i.e., new letters, word spellings), as the main source of character, word, or word segmentation transcription errors. For digital corpora of historical prints, the errors are further exacerbated due to low scan quality and lack of language standardization.
> For the task of OCR post-hoc correction, we propose a neural approach based on a combination of recurrent (RNN) and deep convolutional network (ConvNet) to correct OCR transcription errors. At character level we flexibly capture errors, and decode the corrected output based on a novel attention mechanism. Accounting for the input and output similarity, we propose a new loss function that rewards the model's correcting behavior.
> Evaluation on a historical book corpus in German language shows that our models are robust in capturing diverse OCR transcription errors and reduce the word error rate of 32.3% by more than 89%.

| Comments: | To appear at TACL                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.00583](https://arxiv.org/abs/2102.00583) [cs.CL]** |
|           | (or **[arXiv:2102.00583v1](https://arxiv.org/abs/2102.00583v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-6">6. GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer</h2>

Title: [GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer](https://arxiv.org/abs/2102.00769)

Authors: [Yukai Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Sen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Chenxing Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Xiaodan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+X), [Xiaojun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Liang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L)

> Non-parallel text style transfer has attracted increasing research interests in recent years. Despite successes in transferring the style based on the encoder-decoder framework, current approaches still lack the ability to preserve the content and even logic of original sentences, mainly due to the large unconstrained model space or too simplified assumptions on latent embedding space. Since language itself is an intelligent product of humans with certain grammars and has a limited rule-based model space by its nature, relieving this problem requires reconciling the model capacity of deep neural networks with the intrinsic model constraints from human linguistic rules. To this end, we propose a method called Graph Transformer based Auto Encoder (GTAE), which models a sentence as a linguistic graph and performs feature extraction and style transfer at the graph level, to maximally retain the content and the linguistic structure of original sentences. Quantitative experiment results on three non-parallel text style transfer tasks show that our model outperforms state-of-the-art methods in content preservation, while achieving comparable performance on transfer accuracy and sentence naturalness.

| Comments: | The first two authors share equal-authorship; Code:[this https URL](https://github.com/SenZHANG-GitHub/graph-text-style-transfer) ; benchmark: [this https URL](https://github.com/ykshi/text-style-transfer-benchmark) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2102.00769](https://arxiv.org/abs/2102.00769) [cs.CL]** |
|           | (or **[arXiv:2102.00769v1](https://arxiv.org/abs/2102.00769v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-7">7. Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models</h2>

Title: [Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](https://arxiv.org/abs/2102.00894)

Authors: [Nora Kassner](https://arxiv.org/search/cs?searchtype=author&query=Kassner%2C+N), [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as "Paris is the capital of [MASK]" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2102.00894](https://arxiv.org/abs/2102.00894) [cs.CL]** |
|           | (or **[arXiv:2102.00894v1](https://arxiv.org/abs/2102.00894v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-8">8. End2End Acoustic to Semantic Transduction</h2>

Title: [End2End Acoustic to Semantic Transduction](https://arxiv.org/abs/2102.01013)

Authors: [Valentin Pelloin](https://arxiv.org/search/cs?searchtype=author&query=Pelloin%2C+V), [Nathalie Camelin](https://arxiv.org/search/cs?searchtype=author&query=Camelin%2C+N), [Antoine Laurent](https://arxiv.org/search/cs?searchtype=author&query=Laurent%2C+A), [Renato De Mori](https://arxiv.org/search/cs?searchtype=author&query=De+Mori%2C+R), [Antoine Caubrière](https://arxiv.org/search/cs?searchtype=author&query=Caubrière%2C+A), [Yannick Estève](https://arxiv.org/search/cs?searchtype=author&query=Estève%2C+Y), [Sylvain Meignier](https://arxiv.org/search/cs?searchtype=author&query=Meignier%2C+S)

> In this paper, we propose a novel end-to-end sequence-to-sequence spoken language understanding model using an attention mechanism. It reliably selects contextual acoustic features in order to hypothesize semantic contents. An initial architecture capable of extracting all pronounced words and concepts from acoustic spans is designed and tested. With a shallow fusion language model, this system reaches a 13.6 concept error rate (CER) and an 18.5 concept value error rate (CVER) on the French MEDIA corpus, achieving an absolute 2.8 points reduction compared to the state-of-the-art. Then, an original model is proposed for hypothesizing concepts and their values. This transduction reaches a 15.4 CER and a 21.6 CVER without any new type of context.

| Comments: | Accepted at IEEE ICASSP 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2102.01013](https://arxiv.org/abs/2102.01013) [cs.CL]** |
|           | (or **[arXiv:2102.01013v1](https://arxiv.org/abs/2102.01013v1) [cs.CL]** for this version) |





<h2 id="2021-02-02-9">9. Measuring and Improving Consistency in Pretrained Language Models</h2>

Title: [Measuring and Improving Consistency in Pretrained Language Models](https://arxiv.org/abs/2102.01017)

Authors: [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Nora Kassner](https://arxiv.org/search/cs?searchtype=author&query=Kassner%2C+N), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Abhilasha Ravichander](https://arxiv.org/search/cs?searchtype=author&query=Ravichander%2C+A), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for thirty-eight relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge in a robust way. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.01017](https://arxiv.org/abs/2102.01017) [cs.CL]** |
|           | (or **[arXiv:2102.01017v1](https://arxiv.org/abs/2102.01017v1) [cs.CL]** for this version) |











# 2021-02-01

[Return to Index](#Index)



<h2 id="2021-02-01-1">1. Combining pre-trained language models and structured knowledge</h2>

Title: [Combining pre-trained language models and structured knowledge](https://arxiv.org/abs/2101.12294)

Authors: [Pedro Colon-Hernandez](https://arxiv.org/search/cs?searchtype=author&query=Colon-Hernandez%2C+P), [Catherine Havasi](https://arxiv.org/search/cs?searchtype=author&query=Havasi%2C+C), [Jason Alonso](https://arxiv.org/search/cs?searchtype=author&query=Alonso%2C+J), [Matthew Huggins](https://arxiv.org/search/cs?searchtype=author&query=Huggins%2C+M), [Cynthia Breazeal](https://arxiv.org/search/cs?searchtype=author&query=Breazeal%2C+C)

> In recent years, transformer-based language models have achieved state of the art performance in various NLP benchmarks. These models are able to extract mostly distributional information with some semantics from unstructured text, however it has proven challenging to integrate structured information, such as knowledge graphs into these models. We examine a variety of approaches to integrate structured knowledge into current language models and determine challenges, and possible opportunities to leverage both structured and unstructured information sources. From our survey, we find that there are still opportunities at exploiting adapter-based injections and that it may be possible to further combine various of the explored approaches into one system.

| Comments: | Initial Submission                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.12294](https://arxiv.org/abs/2101.12294) [cs.CL]** |
|           | (or **[arXiv:2101.12294v1](https://arxiv.org/abs/2101.12294v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-2">2. Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning</h2>

Title: [Few-Shot Domain Adaptation for Grammatical Error Correction via Meta-Learning](https://arxiv.org/abs/2101.12409)

Authors: [Shengsheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yaping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Chencheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Erhong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+E)

> Most existing Grammatical Error Correction (GEC) methods based on sequence-to-sequence mainly focus on how to generate more pseudo data to obtain better performance. Few work addresses few-shot GEC domain adaptation. In this paper, we treat different GEC domains as different GEC tasks and propose to extend meta-learning to few-shot GEC domain adaptation without using any pseudo data. We exploit a set of data-rich source domains to learn the initialization of model parameters that facilitates fast adaptation on new resource-poor target domains. We adapt GEC model to the first language (L1) of the second language learner. To evaluate the proposed method, we use nine L1s as source domains and five L1s as target domains. Experiment results on the L1 GEC domain adaptation dataset demonstrate that the proposed approach outperforms the multi-task transfer learning baseline by 0.50 F0.5 score on average and enables us to effectively adapt to a new L1 domain with only 200 parallel sentences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.12409](https://arxiv.org/abs/2101.12409) [cs.CL]** |
|           | (or **[arXiv:2101.12409v1](https://arxiv.org/abs/2101.12409v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-3">3. Synthesizing Monolingual Data for Neural Machine Translation</h2>

Title: [Synthesizing Monolingual Data for Neural Machine Translation](https://arxiv.org/abs/2101.12462)

Authors: [Benjamin Marie](https://arxiv.org/search/cs?searchtype=author&query=Marie%2C+B), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> In neural machine translation (NMT), monolingual data in the target language are usually exploited through a method so-called "back-translation" to synthesize additional training parallel data. The synthetic data have been shown helpful to train better NMT, especially for low-resource language pairs and domains. Nonetheless, large monolingual data in the target domains or languages are not always available to generate large synthetic parallel data. In this work, we propose a new method to generate large synthetic parallel data leveraging very small monolingual data in a specific domain. We fine-tune a pre-trained GPT-2 model on such small in-domain monolingual data and use the resulting model to generate a large amount of synthetic in-domain monolingual data. Then, we perform back-translation, or forward translation, to generate synthetic in-domain parallel data. Our preliminary experiments on three language pairs and five domains show the effectiveness of our method to generate fully synthetic but useful in-domain parallel data for improving NMT in all configurations. We also show promising results in extreme adaptation for personalized NMT.

| Comments: | Preliminary work                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.12462](https://arxiv.org/abs/2101.12462) [cs.CL]** |
|           | (or **[arXiv:2101.12462v1](https://arxiv.org/abs/2101.12462v1) [cs.CL]** for this version) |





<h2 id="2021-02-01-4">4. Transition based Graph Decoder for Neural Machine Translation</h2>

Title: [Transition based Graph Decoder for Neural Machine Translation](https://arxiv.org/abs/2101.12640)

Authors: [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O)

> While a number of works showed gains from incorporating source-side symbolic syntactic and semantic structure into neural machine translation (NMT), much fewer works addressed the decoding of such structure.
> We propose a general Transformer-based approach for tree and graph decoding based on generating a sequence of transitions, inspired by a similar approach that uses RNNs by Dyer (2016).
> Experiments with using the proposed decoder with Universal Dependencies syntax on English-German, German-English and English-Russian show improved performance over the standard Transformer decoder, as well as over ablated versions of the model.\tacltxt{\footnote{All code implementing the presented models will be released upon acceptance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.12640](https://arxiv.org/abs/2101.12640) [cs.CL]** |
|           | (or **[arXiv:2101.12640v1](https://arxiv.org/abs/2101.12640v1) [cs.CL]** for this version) |