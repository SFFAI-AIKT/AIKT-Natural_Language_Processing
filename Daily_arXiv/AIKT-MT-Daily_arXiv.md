# Daily arXiv: Machine Translation - March, 2021

# Index

- [2021-03-31](#2021-03-31)	
  - [1. Diagnosing Vision-and-Language Navigation: What Really Matters](#2021-03-31-1)
  - [2. Unsupervised Machine Translation On Dravidian Languages](#2021-03-31-2)
  - [3. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors](#2021-03-31-3)
  - [4. Auto Correcting in the Process of Translation -- Multi-task Learning Improves Dialogue Machine Translation](#2021-03-31-4)
  - [5. Representing ELMo embeddings as two-dimensional text online](#2021-03-31-5)
- [2021-03-30](#2021-03-30)
  - [1. Bridging Vision and Language from the Video-to-Text Perspective: A Comprehensive Review](#2021-03-30-1)
  - [2. On Hallucination and Predictive Uncertainty in Conditional Language Generation](#2021-03-30-2)
  - [3. PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation](#2021-03-30-3)
  - [4. Changing the Mind of Transformers for Topically-Controllable Language Generation](#2021-03-30-4)
  - [5. NLP for Ghanaian Languages](#2021-03-30-5)
  - [6. Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models](#2021-03-30-6)
  - [7. English-Twi Parallel Corpus for Machine Translation](#2021-03-30-7)
- [2021-03-29](#2021-03-29)
  - [1. Turning transformer attention weights into zero-shot sequence labelers](#2021-03-29-1)
  - [2. Unsupervised Document Embedding via Contrastive Augmentation](#2021-03-29-2)
  - [3. Correcting Automated and Manual Speech Transcription Errors using Warped Language Models](#2021-03-29-3)
  - [4. Dodrio: Exploring Transformer Models with Interactive Visualization](#2021-03-29-4)
- [2021-03-26](#2021-03-26)
  - [1. Mask Attention Networks: Rethinking and Strengthen Transformer](#2021-03-26-1)
  - [2. An Approach to Improve Robustness of NLP Systems against ASR Errors](#2021-03-26-2)
  - [3. Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation](#2021-03-26-3)
  - [4. Visual Grounding Strategies for Text-Only Natural Language Processing](#2021-03-26-4)
- [2021-03-25](#2021-03-25)
  - [1. Repairing Pronouns in Translation with BERT-Based Post-Editing](#2021-03-25-1)
  - [2. Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2](#2021-03-25-2)
  - [3. Finetuning Pretrained Transformers into RNNs](#2021-03-25-3)
  - [4. Representing Numbers in NLP: a Survey and a Vision](#2021-03-25-4)
  - [5. Low-Resource Machine Translation for Low-Resource Languages: Leveraging Comparable Data, Code-Switching and Compute Resources](#2021-03-25-5)
  - [6. Are Multilingual Models Effective in Code-Switching?](#2021-03-25-6)
- [2021-03-23](#2021-03-23)
  - [1. Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals](#2021-03-23-1)
  - [2. Attribute Alignment: Controlling Text Generation from Pre-trained Language Models](#2021-03-23-2)
  - [3. Local Interpretations for Explainable Natural Language Processing: A Survey](#2021-03-23-3)
  - [4. Token-wise Curriculum Learning for Neural Machine Translation](#2021-03-23-4)
  - [5. Dependency Graph-to-String Statistical Machine Translation](#2021-03-23-5)
  - [6. The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation](#2021-03-23-6)
  - [7. Non-Autoregressive Translation by Learning Target Categorical Codes](#2021-03-23-7)
  - [8. SparseGAN: Sparse Generative Adversarial Network for Text Generation](#2021-03-23-8)
  - [9. Monolingual and Parallel Corpora for Kangri Low Resource Language](#2021-03-23-9)
  - [10. Simpson's Bias in NLP Training](#2021-03-23-10)
  - [11. BlonD: An Automatic Evaluation Metric for Document-level MachineTranslation](#2021-03-23-11)
  - [12. BERT: A Review of Applications in Natural Language Processing and Understanding](#2021-03-23-12)
- [2021-03-22](#2021-03-22)
  - [1. Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation](#2021-03-22-1)
  - [2. MuRIL: Multilingual Representations for Indian Languages](#2021-03-22-2)
  - [3. Congolese Swahili Machine Translation for Humanitarian Response](#2021-03-22-3)
- [2021-03-19](#2021-03-19)
  - [1. Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!](#2021-03-19-1)
  - [2. All NLP Tasks Are Generation Tasks: A General Pretraining Framework](#2021-03-19-2)
  - [3. GPT Understands, Too](#2021-03-19-3)
- [2021-03-18](#2021-03-18)
  - [1. Endangered Languages are not Low-Resourced!](#2021-03-18-1)
- [2021-03-17](#2021-03-17)
  - [1. Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models](#2021-03-17-1)
  - [2. MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation](#2021-03-17-2)
  - [3. LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval](#2021-03-17-3)
  - [4. Gumbel-Attention for Multi-modal Machine Translation](#2021-03-17-4)
- [2021-03-16](#2021-03-16)
  - [1. SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels](#2021-03-16-1)
  - [2. A Systematic Review of Reproducibility Research in Natural Language Processing](#2021-03-16-2)
  - [3. Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language](#2021-03-16-3)
  - [4. Towards the evaluation of simultaneous speech translation from a communicative perspective](#2021-03-16-4)
  - [5. Multi-view Subword Regularization](#2021-03-16-5)
  - [6. A Study of Automatic Metrics for the Evaluation of Natural Language Explanations](#2021-03-16-6)
- [2021-03-15](#2021-03-15)
  - [1. Preregistering NLP Research](#2021-03-15-1)
  - [2. Learning Policies for Multilingual Training of Neural Machine Translation Systems](#2021-03-15-2)
  - [3. Bilingual Dictionary-based Language Model Pretraining for Neural Machine Translation](#2021-03-15-3)
  - [4. Constrained Text Generation with Global Guidance -- Case Study on CommonGen](#2021-03-15-4)
  - [5. Improving Translation Robustness with Visual Cues and Error Correction](#2021-03-15-5)
- [2021-03-12](#2021-03-12)
  - [1. FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders](#2021-03-12-1)
  - [2. LightMBERT: A Simple Yet Effective Method for Multilingual BERT Distillation](#2021-03-12-2)
  - [3. Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings](#2021-03-12-3)
  - [4. Active2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation](#2021-03-12-4)
  - [5. The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](#2021-03-12-5)
  - [6. Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings](#2021-03-12-6)
  - [7. Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution](#2021-03-12-7)
  - [8. CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](#2021-03-12-8)
- [2021-03-11](#2021-03-11)
  - [1. Self-Learning for Zero Shot Neural Machine Translation](#2021-03-11-1)
  - [2. CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review](#2021-03-11-2)
- [2021-03-10](#2021-03-10)
  - [1. AfriVEC: Word Embedding Models for African Languages. Case Study of Fon and Nobiin](#2021-03-10-1)
- [2021-03-09](#2021-03-09)
  - [1. Translating the Unseen? Yorùbá → English MT in Low-Resource, Morphologically-Unmarked Settings](#2021-03-09-1)
- [2021-03-08](#2021-03-08)
  - [1. Hierarchical Transformer for Multilingual Machine Translation](#2021-03-08-1)
  - [2. WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings](#2021-03-08-2)
  - [3. Overcoming Poor Word Embeddings with Word Definitions](#2021-03-08-3)
- [2021-03-05](#2021-03-05)
  - [1. An empirical analysis of phrase-based and neural machine translation](#2021-03-05-1)
  - [2. An Empirical Study of End-to-end Simultaneous Speech Translation Decoding Strategies](#2021-03-05-2)
- [2021-03-04](#2021-03-04)
  - [1. Random Feature Attention](#2021-03-04-1)
  - [2. Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation](#2021-03-04-2)
  - [3. Lex2vec: making Explainable Word Embedding via Distant Supervision](#2021-03-04-3)
  - [4. NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands](#2021-03-04-4)
- [2021-03-03](#2021-03-03)
  - [1. WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning](#2021-03-03-1)
  - [2. On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions](#2021-03-03-2)
  - [3. Contrastive Explanations for Model Interpretability](#2021-03-03-3)
  - [4. MultiSubs: A Large-scale Multimodal and Multilingual Dataset](#2021-03-03-4)
- [2021-03-02](#2021-03-02)
  - [1. Generative Adversarial Transformers](#2021-03-02-1)
  - [2. Token-Modification Adversarial Attacks for Natural Language Processing: A Survey](#2021-03-02-2)
  - [3. M6: A Chinese Multimodal Pretrainer](#2021-03-02-3)
- [2021-03-01](#2021-03-01)
  - [1. Automated essay scoring using efficient transformer-based language models](#2021-03-01-1)
  - [2. Learning Chess Blindfolded: Evaluating Language Models on State Tracking](#2021-03-01-2)
  - [3. Gradient-guided Loss Masking for Neural Machine Translation](#2021-03-01-3)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-03-31

[Return to Index](#Index)



<h2 id="2021-03-31-1">1. Diagnosing Vision-and-Language Navigation: What Really Matters
</h2>

Title: [Diagnosing Vision-and-Language Navigation: What Really Matters](https://arxiv.org/abs/2103.16561)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Yuankai Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y), [Pradyumna Narayana](https://arxiv.org/search/cs?searchtype=author&query=Narayana%2C+P), [Kazoo Sone](https://arxiv.org/search/cs?searchtype=author&query=Sone%2C+K), [Sugato Basu](https://arxiv.org/search/cs?searchtype=author&query=Basu%2C+S), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Miguel Eckstein](https://arxiv.org/search/cs?searchtype=author&query=Eckstein%2C+M), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, recent studies witness a slow-down in the performance improvements in both indoor and outdoor VLN tasks, and the agents' inner mechanisms for making navigation decisions remain unclear. To the best of our knowledge, the way the agents perceive the multimodal input is under-studied and clearly needs investigations. In this work, we conduct a series of diagnostic experiments to unveil agents' focus during navigation. Results show that indoor navigation agents refer to both object tokens and direction tokens in the instruction when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and have a poor understanding of the object tokens. Furthermore, instead of merely staring at surrounding objects, indoor navigation agents can set their sights on objects further from the current viewpoint. When it comes to vision-and-language alignments, many models claim that they are able to align object tokens with certain visual targets, but we cast doubt on the reliability of such alignments.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.16561](https://arxiv.org/abs/2103.16561) [cs.CV]** |
|           | (or **[arXiv:2103.16561v1](https://arxiv.org/abs/2103.16561v1) [cs.CV]** for this version) |





<h2 id="2021-03-31-2">2. Unsupervised Machine Translation On Dravidian Languages
</h2>

Title: [Unsupervised Machine Translation On Dravidian Languages](https://arxiv.org/abs/2103.15877)

Authors:[Sai Koneru](https://arxiv.org/search/cs?searchtype=author&query=Koneru%2C+S), [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

> Unsupervised neural machine translation (UNMT) is beneficial especially for low resource languages such as those from the Dravidian family. However, UNMT systems tend to fail in realistic scenarios involving actual low resource languages. Recent works propose to utilize auxiliary parallel data and have achieved state-of-the-art results. In this work, we focus on unsupervised translation between English and Kannada, a low resource Dravidian language. We additionally utilize a limited amount of auxiliary data between English and other related Dravidian languages. We show that unifying the writing systems is essential in unsupervised translation between the Dravidian languages. We explore several model architectures that use the auxiliary data in order to maximize knowledge sharing and enable UNMT for distant language pairs. Our experiments demonstrate that it is crucial to include auxiliary languages that are similar to our focal language, Kannada. Furthermore, we propose a metric to measure language similarity and show that it serves as a good indicator for selecting the auxiliary languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.15877](https://arxiv.org/abs/2103.15877) [cs.CL]** |
|           | (or **[arXiv:2103.15877v1](https://arxiv.org/abs/2103.15877v1) [cs.CL]** for this version) |





<h2 id="2021-03-31-3">3. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors
</h2>

Title: [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors](https://arxiv.org/abs/2103.15949)

Authors:[Zeyu Yun](https://arxiv.org/search/cs?searchtype=author&query=Yun%2C+Z), [Yubei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Bruno A Olshausen](https://arxiv.org/search/cs?searchtype=author&query=Olshausen%2C+B+A), [Yann LeCun](https://arxiv.org/search/cs?searchtype=author&query=LeCun%2C+Y)

> Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these `black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g. word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.15949](https://arxiv.org/abs/2103.15949) [cs.CL]** |
|           | (or **[arXiv:2103.15949v1](https://arxiv.org/abs/2103.15949v1) [cs.CL]** for this version) |





<h2 id="2021-03-31-4">4. Auto Correcting in the Process of Translation -- Multi-task Learning Improves Dialogue Machine Translation
</h2>

Title: [Auto Correcting in the Process of Translation -- Multi-task Learning Improves Dialogue Machine Translation](https://arxiv.org/abs/2103.16189)

Authors:[Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

> Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (\droppro), punctuation dropping (\droppun), and typos (\typo). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%. We will publish the code and dataset publicly at [this https URL](https://github.com/rgwt123/DialogueMT).

| Comments:    | 8 pages, 3 figures, 7 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2103.16189](https://arxiv.org/abs/2103.16189) [cs.CL]** |
|              | (or **[arXiv:2103.16189v1](https://arxiv.org/abs/2103.16189v1) [cs.CL]** for this version) |





<h2 id="2021-03-31-5">5. Representing ELMo embeddings as two-dimensional text online
</h2>

Title: [Representing ELMo embeddings as two-dimensional text online](https://arxiv.org/abs/2103.16414)

Authors:[Andrey Kutuzov](https://arxiv.org/search/cs?searchtype=author&query=Kutuzov%2C+A), [Elizaveta Kuzmenko](https://arxiv.org/search/cs?searchtype=author&query=Kuzmenko%2C+E)

> We describe a new addition to the WebVectors toolkit which is used to serve word embedding models over the Web. The new ELMoViz module adds support for contextualized embedding architectures, in particular for ELMo models. The provided visualizations follow the metaphor of `two-dimensional text' by showing lexical substitutes: words which are most semantically similar in context to the words of the input sentence. The system allows the user to change the ELMo layers from which token embeddings are inferred. It also conveys corpus information about the query words and their lexical substitutes (namely their frequency tiers and parts of speech). The module is well integrated into the rest of the WebVectors toolkit, providing lexical hyperlinks to word representations in static embedding models. Two web services have already implemented the new functionality with pre-trained ELMo models for Russian, Norwegian and English.

| Comments: | EACL'2021 demo paper                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16414](https://arxiv.org/abs/2103.16414) [cs.CL]** |
|           | (or **[arXiv:2103.16414v1](https://arxiv.org/abs/2103.16414v1) [cs.CL]** for this version) |









# 2021-03-30

[Return to Index](#Index)



<h2 id="2021-03-30-1">1. Bridging Vision and Language from the Video-to-Text Perspective: A Comprehensive Review
</h2>

Title: [Bridging Vision and Language from the Video-to-Text Perspective: A Comprehensive Review](https://arxiv.org/abs/2103.14785)

Authors: [Jesus Perez-Martin](https://arxiv.org/search/cs?searchtype=author&query=Perez-Martin%2C+J), [Benjamin Bustos](https://arxiv.org/search/cs?searchtype=author&query=Bustos%2C+B), [Silvio Jamil F. Guimarães](https://arxiv.org/search/cs?searchtype=author&query=Guimarães%2C+S+J+F), [Ivan Sipiran](https://arxiv.org/search/cs?searchtype=author&query=Sipiran%2C+I), [Jorge Pérez](https://arxiv.org/search/cs?searchtype=author&query=Pérez%2C+J), [Grethel Coello Said](https://arxiv.org/search/cs?searchtype=author&query=Said%2C+G+C)

> Research in the area of Vision and Language encompasses challenging topics that seek to connect visual and textual information. The video-to-text problem is one of these topics, in which the goal is to connect an input video with its textual description. This connection can be mainly made by retrieving the most significant descriptions from a corpus or generating a new one given a context video. These two ways represent essential tasks for Computer Vision and Natural Language Processing communities, called text retrieval from video task and video captioning/description task. These two tasks are substantially more complex than predicting or retrieving a single sentence from an image. The spatiotemporal information present in videos introduces diversity and complexity regarding the visual content and the structure of associated language descriptions. This review categorizes and describes the state-of-the-art techniques for the video-to-text problem. It covers the main video-to-text methods and the ways to evaluate their performance. We analyze how the most reported benchmark datasets have been created, showing their drawbacks and strengths for the problem requirements. We also show the impressive progress that researchers have made on each dataset, and we analyze why, despite this progress, the video-to-text conversion is still unsolved. State-of-the-art techniques are still a long way from achieving human-like performance in generating or retrieving video descriptions. We cover several significant challenges in the field and discuss future research directions.

| Comments: | 66 pages, 5 figures. Submitted to Artificial Intelligence Review |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2103.14785](https://arxiv.org/abs/2103.14785) [cs.CV]** |
|           | (or **[arXiv:2103.14785v1](https://arxiv.org/abs/2103.14785v1) [cs.CV]** for this version) |





<h2 id="2021-03-30-2">2. On Hallucination and Predictive Uncertainty in Conditional Language Generation
</h2>

Title: [On Hallucination and Predictive Uncertainty in Conditional Language Generation](https://arxiv.org/abs/2103.15025)

Authors: [Yijun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.15025](https://arxiv.org/abs/2103.15025) [cs.CL]** |
|           | (or **[arXiv:2103.15025v1](https://arxiv.org/abs/2103.15025v1) [cs.CL]** for this version) |





<h2 id="2021-03-30-3">3. PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation
</h2>

Title: [PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation](https://arxiv.org/abs/2103.15075)

Authors: [Dimitris Papadopoulos](https://arxiv.org/search/cs?searchtype=author&query=Papadopoulos%2C+D), [Nikolaos Papadakis](https://arxiv.org/search/cs?searchtype=author&query=Papadakis%2C+N), [Nikolaos Matsatsinis](https://arxiv.org/search/cs?searchtype=author&query=Matsatsinis%2C+N)

> In this paper we present our submission for the EACL 2021 SRW; a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction, showcasing it on the Greek language. The goals of this paper are twofold: First, we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture. Second, we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to Greek. We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language.

| Comments: | 16th conference of the European Chapter of the Association for Computational Linguistics Student Research Workshop (EACL 2021 SRW) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.15075](https://arxiv.org/abs/2103.15075) [cs.CL]** |
|           | (or **[arXiv:2103.15075v1](https://arxiv.org/abs/2103.15075v1) [cs.CL]** for this version) |





<h2 id="2021-03-30-4">4. Changing the Mind of Transformers for Topically-Controllable Language Generation
</h2>

Title: [Changing the Mind of Transformers for Topically-Controllable Language Generation](https://arxiv.org/abs/2103.15335)

Authors: [Haw-Shiuan Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+H), [Jiaming Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+J), [Mohit Iyyer](https://arxiv.org/search/cs?searchtype=author&query=Iyyer%2C+M), [Andrew McCallum](https://arxiv.org/search/cs?searchtype=author&query=McCallum%2C+A)

> Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and (2) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.15335](https://arxiv.org/abs/2103.15335) [cs.CL]** |
|           | (or **[arXiv:2103.15335v1](https://arxiv.org/abs/2103.15335v1) [cs.CL]** for this version) |





<h2 id="2021-03-30-5">5. NLP for Ghanaian Languages
</h2>

Title: [NLP for Ghanaian Languages](https://arxiv.org/abs/2103.15475)

Authors: [Paul Azunre](https://arxiv.org/search/cs?searchtype=author&query=Azunre%2C+P), [Salomey Osei](https://arxiv.org/search/cs?searchtype=author&query=Osei%2C+S), [Salomey Addo](https://arxiv.org/search/cs?searchtype=author&query=Addo%2C+S), [Lawrence Asamoah Adu-Gyamfi](https://arxiv.org/search/cs?searchtype=author&query=Adu-Gyamfi%2C+L+A), [Stephen Moore](https://arxiv.org/search/cs?searchtype=author&query=Moore%2C+S), [Bernard Adabankah](https://arxiv.org/search/cs?searchtype=author&query=Adabankah%2C+B), [Bernard Opoku](https://arxiv.org/search/cs?searchtype=author&query=Opoku%2C+B), [Clara Asare-Nyarko](https://arxiv.org/search/cs?searchtype=author&query=Asare-Nyarko%2C+C), [Samuel Nyarko](https://arxiv.org/search/cs?searchtype=author&query=Nyarko%2C+S), [Cynthia Amoaba](https://arxiv.org/search/cs?searchtype=author&query=Amoaba%2C+C), [Esther Dansoa Appiah](https://arxiv.org/search/cs?searchtype=author&query=Appiah%2C+E+D), [Felix Akwerh](https://arxiv.org/search/cs?searchtype=author&query=Akwerh%2C+F), [Richard Nii Lante Lawson](https://arxiv.org/search/cs?searchtype=author&query=Lawson%2C+R+N+L), [Joel Budu](https://arxiv.org/search/cs?searchtype=author&query=Budu%2C+J), [Emmanuel Debrah](https://arxiv.org/search/cs?searchtype=author&query=Debrah%2C+E), [Nana Boateng](https://arxiv.org/search/cs?searchtype=author&query=Boateng%2C+N), [Wisdom Ofori](https://arxiv.org/search/cs?searchtype=author&query=Ofori%2C+W), [Edwin Buabeng-Munkoh](https://arxiv.org/search/cs?searchtype=author&query=Buabeng-Munkoh%2C+E), [Franklin Adjei](https://arxiv.org/search/cs?searchtype=author&query=Adjei%2C+F), [Isaac Kojo Essel Ampomah](https://arxiv.org/search/cs?searchtype=author&query=Ampomah%2C+I+K+E), [Joseph Otoo](https://arxiv.org/search/cs?searchtype=author&query=Otoo%2C+J), [Reindorf Borkor](https://arxiv.org/search/cs?searchtype=author&query=Borkor%2C+R), [Standylove Birago Mensah](https://arxiv.org/search/cs?searchtype=author&query=Mensah%2C+S+B), [Lucien Mensah](https://arxiv.org/search/cs?searchtype=author&query=Mensah%2C+L), [Mark Amoako Marcel](https://arxiv.org/search/cs?searchtype=author&query=Marcel%2C+M+A), [Anokye Acheampong Amponsah](https://arxiv.org/search/cs?searchtype=author&query=Amponsah%2C+A+A), [James Ben Hayfron-Acquah](https://arxiv.org/search/cs?searchtype=author&query=Hayfron-Acquah%2C+J+B)

> NLP Ghana is an open-source non-profit organization aiming to advance the development and adoption of state-of-the-art NLP techniques and digital language tools to Ghanaian languages and problems. In this paper, we first present the motivation and necessity for the efforts of the organization; by introducing some popular Ghanaian languages while presenting the state of NLP in Ghana. We then present the NLP Ghana organization and outline its aims, scope of work, some of the methods employed and contributions made thus far in the NLP community in Ghana.

| Comments: | 6 pages paper; AfricaNLP 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.15475](https://arxiv.org/abs/2103.15475) [cs.CL]** |
|           | (or **[arXiv:2103.15475v1](https://arxiv.org/abs/2103.15475v1) [cs.CL]** for this version) |





<h2 id="2021-03-30-6">6. Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models
</h2>

Title: [Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models](https://arxiv.org/abs/2103.15543)

Authors: [Wenkai Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Zhiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Bin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+B)

> Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at [this https URL](https://github.com/lancopku/Embedding-Poisoning).

| Comments: | NAACL-HLT 2021, Long Paper                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.15543](https://arxiv.org/abs/2103.15543) [cs.CL]** |
|           | (or **[arXiv:2103.15543v1](https://arxiv.org/abs/2103.15543v1) [cs.CL]** for this version) |





<h2 id="2021-03-30-7">7. English-Twi Parallel Corpus for Machine Translation
</h2>

Title: [English-Twi Parallel Corpus for Machine Translation](https://arxiv.org/abs/2103.15625)

Authors: [Paul Azunre](https://arxiv.org/search/cs?searchtype=author&query=Azunre%2C+P), [Salomey Osei](https://arxiv.org/search/cs?searchtype=author&query=Osei%2C+S), [Salomey Addo](https://arxiv.org/search/cs?searchtype=author&query=Addo%2C+S), [Lawrence Asamoah Adu-Gyamfi](https://arxiv.org/search/cs?searchtype=author&query=Adu-Gyamfi%2C+L+A), [Stephen Moore](https://arxiv.org/search/cs?searchtype=author&query=Moore%2C+S), [Bernard Adabankah](https://arxiv.org/search/cs?searchtype=author&query=Adabankah%2C+B), [Bernard Opoku](https://arxiv.org/search/cs?searchtype=author&query=Opoku%2C+B), [Clara Asare-Nyarko](https://arxiv.org/search/cs?searchtype=author&query=Asare-Nyarko%2C+C), [Samuel Nyarko](https://arxiv.org/search/cs?searchtype=author&query=Nyarko%2C+S), [Cynthia Amoaba](https://arxiv.org/search/cs?searchtype=author&query=Amoaba%2C+C), [Esther Dansoa Appiah](https://arxiv.org/search/cs?searchtype=author&query=Appiah%2C+E+D), [Felix Akwerh](https://arxiv.org/search/cs?searchtype=author&query=Akwerh%2C+F), [Richard Nii Lante Lawson](https://arxiv.org/search/cs?searchtype=author&query=Lawson%2C+R+N+L), [Joel Budu](https://arxiv.org/search/cs?searchtype=author&query=Budu%2C+J), [Emmanuel Debrah](https://arxiv.org/search/cs?searchtype=author&query=Debrah%2C+E), [Nana Boateng](https://arxiv.org/search/cs?searchtype=author&query=Boateng%2C+N), [Wisdom Ofori](https://arxiv.org/search/cs?searchtype=author&query=Ofori%2C+W), [Edwin Buabeng-Munkoh](https://arxiv.org/search/cs?searchtype=author&query=Buabeng-Munkoh%2C+E), [Franklin Adjei](https://arxiv.org/search/cs?searchtype=author&query=Adjei%2C+F), [Isaac Kojo Essel Ampomah](https://arxiv.org/search/cs?searchtype=author&query=Ampomah%2C+I+K+E), [Joseph Otoo](https://arxiv.org/search/cs?searchtype=author&query=Otoo%2C+J), [Reindorf Borkor](https://arxiv.org/search/cs?searchtype=author&query=Borkor%2C+R), [Standylove Birago Mensah](https://arxiv.org/search/cs?searchtype=author&query=Mensah%2C+S+B), [Lucien Mensah](https://arxiv.org/search/cs?searchtype=author&query=Mensah%2C+L), [Mark Amoako Marcel](https://arxiv.org/search/cs?searchtype=author&query=Marcel%2C+M+A), [Anokye Acheampong Amponsah](https://arxiv.org/search/cs?searchtype=author&query=Amponsah%2C+A+A), [James Ben Hayfron-Acquah](https://arxiv.org/search/cs?searchtype=author&query=Hayfron-Acquah%2C+J+B)

> We present a parallel machine translation training corpus for English and Akuapem Twi of 25,421 sentence pairs. We used a transformer-based translator to generate initial translations in Akuapem Twi, which were later verified and corrected where necessary by native speakers to eliminate any occurrence of translationese. In addition, 697 higher quality crowd-sourced sentences are provided for use as an evaluation set for downstream Natural Language Processing (NLP) tasks. The typical use case for the larger human-verified dataset is for further training of machine translation models in Akuapem Twi. The higher quality 697 crowd-sourced dataset is recommended as a testing dataset for machine translation of English to Twi and Twi to English models. Furthermore, the Twi part of the crowd-sourced data may also be used for other tasks, such as representation learning, classification, etc. We fine-tune the transformer translation model on the training corpus and report benchmarks on the crowd-sourced test set.

| Comments: | 9 pages paper, Accepted at African NLP workshop @EACL 2021   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.15625](https://arxiv.org/abs/2103.15625) [cs.CL]** |
|           | (or **[arXiv:2103.15625v1](https://arxiv.org/abs/2103.15625v1) [cs.CL]** for this version) |







# 2021-03-29

[Return to Index](#Index)



<h2 id="2021-03-29-1">1. Turning transformer attention weights into zero-shot sequence labelers
</h2>

Title: [Turning transformer attention weights into zero-shot sequence labelers](https://arxiv.org/abs/2103.14465)

Authors: [Kamil Bujel](https://arxiv.org/search/cs?searchtype=author&query=Bujel%2C+K), [Helen Yannakoudakis](https://arxiv.org/search/cs?searchtype=author&query=Yannakoudakis%2C+H), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M)

> We demonstrate how transformer-based models can be redesigned in order to capture inductive biases across tasks on different granularities and perform inference in a zero-shot manner. Specifically, we show how sentence-level transformers can be modified into effective sequence labelers at the token level without any direct supervision. We compare against a range of diverse and previously proposed methods for generating token-level labels, and present a simple yet effective modified attention layer that significantly advances the current state of the art.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.14465](https://arxiv.org/abs/2103.14465) [cs.CL]** |
|           | (or **[arXiv:2103.14465v1](https://arxiv.org/abs/2103.14465v1) [cs.CL]** for this version) |





<h2 id="2021-03-29-2">2. Unsupervised Document Embedding via Contrastive Augmentation
</h2>

Title: [Unsupervised Document Embedding via Contrastive Augmentation](https://arxiv.org/abs/2103.14542)

Authors: [Dongsheng Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+D), [Wei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+W), [Jingchao Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Wenchao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Xuchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Bo Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+B), [Yanchi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Zhengzhang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Dongjin Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D), [Haifeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Xiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X)

> We present a contrasting learning approach with data augmentation techniques to learn document representations in an unsupervised manner. Inspired by recent contrastive self-supervised learning algorithms used for image and NLP pretraining, we hypothesize that high-quality document embedding should be invariant to diverse paraphrases that preserve the semantics of the original document. With different backbones and contrastive learning frameworks, our study reveals the enormous benefits of contrastive augmentation for document representation learning with two additional insights: 1) including data augmentation in a contrastive way can substantially improve the embedding quality in unsupervised document representation learning, and 2) in general, stochastic augmentations generated by simple word-level manipulation work much better than sentence-level and document-level ones. We plug our method into a classifier and compare it with a broad range of baseline methods on six benchmark datasets. Our method can decrease the classification error rate by up to 6.4% over the SOTA approaches on the document classification task, matching or even surpassing fully-supervised methods.

| Comments: | 13 pages; under review                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.14542](https://arxiv.org/abs/2103.14542) [cs.CL]** |
|           | (or **[arXiv:2103.14542v1](https://arxiv.org/abs/2103.14542v1) [cs.CL]** for this version) |







<h2 id="2021-03-29-3">3. Correcting Automated and Manual Speech Transcription Errors using Warped Language Models
</h2>

Title: [Correcting Automated and Manual Speech Transcription Errors using Warped Language Models](https://arxiv.org/abs/2103.14580)

Authors: [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [John Malik](https://arxiv.org/search/cs?searchtype=author&query=Malik%2C+J), [Li Erran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+E), [Gokhan Tur](https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G), [Dilek Hakkani Tür](https://arxiv.org/search/cs?searchtype=author&query=Tür%2C+D+H)

> Masked language models have revolutionized natural language processing systems in the past few years. A recently introduced generalization of masked language models called warped language models are trained to be more robust to the types of errors that appear in automatic or manual transcriptions of spoken language by exposing the language model to the same types of errors during training. In this work we propose a novel approach that takes advantage of the robustness of warped language models to transcription noise for correcting transcriptions of spoken language. We show that our proposed approach is able to achieve up to 10% reduction in word error rates of both automatic and manual transcriptions of spoken language.

| Comments: | Submitted to INTERSPEECH                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.14580](https://arxiv.org/abs/2103.14580) [cs.CL]** |
|           | (or **[arXiv:2103.14580v1](https://arxiv.org/abs/2103.14580v1) [cs.CL]** for this version) |







<h2 id="2021-03-29-4">4. Dodrio: Exploring Transformer Models with Interactive Visualization
</h2>

Title: [Dodrio: Exploring Transformer Models with Interactive Visualization](https://arxiv.org/abs/2103.14625)

Authors: [Zijie J. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z+J), [Robert Turko](https://arxiv.org/search/cs?searchtype=author&query=Turko%2C+R), [Duen Horng Chau](https://arxiv.org/search/cs?searchtype=author&query=Chau%2C+D+H)

> Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism's ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights with longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at [this https URL](https://poloclub.github.io/dodrio/).

| Comments: | For a demo video, see [this https URL](https://youtu.be/uboTKqPNU5Y) . For a live demo, see [this https URL](https://poloclub.github.io/dodrio/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.14625](https://arxiv.org/abs/2103.14625) [cs.CL]** |
|           | (or **[arXiv:2103.14625v1](https://arxiv.org/abs/2103.14625v1) [cs.CL]** for this version) |





# 2021-03-26

[Return to Index](#Index)



<h2 id="2021-03-26-1">1. Mask Attention Networks: Rethinking and Strengthen Transformer
</h2>

Title: [Mask Attention Networks: Rethinking and Strengthen Transformer](https://arxiv.org/abs/2103.13597)

Authors: [Zhihao Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Z), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Zhongyu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Siyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jian Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Ruofei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.

| Comments: | Accepted as a long paper to NAACL 2021                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.13597](https://arxiv.org/abs/2103.13597) [cs.CL]** |
|           | (or **[arXiv:2103.13597v1](https://arxiv.org/abs/2103.13597v1) [cs.CL]** for this version) |





<h2 id="2021-03-26-2">2. An Approach to Improve Robustness of NLP Systems against ASR Errors
</h2>

Title: [An Approach to Improve Robustness of NLP Systems against ASR Errors](https://arxiv.org/abs/2103.13610)

Authors: [Tong Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+T), [Jinghui Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Speech-enabled systems typically first convert audio to text through an automatic speech recognition (ASR) model and then feed the text to downstream natural language processing (NLP) modules. The errors of the ASR system can seriously downgrade the performance of the NLP modules. Therefore, it is essential to make them robust to the ASR errors. Previous work has shown it is effective to employ data augmentation methods to solve this problem by injecting ASR noise during the training process. In this paper, we utilize the prevalent pre-trained language model to generate training samples with ASR-plausible noise. Compare to the previous methods, our approach generates ASR noise that better fits the real-world error distribution. Experimental results on spoken language translation(SLT) and spoken language understanding (SLU) show that our approach effectively improves the system robustness against the ASR errors and achieves state-of-the-art results on both tasks.

| Comments: | 9 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.13610](https://arxiv.org/abs/2103.13610) [cs.CL]** |
|           | (or **[arXiv:2103.13610v1](https://arxiv.org/abs/2103.13610v1) [cs.CL]** for this version) |







<h2 id="2021-03-26-3">3. Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation
</h2>

Title: [Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation](https://arxiv.org/abs/2103.13678)

Authors: [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W)

> Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both the general-domain and in-domain. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, domain divergence, and model explosion. To address these three problems, we propose a method of "divide and conquer" which is based on the importance of neurons or parameters in the translation model. In our method, we first prune the model and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original unpruned model with the knowledge distillation method. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conduct experiments on different languages and domains and the results show that our method can achieve significant improvements compared with several strong baselines.

| Comments: | NAACL 2021 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.13678](https://arxiv.org/abs/2103.13678) [cs.CL]** |
|           | (or **[arXiv:2103.13678v1](https://arxiv.org/abs/2103.13678v1) [cs.CL]** for this version) |







<h2 id="2021-03-26-4">4. Visual Grounding Strategies for Text-Only Natural Language Processing
</h2>

Title: [Visual Grounding Strategies for Text-Only Natural Language Processing](https://arxiv.org/abs/2103.13942)

Authors: [Damien Sileo](https://arxiv.org/search/cs?searchtype=author&query=Sileo%2C+D)

> Visual grounding is a promising path toward more robust and accurate Natural Language Processing (NLP) models. Many multimodal extensions of BERT (e.g., VideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that lead to state-of-the-art results on multimodal tasks such as Visual Question Answering. Here, we leverage multimodal modeling for purely textual tasks (language modeling and classification) with the expectation that the multimodal pretraining provides a grounding that can improve text processing accuracy. We propose possible strategies in this respect. A first type of strategy, referred to as {\it transferred grounding} consists in applying multimodal models to text-only tasks using a placeholder to replace image input. The second one, which we call {\it associative grounding}, harnesses image retrieval to match texts with related images during both pretraining and text-only downstream tasks. We draw further distinctions into both strategies and then compare them according to their impact on language modeling and commonsense-related downstream tasks, showing improvement over text-only baselines.

| Comments: | Accepted at LANTERN2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.13942](https://arxiv.org/abs/2103.13942) [cs.CL]** |
|           | (or **[arXiv:2103.13942v1](https://arxiv.org/abs/2103.13942v1) [cs.CL]** for this version) |










# 2021-03-25

[Return to Index](#Index)



<h2 id="2021-03-25-1">1. Repairing Pronouns in Translation with BERT-Based Post-Editing
</h2>

Title: [Repairing Pronouns in Translation with BERT-Based Post-Editing](https://arxiv.org/abs/2103.12838)

Authors: [Reid Pryzant](https://arxiv.org/search/cs?searchtype=author&query=Pryzant%2C+R), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Hideto Kazawa](https://arxiv.org/search/cs?searchtype=author&query=Kazawa%2C+H)

> Pronouns are important determinants of a text's meaning but difficult to translate. This is because pronoun choice can depend on entities described in previous sentences, and in some languages pronouns may be dropped when the referent is inferrable from the context. These issues can lead Neural Machine Translation (NMT) systems to make critical errors on pronouns that impair intelligibility and even reinforce gender bias. We investigate the severity of this pronoun issue, showing that (1) in some domains, pronoun choice can account for more than half of a NMT systems' errors, and (2) pronouns have a disproportionately large impact on perceived translation quality. We then investigate a possible solution: fine-tuning BERT on a pronoun prediction task using chunks of source-side sentences, then using the resulting classifier to repair the translations of an existing NMT model. We offer an initial case study of this approach for the Japanese-English language pair, observing that a small number of translations are significantly improved according to human evaluators.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.12838](https://arxiv.org/abs/2103.12838) [cs.CL]** |
|           | (or **[arXiv:2103.12838v1](https://arxiv.org/abs/2103.12838v1) [cs.CL]** for this version) |





<h2 id="2021-03-25-2">2. Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2
</h2>

Title: [Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2](https://arxiv.org/abs/2103.13033)

Authors: [Gregor Betz](https://arxiv.org/search/cs?searchtype=author&query=Betz%2C+G), [Kyle Richardson](https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+K), [Christian Voigt](https://arxiv.org/search/cs?searchtype=author&query=Voigt%2C+C)

> Thinking aloud is an effective meta-cognitive strategy human reasoners apply to solve difficult problems. We suggest to improve the reasoning ability of pre-trained neural language models in a similar way, namely by expanding a task's context with problem elaborations that are dynamically generated by the language model itself. Our main result is that dynamic problem elaboration significantly improves the zero-shot performance of GPT-2 in a deductive reasoning and natural language inference task: While the model uses a syntactic heuristic for predicting an answer, it is capable (to some degree) of generating reasoned additional context which facilitates the successful application of its heuristic. We explore different ways of generating elaborations, including fewshot learning, and find that their relative performance varies with the specific problem characteristics (such as problem difficulty). Moreover, the effectiveness of an elaboration can be explained in terms of the degree to which the elaboration semantically coheres with the corresponding problem. In particular, elaborations that are most faithful to the original problem description may boost accuracy by up to 24%.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.13033](https://arxiv.org/abs/2103.13033) [cs.CL]** |
|           | (or **[arXiv:2103.13033v1](https://arxiv.org/abs/2103.13033v1) [cs.CL]** for this version) |





<h2 id="2021-03-25-3">3. Finetuning Pretrained Transformers into RNNs
</h2>

Title: [Finetuning Pretrained Transformers into RNNs](https://arxiv.org/abs/2103.13076)

Authors: [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Hao Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+H), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Gabriel Ilharco](https://arxiv.org/search/cs?searchtype=author&query=Ilharco%2C+G), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N), [Yi Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Y), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. This comes with a significant computational overhead, as the attention mechanism scales with a quadratic complexity in sequence length. Efficient transformer variants have received increasing interest from recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train or yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving the efficiency while retaining the accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process needs lower training cost than training these recurrent variants from scratch. As many recent models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.13076](https://arxiv.org/abs/2103.13076) [cs.CL]** |
|           | (or **[arXiv:2103.13076v1](https://arxiv.org/abs/2103.13076v1) [cs.CL]** for this version) |





<h2 id="2021-03-25-4">4. Representing Numbers in NLP: a Survey and a Vision
</h2>

Title: [Representing Numbers in NLP: a Survey and a Vision](https://arxiv.org/abs/2103.13136)

Authors: [Avijit Thawani](https://arxiv.org/search/cs?searchtype=author&query=Thawani%2C+A), [Jay Pujara](https://arxiv.org/search/cs?searchtype=author&query=Pujara%2C+J), [Pedro A. Szekely](https://arxiv.org/search/cs?searchtype=author&query=Szekely%2C+P+A), [Filip Ilievski](https://arxiv.org/search/cs?searchtype=author&query=Ilievski%2C+F)

> NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by 18 previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.

| Comments:    | Accepted at NAACL 2021                                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2103.13136](https://arxiv.org/abs/2103.13136) [cs.CL]** |
|              | (or **[arXiv:2103.13136v1](https://arxiv.org/abs/2103.13136v1) [cs.CL]** for this version) |





<h2 id="2021-03-25-5">5. Low-Resource Machine Translation for Low-Resource Languages: Leveraging Comparable Data, Code-Switching and Compute Resources
</h2>

Title: [Low-Resource Machine Translation for Low-Resource Languages: Leveraging Comparable Data, Code-Switching and Compute Resources]()

Authors: [Garry Kuwanto](https://arxiv.org/search/cs?searchtype=author&query=Kuwanto%2C+G), [Afra Feyza Akyürek](https://arxiv.org/search/cs?searchtype=author&query=Akyürek%2C+A+F), [Isidora Chara Tourni](https://arxiv.org/search/cs?searchtype=author&query=Tourni%2C+I+C), [Siyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Derry Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D)

> We conduct an empirical study of unsupervised neural machine translation (NMT) for truly low resource languages, exploring the case when both parallel training data and compute resource are lacking, reflecting the reality of most of the world's languages and the researchers working on these languages. We propose a simple and scalable method to improve unsupervised NMT, showing how adding comparable data mined using a bilingual dictionary along with modest additional compute resource to train the model can significantly improve its performance. We also demonstrate how the use of the dictionary to code-switch monolingual data to create more comparable data can further improve performance. With this weak supervision, our best method achieves BLEU scores that improve over supervised results for English→Gujarati (+18.88), English→Kazakh (+5.84), and English→Somali (+1.16), showing the promise of weakly-supervised NMT for many low resource languages with modest compute resource in the world. To the best of our knowledge, our work is the first to quantitatively showcase the impact of different modest compute resource in low resource NMT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.13272](https://arxiv.org/abs/2103.13272) [cs.CL]** |
|           | (or **[arXiv:2103.13272v1](https://arxiv.org/abs/2103.13272v1) [cs.CL]** for this version) |





<h2 id="2021-03-25-6">6. Are Multilingual Models Effective in Code-Switching?
</h2>

Title: [Are Multilingual Models Effective in Code-Switching?](https://arxiv.org/abs/2103.13309)

Authors: [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Samuel Cahyawijaya](https://arxiv.org/search/cs?searchtype=author&query=Cahyawijaya%2C+S), [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Andrea Madotto](https://arxiv.org/search/cs?searchtype=author&query=Madotto%2C+A), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Multilingual language models have shown decent performance in multilingual and cross-lingual natural language understanding tasks. However, the power of these multilingual models in code-switching tasks has not been fully explored. In this paper, we study the effectiveness of multilingual language models to understand their capability and adaptability to the mixed-language setting by considering the inference speed, performance, and number of parameters to measure their practicality. We conduct experiments in three language pairs on named entity recognition and part-of-speech tagging and compare them with existing methods, such as using bilingual embeddings and multilingual meta-embeddings. Our findings suggest that pre-trained multilingual models do not necessarily guarantee high-quality representations on code-switching, while using meta-embeddings achieves similar results with significantly fewer parameters.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.13309](https://arxiv.org/abs/2103.13309) [cs.CL]** |
|           | (or **[arXiv:2103.13309v1](https://arxiv.org/abs/2103.13309v1) [cs.CL]** for this version) |







# 2021-03-23

[Return to Index](#Index)



<h2 id="2021-03-23-1">1. Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals
</h2>

Title: [Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals](https://arxiv.org/abs/2103.11011)

Authors: [Dani Kiyasseh](https://arxiv.org/search/cs?searchtype=author&query=Kiyasseh%2C+D), [Tingting Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+T), [David Clifton](https://arxiv.org/search/cs?searchtype=author&query=Clifton%2C+D)

> Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.11011](https://arxiv.org/abs/2103.11011) [cs.CL]** |
|           | (or **[arXiv:2103.11011v1](https://arxiv.org/abs/2103.11011v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-2">2. Attribute Alignment: Controlling Text Generation from Pre-trained Language Models
</h2>

Title: [Attribute Alignment: Controlling Text Generation from Pre-trained Language Models](https://arxiv.org/abs/2103.11070)

Authors: [Dian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Kenji Sagae](https://arxiv.org/search/cs?searchtype=author&query=Sagae%2C+K), [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z)

> Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.11070](https://arxiv.org/abs/2103.11070) [cs.CL]** |
|           | (or **[arXiv:2103.11070v1](https://arxiv.org/abs/2103.11070v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-3">3. Local Interpretations for Explainable Natural Language Processing: A Survey
</h2>

Title: [Local Interpretations for Explainable Natural Language Processing: A Survey](https://arxiv.org/abs/2103.11072)

Authors: [Siwen Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Hamish Ivison](https://arxiv.org/search/cs?searchtype=author&query=Ivison%2C+H), [Caren Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+C), [Josiah Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+J)

> As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term \textit{interpretability} and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model's predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.

| Comments: | This work is an initial draft                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.11072](https://arxiv.org/abs/2103.11072) [cs.CL]** |
|           | (or **[arXiv:2103.11072v1](https://arxiv.org/abs/2103.11072v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-4">4. Token-wise Curriculum Learning for Neural Machine Translation
</h2>

Title: [Token-wise Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2103.11088)

Authors: [Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [Haoming Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Tuo Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

> Existing curriculum learning approaches to Neural Machine Translation (NMT) require sampling sufficient amounts of "easy" samples from training data at the early training stage. This is not always achievable for low-resource languages where the amount of training data is limited. To address such limitation, we propose a novel token-wise curriculum learning approach that creates sufficient amounts of easy samples. Specifically, the model learns to predict a short sub-sequence from the beginning part of each target sentence at the early stage of training, and then the sub-sequence is gradually expanded as the training progresses. Such a new curriculum design is inspired by the cumulative effect of translation errors, which makes the latter tokens more difficult to predict than the beginning ones. Extensive experiments show that our approach can consistently outperform baselines on 5 language pairs, especially for low-resource languages. Combining our approach with sentence-level methods further improves the performance on high-resource languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.11088](https://arxiv.org/abs/2103.11088) [cs.CL]** |
|           | (or **[arXiv:2103.11088v1](https://arxiv.org/abs/2103.11088v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-5">5. Dependency Graph-to-String Statistical Machine Translation
</h2>

Title: [Dependency Graph-to-String Statistical Machine Translation](https://arxiv.org/abs/2103.11089)

Authors: [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> We present graph-based translation models which translate source graphs into target strings. Source graphs are constructed from dependency trees with extra links so that non-syntactic phrases are connected. Inspired by phrase-based models, we first introduce a translation model which segments a graph into a sequence of disjoint subgraphs and generates a translation by combining subgraph translations left-to-right using beam search. However, similar to phrase-based models, this model is weak at phrase reordering. Therefore, we further introduce a model based on a synchronous node replacement grammar which learns recursive translation rules. We provide two implementations of the model with different restrictions so that source graphs can be parsed efficiently. Experiments on Chinese--English and German--English show that our graph-based models are significantly better than corresponding sequence- and tree-based baselines.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.11089](https://arxiv.org/abs/2103.11089) [cs.CL]** |
|           | (or **[arXiv:2103.11089v1](https://arxiv.org/abs/2103.11089v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-6">6. The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation
</h2>

Title: [The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation](https://arxiv.org/abs/2103.11189)

Authors: [Jonne Sälevä](https://arxiv.org/search/cs?searchtype=author&query=Sälevä%2C+J), [Constantine Lignos](https://arxiv.org/search/cs?searchtype=author&query=Lignos%2C+C)

> This paper evaluates the performance of several modern subword segmentation methods in a low-resource neural machine translation setting. We compare segmentations produced by applying BPE at the token or sentence level with morphologically-based segmentations from LMVR and MORSEL. We evaluate translation tasks between English and each of Nepali, Sinhala, and Kazakh, and predict that using morphologically-based segmentation methods would lead to better performance in this setting. However, comparing to BPE, we find that no consistent and reliable differences emerge between the segmentation methods. While morphologically-based methods outperform BPE in a few cases, what performs best tends to vary across tasks, and the performance of segmentation methods is often statistically indistinguishable.

| Comments: | EACL 2021 Student Research Workshop                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.11189](https://arxiv.org/abs/2103.11189) [cs.CL]** |
|           | (or **[arXiv:2103.11189v1](https://arxiv.org/abs/2103.11189v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-7">7. Non-Autoregressive Translation by Learning Target Categorical Codes
</h2>

Title: [Non-Autoregressive Translation by Learning Target Categorical Codes](https://arxiv.org/abs/2103.11405)

Authors: [Yu Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Y), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Dongqi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Xinyu Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks, compared with several strong baselines.

| Comments: | 11 pages, 3 figures, 7 tables. Accepted by NAACL-2021        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.11405](https://arxiv.org/abs/2103.11405) [cs.CL]** |
|           | (or **[arXiv:2103.11405v1](https://arxiv.org/abs/2103.11405v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-8">8. SparseGAN: Sparse Generative Adversarial Network for Text Generation
</h2>

Title: [SparseGAN: Sparse Generative Adversarial Network for Text Generation](https://arxiv.org/abs/2103.11578)

Authors: [Liping Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+L), [Jiehang Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J), [Xiaoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X)

> It is still a challenging task to learn a neural text generation model under the framework of generative adversarial networks (GANs) since the entire training process is not differentiable. The existing training strategies either suffer from unreliable gradient estimations or imprecise sentence representations. Inspired by the principle of sparse coding, we propose a SparseGAN that generates semantic-interpretable, but sparse sentence representations as inputs to the discriminator. The key idea is that we treat an embedding matrix as an over-complete dictionary, and use a linear combination of very few selected word embeddings to approximate the output feature representation of the generator at each time step. With such semantic-rich representations, we not only reduce unnecessary noises for efficient adversarial training, but also make the entire training process fully differentiable. Experiments on multiple text generation datasets yield performance improvements, especially in sequence-level metrics, such as BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.11578](https://arxiv.org/abs/2103.11578) [cs.CL]** |
|           | (or **[arXiv:2103.11578v1](https://arxiv.org/abs/2103.11578v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-9">9. Monolingual and Parallel Corpora for Kangri Low Resource Language
</h2>

Title: [Monolingual and Parallel Corpora for Kangri Low Resource Language](https://arxiv.org/abs/2103.11596)

Authors: [Shweta Chauhan](https://arxiv.org/search/cs?searchtype=author&query=Chauhan%2C+S), [Shefali Saxena](https://arxiv.org/search/cs?searchtype=author&query=Saxena%2C+S), [Philemon Daniel](https://arxiv.org/search/cs?searchtype=author&query=Daniel%2C+P)

> In this paper we present the dataset of Himachali low resource endangered language, Kangri (ISO 639-3xnr) listed in the United Nations Educational, Scientific and Cultural Organization (UNESCO). The compilation of kangri corpus has been a challenging task due to the non-availability of the digitalized resources. The corpus contains 1,81,552 Monolingual and 27,362 Hindi-Kangri Parallel corpora. We shared pre-trained kangri word embeddings. We also reported the Bilingual Evaluation Understudy (BLEU) score and Metric for Evaluation of Translation with Explicit ORdering (METEOR) score of Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) results for the corpus. The corpus is freely available for non-commercial usages and research. To the best of our knowledge, this is the first Himachali low resource endangered language corpus. The resources are available at ([this https URL](https://github.com/chauhanshweta/Kangri_corpus))

| Comments: | 7 pages, 6 Tables, 1 Figure                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.11596](https://arxiv.org/abs/2103.11596) [cs.CL]** |
|           | (or **[arXiv:2103.11596v1](https://arxiv.org/abs/2103.11596v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-10">10. Simpson's Bias in NLP Training
</h2>

Title: [Simpson's Bias in NLP Training](https://arxiv.org/abs/2103.11795)

Authors: [Fei Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+F), [Longtu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Huang Bojun](https://arxiv.org/search/cs?searchtype=author&query=Bojun%2C+H), [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y)

> In most machine learning tasks, we evaluate a model M on a given data population S by measuring a population-level metric F(S;M). Examples of such evaluation metric F include precision/recall for (binary) recognition, the F1 score for multi-class classification, and the BLEU metric for language generation. On the other hand, the model M is trained by optimizing a sample-level loss G(St;M) at each learning step t, where St is a subset of S (a.k.a. the mini-batch). Popular choices of G include cross-entropy loss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption behind this paradigm is that the mean value of the sample-level loss G, if averaged over all possible samples, should effectively represent the population-level metric F of the task, such as, that 𝔼[G(St;M)]≈F(S;M).
> In this paper, we systematically investigate the above assumption in several NLP tasks. We show, both theoretically and experimentally, that some popular designs of the sample-level loss G may be inconsistent with the true population-level metric F of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson's bias, due to its deep connections with the classic paradox known as Simpson's reversal paradox in statistics and social sciences.

| Comments: | AAAI 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.11795](https://arxiv.org/abs/2103.11795) [cs.CL]** |
|           | (or **[arXiv:2103.11795v1](https://arxiv.org/abs/2103.11795v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-11">11. BlonD: An Automatic Evaluation Metric for Document-level MachineTranslation
</h2>

Title: [BlonD: An Automatic Evaluation Metric for Document-level MachineTranslation](https://arxiv.org/abs/2103.11878)

Authors: [Yuchen Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> Standard automatic metrics (such as BLEU) are problematic for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones nor can they identify the specific discourse phenomena that caused the translation errors. To address these problems, we propose an automatic metric BlonD for document-level machine translation evaluation. BlonD takes discourse coherence into consideration by calculating the recall and distance of check-pointing phrases and tags, and further provides comprehensive evaluation scores by combining with n-gram. Extensive comparisons between BlonD and existing evaluation metrics are conducted to illustrate their critical distinctions. Experimental results show that BlonD has a much higher document-level sensitivity with respect to previous metrics. The human evaluation also reveals high Pearson R correlation values between BlonD scores and manual quality judgments.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.11878](https://arxiv.org/abs/2103.11878) [cs.CL]** |
|           | (or **[arXiv:2103.11878v1](https://arxiv.org/abs/2103.11878v1) [cs.CL]** for this version) |





<h2 id="2021-03-23-12">12. BERT: A Review of Applications in Natural Language Processing and Understanding
</h2>

Title: [BERT: A Review of Applications in Natural Language Processing and Understanding](https://arxiv.org/abs/2103.11943)

Authors: [M. V. Koroteev](https://arxiv.org/search/cs?searchtype=author&query=Koroteev%2C+M+V)

> In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.

| Comments: | 18 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.11943](https://arxiv.org/abs/2103.11943) [cs.CL]** |
|           | (or **[arXiv:2103.11943v1](https://arxiv.org/abs/2103.11943v1) [cs.CL]** for this version) |







# 2021-03-22

[Return to Index](#Index)



<h2 id="2021-03-22-1">1. Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation
</h2>

Title: [Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation](https://arxiv.org/abs/2103.10531)

Authors:[Alexandra Chronopoulou](https://arxiv.org/search/cs?searchtype=author&query=Chronopoulou%2C+A), [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource, distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to an established UNMT baseline.

| Comments: | Accepted at NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.10531](https://arxiv.org/abs/2103.10531) [cs.CL]** |
|           | (or **[arXiv:2103.10531v1](https://arxiv.org/abs/2103.10531v1) [cs.CL]** for this version) |



<h2 id="2021-03-22-2">2. MuRIL: Multilingual Representations for Indian Languages
</h2>

Title: [MuRIL: Multilingual Representations for Indian Languages](https://arxiv.org/abs/2103.10730)

Authors:[Simran Khanuja](https://arxiv.org/search/cs?searchtype=author&query=Khanuja%2C+S), [Diksha Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+D), [Sarvesh Mehtani](https://arxiv.org/search/cs?searchtype=author&query=Mehtani%2C+S), [Savya Khosla](https://arxiv.org/search/cs?searchtype=author&query=Khosla%2C+S), [Atreyee Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+A), [Balaji Gopalan](https://arxiv.org/search/cs?searchtype=author&query=Gopalan%2C+B), [Dilip Kumar Margam](https://arxiv.org/search/cs?searchtype=author&query=Margam%2C+D+K), [Pooja Aggarwal](https://arxiv.org/search/cs?searchtype=author&query=Aggarwal%2C+P), [Rajiv Teja Nagipogu](https://arxiv.org/search/cs?searchtype=author&query=Nagipogu%2C+R+T), [Shachi Dave](https://arxiv.org/search/cs?searchtype=author&query=Dave%2C+S), [Shruti Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+S), [Subhash Chandra Bose Gali](https://arxiv.org/search/cs?searchtype=author&query=Gali%2C+S+C+B), [Vish Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+V), [Partha Talukdar](https://arxiv.org/search/cs?searchtype=author&query=Talukdar%2C+P)

> India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011). India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today's state-of-the-art multilingual systems perform suboptimally on Indian (IN) languages. This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn't help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both translated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native to Latin script) test sets of the chosen datasets and demonstrate the efficacy of MuRIL in handling transliterated data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.10730](https://arxiv.org/abs/2103.10730) [cs.CL]** |
|           | (or **[arXiv:2103.10730v1](https://arxiv.org/abs/2103.10730v1) [cs.CL]** for this version) |





<h2 id="2021-03-22-3">3. Congolese Swahili Machine Translation for Humanitarian Response
</h2>

Title: [Congolese Swahili Machine Translation for Humanitarian Response](https://arxiv.org/abs/2103.10734)

Authors:[Alp Öktem](https://arxiv.org/search/cs?searchtype=author&query=Öktem%2C+A), [Eric DeLuca](https://arxiv.org/search/cs?searchtype=author&query=DeLuca%2C+E), [Rodrigue Bashizi](https://arxiv.org/search/cs?searchtype=author&query=Bashizi%2C+R), [Eric Paquin](https://arxiv.org/search/cs?searchtype=author&query=Paquin%2C+E), [Grace Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+G)

> In this paper we describe our efforts to make a bidirectional Congolese Swahili (SWC) to French (FRA) neural machine translation system with the motivation of improving humanitarian translation workflows. For training, we created a 25,302-sentence general domain parallel corpus and combined it with publicly available data. Experimenting with low-resource methodologies like cross-dialect transfer and semi-supervised learning, we recorded improvements of up to 2.4 and 3.5 BLEU points in the SWC-FRA and FRA-SWC directions, respectively. We performed human evaluations to assess the usability of our models in a COVID-domain chatbot that operates in the Democratic Republic of Congo (DRC). Direct assessment in the SWC-FRA direction demonstrated an average quality ranking of 6.3 out of 10 with 75% of the target strings conveying the main message of the source text. For the FRA-SWC direction, our preliminary tests on post-editing assessment showed its potential usefulness for machine-assisted translation. We make our models, datasets containing up to 1 million sentences, our development pipeline, and a translator web-app available for public use.

| Comments: | Accepted to Africa NLP workshop organized within the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.10734](https://arxiv.org/abs/2103.10734) [cs.CL]** |
|           | (or **[arXiv:2103.10734v1](https://arxiv.org/abs/2103.10734v1) [cs.CL]** for this version) |





# 2021-03-19

[Return to Index](#Index)



<h2 id="2021-03-19-1">1. Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!
</h2>

Title: [Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!](https://arxiv.org/abs/2103.10013)

Authors: [Xuanli He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Lingjuan Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+L), [Qiongkai Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Lichao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L)

> Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pre-trained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model and find that unless the performance of the victim model is sacrificed, both model ex-traction and adversarial transferability can effectively compromise the target models

| Comments: | accepted to NAACL2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.10013](https://arxiv.org/abs/2103.10013) [cs.CL]** |
|           | (or **[arXiv:2103.10013v1](https://arxiv.org/abs/2103.10013v1) [cs.CL]** for this version) |





<h2 id="2021-03-19-2">2. All NLP Tasks Are Generation Tasks: A General Pretraining Framework</h2>

Title: [All NLP Tasks Are Generation Tasks: A General Pretraining Framework](https://arxiv.org/abs/2103.10360)

Authors: [Zhengxiao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Z), [Yujie Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+Y), [Xiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Ming Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M), [Jiezhong Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+J), [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> There have been various types of pretraining architectures including autoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks are different in nature, with three main categories being classification, unconditional generation, and conditional generation. However, none of the pretraining frameworks performs the best for all tasks, which introduces inconvenience for model development and selection. We propose a novel pretraining framework GLM (General Language Model) to address this challenge. Compared to previous work, our architecture has three major benefits: (1) it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; (2) it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; (3) it naturally handles variable-length blank filling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pre-training data. Moreover, GLM with 1.25x parameters of BERT-Large achieves the best performance in NLU, conditional and unconditional generation at the same time, which demonstrates its generalizability to different downstream tasks.

| Comments: | 14 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.10360](https://arxiv.org/abs/2103.10360) [cs.CL]** |
|           | (or **[arXiv:2103.10360v1](https://arxiv.org/abs/2103.10360v1) [cs.CL]** for this version) |





<h2 id="2021-03-19-3">3. GPT Understands, Too</h2>

Title: [GPT Understands, Too](https://arxiv.org/abs/2103.10385)

Authors: [Xiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Yanan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Zhengxiao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Z), [Ming Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M), [Yujie Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+Y), [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.10385](https://arxiv.org/abs/2103.10385) [cs.CL]** |
|           | (or **[arXiv:2103.10385v1](https://arxiv.org/abs/2103.10385v1) [cs.CL]** for this version) |





# 2021-03-18

[Return to Index](#Index)



<h2 id="2021-03-18-1">1. Endangered Languages are not Low-Resourced!</h2>

Title: [Endangered Languages are not Low-Resourced!](https://arxiv.org/abs/2103.09567)

Authors: [Mika Hämäläinen](https://arxiv.org/search/cs?searchtype=author&query=Hämäläinen%2C+M)

> The term low-resourced has been tossed around in the field of natural language processing to a degree that almost any language that is not English can be called "low-resourced"; sometimes even just for the sake of making a mundane or mediocre paper appear more interesting and insightful. In a field where English is a synonym for language and low-resourced is a synonym for anything not English, calling endangered languages low-resourced is a bit of an overstatement. In this paper, I inspect the relation of the endangered with the low-resourced from my own experiences.

| Comments: | In Multilingual Facilitation (2021)                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.31885/9789515150257.1](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.31885%2F9789515150257.1&v=32d3d8fb) |
| Cite as:  | **[arXiv:2103.09567](https://arxiv.org/abs/2103.09567) [cs.CL]** |
|           | (or **[arXiv:2103.09567v1](https://arxiv.org/abs/2103.09567v1) [cs.CL]** for this version) |





# 2021-03-17

[Return to Index](#Index)



<h2 id="2021-03-17-1">1. Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</h2>

Title: [Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models](https://arxiv.org/abs/2103.08849)

Authors: [Po-Yao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Mandela Patrick](https://arxiv.org/search/cs?searchtype=author&query=Patrick%2C+M), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Alexander Hauptmann](https://arxiv.org/search/cs?searchtype=author&query=Hauptmann%2C+A)

> This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextualized multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (MultiHowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at [this http URL](http://github.com/berniebear/Mutli-HT100M).

| Comments: | accepted by NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2103.08849](https://arxiv.org/abs/2103.08849) [cs.CV]** |
|           | (or **[arXiv:2103.08849v1](https://arxiv.org/abs/2103.08849v1) [cs.CV]** for this version) |





<h2 id="2021-03-17-2">2. MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation</h2>

Title: [MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation](https://arxiv.org/abs/2103.08647)

Authors: [David I. Adelani](https://arxiv.org/search/cs?searchtype=author&query=Adelani%2C+D+I), [Dana Ruiter](https://arxiv.org/search/cs?searchtype=author&query=Ruiter%2C+D), [Jesujoba O. Alabi](https://arxiv.org/search/cs?searchtype=author&query=Alabi%2C+J+O), [Damilola Adebonojo](https://arxiv.org/search/cs?searchtype=author&query=Adebonojo%2C+D), [Adesina Ayeni](https://arxiv.org/search/cs?searchtype=author&query=Ayeni%2C+A), [Mofe Adeyemi](https://arxiv.org/search/cs?searchtype=author&query=Adeyemi%2C+M), [Ayodele Awokoya](https://arxiv.org/search/cs?searchtype=author&query=Awokoya%2C+A), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C)

> Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource Yorùbá--English (yo--en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook's M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.

| Comments: | Accepted to the AfricaNLP 2021 Workshop @EACL 2021           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.08647](https://arxiv.org/abs/2103.08647) [cs.CL]** |
|           | (or **[arXiv:2103.08647v1](https://arxiv.org/abs/2103.08647v1) [cs.CL]** for this version) |





<h2 id="2021-03-17-3">3. LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</h2>

Title: [LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval](https://arxiv.org/abs/2103.08784)

Authors: [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computation cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, LightningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000x magnitude of computational hours. Code and pre-training checkpoints are available at [this https URL](https://github.com/intersun/LightningDOT).

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.08784](https://arxiv.org/abs/2103.08784) [cs.CL]** |
|           | (or **[arXiv:2103.08784v1](https://arxiv.org/abs/2103.08784v1) [cs.CL]** for this version) |





<h2 id="2021-03-17-4">4. Gumbel-Attention for Multi-modal Machine Translation</h2>

Title: [Gumbel-Attention for Multi-modal Machine Translation](https://arxiv.org/abs/2103.08862)

Authors: [Pengbo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Hailong Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+H), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

> Multi-modal machine translation (MMT) improves translation quality by introducing visual information. However, the existing MMT model ignores the problem that the image will bring information irrelevant to the text, causing much noise to the model and affecting the translation quality. In this paper, we propose a novel Gumbel-Attention for multi-modal machine translation, which selects the text-related parts of the image features. Specifically, different from the previous attention-based method, we first use a differentiable method to select the image information and automatically remove the useless parts of the image features. Through the score matrix of Gumbel-Attention and image features, the image-aware text representation is generated. And then, we independently encode the text representation and the image-aware text representation with the multi-modal encoder. Finally, the final output of the encoder is obtained through multi-modal gated fusion. Experiments and case analysis proves that our method retains the image features related to the text, and the remaining parts help the MMT model generates better translations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.08862](https://arxiv.org/abs/2103.08862) [cs.CL]** |
|           | (or **[arXiv:2103.08862v1](https://arxiv.org/abs/2103.08862v1) [cs.CL]** for this version) |





# 2021-03-16

[Return to Index](#Index)



<h2 id="2021-03-16-1">1. SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels</h2>

Title: [SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels](https://arxiv.org/abs/2103.07829)

Authors: [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S)

> Vision-language pre-training (VLP) on large-scale image-text pairs has recently witnessed rapid progress for learning cross-modal representations. Existing pre-training methods either directly concatenate image representation and text representation at a feature level as input to a single-stream Transformer, or use a two-stream cross-modal Transformer to align the image-text representation at a high-level semantic space. In real-world image-text data, we observe that it is easy for some of the image-text pairs to align simple semantics on both modalities, while others may be related after higher-level abstraction. Therefore, in this paper, we propose a new pre-training method SemVLP, which jointly aligns both the low-level and high-level semantics between image and text representations. The model is pre-trained iteratively with two prevalent fashions: single-stream pre-training to align at a fine-grained feature level and two-stream pre-training to align high-level semantics, by employing a shared Transformer network with a pluggable cross-modal attention module. An extensive set of experiments have been conducted on four well-established vision-language understanding tasks to demonstrate the effectiveness of the proposed SemVLP in aligning cross-modal representations towards different semantic granularities.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.07829](https://arxiv.org/abs/2103.07829) [cs.CL]** |
|           | (or **[arXiv:2103.07829v1](https://arxiv.org/abs/2103.07829v1) [cs.CL]** for this version) |





<h2 id="2021-03-16-2">2. A Systematic Review of Reproducibility Research in Natural Language Processing</h2>

Title: [A Systematic Review of Reproducibility Research in Natural Language Processing](https://arxiv.org/abs/2103.07929)

Authors: [Anya Belz](https://arxiv.org/search/cs?searchtype=author&query=Belz%2C+A), [Shubham Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+S), [Anastasia Shimorina](https://arxiv.org/search/cs?searchtype=author&query=Shimorina%2C+A), [Ehud Reiter](https://arxiv.org/search/cs?searchtype=author&query=Reiter%2C+E)

> Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP, delineating differences and similarities, and providing pointers to common denominators.

| Comments: | To be published in proceedings of EACL'21                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.07929](https://arxiv.org/abs/2103.07929) [cs.CL]** |
|           | (or **[arXiv:2103.07929v1](https://arxiv.org/abs/2103.07929v1) [cs.CL]** for this version) |





<h2 id="2021-03-16-3">3. Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language</h2>

Title: [Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language](https://arxiv.org/abs/2103.08052)

Authors: [Bonaventure F. P. Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B+F+P), [Chris C. Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C+C)

> Building effective neural machine translation (NMT) models for very low-resourced and morphologically rich African indigenous languages is an open challenge. Besides the issue of finding available resources for them, a lot of work is put into preprocessing and tokenization. Recent studies have shown that standard tokenization methods do not always adequately deal with the grammatical, diacritical, and tonal properties of some African languages. That, coupled with the extremely low availability of training samples, hinders the production of reliable NMT models. In this paper, using Fon language as a case study, we revisit standard tokenization methods and introduce Word-Expressions-Based (WEB) tokenization, a human-involved super-words tokenization strategy to create a better representative vocabulary for training. Furthermore, we compare our tokenization strategy to others on the Fon-French and French-Fon translation tasks.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | African NLP, EACL 2021                                       |
| Cite as:           | **[arXiv:2103.08052](https://arxiv.org/abs/2103.08052) [cs.CL]** |
|                    | (or **[arXiv:2103.08052v1](https://arxiv.org/abs/2103.08052v1) [cs.CL]** for this version) |





<h2 id="2021-03-16-4">4. Towards the evaluation of simultaneous speech translation from a communicative perspective</h2>

Title: [Towards the evaluation of simultaneous speech translation from a communicative perspective](https://arxiv.org/abs/2103.08364)

Authors: [claudio Fantinuoli](https://arxiv.org/search/cs?searchtype=author&query=Fantinuoli%2C+c), [Bianca Prandi](https://arxiv.org/search/cs?searchtype=author&query=Prandi%2C+B)

> In recent years, machine speech-to-speech and speech-to-text translation has gained momentum thanks to advances in artificial intelligence, especially in the domains of speech recognition and machine translation. The quality of such applications is commonly tested with automatic metrics, such as BLEU, primarily with the goal of assessing improvements of releases or in the context of evaluation campaigns. However, little is known about how such systems compare to human performances in similar communicative tasks or how the performance of such systems is perceived by final users.
> In this paper, we present the results of an experiment aimed at evaluating the quality of a simultaneous speech translation engine by comparing it to the performance of professional interpreters. To do so, we select a framework developed for the assessment of human interpreters and use it to perform a manual evaluation on both human and machine performances. In our sample, we found better performance for the human interpreters in terms of intelligibility, while the machine performs slightly better in terms of informativeness. The limitations of the study and the possible enhancements of the chosen framework are discussed. Despite its intrinsic limitations, the use of this framework represents a first step towards a user-centric and communication-oriented methodology for evaluating simultaneous speech translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.08364](https://arxiv.org/abs/2103.08364) [cs.CL]** |
|           | (or **[arXiv:2103.08364v1](https://arxiv.org/abs/2103.08364v1) [cs.CL]** for this version) |





<h2 id="2021-03-16-5">5. Multi-view Subword Regularization</h2>

Title: [Multi-view Subword Regularization](https://arxiv.org/abs/2103.08490)

Authors: [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods(Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark(Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.

| Comments: | Accepted at NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.08490](https://arxiv.org/abs/2103.08490) [cs.CL]** |
|           | (or **[arXiv:2103.08490v1](https://arxiv.org/abs/2103.08490v1) [cs.CL]** for this version) |





<h2 id="2021-03-16-6">6. A Study of Automatic Metrics for the Evaluation of Natural Language Explanations</h2>

Title: [A Study of Automatic Metrics for the Evaluation of Natural Language Explanations](https://arxiv.org/abs/2103.08545)

Authors: [Miruna Clinciu](https://arxiv.org/search/cs?searchtype=author&query=Clinciu%2C+M), [Arash Eshghi](https://arxiv.org/search/cs?searchtype=author&query=Eshghi%2C+A), [Helen Hastie](https://arxiv.org/search/cs?searchtype=author&query=Hastie%2C+H)

> As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2103.08545](https://arxiv.org/abs/2103.08545) [cs.CL]** |
|           | (or **[arXiv:2103.08545v1](https://arxiv.org/abs/2103.08545v1) [cs.CL]** for this version) |





# 2021-03-15

[Return to Index](#Index)



<h2 id="2021-03-15-1">1. Preregistering NLP Research</h2>

Title: [Preregistering NLP Research](https://arxiv.org/abs/2103.06944)

Authors: [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Chris van der Lee](https://arxiv.org/search/cs?searchtype=author&query=van+der+Lee%2C+C), [Emiel Krahmer](https://arxiv.org/search/cs?searchtype=author&query=Krahmer%2C+E)

> Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.

| Comments: | Accepted at NAACL2021; pre-final draft, comments welcome     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.06944](https://arxiv.org/abs/2103.06944) [cs.CL]** |
|           | (or **[arXiv:2103.06944v1](https://arxiv.org/abs/2103.06944v1) [cs.CL]** for this version) |





<h2 id="2021-03-15-2">2. Learning Policies for Multilingual Training of Neural Machine Translation Systems</h2>

Title: [Learning Policies for Multilingual Training of Neural Machine Translation Systems](https://arxiv.org/abs/2103.06964)

Authors: [Gaurav Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+G), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Sanjeev Khudanpur](https://arxiv.org/search/cs?searchtype=author&query=Khudanpur%2C+S)

> Low-resource Multilingual Neural Machine Translation (MNMT) is typically tasked with improving the translation performance on one or more language pairs with the aid of high-resource language pairs. In this paper, we propose two simple search based curricula -- orderings of the multilingual training data -- which help improve translation performance in conjunction with existing techniques such as fine-tuning. Additionally, we attempt to learn a curriculum for MNMT from scratch jointly with the training of the translation system with the aid of contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system.

| Comments: | 7 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.06964](https://arxiv.org/abs/2103.06964) [cs.CL]** |
|           | (or **[arXiv:2103.06964v1](https://arxiv.org/abs/2103.06964v1) [cs.CL]** for this version) |





<h2 id="2021-03-15-3">3. Bilingual Dictionary-based Language Model Pretraining for Neural Machine Translation</h2>

Title: [Bilingual Dictionary-based Language Model Pretraining for Neural Machine Translation](https://arxiv.org/abs/2103.07040)

Authors: [Yusen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Jiayong Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Shuaicheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Haoying Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+H)

> Recent studies have demonstrated a perceivable improvement on the performance of neural machine translation by applying cross-lingual language model pretraining (Lample and Conneau, 2019), especially the Translation Language Modeling (TLM). To alleviate the need for expensive parallel corpora by TLM, in this work, we incorporate the translation information from dictionaries into the pretraining process and propose a novel Bilingual Dictionary-based Language Model (BDLM). We evaluate our BDLM in Chinese, English, and Romanian. For Chinese-English, we obtained a 55.0 BLEU on WMT-News19 (Tiedemann, 2012) and a 24.3 BLEU on WMT20 news-commentary, outperforming the Vanilla Transformer (Vaswani et al., 2017) by more than 8.4 BLEU and 2.3 BLEU, respectively. According to our results, the BDLM also has advantages on convergence speed and predicting rare words. The increase in BLEU for WMT16 Romanian-English also shows its effectiveness in low-resources language translation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.07040](https://arxiv.org/abs/2103.07040) [cs.CL]** |
|           | (or **[arXiv:2103.07040v1](https://arxiv.org/abs/2103.07040v1) [cs.CL]** for this version) |





<h2 id="2021-03-15-4">4. Constrained Text Generation with Global Guidance -- Case Study on CommonGen</h2>

Title: [Constrained Text Generation with Global Guidance -- Case Study on CommonGen](https://arxiv.org/abs/2103.07170)

Authors: [Yixian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Liwen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Wenjuan Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+W), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Kewei Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+K)

> This paper studies constrained text generation, which is to generate sentences under certain pre-conditions. We focus on CommonGen, the task of generating text based on a set of concepts, as a representative task of constrained text generation. Traditional methods mainly rely on supervised training to maximize the likelihood of target sentences.However, global constraints such as common sense and coverage cannot be incorporated into the likelihood objective of the autoregressive decoding process. In this paper, we consider using reinforcement learning to address the limitation, measuring global constraints including fluency, common sense and concept coverage with a comprehensive score, which serves as the reward for reinforcement learning. Besides, we design a guided decoding method at the word, fragment and sentence levels. Experiments demonstrate that our method significantly increases the concept coverage and outperforms existing models in various automatic evaluations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.07170](https://arxiv.org/abs/2103.07170) [cs.CL]** |
|           | (or **[arXiv:2103.07170v1](https://arxiv.org/abs/2103.07170v1) [cs.CL]** for this version) |





<h2 id="2021-03-15-5">5. Improving Translation Robustness with Visual Cues and Error Correction</h2>

Title: [Improving Translation Robustness with Visual Cues and Error Correction](https://arxiv.org/abs/2103.07352)

Authors: [Zhenhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Neural Machine Translation models are brittle to input noise. Current robustness techniques mostly adapt models to existing noisy texts, but these models generally fail when faced with unseen noise and their performance degrades on clean texts. In this paper, we introduce the idea of visual context to improve translation robustness against noisy texts. In addition, we propose a novel error correction training regime by treating error correction as an auxiliary task to further improve robustness. Experiments on English-French and English-German translation show that both multimodality and error correction training are beneficial for model robustness to known and new types of errors, while keeping the quality on clean texts.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.07352](https://arxiv.org/abs/2103.07352) [cs.CL]** |
|           | (or **[arXiv:2103.07352v1](https://arxiv.org/abs/2103.07352v1) [cs.CL]** for this version) |





# 2021-03-12

[Return to Index](#Index)



<h2 id="2021-03-12-1">1. FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders</h2>

Title: [FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders](https://arxiv.org/abs/2103.06413)

Authors: [Pengyu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+P), [Weituo Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+W), [Siyang Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+S), [Shijing Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+S), [Lawrence Carin](https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L)

> Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.

| Comments: | Accepted by the 9th International Conference on Learning Representations (ICLR 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.06413](https://arxiv.org/abs/2103.06413) [cs.CL]** |
|           | (or **[arXiv:2103.06413v1](https://arxiv.org/abs/2103.06413v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-2">2. LightMBERT: A Simple Yet Effective Method for Multilingual BERT Distillation</h2>

Title: [LightMBERT: A Simple Yet Effective Method for Multilingual BERT Distillation](https://arxiv.org/abs/2103.06418)

Authors: [Xiaoqi Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+X), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Linlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Fang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> The multilingual pre-trained language models (e.g, mBERT, XLM and XLM-R) have shown impressive performance on cross-lingual natural language understanding tasks. However, these models are computationally intensive and difficult to be deployed on resource-restricted devices. In this paper, we propose a simple yet effective distillation method (LightMBERT) for transferring the cross-lingual generalization ability of the multilingual BERT to a small student model. The experiment results empirically demonstrate the efficiency and effectiveness of LightMBERT, which is significantly better than the baselines and performs comparable to the teacher mBERT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.06418](https://arxiv.org/abs/2103.06418) [cs.CL]** |
|           | (or **[arXiv:2103.06418v1](https://arxiv.org/abs/2103.06418v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-3">3. Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings</h2>

Title: [Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings](https://arxiv.org/abs/2103.06459)

Authors: [Linlin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Thien Hai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Lidong Bing](https://arxiv.org/search/cs?searchtype=author&query=Bing%2C+L), [Luo Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+L)

> Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.06459](https://arxiv.org/abs/2103.06459) [cs.CL]** |
|           | (or **[arXiv:2103.06459v1](https://arxiv.org/abs/2103.06459v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-4">4. Active2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation</h2>

Title: [Active2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation](https://arxiv.org/abs/2103.06490)

Authors: [Rishi Hazra](https://arxiv.org/search/cs?searchtype=author&query=Hazra%2C+R), [Parag Dutta](https://arxiv.org/search/cs?searchtype=author&query=Dutta%2C+P), [Shubham Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+S), [Mohammed Abdul Qaathir](https://arxiv.org/search/cs?searchtype=author&query=Qaathir%2C+M+A), [Ambedkar Dukkipati](https://arxiv.org/search/cs?searchtype=author&query=Dukkipati%2C+A)

> While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active**2** Learning (A**2**L), actively adapts to the deep learning model being trained to eliminate further such redundant examples chosen by an AL strategy. We show that A**2**L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by an absolute percentage reduction of ≈**3****−****25****%** on multiple NLP tasks while achieving the same performance with no additional computation overhead.

| Comments: | Accepted in NAACL-HLT-2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2103.06490](https://arxiv.org/abs/2103.06490) [cs.CL]** |
|           | (or **[arXiv:2103.06490v1](https://arxiv.org/abs/2103.06490v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-5">5. The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models</h2>

Title: [The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)

Authors: [Go Inoue](https://arxiv.org/search/cs?searchtype=author&query=Inoue%2C+G), [Bashar Alhafni](https://arxiv.org/search/cs?searchtype=author&query=Alhafni%2C+B), [Nurpeiis Baimukan](https://arxiv.org/search/cs?searchtype=author&query=Baimukan%2C+N), [Houda Bouamor](https://arxiv.org/search/cs?searchtype=author&query=Bouamor%2C+H), [Nizar Habash](https://arxiv.org/search/cs?searchtype=author&query=Habash%2C+N)

> In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.

| Comments: | Accepted to WANLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.06678](https://arxiv.org/abs/2103.06678) [cs.CL]** |
|           | (or **[arXiv:2103.06678v1](https://arxiv.org/abs/2103.06678v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-6">6. Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings</h2>

Title: [Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings](https://arxiv.org/abs/2103.06689)

Authors: [Carlos Mullov](https://arxiv.org/search/cs?searchtype=author&query=Mullov%2C+C), [Ngoc-Quan Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+N), [Alexander Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

> In this work we look into adding a new language to a multilingual NMT system in an unsupervised fashion. Under the utilization of pre-trained cross-lingual word embeddings we seek to exploit a language independent multilingual sentence representation to easily generalize to a new language. While using cross-lingual embeddings for word lookup we decode from a yet entirely unseen source language in a process we call blind decoding. Blindly decoding from Portuguese using a basesystem containing several Romance languages we achieve scores of 36.4 BLEU for Portuguese-English and 12.8 BLEU for Russian-English. In an attempt to train the mapping from the encoder sentence representation to a new target language we use our model as an autoencoder. Merely training to translate from Portuguese to Portuguese while freezing the encoder we achieve 26 BLEU on English-Portuguese, and up to 28 BLEU when adding artificial noise to the input. Lastly we explore a more practical adaptation approach through non-iterative backtranslation, exploiting our model's ability to produce high quality translations through blind decoding. This yields us up to 34.6 BLEU on English-Portuguese, attaining near parity with a model adapted on real bilingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.06689](https://arxiv.org/abs/2103.06689) [cs.CL]** |
|           | (or **[arXiv:2103.06689v1](https://arxiv.org/abs/2103.06689v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-7">7. Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution</h2>

Title: [Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution](https://arxiv.org/abs/2103.06799)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.

| Comments: | Accepted at NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.06799](https://arxiv.org/abs/2103.06799) [cs.CL]** |
|           | (or **[arXiv:2103.06799v1](https://arxiv.org/abs/2103.06799v1) [cs.CL]** for this version) |





<h2 id="2021-03-12-8">8. CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</h2>

Title: [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)

Authors: [Jonathan H. Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+J+H), [Dan Garrette](https://arxiv.org/search/cs?searchtype=author&query=Garrette%2C+D), [Iulia Turc](https://arxiv.org/search/cs?searchtype=author&query=Turc%2C+I), [John Wieting](https://arxiv.org/search/cs?searchtype=author&query=Wieting%2C+J)

> Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token [this http URL](http://boundaries.to/) use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes con-text. CANINE outperforms a comparable mBERT model by >=1 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28% fewer model parameters.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.06874](https://arxiv.org/abs/2103.06874) [cs.CL]** |
|           | (or **[arXiv:2103.06874v1](https://arxiv.org/abs/2103.06874v1) [cs.CL]** for this version) |



# 2021-03-11

[Return to Index](#Index)



<h2 id="2021-03-11-1">1. Self-Learning for Zero Shot Neural Machine Translation</h2>

Title: [Self-Learning for Zero Shot Neural Machine Translation](https://arxiv.org/abs/2103.05951)

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Neural Machine Translation (NMT) approaches employing monolingual data are showing steady improvements in resource rich conditions. However, evaluations using real-world low-resource languages still result in unsatisfactory performance. This work proposes a novel zero-shot NMT modeling approach that learns without the now-standard assumption of a pivot language sharing parallel data with the zero-shot source and target languages. Our approach is based on three stages: initialization from any pre-trained NMT model observing at least the target language, augmentation of source sides leveraging target monolingual data, and learning to optimize the initial model to the zero-shot pair, where the latter two constitute a self-learning cycle. Empirical findings involving four diverse (in terms of a language family, script and relatedness) zero-shot pairs show the effectiveness of our approach with up to +5.93 BLEU improvement against a supervised bilingual baseline. Compared to unsupervised NMT, consistent improvements are observed even in a domain-mismatch setting, attesting to the usability of our method.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.05951](https://arxiv.org/abs/2103.05951) [cs.CL]** |
|           | (or **[arXiv:2103.05951v1](https://arxiv.org/abs/2103.05951v1) [cs.CL]** for this version) |





<h2 id="2021-03-11-2">2. CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review</h2>

Title: [CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review](https://arxiv.org/abs/2103.06268)

Authors: [Dan Hendrycks](https://arxiv.org/search/cs?searchtype=author&query=Hendrycks%2C+D), [Collin Burns](https://arxiv.org/search/cs?searchtype=author&query=Burns%2C+C), [Anya Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+A), [Spencer Ball](https://arxiv.org/search/cs?searchtype=author&query=Ball%2C+S)

> Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.

| Comments: | Code and the CUAD dataset are available at [this https URL](https://github.com/TheAtticusProject/cuad/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.06268](https://arxiv.org/abs/2103.06268) [cs.CL]** |
|           | (or **[arXiv:2103.06268v1](https://arxiv.org/abs/2103.06268v1) [cs.CL]** for this version) |



# 2021-03-10

[Return to Index](#Index)



<h2 id="2021-03-10-1">1. AfriVEC: Word Embedding Models for African Languages. Case Study of Fon and Nobiin</h2>

Title: [AfriVEC: Word Embedding Models for African Languages. Case Study of Fon and Nobiin](https://arxiv.org/abs/2103.05132)

Authors: [Bonaventure F. P. Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B+F+P), [Mohammed Sabry](https://arxiv.org/search/cs?searchtype=author&query=Sabry%2C+M)

> From Word2Vec to GloVe, word embedding models have played key roles in the current state-of-the-art results achieved in Natural Language Processing. Designed to give significant and unique vectorized representations of words and entities, those models have proven to efficiently extract similarities and establish relationships reflecting semantic and contextual meaning among words and entities. African Languages, representing more than 31% of the worldwide spoken languages, have recently been subject to lots of research. However, to the best of our knowledge, there are currently very few to none word embedding models for those languages words and entities, and none for the languages under study in this paper. After describing Glove, Word2Vec, and Poincaré embeddings functionalities, we build Word2Vec and Poincaré word embedding models for Fon and Nobiin, which show promising results. We test the applicability of transfer learning between these models as a landmark for African Languages to jointly involve in mitigating the scarcity of their resources, and attempt to provide linguistic and social interpretations of our results. Our main contribution is to arouse more interest in creating word embedding models proper to African Languages, ready for use, and that can significantly improve the performances of Natural Language Processing downstream tasks on them. The official repository and implementation is at [this https URL](https://github.com/bonaventuredossou/afrivec)

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Africa NLP, EACL 2021                                        |
| Cite as:           | **[arXiv:2103.05132](https://arxiv.org/abs/2103.05132) [cs.CL]** |
|                    | (or **[arXiv:2103.05132v1](https://arxiv.org/abs/2103.05132v1) [cs.CL]** for this version) |





# 2021-03-09

[Return to Index](#Index)



<h2 id="2021-03-09-1">1. Hierarchical Transformer for Multilingual Machine Translation</h2>

Title: [Translating the Unseen? Yorùbá → English MT in Low-Resource, Morphologically-Unmarked Settings](https://arxiv.org/abs/2103.04225)

Authors: [Ife Adebara Miikka Silfverberg Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+I+A+M+S+M)

> Translating between languages where certain features are marked morphologically in one but absent or marked contextually in the other is an important test case for machine translation. When translating into English which marks (in)definiteness morphologically, from Yorùbá which uses bare nouns but marks these features contextually, ambiguities arise. In this work, we perform fine-grained analysis on how an SMT system compares with two NMT systems (BiLSTM and Transformer) when translating bare nouns in Yorùbá into English. We investigate how the systems what extent they identify BNs, correctly translate them, and compare with human translation patterns. We also analyze the type of errors each model makes and provide a linguistic description of these errors. We glean insights for evaluating model performance in low-resource settings. In translating bare nouns, our results show the transformer model outperforms the SMT and BiLSTM models for 4 categories, the BiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the NMT models for 1 category.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | AfricanNLP @ EACL2021                                        |
| Cite as:           | **[arXiv:2103.04225](https://arxiv.org/abs/2103.04225) [cs.CL]** |
|                    | (or **[arXiv:2103.04225v1](https://arxiv.org/abs/2103.04225v1) [cs.CL]** for this version) |







# 2021-03-08

[Return to Index](#Index)



<h2 id="2021-03-08-1">1. Hierarchical Transformer for Multilingual Machine Translation</h2>

Title: [Hierarchical Transformer for Multilingual Machine Translation](https://arxiv.org/abs/2103.03589)

Authors: [Albina Khusainova](https://arxiv.org/search/cs?searchtype=author&query=Khusainova%2C+A), [Adil Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A), [Adín Ramírez Rivera](https://arxiv.org/search/cs?searchtype=author&query=Rivera%2C+A+R), [Vitaly Romanov](https://arxiv.org/search/cs?searchtype=author&query=Romanov%2C+V)

> The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.

| Comments: | Accepted to VarDial 2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.03589](https://arxiv.org/abs/2103.03589) [cs.CL]** |
|           | (or **[arXiv:2103.03589v1](https://arxiv.org/abs/2103.03589v1) [cs.CL]** for this version) |





<h2 id="2021-03-08-2">2. WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings</h2>

Title: [WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings](https://arxiv.org/abs/2103.03598)

Authors: [Bhavya Ghai](https://arxiv.org/search/cs?searchtype=author&query=Ghai%2C+B), [Md Naimul Hoque](https://arxiv.org/search/cs?searchtype=author&query=Hoque%2C+M+N), [Klaus Mueller](https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+K)

> Intersectional bias is a bias caused by an overlap of multiple social factors like gender, sexuality, race, disability, religion, etc. A recent study has shown that word embedding models can be laden with biases against intersectional groups like African American females, etc. The first step towards tackling such intersectional biases is to identify them. However, discovering biases against different intersectional groups remains a challenging task. In this work, we present WordBias, an interactive visual tool designed to explore biases against intersectional groups encoded in static word embeddings. Given a pretrained static word embedding, WordBias computes the association of each word along different groups based on race, age, etc. and then visualizes them using a novel interactive interface. Using a case study, we demonstrate how WordBias can help uncover biases against intersectional groups like Black Muslim Males, Poor Females, etc. encoded in word embedding. In addition, we also evaluate our tool using qualitative feedback from expert interviews. The source code for this tool can be publicly accessed for reproducibility at [this http URL](http://github.com/bhavyaghai/WordBias).

| Comments: | Accepted to ACM SIGCHI 2021 LBW                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2103.03598](https://arxiv.org/abs/2103.03598) [cs.CL]** |
|           | (or **[arXiv:2103.03598v1](https://arxiv.org/abs/2103.03598v1) [cs.CL]** for this version) |





<h2 id="2021-03-08-3">3. Overcoming Poor Word Embeddings with Word Definitions</h2>

Title: [Overcoming Poor Word Embeddings with Word Definitions](https://arxiv.org/abs/2103.03842)

Authors: [Christopher Malon](https://arxiv.org/search/cs?searchtype=author&query=Malon%2C+C)

> Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a model could learn to use definitions, provided in natural text, to overcome this handicap. Our model's understanding of a definition is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.03842](https://arxiv.org/abs/2103.03842) [cs.CL]** |
|           | (or **[arXiv:2103.03842v1](https://arxiv.org/abs/2103.03842v1) [cs.CL]** for this version) |



# 2021-03-05

[Return to Index](#Index)



<h2 id="2021-03-05-1">1. An empirical analysis of phrase-based and neural machine translation</h2>

Title: [An empirical analysis of phrase-based and neural machine translation](https://arxiv.org/abs/2103.03108)

Authors: [Hamidreza Ghader](https://arxiv.org/search/cs?searchtype=author&query=Ghader%2C+H)

> Two popular types of machine translation (MT) are phrase-based and neural machine translation systems. Both of these types of systems are composed of multiple complex models or layers. Each of these models and layers learns different linguistic aspects of the source language. However, for some of these models and layers, it is not clear which linguistic phenomena are learned or how this information is learned. For phrase-based MT systems, it is often clear what information is learned by each model, and the question is rather how this information is learned, especially for its phrase reordering model. For neural machine translation systems, the situation is even more complex, since for many cases it is not exactly clear what information is learned and how it is learned.
> To shed light on what linguistic phenomena are captured by MT systems, we analyze the behavior of important models in both phrase-based and neural MT systems. We consider phrase reordering models from phrase-based MT systems to investigate which words from inside of a phrase have the biggest impact on defining the phrase reordering behavior. Additionally, to contribute to the interpretability of neural MT systems we study the behavior of the attention model, which is a key component in neural MT systems and the closest model in functionality to phrase reordering models in phrase-based systems. The attention model together with the encoder hidden state representations form the main components to encode source side linguistic information in neural MT. To this end, we also analyze the information captured in the encoder hidden state representations of a neural MT system. We investigate the extent to which syntactic and lexical-semantic information from the source side is captured by hidden state representations of different neural MT architectures.

| Comments: | PhD thesis, University of Amsterdam, October 2020. [this https URL](https://pure.uva.nl/ws/files/51388868/Thesis.pdf) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2103.03108](https://arxiv.org/abs/2103.03108) [cs.CL]** |
|           | (or **[arXiv:2103.03108v1](https://arxiv.org/abs/2103.03108v1) [cs.CL]** for this version) |





<h2 id="2021-03-05-2">2. An Empirical Study of End-to-end Simultaneous Speech Translation Decoding Strategies</h2>

Title: [An Empirical Study of End-to-end Simultaneous Speech Translation Decoding Strategies](https://arxiv.org/abs/2103.03233)

Authors: [Ha Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Yannick Estève](https://arxiv.org/search/cs?searchtype=author&query=Estève%2C+Y), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> This paper proposes a decoding strategy for end-to-end simultaneous speech translation. We leverage end-to-end models trained in offline mode and conduct an empirical study for two language pairs (English-to-German and English-to-Portuguese). We also investigate different output token granularities including characters and Byte Pair Encoding (BPE) units. The results show that the proposed decoding approach allows to control BLEU/Average Lagging trade-off along different latency regimes. Our best decoding settings achieve comparable results with a strong cascade model evaluated on the simultaneous translation track of IWSLT 2020 shared task.

| Comments: | This paper has been accepted for presentation at IEEE ICASSP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.03233](https://arxiv.org/abs/2103.03233) [cs.CL]** |
|           | (or **[arXiv:2103.03233v1](https://arxiv.org/abs/2103.03233v1) [cs.CL]** for this version) |













# 2021-03-04

[Return to Index](#Index)



<h2 id="2021-03-04-1">1. Random Feature Attention</h2>

Title: [Random Feature Attention](https://arxiv.org/abs/2103.02143)

Authors: [Hao Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+H), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L)

> Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.

| Comments: | ICLR 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.02143](https://arxiv.org/abs/2103.02143) [cs.CL]** |
|           | (or **[arXiv:2103.02143v1](https://arxiv.org/abs/2103.02143v1) [cs.CL]** for this version) |





<h2 id="2021-03-04-2">2. Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation</h2>

Title: [Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation](https://arxiv.org/abs/2103.02262)

Authors: [Runzhe Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+R), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> Meta-learning has been sufficiently validated to be beneficial for low-resource neural machine translation (NMT). However, we find that meta-trained NMT fails to improve the translation performance of the domain unseen at the meta-training stage. In this paper, we aim to alleviate this issue by proposing a novel meta-curriculum learning for domain adaptation in NMT. During meta-training, the NMT first learns the similar curricula from each domain to avoid falling into a bad local optimum early, and finally learns the curricula of individualities to improve the model robustness for learning domain-specific knowledge. Experimental results on 10 different low-resource domains show that meta-curriculum learning can improve the translation performance of both familiar and unfamiliar domains. All the codes and data are freely available at [this https URL](https://github.com/NLP2CT/Meta-Curriculum).

| Comments: | Accepted to AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.02262](https://arxiv.org/abs/2103.02262) [cs.CL]** |
|           | (or **[arXiv:2103.02262v1](https://arxiv.org/abs/2103.02262v1) [cs.CL]** for this version) |





<h2 id="2021-03-04-3">3. Lex2vec: making Explainable Word Embedding via Distant Supervision</h2>

Title: [Lex2vec: making Explainable Word Embedding via Distant Supervision](https://arxiv.org/abs/2103.02269)

Authors: [Fabio Celli](https://arxiv.org/search/cs?searchtype=author&query=Celli%2C+F)

> In this technical report we propose an algorithm, called Lex2vec, that exploits lexical resources to inject information into word embeddings and name the embedding dimensions by means of distant supervision. We evaluate the optimal parameters to extract a number of informative labels that is readable and has a good coverage for the embedding dimensions.

| Comments: | 3 pages, 1 figure, 1 table                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.02269](https://arxiv.org/abs/2103.02269) [cs.CL]** |
|           | (or **[arXiv:2103.02269v1](https://arxiv.org/abs/2103.02269v1) [cs.CL]** for this version) |





<h2 id="2021-03-04-4">4. NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands</h2>

Title: [NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands](https://arxiv.org/abs/2103.02523)

Authors: [Mayank Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+M), [Tathagata Chakraborti](https://arxiv.org/search/cs?searchtype=author&query=Chakraborti%2C+T), [Quchen Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Q), [David Gros](https://arxiv.org/search/cs?searchtype=author&query=Gros%2C+D), [Xi Victoria Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X+V), [Jaron Maene](https://arxiv.org/search/cs?searchtype=author&query=Maene%2C+J), [Kartik Talamadupula](https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K), [Zhongwei Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Jules White](https://arxiv.org/search/cs?searchtype=author&query=White%2C+J)

> The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line. Participants were tasked with building models that can transform descriptions of command line tasks in English to their Bash syntax. This is a report on the competition with details of the task, metrics, data, attempted solutions, and lessons learned.

| Comments: | Competition URL: [this http URL](http://ibm.biz/nlc2cmd)     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.02523](https://arxiv.org/abs/2103.02523) [cs.CL]** |
|           | (or **[arXiv:2103.02523v1](https://arxiv.org/abs/2103.02523v1) [cs.CL]** for this version) |



# 2021-03-03

[Return to Index](#Index)



<h2 id="2021-03-03-1">1. WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</h2>

Title: [WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning](https://arxiv.org/abs/2103.01913)

Authors: [Krishna Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+K), [Karthik Raman](https://arxiv.org/search/cs?searchtype=author&query=Raman%2C+K), [Jiecao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Michael Bendersky](https://arxiv.org/search/cs?searchtype=author&query=Bendersky%2C+M), [Marc Najork](https://arxiv.org/search/cs?searchtype=author&query=Najork%2C+M)

> The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information (across image and text modalities). In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset\footnote{\url{[this https URL](https://github.com/google-research-datasets/wit)}} to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.01913](https://arxiv.org/abs/2103.01913) [cs.CV]** |
|           | (or **[arXiv:2103.01913v1](https://arxiv.org/abs/2103.01913v1) [cs.CV]** for this version) |





<h2 id="2021-03-03-2">2. On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions</h2>

Title: [On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions](https://arxiv.org/abs/2103.01273)

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Recent complementary strands of research have shown that leveraging information on the data source through encoding their properties into embeddings can lead to performance increase when training a single model on heterogeneous data sources. However, it remains unclear in which situations these dataset embeddings are most effective, because they are used in a large variety of settings, languages and tasks. Furthermore, it is usually assumed that gold information on the data source is available, and that the test data is from a distribution seen during training. In this work, we compare the effect of dataset embeddings in mono-lingual settings, multi-lingual settings, and with predicted data source label in a zero-shot setting. We evaluate on three morphosyntactic tasks: morphological tagging, lemmatization, and dependency parsing, and use 104 datasets, 66 languages, and two different dataset grouping strategies. Performance increases are highest when the datasets are of the same language, and we know from which distribution the test-instance is drawn. In contrast, for setups where the data is from an unseen distribution, performance increase vanishes.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.01273](https://arxiv.org/abs/2103.01273) [cs.CL]** |
|           | (or **[arXiv:2103.01273v1](https://arxiv.org/abs/2103.01273v1) [cs.CL]** for this version) |





<h2 id="2021-03-03-3">3. Contrastive Explanations for Model Interpretability</h2>

Title: [Contrastive Explanations for Model Interpretability](https://arxiv.org/abs/2103.01378)

Authors: [Alon Jacovi](https://arxiv.org/search/cs?searchtype=author&query=Jacovi%2C+A), [Swabha Swayamdipta](https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.01378](https://arxiv.org/abs/2103.01378) [cs.CL]** |
|           | (or **[arXiv:2103.01378v1](https://arxiv.org/abs/2103.01378v1) [cs.CL]** for this version) |





<h2 id="2021-03-03-4">4. MultiSubs: A Large-scale Multimodal and Multilingual Dataset</h2>

Title: [MultiSubs: A Large-scale Multimodal and Multilingual Dataset](https://arxiv.org/abs/2103.01910)

Authors: [Josiah Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Josiel Figueiredo](https://arxiv.org/search/cs?searchtype=author&query=Figueiredo%2C+J), [Chiraag Lala](https://arxiv.org/search/cs?searchtype=author&query=Lala%2C+C), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. We show the utility of the dataset on two automatic tasks: (i) fill-in-the blank; (ii) lexical translation. Results of the human evaluation and automatic models demonstrate that images can be a useful complement to the textual context. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.01910](https://arxiv.org/abs/2103.01910) [cs.CL]** |
|           | (or **[arXiv:2103.01910v1](https://arxiv.org/abs/2103.01910v1) [cs.CL]** for this version) |







# 2021-03-02

[Return to Index](#Index)



<h2 id="2021-03-02-1">1. Generative Adversarial Transformers</h2>

Title: [Generative Adversarial Transformers](https://arxiv.org/abs/2103.01209)

Authors: [Drew A. Hudson](https://arxiv.org/search/cs?searchtype=author&query=Hudson%2C+D+A), [C. Lawrence Zitnick](https://arxiv.org/search/cs?searchtype=author&query=Zitnick%2C+C+L)

> We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at [this http URL](http://github.com/dorarad/gansformer).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.01209](https://arxiv.org/abs/2103.01209) [cs.CV]** |
|           | (or **[arXiv:2103.01209v1](https://arxiv.org/abs/2103.01209v1) [cs.CV]** for this version) |







<h2 id="2021-03-02-2">2. Token-Modification Adversarial Attacks for Natural Language Processing: A Survey</h2>

Title: [Token-Modification Adversarial Attacks for Natural Language Processing: A Survey](https://arxiv.org/abs/2103.00676)

Authors: [Tom Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+T), [Yansong Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Alsharif Abuadbba](https://arxiv.org/search/cs?searchtype=author&query=Abuadbba%2C+A), [Surya Nepal](https://arxiv.org/search/cs?searchtype=author&query=Nepal%2C+S), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W)

> There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a \textit{token-modification} attack. Each token-modification attack is defined by a specific combination of fundamental \textit{components}, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. We hope this survey will guide new researchers to this field and spark further research into the individual attack components.

| Comments: | 8 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.00676](https://arxiv.org/abs/2103.00676) [cs.CL]** |
|           | (or **[arXiv:2103.00676v1](https://arxiv.org/abs/2103.00676v1) [cs.CL]** for this version) |







<h2 id="2021-03-02-3">3. M6: A Chinese Multimodal Pretrainer</h2>

Title: [M6: A Chinese Multimodal Pretrainer](https://arxiv.org/abs/2103.00823)

Authors: [Junyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Rui Men](https://arxiv.org/search/cs?searchtype=author&query=Men%2C+R), [An Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+A), [Chang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Ming Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M), [Yichang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Peng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Ang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+A), [Le Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Xianyan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+X), [Jie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Jianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Xu Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+X), [Zhikang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Xiaodong Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+X), [Jie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Jinbao Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+J), [Huiling Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Jianxin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J), [Jin Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Yong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Wei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W), [Jingren Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [J ie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J+i), [Hongxia Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H)

> In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.

| Comments: | 12 pages, technical report                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.00823](https://arxiv.org/abs/2103.00823) [cs.CL]** |
|           | (or **[arXiv:2103.00823v1](https://arxiv.org/abs/2103.00823v1) [cs.CL]** for this version) |







# 2021-03-01

[Return to Index](#Index)



<h2 id="2021-03-01-1">1. Automated essay scoring using efficient transformer-based language models</h2>

Title: [Automated essay scoring using efficient transformer-based language models](https://arxiv.org/abs/2102.13136)

Authors: [Christopher M Ormerod](https://arxiv.org/search/cs?searchtype=author&query=Ormerod%2C+C+M), [Akanksha Malhotra](https://arxiv.org/search/cs?searchtype=author&query=Malhotra%2C+A), [Amir Jafari](https://arxiv.org/search/cs?searchtype=author&query=Jafari%2C+A)

> Automated Essay Scoring (AES) is a cross-disciplinary effort involving Education, Linguistics, and Natural Language Processing (NLP). The efficacy of an NLP model in AES tests it ability to evaluate long-term dependencies and extrapolate meaning even when text is poorly written. Large pretrained transformer-based language models have dominated the current state-of-the-art in many NLP tasks, however, the computational requirements of these models make them expensive to deploy in practice. The goal of this paper is to challenge the paradigm in NLP that bigger is better when it comes to AES. To do this, we evaluate the performance of several fine-tuned pretrained NLP models with a modest number of parameters on an AES dataset. By ensembling our models, we achieve excellent results with fewer parameters than most pretrained transformer-based models.

| Comments: | 11 pages, 1 figure, 3 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2102.13136](https://arxiv.org/abs/2102.13136) [cs.CL]** |
|           | (or **[arXiv:2102.13136v1](https://arxiv.org/abs/2102.13136v1) [cs.CL]** for this version) |





<h2 id="2021-03-01-2">2. Learning Chess Blindfolded: Evaluating Language Models on State Tracking</h2>

Title: [Learning Chess Blindfolded: Evaluating Language Models on State Tracking](https://arxiv.org/abs/2102.13249)

Authors: [Shubham Toshniwal](https://arxiv.org/search/cs?searchtype=author&query=Toshniwal%2C+S), [Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Karen Livescu](https://arxiv.org/search/cs?searchtype=author&query=Livescu%2C+K), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. "full attention". Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.13249](https://arxiv.org/abs/2102.13249) [cs.CL]** |
|           | (or **[arXiv:2102.13249v1](https://arxiv.org/abs/2102.13249v1) [cs.CL]** for this version) |



<h2 id="2021-03-01-3">3. Gradient-guided Loss Masking for Neural Machine Translation</h2>

Title: [Gradient-guided Loss Masking for Neural Machine Translation](https://arxiv.org/abs/2102.13549)

Authors: [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> To mitigate the negative effect of low quality training data on the performance of neural machine translation models, most existing strategies focus on filtering out harmful data before training starts. In this paper, we explore strategies that dynamically optimize data usage during the training process using the model's gradients on a small set of clean data. At each training step, our algorithm calculates the gradient alignment between the training data and the clean data to mask out data with negative alignment. Our method has a natural intuition: good training data should update the model parameters in a similar direction as the clean data. Experiments on three WMT language pairs show that our method brings significant improvement over strong baselines, and the improvements are generalizable across test data from different domains.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2102.13549](https://arxiv.org/abs/2102.13549) [cs.CL]** |
|           | (or **[arXiv:2102.13549v1](https://arxiv.org/abs/2102.13549v1) [cs.CL]** for this version) |