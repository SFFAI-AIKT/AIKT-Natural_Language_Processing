# Daily arXiv: Machine Translation - Jan., 2020

# Index

- [2020-02-18](#2020-02-18)
  - [1. Supervised Phrase-boundary Embeddings](#2020-02-18-1)
  - [2. Neural Machine Translation with Joint Representation](#2020-02-18-2)
  - [3. Multi-layer Representation Fusion for Neural Machine Translation](#2020-02-18-3)
  - [4. Incorporating BERT into Neural Machine Translation](#2020-02-18-4)
- [2020-02-17](#2020-02-17)
  - [1. Transformers as Soft Reasoners over Language](#2020-02-17-1)
  - [2. Transformer on a Diet](#2020-02-17-2)
- [2020-02-13](#2020-02-13)
  - [1. Superbloom: Bloom filter meets Transformer](#2020-02-13-1)
  - [2. On Layer Normalization in the Transformer Architecture](#2020-02-13-2)
- [2020-02-12](#2020-02-12)
  - [1. Learning Coupled Policies for Simultaneous Machine Translation](#2020-02-12-1)
- [2020-02-11](#2020-02-11)
  - [1. Blank Language Models](#2020-02-11-1)
  - [2. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention](#2020-02-11-2)
  - [3. Multilingual Alignment of Contextual Word Representations](#2020-02-11-3)


- [2020-02-10](#2020-02-10)

  - [1. Neural Machine Translation System of Indic Languages -- An Attention based Approach](#2020-02-10-1)
  - [2. A Multilingual View of Unsupervised Machine Translation](#2020-02-10-2)
- [2020-02-06](#2020-02-06)
- [1. Multilingual acoustic word embedding models for processing zero-resource languages](#2020-02-06-1)
  - [2. Irony Detection in a Multilingual Context](#2020-02-06-2)
- [2020-02-05](#2020-02-05)

  - [1. CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus](#2020-02-05-1)
- [2020-02-04](#2020-02-04)

  - [1. Unsupervised Bilingual Lexicon Induction Across Writing Systems](#2020-02-04-1)
  - [2. Citation Text Generation](#2020-02-04-2)
  - [3. Unsupervised Multilingual Alignment using Wasserstein Barycenter](#2020-02-04-3)
  - [4. Joint Contextual Modeling for ASR Correction and Language Understanding](#2020-02-04-4)
  - [5. FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications](#2020-02-04-5)
  - [6. Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance](#2020-02-04-6)
- [2020-02-03](#2020-02-03)

  - [1. Self-Adversarial Learning with Comparative Discrimination for Text Generation](#2020-02-03-1)
  - [2. Teaching Machines to Converse](#2020-02-03-2)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-02-18

[Return to Index](#Index)



<h2 id="2020-02-18-1">1. Supervised Phrase-boundary Embeddings</h2>

Title: [Supervised Phrase-boundary Embeddings](https://arxiv.org/abs/2002.06450)

Authors: [Manni Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M), [David Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+D), [Mark Levene](https://arxiv.org/search/cs?searchtype=author&query=Levene%2C+M)

*(Submitted on 15 Feb 2020)*

> We propose a new word embedding model, called SPhrase, that incorporates supervised phrase information. Our method modifies traditional word embeddings by ensuring that all target words in a phrase have exactly the same context. We demonstrate that including this information within a context window produces superior embeddings for both intrinsic evaluation tasks and downstream extrinsic tasks.

| Comments: | 12 pages, 3 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | [arXiv:2002.06450](https://arxiv.org/abs/2002.06450) [cs.AI] |
|           | (or [arXiv:2002.06450v1](https://arxiv.org/abs/2002.06450v1) [cs.AI] for this version) |





<h2 id="2020-02-18-2">2. Neural Machine Translation with Joint Representation</h2>

Title: [Neural Machine Translation with Joint Representation](https://arxiv.org/abs/2002.06546)

Authors: [YanYang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Tongran Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 16 Feb 2020)*

> Though early successes of Statistical Machine Translation (SMT) systems are attributed in part to the explicit modelling of the interaction between any two source and target units, e.g., alignment, the recent Neural Machine Translation (NMT) systems resort to the attention which partially encodes the interaction for efficiency. In this paper, we employ Joint Representation that fully accounts for each possible interaction. We sidestep the inefficiency issue by refining representations with the proposed efficient attention operation. The resulting Reformer models offer a new Sequence-to- Sequence modelling paradigm besides the Encoder-Decoder framework and outperform the Transformer baseline in either the small scale IWSLT14 German-English, English-German and IWSLT15 Vietnamese-English or the large scale NIST12 Chinese-English translation tasks by about 1 BLEU point.We also propose a systematic model scaling approach, allowing the Reformer model to beat the state-of-the-art Transformer in IWSLT14 German-English and NIST12 Chinese-English with about 50% fewer parameters. The code is publicly available at [this https URL](https://github.com/lyy1994/reformer).

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06546](https://arxiv.org/abs/2002.06546) [cs.CL] |
|           | (or [arXiv:2002.06546v1](https://arxiv.org/abs/2002.06546v1) [cs.CL] for this version) |





<h2 id="2020-02-18-3">3. Multi-layer Representation Fusion for Neural Machine Translation</h2>

Title: [Multi-layer Representation Fusion for Neural Machine Translation](https://arxiv.org/abs/2002.06714)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Fuxue Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Yanyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yinqiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 16 Feb 2020)*

> Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation.

| Comments: | COLING 2018                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06714](https://arxiv.org/abs/2002.06714) [cs.CL] |
|           | (or [arXiv:2002.06714v1](https://arxiv.org/abs/2002.06714v1) [cs.CL] for this version) |





<h2 id="2020-02-18-4">4. Incorporating BERT into Neural Machine Translation</h2>

Title: [Incorporating BERT into Neural Machine Translation](https://arxiv.org/abs/2002.06823)

Authors: [Jinhua Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Wengang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Houqiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 17 Feb 2020)*

> The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \url{[this https URL](https://github.com/bert-nmt/bert-nmt)}.

| Comments: | Accepted to ICLR-2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06823](https://arxiv.org/abs/2002.06823) [cs.CL] |
|           | (or [arXiv:2002.06823v1](https://arxiv.org/abs/2002.06823v1) [cs.CL] for this version) |



# 2020-02-17

[Return to Index](#Index)



<h2 id="2020-02-17-1">1. Transformers as Soft Reasoners over Language</h2>

Title: [Transformers as Soft Reasoners over Language](https://arxiv.org/abs/2002.05867)

Authors: [Peter Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P), [Oyvind Tafjord](https://arxiv.org/search/cs?searchtype=author&query=Tafjord%2C+O), [Kyle Richardson](https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+K)

*(Submitted on 14 Feb 2020)*

> AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at [this http URL](http://rule-reasoning.apps.allenai.org/)

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.05867](https://arxiv.org/abs/2002.05867) [cs.CL] |
|           | (or [arXiv:2002.05867v1](https://arxiv.org/abs/2002.05867v1) [cs.CL] for this version) |





<h2 id="2020-02-17-2">2. Transformer on a Diet</h2>

Title: [Transformer on a Diet](https://arxiv.org/abs/2002.06170)

Authors: [Chenguang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Zihao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A), [Zheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J)

*(Submitted on 14 Feb 2020)*

> Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available.

| Comments: | 6 pages, 2 tables, 1 figure                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.06170](https://arxiv.org/abs/2002.06170) [cs.CL] |
|           | (or [arXiv:2002.06170v1](https://arxiv.org/abs/2002.06170v1) [cs.CL] for this version) |





# 2020-02-13

[Return to Index](#Index)



<h2 id="2020-02-13-1">1. Superbloom: Bloom filter meets Transformer</h2>

Title: [Superbloom: Bloom filter meets Transformer](https://arxiv.org/abs/2002.04723)

Authors: [John Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+J), [Qingqing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q), [Walid Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+W), [Steffen Rendle](https://arxiv.org/search/cs?searchtype=author&query=Rendle%2C+S), [Li Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

*(Submitted on 11 Feb 2020)*

> We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.04723](https://arxiv.org/abs/2002.04723) [cs.LG] |
|           | (or [arXiv:2002.04723v1](https://arxiv.org/abs/2002.04723v1) [cs.LG] for this version) |





<h2 id="2020-02-13-2">2. On Layer Normalization in the Transformer Architecture</h2>

Title: [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)

Authors:[Ruibin Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+R), [Yunchang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Kai Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+K), [Shuxin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S), [Chen Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+C), [Huishuai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yanyan Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Y), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 12 Feb 2020)*

> The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.04745](https://arxiv.org/abs/2002.04745) [cs.LG] |
|           | (or [arXiv:2002.04745v1](https://arxiv.org/abs/2002.04745v1) [cs.LG] for this version) |



# 2020-02-12

[Return to Index](#Index)



<h2 id="2020-02-12-1">1. Learning Coupled Policies for Simultaneous Machine Translation</h2>

Title: [Learning Coupled Policies for Simultaneous Machine Translation](https://arxiv.org/abs/2002.04306)

Authors: [Philip Arthur](https://arxiv.org/search/cs?searchtype=author&query=Arthur%2C+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 11 Feb 2020)*

> Abstract: In simultaneous machine translation, the system needs to incrementally generate the output translation before the input sentence ends. This is a coupled decision process consisting of a programmer and interpreter. The programmer's policy decides about when to WRITE the next output or READ the next input, and the interpreter's policy decides what word to write. We present an imitation learning (IL) approach to efficiently learn effective coupled programmer-interpreter policies. To enable IL, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. We attribute the effectiveness of the learned coupled policies to (i) scheduled sampling addressing the coupled exposure bias, and (ii) quality of oracle actions capturing enough information from the partial input before writing the output. Experiments show our method outperforms strong baselines in terms of translation quality and delay, when translating from German/Arabic/Czech/Bulgarian/Romanian to English.

| Comments: | 7 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.04306](https://arxiv.org/abs/2002.04306) [cs.CL] |
|           | (or [arXiv:2002.04306v1](https://arxiv.org/abs/2002.04306v1) [cs.CL] for this version) |





# 2020-02-11

[Return to Index](#Index)



<h2 id="2020-02-11-1">1. Blank Language Models</h2>

Title: [Blank Language Models](https://arxiv.org/abs/2002.03079)

Authors: [Tianxiao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+T), [Victor Quach](https://arxiv.org/search/cs?searchtype=author&query=Quach%2C+V), [Regina Barzilay](https://arxiv.org/search/cs?searchtype=author&query=Barzilay%2C+R), [Tommi Jaakkola](https://arxiv.org/search/cs?searchtype=author&query=Jaakkola%2C+T)

*(Submitted on 8 Feb 2020)*

> We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.03079](https://arxiv.org/abs/2002.03079) [cs.CL] |
|           | (or [arXiv:2002.03079v1](https://arxiv.org/abs/2002.03079v1) [cs.CL] for this version) |





<h2 id="2020-02-11-2">2. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention</h2>

Title: [LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention](https://arxiv.org/abs/2002.03084)

Authors: [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Arianna Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

*(Submitted on 8 Feb 2020)*

> Non-autoregressive translation (NAT) models generate multiple tokens in one forward pass and is highly efficient at inference stage compared with autoregressive translation (AT) methods. However, NAT models often suffer from the multimodality problem, i.e., generating duplicated tokens or missing tokens. In this paper, we propose two novel methods to address this issue, the Look-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The Look-Around strategy predicts the neighbor tokens in order to predict the current token, and the Vocabulary Attention models long-term token dependencies inside the decoder by attending the whole vocabulary for each position to acquire knowledge of which token is about to generate. %We also propose a dynamic bidirectional decoding approach to accelerate the inference process of the LAVA model while preserving the high-quality of the generated output. Our proposed model uses significantly less time during inference compared with autoregressive models and most other NAT models. Our experiments on four benchmarks (WMT14 En*[Math Processing Error]*De, WMT14 De*[Math Processing Error]*En, WMT16 Ro*[Math Processing Error]*En and IWSLT14 De*[Math Processing Error]*En) show that the proposed model achieves competitive performance compared with the state-of-the-art non-autoregressive and autoregressive models while significantly reducing the time cost in inference phase.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.03084](https://arxiv.org/abs/2002.03084) [cs.CL] |
|           | (or [arXiv:2002.03084v1](https://arxiv.org/abs/2002.03084v1) [cs.CL] for this version) |





<h2 id="2020-02-11-3">3. Multilingual Alignment of Contextual Word Representations</h2>

Title: [Multilingual Alignment of Contextual Word Representations](https://arxiv.org/abs/2002.03518)

Authors: [Steven Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S), [Nikita Kitaev](https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

*(Submitted on 10 Feb 2020)*

> We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models.

| Comments: | ICLR 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.03518](https://arxiv.org/abs/2002.03518) [cs.CL] |
|           | (or [arXiv:2002.03518v1](https://arxiv.org/abs/2002.03518v1) [cs.CL] for this version) |



# 2020-02-10

[Return to Index](#Index)



<h2 id="2020-02-10-1">1. Neural Machine Translation System of Indic Languages -- An Attention based Approach</h2>

Title: [Neural Machine Translation System of Indic Languages -- An Attention based Approach]()

Authors: [Parth Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+P), [Vishvajit Bakrola](https://arxiv.org/search/cs?searchtype=author&query=Bakrola%2C+V)

*(Submitted on 2 Feb 2020)*

> Neural machine translation (NMT) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques. Proposed neural machine translation model developed for the Gujarati language contains encoder-decoder with attention mechanism. In India, almost all the languages are originated from their ancestral language - Sanskrit. They are having inevitable similarities including lexical and named entity similarity. Translating into Indic languages is always be a challenging task. In this paper, we have presented the neural machine translation system (NMT) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58.49 percentage of total speakers in the country. We have compared the performance of our NMT model with automatic evaluation matrices such as BLEU, perplexity and TER matrix. The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 BLEU score on English-Gujarati translation.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP), Gangtok, India, 2019, pp. 1-5 |
| DOI:               | [10.1109/ICACCP.2019.8882969](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FICACCP.2019.8882969&v=a230a5bc) |
| Cite as:           | [arXiv:2002.02758](https://arxiv.org/abs/2002.02758) [cs.CL] |
|                    | (or [arXiv:2002.02758v1](https://arxiv.org/abs/2002.02758v1) [cs.CL] for this version) |







<h2 id="2020-02-10-2">2. A Multilingual View of Unsupervised Machine Translation</h2>

Title: [A Multilingual View of Unsupervised Machine Translation](https://arxiv.org/abs/2002.02955)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Pierre Foret](https://arxiv.org/search/cs?searchtype=author&query=Foret%2C+P), [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

*(Submitted on 7 Feb 2020)*

> We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT'14 English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in most directions. In particular, we obtain a +1.65 BLEU advantage over the best-performing unsupervised model in the Romanian-English direction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.02955](https://arxiv.org/abs/2002.02955) [cs.CL] |
|           | (or [arXiv:2002.02955v1](https://arxiv.org/abs/2002.02955v1) [cs.CL] for this version) |







# 2020-02-06

[Return to Index](#Index)



<h2 id="2020-02-06-1">1. Multilingual acoustic word embedding models for processing zero-resource languages</h2>

Title: [Multilingual acoustic word embedding models for processing zero-resource languages](https://arxiv.org/abs/2002.02109)

Authors: [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Yevgen Matusevych](https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

*(Submitted on 6 Feb 2020)*

> Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. In settings where unlabelled speech is the only available resource, such embeddings can be used in "zero-resource" speech search, indexing and discovery systems. Here we propose to train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. For this transfer learning approach, we consider two multilingual recurrent neural network models: a discriminative classifier trained on the joint vocabularies of all training languages, and a correspondence autoencoder trained to reconstruct word pairs. We test these using a word discrimination task on six target zero-resource languages. When trained on seven well-resourced languages, both models perform similarly and outperform unsupervised models trained on the zero-resource languages. With just a single training language, the second model works better, but performance depends more on the particular training--testing language pair.

| Comments: | 5 pages, 4 figures, 1 table; accepted to ICASSP 2020. arXiv admin note: text overlap with [arXiv:1811.00403](https://arxiv.org/abs/1811.00403) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2002.02109](https://arxiv.org/abs/2002.02109) [cs.CL] |
|           | (or [arXiv:2002.02109v1](https://arxiv.org/abs/2002.02109v1) [cs.CL] for this version) |





<h2 id="2020-02-06-2">2. Irony Detection in a Multilingual Context</h2>

Title: [Irony Detection in a Multilingual Context](https://arxiv.org/abs/2002.02427)

Authors: [Bilal Ghanem](https://arxiv.org/search/cs?searchtype=author&query=Ghanem%2C+B), [Jihen Karoui](https://arxiv.org/search/cs?searchtype=author&query=Karoui%2C+J), [Farah Benamara](https://arxiv.org/search/cs?searchtype=author&query=Benamara%2C+F), [Paolo Rosso](https://arxiv.org/search/cs?searchtype=author&query=Rosso%2C+P), [Véronique Moriceau](https://arxiv.org/search/cs?searchtype=author&query=Moriceau%2C+V)

*(Submitted on 6 Feb 2020)*

> This paper proposes the first multilingual (French, English and Arabic) and multicultural (Indo-European languages vs. less culturally close languages) irony detection system. We employ both feature-based models and neural architectures using monolingual word representation. We compare the performance of these systems with state-of-the-art systems to identify their capabilities. We show that these monolingual models trained separately on different languages using multilingual word representation or text-based features can open the door to irony detection in languages that lack of annotated data for irony.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.02427](https://arxiv.org/abs/2002.02427) [cs.CL] |
|           | (or [arXiv:2002.02427v1](https://arxiv.org/abs/2002.02427v1) [cs.CL] for this version) |



# 2020-02-05

[Return to Index](#Index)



<h2 id="2020-02-05-1">1. CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus</h2>

Title: [CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus](https://arxiv.org/abs/2002.01320)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 4 Feb 2020)*

> Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets involve language pairs with English as a source language, involve very specific domains or are low resource. We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free to use. We also provide additional evaluation data derived from Tatoeba under CC licenses.

| Comments: | Submitted to LREC 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.01320](https://arxiv.org/abs/2002.01320) [cs.CL] |
|           | (or [arXiv:2002.01320v1](https://arxiv.org/abs/2002.01320v1) [cs.CL] for this version) |



# 2020-02-04

[Return to Index](#Index)



<h2 id="2020-02-04-1">1. Unsupervised Bilingual Lexicon Induction Across Writing Systems</h2>

Title: [Unsupervised Bilingual Lexicon Induction Across Writing Systems](https://arxiv.org/abs/2002.00037)

Authors: [Parker Riley](https://arxiv.org/search/cs?searchtype=author&query=Riley%2C+P), [Daniel Gildea](https://arxiv.org/search/cs?searchtype=author&query=Gildea%2C+D)

*(Submitted on 31 Jan 2020)*

> Recent embedding-based methods in unsupervised bilingual lexicon induction have shown good results, but generally have not leveraged orthographic (spelling) information, which can be helpful for pairs of related languages. This work augments a state-of-the-art method with orthographic features, and extends prior work in this space by proposing methods that can learn and utilize orthographic correspondences even between languages with different scripts. We demonstrate this by experimenting on three language pairs with different scripts and varying degrees of lexical similarity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00037](https://arxiv.org/abs/2002.00037) [cs.CL] |
|           | (or [arXiv:2002.00037v1](https://arxiv.org/abs/2002.00037v1) [cs.CL] for this version) |





<h2 id="2020-02-04-2">2. Citation Text Generation</h2>

Title: [Citation Text Generation](https://arxiv.org/abs/2002.00317)

Authors: [Kelvin Luu](https://arxiv.org/search/cs?searchtype=author&query=Luu%2C+K), [Rik Koncel-Kedziorski](https://arxiv.org/search/cs?searchtype=author&query=Koncel-Kedziorski%2C+R), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Isabel Cachola](https://arxiv.org/search/cs?searchtype=author&query=Cachola%2C+I), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 2 Feb 2020)*

> We introduce the task of citation text generation: given a pair of scientific documents, explain their relationship in natural language text in the manner of a citation from one text to the other. This task encourages systems to learn rich relationships between scientific texts and to express them concretely in natural language. Models for citation text generation will require robust document understanding including the capacity to quickly adapt to new vocabulary and to reason about document content. We believe this challenging direction of research will benefit high-impact applications such as automatic literature review or scientific writing assistance systems. In this paper we establish the task of citation text generation with a standard evaluation corpus and explore several baseline models.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.00317](https://arxiv.org/abs/2002.00317) [cs.CL] |
|           | (or [arXiv:2002.00317v1](https://arxiv.org/abs/2002.00317v1) [cs.CL] for this version) |





<h2 id="2020-02-04-3">3. Unsupervised Multilingual Alignment using Wasserstein Barycenter</h2>

Title: [Unsupervised Multilingual Alignment using Wasserstein Barycenter](https://arxiv.org/abs/2002.00743)

Authors: [Xin Lian](https://arxiv.org/search/cs?searchtype=author&query=Lian%2C+X), [Kshitij Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+K), [Jakub Truszkowski](https://arxiv.org/search/cs?searchtype=author&query=Truszkowski%2C+J), [Pascal Poupart](https://arxiv.org/search/cs?searchtype=author&query=Poupart%2C+P), [Yaoliang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y)

*(Submitted on 28 Jan 2020)*

> We study unsupervised multilingual alignment, the problem of finding word-to-word translations between multiple languages without using any parallel data. One popular strategy is to reduce multilingual alignment to the much simplified bilingual setting, by picking one of the input languages as the pivot language that we transit through. However, it is well-known that transiting through a poorly chosen pivot language (such as English) may severely degrade the translation quality, since the assumed transitive relations among all pairs of languages may not be enforced in the training process. Instead of going through a rather arbitrarily chosen pivot language, we propose to use the Wasserstein barycenter as a more informative ''mean'' language: it encapsulates information from all languages and minimizes all pairwise transportation costs. We evaluate our method on standard benchmarks and demonstrate state-of-the-art performances.

| Comments: | Work in progress; comments welcome!                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2002.00743](https://arxiv.org/abs/2002.00743) [cs.CL] |
|           | (or [arXiv:2002.00743v1](https://arxiv.org/abs/2002.00743v1) [cs.CL] for this version) |





<h2 id="2020-02-04-4">4. Joint Contextual Modeling for ASR Correction and Language Understanding</h2>

Title: [Joint Contextual Modeling for ASR Correction and Language Understanding](https://arxiv.org/abs/2002.00750)

Authors: [Yue Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+Y), [Sai Sumanth Miryala](https://arxiv.org/search/cs?searchtype=author&query=Miryala%2C+S+S), [Chandra Khatri](https://arxiv.org/search/cs?searchtype=author&query=Khatri%2C+C), [Runze Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Huaixiu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Piero Molino](https://arxiv.org/search/cs?searchtype=author&query=Molino%2C+P), [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [Alexandros Papangelis](https://arxiv.org/search/cs?searchtype=author&query=Papangelis%2C+A), [Hugh Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+H), [Franziska Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+F), [Gokhan Tur](https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G)

*(Submitted on 28 Jan 2020)*

> The quality of automatic speech recognition (ASR) is critical to Dialogue Systems as ASR errors propagate to and directly impact downstream tasks such as language understanding (LU). In this paper, we propose multi-task neural approaches to perform contextual language correction on ASR outputs jointly with LU to improve the performance of both tasks simultaneously. To measure the effectiveness of this approach we used a public benchmark, the 2nd Dialogue State Tracking (DSTC2) corpus. As a baseline approach, we trained task-specific Statistical Language Models (SLM) and fine-tuned state-of-the-art Generalized Pre-training (GPT) Language Model to re-rank the n-best ASR hypotheses, followed by a model to identify the dialog act and slots. i) We further trained ranker models using GPT and Hierarchical CNN-RNN models with discriminatory losses to detect the best output given n-best hypotheses. We extended these ranker models to first select the best ASR output and then identify the dialogue act and slots in an end to end fashion. ii) We also proposed a novel joint ASR error correction and LU model, a word confusion pointer network (WCN-Ptr) with multi-head self-attention on top, which consumes the word confusions populated from the n-best. We show that the error rates of off the shelf ASR and following LU systems can be reduced significantly by 14% relative with joint models trained using small amounts of in-domain data.

| Comments: | Accepted at IEEE ICASSP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2002.00750](https://arxiv.org/abs/2002.00750) [cs.CL] |
|           | (or [arXiv:2002.00750v1](https://arxiv.org/abs/2002.00750v1) [cs.CL] for this version) |





<h2 id="2020-02-04-5">5. FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications</h2>

Title: [FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications](https://arxiv.org/abs/2002.00760)

Authors: [Dou Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+D), [Lv Zhonghou](https://arxiv.org/search/cs?searchtype=author&query=Zhonghou%2C+L), [Wang minghua](https://arxiv.org/search/cs?searchtype=author&query=minghua%2C+W)

*(Submitted on 31 Jan 2020)*

> In this paper, we present a novel algorithm, FastWordBug, to efficiently generate small text perturbations in a black-box setting that forces a sentiment analysis or text classification mode to make an incorrect prediction. By combining the part of speech attributes of words, we propose a scoring method that can quickly identify important words that affect text classification. We evaluate FastWordBug on three real-world text datasets and two state-of-the-art machine learning models under black-box setting. The results show that our method can significantly reduce the accuracy of the model, and at the same time, we can call the model as little as possible, with the highest attack efficiency. We also attack two popular real-world cloud services of NLP, and the results show that our method works as well.

| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00760](https://arxiv.org/abs/2002.00760) [cs.CL] |
|           | (or [arXiv:2002.00760v1](https://arxiv.org/abs/2002.00760v1) [cs.CL] for this version) |





<h2 id="2020-02-04-6">6. Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance</h2>

Title: [Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance](https://arxiv.org/abs/2002.00761)

Authors: [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F)

*(Submitted on 31 Jan 2020)*

> Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel bitexts for machine translation training. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low, mid, and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents, we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs, 15% on mid-resource language pairs, and 22% on low-resource language pairs

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00761](https://arxiv.org/abs/2002.00761) [cs.CL] |
|           | (or [arXiv:2002.00761v1](https://arxiv.org/abs/2002.00761v1) [cs.CL] for this version) |



# 2020-02-03
[Return to Index](#Index)



<h2 id="2020-02-03-1">1. Self-Adversarial Learning with Comparative Discrimination for Text Generation</h2>

Title: [Self-Adversarial Learning with Comparative Discrimination for Text Generation](https://arxiv.org/abs/2001.11691)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 31 Jan 2020)*

> Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.

| Comments: | to be published in ICLR 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.11691](https://arxiv.org/abs/2001.11691) [cs.CL] |
|           | (or [arXiv:2001.11691v1](https://arxiv.org/abs/2001.11691v1) [cs.CL] for this version) |





<h2 id="2020-02-03-2">2. Teaching Machines to Converse</h2>

Title: [Teaching Machines to Converse](https://arxiv.org/abs/2001.11701)

Authors: [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

*(Submitted on 31 Jan 2020)*

> The ability of a machine to communicate with humans has long been associated with the general success of AI. This dates back to Alan Turing's epoch-making work in the early 1950s, which proposes that a machine's intelligence can be tested by how well it, the machine, can fool a human into believing that the machine is a human through dialogue conversations. Many systems learn generation rules from a minimal set of authored rules or labels on top of hand-coded rules or templates, and thus are both expensive and difficult to extend to open-domain scenarios. Recently, the emergence of neural network models the potential to solve many of the problems in dialogue learning that earlier systems cannot tackle: the end-to-end neural frameworks offer the promise of scalability and language-independence, together with the ability to track the dialogue state and then mapping between states and dialogue actions in a way not possible with conventional systems. On the other hand, neural systems bring about new challenges: they tend to output dull and generic responses; they lack a consistent or a coherent persona; they are usually optimized through single-turn conversations and are incapable of handling the long-term success of a conversation; and they are not able to take the advantage of the interactions with humans. This dissertation attempts to tackle these challenges: Contributions are two-fold: (1) we address new challenges presented by neural network models in open-domain dialogue generation systems; (2) we develop interactive question-answering dialogue systems by (a) giving the agent the ability to ask questions and (b) training a conversation agent through interactions with humans in an online fashion, where a bot improves through communicating with humans and learning from the mistakes that it makes.

| Comments: | phd thesis                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.11701](https://arxiv.org/abs/2001.11701) [cs.CL] |
|           | (or [arXiv:2001.11701v1](https://arxiv.org/abs/2001.11701v1) [cs.CL] for this version) |


