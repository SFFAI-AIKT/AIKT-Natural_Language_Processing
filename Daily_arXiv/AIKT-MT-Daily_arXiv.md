# Daily arXiv: Machine Translation - Dec., 2019

# Index

- [2019-12-30](#2019-12-30)
  - [1. A Study of Multilingual Neural Machine Translation](#2019-12-30-1)
  - [2. Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation](#2019-12-30-2)
  - [3. Explicit Sentence Compression for Neural Machine Translation](#2019-12-30-3)
  - [4. Visual Agreement Regularized Training for Multi-Modal Machine Translation](#2019-12-30-4)
- [2019-12-24](#2019-12-24)
  - [1. Machine Translation with Cross-lingual Word Embeddings](#2019-12-24-1)
  - [2. Two Way Adversarial Unsupervised Word Translation](#2019-12-24-2)
  - [3. A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings](#2019-12-24-3)
  - [4. Tag-less Back-Translation](#2019-12-24-4)
  - [5. Siamese Networks for Large-Scale Author Identification](#2019-12-24-5)
- [2019-12-23](#2019-12-23)
  - [1. Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin using Recursive Neural Networks](#2019-12-23-1)
- [2019-12-19](#2019-12-19)
  - [1. MALA: Cross-Domain Dialogue Generation with Action Learning](#2019-12-19-1)
  - [2. A Survey on Document-level Machine Translation: Methods and Evaluation](#2019-12-19-2)
- [2019-12-18](#2019-12-18)
  - [1. Cross-Lingual Ability of Multilingual BERT: An Empirical Study](#2019-12-18)
- [2019-12-17](#2019-12-17)
  - [1. Iterative Dual Domain Adaptation for Neural Machine Translation](#2019-12-17-1)
  - [2. Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding](#2019-12-17-2)
- [2019-12-16](#2019-12-16)
  - [1. Document Sub-structure in Neural Machine Translation](#2019-12-16-1)
- [2019-12-13](#2019-12-13)
  - [1. Improving Interpretability of Word Embeddings by Generating Definition and Usage](#2019-12-13-1)
- [2019-12-12](#2019-12-12)
  - [1. Lifelong learning for text retrieval and recognition in historical handwritten document collections](#2019-12-12-1)
  - [2. Unsupervised Neural Dialect Translation with Commonality and Diversity Modeling](#2019-12-12-2)
  - [3. Automatic Spanish Translation of the SQuAD Dataset for Multilingual Question Answering](#2019-12-12-3)
  - [4. MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low Resource Machine Translation](#2019-12-12-4)
- [2019-12-11](#2019-12-11)
  - [1. Cross-Language Aphasia Detection using Optimal Transport Domain Adaptation](#2019-12-11-1)
  - [2. GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies](#2019-12-11-2)
- [2019-12-10](#2019-12-10)
  - [1. Bidirectional Scene Text Recognition with a Single Decoder](#2019-12-10-1)
  - [2. Explaining Sequence-Level Knowledge Distillation as Data-Augmentation for Neural Machine Translation](#2019-12-10-2)
  - [3. Re-Translation Strategies For Long Form, Simultaneous, Spoken Language Translation](#2019-12-10-3)
  - [4. PidginUNMT: Unsupervised Neural Machine Translation from West African Pidgin to English](#2019-12-10-4)
- [2019-12-09](#2019-12-09)
  - [1. Machine Translation Evaluation Meets Community Question Answering](#2019-12-09-1)
  - [2. Pairwise Neural Machine Translation Evaluation](#2019-12-09-2)
- [2019-12-06](#2019-12-06)
  - [1. Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar](#2019-12-06-1)
- [2019-12-05](#2019-12-05)
  - [1. Neural Machine Translation: A Review](#2019-12-05-1)
  - [2. A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well](#2019-12-05-2)
  - [3. Acquiring Knowledge from Pre-trained Model to Neural Machine Translation](#2019-12-05-3)
- [2019-12-04](#2019-12-04)
  - [1. Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation](#2019-12-04-1)
- [2019-12-03](#2019-12-03)
  - [1. Not All Attention Is Needed: Gated Attention Network for Sequence Data](#2019-12-03-1)
  - [2. Modeling Fluency and Faithfulness for Diverse Neural Machine Translation](#2019-12-03-2)
  - [3. Merging External Bilingual Pairs into Neural Machine Translation](#2019-12-03-3)
- [2019-12-02](#2019-12-02)
  - [1. DiscoTK: Using Discourse Structure for Machine Translation Evaluation](#2019-12-02-1)
  - [2. Multimodal Machine Translation through Visuals and Speech](#2019-12-02-2)
  - [3. GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors](#2019-12-02-3)
  - [4. Neural Chinese Word Segmentation as Sequence to Sequence Translation](#2019-12-02-4)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-12-30

[Return to Index](#Index)



<h2 id="2019-12-30-1">1. A Study of Multilingual Neural Machine Translation</h2>

Title: [A Study of Multilingual Neural Machine Translation](https://arxiv.org/abs/1912.11625)

Authors: [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Yichong Leng](https://arxiv.org/search/cs?searchtype=author&query=Leng%2C+Y), [Jiale Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Yi Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 25 Dec 2019)*

> Multilingual neural machine translation (NMT) has recently been investigated from different aspects (e.g., pivot translation, zero-shot translation, fine-tuning, or training from scratch) and in different settings (e.g., rich resource and low resource, one-to-many, and many-to-one translation). This paper concentrates on a deep understanding of multilingual NMT and conducts a comprehensive study on a multilingual dataset with more than 20 languages. Our results show that (1) low-resource language pairs benefit much from multilingual training, while rich-resource language pairs may get hurt under limited model capacity and training with similar languages benefits more than dissimilar languages; (2) fine-tuning performs better than training from scratch in the one-to-many setting while training from scratch performs better in the many-to-one setting; (3) the bottom layers of the encoder and top layers of the decoder capture more language-specific information, and just fine-tuning these parts can achieve good accuracy for low-resource language pairs; (4) direct translation is better than pivot translation when the source language is similar to the target language (e.g., in the same language branch), even when the size of direct training data is much smaller; (5) given a fixed training data budget, it is better to introduce more languages into multilingual training for zero-shot translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.11625](https://arxiv.org/abs/1912.11625) [cs.CL] |
|           | (or [arXiv:1912.11625v1](https://arxiv.org/abs/1912.11625v1) [cs.CL] for this version) |





<h2 id="2019-12-30-2">2. Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation</h2>

Title: [Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation](https://arxiv.org/abs/1912.11739)

Authors: [Haiyue Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H), [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

*(Submitted on 26 Dec 2019)*

> Lectures translation is a case of spoken language translation and there is a lack of publicly available parallel corpora for this purpose. To address this, we examine a language independent framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on machine translation and cosine similarity over continuous-space sentence representations. We also show how to use the resulting corpora in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For Japanese--English lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we will release our code for parallel data creation.

| Comments: | 10 pages, 1 figure, 9 tables, under review by LREC2020       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.11739](https://arxiv.org/abs/1912.11739) [cs.CL] |
|           | (or [arXiv:1912.11739v1](https://arxiv.org/abs/1912.11739v1) [cs.CL] for this version) |





<h2 id="2019-12-30-3">3. Explicit Sentence Compression for Neural Machine Translation</h2>

Title: [Explicit Sentence Compression for Neural Machine Translation](https://arxiv.org/abs/1912.11980)

Authors: [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

*(Submitted on 27 Dec 2019)*

> State-of-the-art Transformer-based neural machine translation (NMT) systems still follow a standard encoder-decoder framework, in which source sentence representation can be well done by an encoder with self-attention mechanism. Though Transformer-based encoder may effectively capture general information in its resulting source sentence representation, the backbone information, which stands for the gist of a sentence, is not specifically focused on. In this paper, we propose an explicit sentence compression method to enhance the source sentence representation for NMT. In practice, an explicit sentence compression goal used to learn the backbone information in a sentence. We propose three ways, including backbone source-side fusion, target-side fusion, and both-side fusion, to integrate the compressed sentence into NMT. Our empirical tests on the WMT English-to-French and English-to-German translation tasks show that the proposed sentence compression method significantly improves the translation performances over strong baselines.

| Comments: | Working in progress, part of this work is accepted in AAAI-2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.11980](https://arxiv.org/abs/1912.11980) [cs.CL] |
|           | (or [arXiv:1912.11980v1](https://arxiv.org/abs/1912.11980v1) [cs.CL] for this version) |





<h2 id="2019-12-30-4">4. Visual Agreement Regularized Training for Multi-Modal Machine Translation</h2>

Title: [Visual Agreement Regularized Training for Multi-Modal Machine Translation](https://arxiv.org/abs/1912.12014)

Authors: [Pengcheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+P), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Pei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

*(Submitted on 27 Dec 2019)*

> Multi-modal machine translation aims at translating the source sentence into a different language in the presence of the paired image. Previous work suggests that additional visual information only provides dispensable help to translation, which is needed in several very special cases such as translating ambiguous words. To make better use of visual information, this work presents visual agreement regularized training. The proposed approach jointly trains the source-to-target and target-to-source translation models and encourages them to share the same focus on the visual information when generating semantically equivalent visual words (e.g. "ball" in English and "ballon" in French). Besides, a simple yet effective multi-head co-attention model is also introduced to capture interactions between visual and textual features. The results show that our approaches can outperform competitive baselines by a large margin on the Multi30k dataset. Further analysis demonstrates that the proposed regularized training can effectively improve the agreement of attention on the image, leading to better use of visual information.

| Comments: | Accepted by AAAI 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | [arXiv:1912.12014](https://arxiv.org/abs/1912.12014) [cs.CL] |
|           | (or [arXiv:1912.12014v1](https://arxiv.org/abs/1912.12014v1) [cs.CL] for this version) |









# 2019-12-24

[Return to Index](#Index)



<h2 id="2019-12-24-1">1. Machine Translation with Cross-lingual Word Embeddings</h2>
Title: [Machine Translation with Cross-lingual Word Embeddings](https://arxiv.org/abs/1912.10167)

Authors: [Marco Berlot](https://arxiv.org/search/cs?searchtype=author&query=Berlot%2C+M), [Evan Kaplan](https://arxiv.org/search/cs?searchtype=author&query=Kaplan%2C+E)

*(Submitted on 10 Dec 2019)*

> Learning word embeddings using distributional information is a task that has been studied by many researchers, and a lot of studies are reported in the literature. On the contrary, less studies were done for the case of multiple languages. The idea is to focus on a single representation for a pair of languages such that semantically similar words are closer to one another in the induced representation irrespective of the language. In this way, when data are missing for a particular language, classifiers from another language can be used.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.10167](https://arxiv.org/abs/1912.10167) [cs.CL] |
|           | (or [arXiv:1912.10167v1](https://arxiv.org/abs/1912.10167v1) [cs.CL] for this version) |





<h2 id="2019-12-24-2">2. Two Way Adversarial Unsupervised Word Translation</h2>
Title: [Two Way Adversarial Unsupervised Word Translation](https://arxiv.org/abs/1912.10168)

Authors: [Blaine Cole](https://arxiv.org/search/cs?searchtype=author&query=Cole%2C+B)

*(Submitted on 12 Dec 2019)*

> Word translation is a problem in machine translation that seeks to build models that recover word level correspondence between languages. Recent approaches to this problem have shown that word translation models can learned with very small seeding dictionaries, and even without any starting supervision. In this paper we propose a method to jointly find translations between a pair of languages. Not only does our method learn translations in both directions but it improves accuracy of those translations over past methods.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.10168](https://arxiv.org/abs/1912.10168) [cs.CL] |
|           | (or [arXiv:1912.10168v1](https://arxiv.org/abs/1912.10168v1) [cs.CL] for this version) |





<h2 id="2019-12-24-3">3. A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings</h2>
Title: [A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings](https://arxiv.org/abs/1912.10169)

Authors: [Niels van der Heijden](https://arxiv.org/search/cs?searchtype=author&query=van+der+Heijden%2C+N), [Samira Abnar](https://arxiv.org/search/cs?searchtype=author&query=Abnar%2C+S), [Ekaterina Shutova](https://arxiv.org/search/cs?searchtype=author&query=Shutova%2C+E)

*(Submitted on 15 Dec 2019)*

> The lack of annotated data in many languages is a well-known challenge within the field of multilingual natural language processing (NLP). Therefore, many recent studies focus on zero-shot transfer learning and joint training across languages to overcome data scarcity for low-resource languages. In this work we (i) perform a comprehensive comparison of state-ofthe-art multilingual word and sentence encoders on the tasks of named entity recognition (NER) and part of speech (POS) tagging; and (ii) propose a new method for creating multilingual contextualized word embeddings, compare it to multiple baselines and show that it performs at or above state-of-theart level in zero-shot transfer settings. Finally, we show that our method allows for better knowledge sharing across languages in a joint training setting.

| Comments: | 7 pages, 6 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.10169](https://arxiv.org/abs/1912.10169) [cs.CL] |
|           | (or [arXiv:1912.10169v1](https://arxiv.org/abs/1912.10169v1) [cs.CL] for this version) |





<h2 id="2019-12-24-4">4. Tag-less Back-Translation</h2>
Title: [Tag-less Back-Translation](https://arxiv.org/abs/1912.10514)

Authors: [Idris Abdulmumin](https://arxiv.org/search/cs?searchtype=author&query=Abdulmumin%2C+I), [Bashir Shehu Galadanci](https://arxiv.org/search/cs?searchtype=author&query=Galadanci%2C+B+S), [Aliyu Garba](https://arxiv.org/search/cs?searchtype=author&query=Garba%2C+A)

*(Submitted on 22 Dec 2019)*

> An effective method to generate a large number of parallel sentences for training improved neural machine translation (NMT) systems is the use of back-translations of the target-side monolingual data. Tagging, or using gates, has been used to enable translation models to distinguish between synthetic and natural data. This improves standard back-translation and also enables the use of iterative back-translation on language pairs that underperformed using standard back-translation. This work presents a simplified approach of differentiating between the two data using pretraining and finetuning. The approach - tag-less back-translation - trains the model on the synthetic data and finetunes it on the natural data. Preliminary experiments have shown the approach to continuously outperform the tagging approach on low resource English-Vietnamese neural machine translation. While the need for tagging (noising) the dataset has been removed, the approach outperformed the tagged back-translation approach by an average of 0.4 BLEU.

| Comments: | Submitted to 2020 International Conference on Computer and Information Sciences, 5 pages, 2 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.10514](https://arxiv.org/abs/1912.10514) [cs.CL] |
|           | (or [arXiv:1912.10514v1](https://arxiv.org/abs/1912.10514v1) [cs.CL] for this version) |





<h2 id="2019-12-24-5">5. Siamese Networks for Large-Scale Author Identification</h2>
Title: [Siamese Networks for Large-Scale Author Identification](https://arxiv.org/abs/1912.10616)

Authors: [Chakaveh Saedi](https://arxiv.org/search/cs?searchtype=author&query=Saedi%2C+C), [Mark Dras](https://arxiv.org/search/cs?searchtype=author&query=Dras%2C+M)

*(Submitted on 23 Dec 2019)*

> Authorship attribution is the process of identifying the author of a text. Classification-based approaches work well for small numbers of candidate authors, but only similarity-based methods are applicable for larger numbers of authors or for authors beyond the training set. While deep learning methods have been applied to classification-based approaches, current similarity-based methods only embody static notions of similarity. Siamese networks have been used to develop learned notions of similarity in one-shot image tasks, and also for tasks of semantic relatedness in NLP. We examine their application to the stylistic task of authorship attribution, and show that they can substantially outperform both classification- and existing similarity-based approaches on datasets with large numbers of authors.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.10616](https://arxiv.org/abs/1912.10616) [cs.CL] |
|           | (or [arXiv:1912.10616v1](https://arxiv.org/abs/1912.10616v1) [cs.CL] for this version) |







# 2019-12-23

[Return to Index](#Index)



<h2 id="2019-12-23-1">1. Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin using Recursive Neural Networks</h2>
Title: [Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin using Recursive Neural Networks](https://arxiv.org/abs/1912.09913)

Authors: [Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M), [Gia H. Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+G+H), [Nancy F. Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+N+F)

*(Submitted on 20 Dec 2019)*

> Logographs (Chinese characters) have recursive structures (i.e. hierarchies of sub-units in logographs) that contain phonological and semantic information, as developmental psychology literature suggests that native speakers leverage on the structures to learn how to read. Exploiting these structures could potentially lead to better embeddings that can benefit many downstream tasks. We propose building hierarchical logograph (character) embeddings from logograph recursive structures using treeLSTM, a recursive neural network. Using recursive neural network imposes a prior on the mapping from logographs to embeddings since the network must read in the sub-units in logographs according to the order specified by the recursive structures. Based on human behavior in language learning and reading, we hypothesize that modeling logographs' structures using recursive neural network should be beneficial. To verify this claim, we consider two tasks (1) predicting logographs' Cantonese pronunciation from logographic structures and (2) language modeling. Empirical results show that the proposed hierarchical embeddings outperform baseline approaches. Diagnostic analysis suggests that hierarchical embeddings constructed using treeLSTM is less sensitive to distractors, thus is more robust, especially on complex logographs.

| Comments: | Accepted by IEEE Transactions on Audio, Speech and Language Processing. Copyright 2019 IEEE |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1109/TASLP.2019.2955246](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FTASLP.2019.2955246&v=0584ccbf) |
| Cite as:  | [arXiv:1912.09913](https://arxiv.org/abs/1912.09913) [cs.CL] |
|           | (or [arXiv:1912.09913v1](https://arxiv.org/abs/1912.09913v1) [cs.CL] for this version) |





# 2019-12-19

[Return to Index](#Index)



<h2 id="2019-12-19-1">1. MALA: Cross-Domain Dialogue Generation with Action Learning</h2>
Title:  [MALA: Cross-Domain Dialogue Generation with Action Learning](https://arxiv.org/abs/1912.08442)

Authors: [Xinting Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jianzhong Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+J), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Rui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R)

*(Submitted on 18 Dec 2019)*

> Response generation for task-oriented dialogues involves two basic components: dialogue planning and surface realization. These two components, however, have a discrepancy in their objectives, i.e., task completion and language quality. To deal with such discrepancy, conditioned response generation has been introduced where the generation process is factorized into action decision and language generation via explicit action representations. To obtain action representations, recent studies learn latent actions in an unsupervised manner based on the utterance lexical similarity. Such an action learning approach is prone to diversities of language surfaces, which may impinge task completion and language quality. To address this issue, we propose multi-stage adaptive latent action learning (MALA) that learns semantic latent actions by distinguishing the effects of utterances on dialogue progress. We model the utterance effect using the transition of dialogue states caused by the utterance and develop a semantic similarity measurement that estimates whether utterances have similar effects. For learning semantic actions on domains without dialogue states, MsALA extends the semantic similarity measurement across domains progressively, i.e., from aligning shared actions to learning domain-specific actions. Experiments using multi-domain datasets, SMD and MultiWOZ, show that our proposed model achieves consistent improvements over the baselines models in terms of both task completion and language quality.

| Comments: | 9 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.08442](https://arxiv.org/abs/1912.08442) [cs.CL] |
|           | (or [arXiv:1912.08442v1](https://arxiv.org/abs/1912.08442v1) [cs.CL] for this version) |





<h2 id="2019-12-19-2">2. A Survey on Document-level Machine Translation: Methods and Evaluation</h2>
Title:  [A Survey on Document-level Machine Translation: Methods and Evaluation](https://arxiv.org/abs/1912.08494)

Authors: [Sameen Maruf](https://arxiv.org/search/cs?searchtype=author&query=Maruf%2C+S), [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 18 Dec 2019)*

> Machine translation (MT) is an important task in natural language processing (NLP) as it automates the translation process and reduces the reliance on human translators. With the advent of neural networks, the translation quality surpasses that of the translations obtained using statistical techniques. Up until three years ago, all neural translation models translated sentences independently, without incorporating any extra-sentential information. The aim of this paper is to highlight the major works that have been undertaken in the space of document-level machine translation before and after the neural revolution so that researchers can recognise where we started from and which direction we are heading in. When talking about the literature in statistical machine translation (SMT), we focus on works which have tried to improve the translation of specific discourse phenomena, while in neural machine translation (NMT), we focus on works which use the wider context explicitly. In addition to this, we also cover the evaluation strategies that have been introduced to account for the improvements in this domain.

| Comments: | This article is under-review at an international journal. This arXiv version has been made available to solicit feedback |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.08494](https://arxiv.org/abs/1912.08494) [cs.CL] |
|           | (or [arXiv:1912.08494v1](https://arxiv.org/abs/1912.08494v1) [cs.CL] for this version) |



# 2019-12-18

[Return to Index](#Index)



<h2 id="2019-12-18-1">1. Cross-Lingual Ability of Multilingual BERT: An Empirical Study</h2>
Title: [Cross-Lingual Ability of Multilingual BERT: An Empirical Study](https://arxiv.org/abs/1912.07840)

Authors: [Karthikeyan K](https://arxiv.org/search/cs?searchtype=author&query=K%2C+K), [Zihan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Stephen Mayhew](https://arxiv.org/search/cs?searchtype=author&query=Mayhew%2C+S), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

*(Submitted on 17 Dec 2019)*

> Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.07840](https://arxiv.org/abs/1912.07840) [cs.CL] |
|           | (or [arXiv:1912.07840v1](https://arxiv.org/abs/1912.07840v1) [cs.CL] for this version) |





# 2019-12-17

[Return to Index](#Index)





<h2 id="2019-12-17-1">1. Iterative Dual Domain Adaptation for Neural Machine Translation</h2>
Title: [Iterative Dual Domain Adaptation for Neural Machine Translation](https://arxiv.org/abs/1912.07239)

Authors: [Jiali Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Yubin Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Yaojie Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

*(Submitted on 16 Dec 2019)*

> Previous studies on the domain adaptation for neural machine translation (NMT) mainly focus on the one-pass transferring out-of-domain translation knowledge to in-domain NMT model. In this paper, we argue that such a strategy fails to fully extract the domain-shared translation knowledge, and repeatedly utilizing corpora of different domains can lead to better distillation of domain-shared translation knowledge. To this end, we propose an iterative dual domain adaptation framework for NMT. Specifically, we first pre-train in-domain and out-of-domain NMT models using their own training corpora respectively, and then iteratively perform bidirectional translation knowledge transfer (from in-domain to out-of-domain and then vice versa) based on knowledge distillation until the in-domain NMT model convergences. Furthermore, we extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the above-mentioned transfer is performed sequentially between the in-domain and each out-of-domain NMT models in the ascending order of their domain similarities. Empirical results on Chinese-English and English-German translation tasks demonstrate the effectiveness of our framework.

| Comments: | EMNLP2019                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.07239](https://arxiv.org/abs/1912.07239) [cs.CL] |
|           | (or [arXiv:1912.07239v1](https://arxiv.org/abs/1912.07239v1) [cs.CL] for this version) |





<h2 id="2019-12-17-2">2. Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding</h2>
Title: [Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding](https://arxiv.org/abs/1912.07240)

Authors: [Yuchen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Hao Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+H), [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Zhongjun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

*(Submitted on 16 Dec 2019)*

> Speech-to-text translation (ST), which translates source language speech into target language text, has attracted intensive attention in recent years. Compared to the traditional pipeline system, the end-to-end ST model has potential benefits of lower latency, smaller model size, and less error propagation. However, it is notoriously difficult to implement such a model without transcriptions as intermediate. Existing works generally apply multi-task learning to improve translation quality by jointly training end-to-end ST along with automatic speech recognition (ASR). However, different tasks in this method cannot utilize information from each other, which limits the improvement. Other works propose a two-stage model where the second model can use the hidden state from the first one, but its cascade manner greatly affects the efficiency of training and inference process. In this paper, we propose a novel interactive attention mechanism which enables ASR and ST to perform synchronously and interactively in a single model. Specifically, the generation of transcriptions and translations not only relies on its previous outputs but also the outputs predicted in the other task. Experiments on TED speech translation corpora have shown that our proposed model can outperform strong baselines on the quality of speech translation and achieve better speech recognition performances as well.

| Comments: | Accepted by AAAI 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.07240](https://arxiv.org/abs/1912.07240) [cs.CL] |
|           | (or [arXiv:1912.07240v1](https://arxiv.org/abs/1912.07240v1) [cs.CL] for this version) |





# 2019-12-16

[Return to Index](#Index)



<h2 id="2019-12-16-1">1. Document Sub-structure in Neural Machine Translation</h2>
Title: [Document Sub-structure in Neural Machine Translation](https://arxiv.org/abs/1912.06598)

Author: [Radina Dobreva](https://arxiv.org/search/cs?searchtype=author&query=Dobreva%2C+R), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R)

*(Submitted on 13 Dec 2019)*

> Current approaches to machine translation (MT) either translate sentences in isolation, disregarding the context they appear in, or model context on the level of the full document, without a notion of any internal structure the document may have. In this work we consider the fact that documents are rarely homogeneous blocks of text, but rather consist of parts covering different topics. Some documents, e.g. biographies and encyclopedia entries have highly predictable, regular structures in which sections are characterised by different topics. We draw inspiration from Louis and Webber (2014) who use this information to improve MT and transfer their proposal into the framework of neural MT. We compare two different methods of including information about the topic of the section within which each sentence is found: one using side constraints and the other using a cache-based model. We create and release the data on which we run our experiments -- parallel corpora for three language pairs (Chinese-English, French-English, Bulgarian-English) from Wikipedia biographies, preserving the boundaries of sections within the articles.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.06598](https://arxiv.org/abs/1912.06598) [cs.CL] |
|           | (or [arXiv:1912.06598v1](https://arxiv.org/abs/1912.06598v1) [cs.CL] for this version) |



# 2019-12-13

[Return to Index](#Index)



<h2 id="2019-12-13-1">1. Improving Interpretability of Word Embeddings by Generating Definition and Usage</h2>
Title: [Improving Interpretability of Word Embeddings by Generating Definition and Usage](https://arxiv.org/abs/1912.05898)

Authors: [Haitong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yongping Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Jiaxin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J), [Qingxiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q)

*(Submitted on 12 Dec 2019)*

> Word Embeddings, which encode semantic and syntactic features, have achieved success in many natural language processing tasks recently. However, the lexical semantics captured by these embeddings are difficult to interpret due to the dense vector representations. In order to improve the interpretability of word vectors, we explore definition modeling task and propose a novel framework (Semantics-Generator) to generate more reasonable and understandable context-dependent definitions. Moreover, we introduce usage modeling and study whether it is possible to utilize distributed representations to generate example sentences of words. These ways of semantics generation are a more direct and explicit expression of embedding's semantics. Two multi-task learning methods are used to combine usage modeling and definition modeling. To verify our approach, we construct Oxford-2019 dataset, where each entry contains word, context, example sentence and corresponding definition. Experimental results show that Semantics-Generator achieves the state-of-the-art result in definition modeling and the multi-task learning methods are helpful for two tasks to improve the performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.05898](https://arxiv.org/abs/1912.05898) [cs.CL] |
|           | (or [arXiv:1912.05898v1](https://arxiv.org/abs/1912.05898v1) [cs.CL] for this version) |



# 2019-12-12

[Return to Index](#Index)



<h2 id="2019-12-12-1">1. Lifelong learning for text retrieval and recognition in historical handwritten document collections</h2>
Title: [Lifelong learning for text retrieval and recognition in historical handwritten document collections](https://arxiv.org/abs/1912.05156)

Authors: [Lambert Schomaker](https://arxiv.org/search/cs?searchtype=author&query=Schomaker%2C+L)

*(Submitted on 11 Dec 2019)*

> This chapter provides an overview of the problems that need to be dealt with when constructing a lifelong-learning retrieval, recognition and indexing engine for large historical document collections in multiple scripts and languages, the Monk system. This application is highly variable over time, since the continuous labeling by end users changes the concept of what a 'ground truth' constitutes. Although current advances in deep learning provide a huge potential in this application domain, the scale of the problem, i.e., more than 520 hugely diverse books, documents and manuscripts precludes the current meticulous and painstaking human effort which is required in designing and developing successful deep-learning systems. The ball-park principle is introduced, which describes the evolution from the sparsely-labeled stage that can only be addressed by traditional methods or nearest-neighbor methods on embedded vectors of pre-trained neural networks, up to the other end of the spectrum where massive labeling allows reliable training of deep-learning methods. Contents: Introduction, Expectation management, Deep learning, The ball-park principle, Technical realization, Work flow, Quality and quantity of material, Industrialization and scalability, Human effort, Algorithms, Object of recognition, Processing pipeline, Performance,Compositionality, Conclusion.

| Comments: | To appear as chapter in book: Handwritten Historical Document Analysis, Recognition, and Retrieval -- State of the Art and Future Trends, in the book series: Series in Machine Perception and Artificial Intelligence World Scientific, ISSN (print): 1793-0839 Original version deposited at Zenodo: [this https URL](https://zenodo.org/record/2346885#.XfCfsq5ytpg) on December 17, 2018 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.05156](https://arxiv.org/abs/1912.05156) [cs.CV] |
|           | (or [arXiv:1912.05156v1](https://arxiv.org/abs/1912.05156v1) [cs.CV] for this version) |





<h2 id="2019-12-12-2">2. Unsupervised Neural Dialect Translation with Commonality and Diversity Modeling</h2>
Title: [Unsupervised Neural Dialect Translation with Commonality and Diversity Modeling](https://arxiv.org/abs/1912.05134)

Authors: [Yu Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Haihua Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+H), [Ben C.H. Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+B+C)

*(Submitted on 11 Dec 2019)*

> As a special machine translation task, dialect translation has two main characteristics: 1) lack of parallel training corpus; and 2) possessing similar grammar between two sides of the translation. In this paper, we investigate how to exploit the commonality and diversity between dialects thus to build unsupervised translation models merely accessing to monolingual data. Specifically, we leverage pivot-private embedding, layer coordination, as well as parameter sharing to sufficiently model commonality and diversity among source and target, ranging from lexical, through syntactic, to semantic levels. In order to examine the effectiveness of the proposed models, we collect 20 million monolingual corpus for each of Mandarin and Cantonese, which are official language and the most widely used dialect in China. Experimental results reveal that our methods outperform rule-based simplified and traditional Chinese conversion and conventional unsupervised translation models over 12 BLEU scores.

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.05134](https://arxiv.org/abs/1912.05134) [cs.CL] |
|           | (or [arXiv:1912.05134v1](https://arxiv.org/abs/1912.05134v1) [cs.CL] for this version) |





<h2 id="2019-12-12-3">3. Automatic Spanish Translation of the SQuAD Dataset for Multilingual Question Answering</h2>
Title: [Automatic Spanish Translation of the SQuAD Dataset for Multilingual Question Answering](https://arxiv.org/abs/1912.05200)

Authors: [Casimiro Pio Carrino](https://arxiv.org/search/cs?searchtype=author&query=Carrino%2C+C+P), [Marta Ruiz Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José Adrián Rodríguez Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R)

*(Submitted on 11 Dec 2019)*

> Recently, multilingual question answering became a crucial research topic, and it is receiving increased interest in the NLP community. However, the unavailability of large-scale datasets makes it challenging to train multilingual QA systems with performance comparable to the English ones. In this work, we develop the Translate Align Retrieve (TAR) method to automatically translate the Stanford Question Answering Dataset (SQuAD) v1.1 to Spanish. We then used this dataset to train Spanish QA systems by fine-tuning a Multilingual-BERT model. Finally, we evaluated our QA models with the recently proposed MLQA and XQuAD benchmarks for cross-lingual Extractive QA. Experimental results show that our models outperform the previous Multilingual-BERT baselines achieving the new state-of-the-art value of 68.1 F1 points on the Spanish MLQA corpus and 77.6 F1 and 61.8 Exact Match points on the Spanish XQuAD corpus. The resulting, synthetically generated SQuAD-es v1.1 corpora, with almost 100% of data contained in the original English version, to the best of our knowledge, is the first large-scale QA training resource for Spanish.

| Comments: | Submitted to LREC 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.05200](https://arxiv.org/abs/1912.05200) [cs.CL] |
|           | (or [arXiv:1912.05200v1](https://arxiv.org/abs/1912.05200v1) [cs.CL] for this version) |





<h2 id="2019-12-12-4">4. MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low Resource Machine Translation</h2>
Title: [MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low Resource Machine Translation](https://arxiv.org/abs/1912.05467)

Authors: [Rumeng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+R), [Xun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Hong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H)

*(Submitted on 11 Dec 2019)*

> Manipulating training data leads to robust neural models for MT.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.05467](https://arxiv.org/abs/1912.05467) [cs.CL] |
|           | (or [arXiv:1912.05467v1](https://arxiv.org/abs/1912.05467v1) [cs.CL] for this version) |





# 2019-12-11

[Return to Index](#Index)



<h2 id="2019-12-11-1">1. Cross-Language Aphasia Detection using Optimal Transport Domain Adaptation</h2>
Title: [Cross-Language Aphasia Detection using Optimal Transport Domain Adaptation](https://arxiv.org/abs/1912.04370)

Authors: [Aparna Balagopalan](https://arxiv.org/search/eess?searchtype=author&query=Balagopalan%2C+A), [Jekaterina Novikova](https://arxiv.org/search/eess?searchtype=author&query=Novikova%2C+J), [Matthew B. A. McDermott](https://arxiv.org/search/eess?searchtype=author&query=McDermott%2C+M+B+A), [Bret Nestor](https://arxiv.org/search/eess?searchtype=author&query=Nestor%2C+B), [Tristan Naumann](https://arxiv.org/search/eess?searchtype=author&query=Naumann%2C+T), [Marzyeh Ghassemi](https://arxiv.org/search/eess?searchtype=author&query=Ghassemi%2C+M)

*(Submitted on 4 Dec 2019)*

> Multi-language speech datasets are scarce and often have small sample sizes in the medical domain. Robust transfer of linguistic features across languages could improve rates of early diagnosis and therapy for speakers of low-resource languages when detecting health conditions from speech. We utilize out-of-domain, unpaired, single-speaker, healthy speech data for training multiple Optimal Transport (OT) domain adaptation systems. We learn mappings from other languages to English and detect aphasia from linguistic characteristics of speech, and show that OT domain adaptation improves aphasia detection over unilingual baselines for French (6% increased F1) and Mandarin (5% increased F1). Further, we show that adding aphasic data to the domain adaptation system significantly increases performance for both French and Mandarin, increasing the F1 scores further (10% and 8% increase in F1 scores for French and Mandarin, respectively, over unilingual baselines).

| Comments: | Accepted to ML4H at NeurIPS 2019                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1912.04370](https://arxiv.org/abs/1912.04370) [eess.AS] |
|           | (or [arXiv:1912.04370v1](https://arxiv.org/abs/1912.04370v1) [eess.AS] for this version) |





<h2 id="2019-12-11-2">2. GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies</h2>
Title: [GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies](https://arxiv.org/abs/1912.04778)

Authors: [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [Pau Li Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+P+L), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C)

*(Submitted on 10 Dec 2019)*

> We introduce GeBioToolkit, a tool for extracting multilingual parallel corpora at sentence level, with document and gender information from Wikipedia biographies. Despite thegender inequalitiespresent in Wikipedia, the toolkit has been designed to extract corpus balanced in gender. While our toolkit is customizable to any number of languages (and different domains), in this work we present a corpus of 2,000 sentences in English, Spanish and Catalan, which has been post-edited by native speakers to become a high-quality dataset for machinetranslation evaluation. While GeBioCorpus aims at being one of the first non-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the path to standardize procedures to produce gender-balanced datasets

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.04778](https://arxiv.org/abs/1912.04778) [cs.CL] |
|           | (or [arXiv:1912.04778v1](https://arxiv.org/abs/1912.04778v1) [cs.CL] for this version) |







# 2019-12-10

[Return to Index](#Index)



<h2 id="2019-12-10-1">1. Bidirectional Scene Text Recognition with a Single Decoder</h2>
Title: [Bidirectional Scene Text Recognition with a Single Decoder](https://arxiv.org/abs/1912.03656)

Authors: [Maurits Bleeker](https://arxiv.org/search/cs?searchtype=author&query=Bleeker%2C+M), [Maarten de Rijke](https://arxiv.org/search/cs?searchtype=author&query=de+Rijke%2C+M)

*(Submitted on 8 Dec 2019)*

> Scene Text Recognition (STR) is the problem of recognizing the correct word or character sequence in a cropped word image. To obtain more robust output sequences, the notion of bidirectional STR has been introduced. So far, bidirectional STRs have been implemented by using two separate decoders; one for left-to-right decoding and one for right-to-left. Having two separate decoders for almost the same task with the same output space is undesirable from a computational and optimization point of view. We introduce the bidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR method with a single decoder for bidirectional text decoding. With its single decoder, Bi-STET outperforms methods that apply bidirectional decoding by using two separate decoders while also being more efficient than those methods, Furthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR benchmarks with Bi-STET. Finally, we provide analyses and insights into the performance of Bi-STET.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.03656](https://arxiv.org/abs/1912.03656) [cs.CV] |
|           | (or [arXiv:1912.03656v1](https://arxiv.org/abs/1912.03656v1) [cs.CV] for this version) |





<h2 id="2019-12-10-2">2. Explaining Sequence-Level Knowledge Distillation as Data-Augmentation for Neural Machine Translation</h2>
Title: [Explaining Sequence-Level Knowledge Distillation as Data-Augmentation for Neural Machine Translation](https://arxiv.org/abs/1912.03334)

Authors: [Mitchell A. Gordon](https://arxiv.org/search/cs?searchtype=author&query=Gordon%2C+M+A), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K)

*(Submitted on 6 Dec 2019)*

> Sequence-level knowledge distillation (SLKD) is a model compression technique that leverages large, accurate teacher models to train smaller, under-parameterized student models. Why does pre-processing MT data with SLKD help us train smaller models? We test the common hypothesis that SLKD addresses a capacity deficiency in students by "simplifying" noisy data points and find it unlikely in our case. Models trained on concatenations of original and "simplified" datasets generalize just as well as baseline SLKD. We then propose an alternative hypothesis under the lens of data augmentation and regularization. We try various augmentation strategies and observe that dropout regularization can become unnecessary. Our methods achieve BLEU gains of 0.7-1.2 on TED Talks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.03334](https://arxiv.org/abs/1912.03334) [cs.CL] |
|           | (or [arXiv:1912.03334v1](https://arxiv.org/abs/1912.03334v1) [cs.CL] for this version) |





<h2 id="2019-12-10-3">3. Re-Translation Strategies For Long Form, Simultaneous, Spoken Language Translation</h2>
Title: [Re-Translation Strategies For Long Form, Simultaneous, Spoken Language Translation](https://arxiv.org/abs/1912.03393)

Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Te I](https://arxiv.org/search/cs?searchtype=author&query=I%2C+T), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W), [Pallavi Baljekar](https://arxiv.org/search/cs?searchtype=author&query=Baljekar%2C+P), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G)

*(Submitted on 6 Dec 2019)*

> We investigate the problem of simultaneous machine translation of long-form speech content. We target a continuous speech-to-text scenario, generating translated captions for a live audio feed, such as a lecture or play-by-play commentary. As this scenario allows for revisions to our incremental translations, we adopt a re-translation approach to simultaneous translation, where the source is repeatedly translated from scratch as it grows. This approach naturally exhibits very low latency and high final quality, but at the cost of incremental instability as the output is continuously refined. We experiment with a pipeline of industry-grade speech recognition and translation tools, augmented with simple inference heuristics to improve stability. We use TED Talks as a source of multilingual test data, developing our techniques on English-to-German spoken language translation. Our minimalist approach to simultaneous translation allows us to easily scale our final evaluation to six more target languages, dramatically improving incremental stability for all of them.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.03393](https://arxiv.org/abs/1912.03393) [cs.CL] |
|           | (or [arXiv:1912.03393v1](https://arxiv.org/abs/1912.03393v1) [cs.CL] for this version) |





<h2 id="2019-12-10-4">4. PidginUNMT: Unsupervised Neural Machine Translation from West African Pidgin to English</h2>
Title: [PidginUNMT: Unsupervised Neural Machine Translation from West African Pidgin to English](https://arxiv.org/abs/1912.03444)

Authors: [Kelechi Ogueji](https://arxiv.org/search/cs?searchtype=author&query=Ogueji%2C+K), [Orevaoghene Ahia](https://arxiv.org/search/cs?searchtype=author&query=Ahia%2C+O)

*(Submitted on 7 Dec 2019)*

> Over 800 languages are spoken across West Africa. Despite the obvious diversity among people who speak these languages, one language significantly unifies them all - West African Pidgin English. There are at least 80 million speakers of West African Pidgin English. However, there is no known natural language processing (NLP) work on this language. In this work, we perform the first NLP work on the most popular variant of the language, providing three major contributions. First, the provision of a Pidgin corpus of over 56000 sentences, which is the largest we know of. Secondly, the training of the first ever cross-lingual embedding between Pidgin and English. This aligned embedding will be helpful in the performance of various downstream tasks between English and Pidgin. Thirdly, the training of an Unsupervised Neural Machine Translation model between Pidgin and English which achieves BLEU scores of 7.93 from Pidgin to English, and 5.18 from English to Pidgin. In all, this work greatly reduces the barrier of entry for future NLP works on West African Pidgin English.

| Comments: | Presented at NeurIPS 2019 Workshop on Machine Learning for the Developing World |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.03444](https://arxiv.org/abs/1912.03444) [cs.CL] |
|           | (or [arXiv:1912.03444v1](https://arxiv.org/abs/1912.03444v1) [cs.CL] for this version) |





# 2019-12-09

[Return to Index](#Index)



<h2 id="2019-12-09-1">1. Machine Translation Evaluation Meets Community Question Answering</h2>
Title: [Machine Translation Evaluation Meets Community Question Answering](https://arxiv.org/abs/1912.02998)

Authors: [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Lluís Màrquez](https://arxiv.org/search/cs?searchtype=author&query=Màrquez%2C+L), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 6 Dec 2019)*

> We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering. In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions. The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.

| Comments:          | community question answering, machine translation evaluation, pairwise ranking, learning to rank |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Annual meeting of the Association for Computational Linguistics (ACL-2016) |
| Cite as:           | [arXiv:1912.02998](https://arxiv.org/abs/1912.02998) [cs.CL] |
|                    | (or [arXiv:1912.02998v1](https://arxiv.org/abs/1912.02998v1) [cs.CL] for this version) |





<h2 id="2019-12-09-2">2. Pairwise Neural Machine Translation Evaluation</h2>
Title: [Pairwise Neural Machine Translation Evaluation](https://arxiv.org/abs/1912.03135)

Authors: [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Lluis Marquez](https://arxiv.org/search/cs?searchtype=author&query=Marquez%2C+L), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 5 Dec 2019)*

> We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.

| Comments:          | machine translation evaluation, machine translation, pairwise ranking, learning to rank. arXiv admin note: substantial text overlap with [arXiv:1710.02095](https://arxiv.org/abs/1710.02095) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Conference of the Association for Computational Linguistics (ACL'2015) |
| Cite as:           | [arXiv:1912.03135](https://arxiv.org/abs/1912.03135) [cs.CL] |
|                    | (or [arXiv:1912.03135v1](https://arxiv.org/abs/1912.03135v1) [cs.CL] for this version) |









# 2019-12-06

[Return to Index](#Index)



<h2 id="2019-12-06-1">1. Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar</h2>
Title: [Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar](https://arxiv.org/abs/1912.02636)

Authors: [Qingxiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Chad Brown](https://arxiv.org/search/cs?searchtype=author&query=Brown%2C+C), [Cezary Kaliszyk](https://arxiv.org/search/cs?searchtype=author&query=Kaliszyk%2C+C), [Josef Urban](https://arxiv.org/search/cs?searchtype=author&query=Urban%2C+J)

*(Submitted on 5 Dec 2019)*

> In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. In our context informal mathematics refers to human-written mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. To train these models we also prepared four informal-to-formal datasets. We compare and analyze our results according to whether the model is supervised or unsupervised. In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation.

| Comments: | Submitted to POPL/CPP'2020                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Logic in Computer Science (cs.LO)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.02636](https://arxiv.org/abs/1912.02636) [cs.LO] |
|           | (or [arXiv:1912.02636v1](https://arxiv.org/abs/1912.02636v1) [cs.LO] for this version) |





# 2019-12-05

[Return to Index](#Index)



<h2 id="2019-12-05-1">1. Neural Machine Translation: A Review</h2>
Title: [Neural Machine Translation: A Review](https://arxiv.org/abs/1912.02047)

Authors: [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F)

*(Submitted on 4 Dec 2019)*

> The field of machine translation (MT), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. Statistical MT, which mainly relies on various count-based models and which used to dominate MT research for decades, has largely been superseded by neural machine translation (NMT), which tackles translation with a single neural network. In this work we will trace back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family. We will conclude with a survey of recent trends in the field.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.02047](https://arxiv.org/abs/1912.02047) [cs.CL] |
|           | (or [arXiv:1912.02047v1](https://arxiv.org/abs/1912.02047v1) [cs.CL] for this version) |





<h2 id="2019-12-05-2">2. A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well</h2>
Title: [A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well](https://arxiv.org/abs/1912.01706)

Authors: [Nicolas Garneau](https://arxiv.org/search/cs?searchtype=author&query=Garneau%2C+N), [Mathieu Godbout](https://arxiv.org/search/cs?searchtype=author&query=Godbout%2C+M), [David Beauchemin](https://arxiv.org/search/cs?searchtype=author&query=Beauchemin%2C+D), [Audrey Durand](https://arxiv.org/search/cs?searchtype=author&query=Durand%2C+A), [Luc Lamontagne](https://arxiv.org/search/cs?searchtype=author&query=Lamontagne%2C+L)

*(Submitted on 3 Dec 2019)*

> In this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. We show that the reproduction of their method is indeed feasible with some minor assumptions. We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper. In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters. We then propose key recommendations applicable to any research project in order to deliver fully reproducible research.

| Comments: | Submitted to REPROLANG@LREC2020                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1912.01706](https://arxiv.org/abs/1912.01706) [cs.LG] |
|           | (or [arXiv:1912.01706v1](https://arxiv.org/abs/1912.01706v1) [cs.LG] for this version) |





<h2 id="2019-12-05-3">3. Acquiring Knowledge from Pre-trained Model to Neural Machine Translation</h2>
Title: [Acquiring Knowledge from Pre-trained Model to Neural Machine Translation](https://arxiv.org/abs/1912.01774)

Authors: [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Shanbo Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

*(Submitted on 4 Dec 2019)*

> Pre-training and fine-tuning have achieved great success in the natural language process field. The standard paradigm of exploiting them includes two steps: first, pre-training a model, e.g. BERT, with a large scale unlabeled monolingual data. Then, fine-tuning the pre-trained model with labeled data from downstream tasks. However, in neural machine translation (NMT), we address the problem that the training objective of the bilingual task is far different from the monolingual pre-trained model. This gap leads that only using fine-tuning in NMT can not fully utilize prior language knowledge. In this paper, we propose an APT framework for acquiring knowledge from the pre-trained model to NMT. The proposed approach includes two modules: 1). a dynamic fusion mechanism to fuse task-specific features adapted from general knowledge into NMT network, 2). a knowledge distillation paradigm to learn language knowledge continuously during the NMT training process. The proposed approach could integrate suitable knowledge from pre-trained models to improve the NMT. Experimental results on WMT English to German, German to English and Chinese to English machine translation tasks show that our model outperforms strong baselines and the fine-tuning counterparts.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.01774](https://arxiv.org/abs/1912.01774) [cs.CL] |
|           | (or [arXiv:1912.01774v1](https://arxiv.org/abs/1912.01774v1) [cs.CL] for this version) |







# 2019-12-04

[Return to Index](#Index)



<h2 id="2019-12-04-1">1. Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation</h2>
Title: [Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation](https://arxiv.org/abs/1912.01214)

Authors: [Baijun Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+B), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xiangyu Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+X), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

*(Submitted on 3 Dec 2019)*

> Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.

| Comments: | Accepted as a conference paper at AAAI 2020 (oral presentation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.01214](https://arxiv.org/abs/1912.01214) [cs.CL] |
|           | (or [arXiv:1912.01214v1](https://arxiv.org/abs/1912.01214v1) [cs.CL] for this version) |





# 2019-12-03

[Return to Index](#Index)



<h2 id="2019-12-03-1">1. Not All Attention Is Needed: Gated Attention Network for Sequence Data</h2>
Title: [Not All Attention Is Needed: Gated Attention Network for Sequence Data](https://arxiv.org/abs/1912.00349)

Authors: [Lanqing Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+L), [Xiaopeng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Nevin L. Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N+L)

*(Submitted on 1 Dec 2019)*

> Although deep neural networks generally have fixed network structures, the concept of dynamic mechanism has drawn more and more attention in recent years. Attention mechanisms compute input-dependent dynamic attention weights for aggregating a sequence of hidden states. Dynamic network configuration in convolutional neural networks (CNNs) selectively activates only part of the network at a time for different inputs. In this paper, we combine the two dynamic mechanisms for text classification tasks. Traditional attention mechanisms attend to the whole sequence of hidden states for an input sentence, while in most cases not all attention is needed especially for long sequences. We propose a novel method called Gated Attention Network (GA-Net) to dynamically select a subset of elements to attend to using an auxiliary network, and compute attention weights to aggregate the selected elements. It avoids a significant amount of unnecessary computation on unattended elements, and allows the model to pay attention to important parts of the sequence. Experiments in various datasets show that the proposed method achieves better performance compared with all baseline models with global or local attention while requiring less computation and achieving better interpretability. It is also promising to extend the idea to more complex attention-based models, such as transformers and seq-to-seq models.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.00349](https://arxiv.org/abs/1912.00349) [cs.LG] |
|           | (or [arXiv:1912.00349v1](https://arxiv.org/abs/1912.00349v1) [cs.LG] for this version) |





<h2 id="2019-12-03-2">2. Modeling Fluency and Faithfulness for Diverse Neural Machine Translation</h2>
Title: [Modeling Fluency and Faithfulness for Diverse Neural Machine Translation](https://arxiv.org/abs/1912.00178)

Authors: [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Wen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Zhengxin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

*(Submitted on 30 Nov 2019)*

> Neural machine translation models usually adopt the teacher forcing strategy for training which requires the predicted sequence matches ground truth word by word and forces the probability of each prediction to approach a 0-1 distribution. However, the strategy casts all the portion of the distribution to the ground truth word and ignores other words in the target vocabulary even when the ground truth word cannot dominate the distribution. To address the problem of teacher forcing, we propose a method to introduce an evaluation module to guide the distribution of the prediction. The evaluation module accesses each prediction from the perspectives of fluency and faithfulness to encourage the model to generate the word which has a fluent connection with its past and future translation and meanwhile tends to form a translation equivalent in meaning to the source. The experiments on multiple translation tasks show that our method can achieve significant improvements over strong baselines.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.00178](https://arxiv.org/abs/1912.00178) [cs.CL] |
|           | (or [arXiv:1912.00178v1](https://arxiv.org/abs/1912.00178v1) [cs.CL] for this version) |



<h2 id="2019-12-03-3">3. Merging External Bilingual Pairs into Neural Machine Translation</h2>
Title: [Merging External Bilingual Pairs into Neural Machine Translation](https://arxiv.org/abs/1912.00567)

Authors: [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Shaohui Kuang](https://arxiv.org/search/cs?searchtype=author&query=Kuang%2C+S), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D), [António Branco](https://arxiv.org/search/cs?searchtype=author&query=Branco%2C+A)

*(Submitted on 2 Dec 2019)*

> As neural machine translation (NMT) is not easily amenable to explicit correction of errors, incorporating pre-specified translations into NMT is widely regarded as a non-trivial challenge. In this paper, we propose and explore three methods to endow NMT with pre-specified bilingual pairs. Instead, for instance, of modifying the beam search algorithm during decoding or making complex modifications to the attention mechanism --- mainstream approaches to tackling this challenge ---, we experiment with the training data being appropriately pre-processed to add information about pre-specified translations. Extra embeddings are also used to distinguish pre-specified tokens from the other tokens. Extensive experimentation and analysis indicate that over 99% of the pre-specified phrases are successfully translated (given a 85% baseline) and that there is also a substantive improvement in translation quality with the methods explored here.

| Comments:    | 7 pages, 3 figures, 5 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:1912.00567](https://arxiv.org/abs/1912.00567) [cs.CL] |
|              | (or [arXiv:1912.00567v1](https://arxiv.org/abs/1912.00567v1) [cs.CL] for this version) |





# 2019-12-02

[Return to Index](#Index)



<h2 id="2019-12-02-1">1. DiscoTK: Using Discourse Structure for Machine Translation Evaluation</h2>
Title: [DiscoTK: Using Discourse Structure for Machine Translation Evaluation](https://arxiv.org/abs/1911.12547)

Authors: [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Lluis Marquez](https://arxiv.org/search/cs?searchtype=author&query=Marquez%2C+L), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 28 Nov 2019)*

> We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.

| Comments:          | machine translation evaluation, machine translation, tree kernels, discourse, convolutional kernels, discourse tree, RST, rhetorical structure theory, ASIYA |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | WMT-2014                                                     |
| Cite as:           | [arXiv:1911.12547](https://arxiv.org/abs/1911.12547) [cs.CL] |
|                    | (or [arXiv:1911.12547v1](https://arxiv.org/abs/1911.12547v1) [cs.CL] for this version) |





<h2 id="2019-12-02-2">2. Multimodal Machine Translation through Visuals and Speech</h2>
Title: [Multimodal Machine Translation through Visuals and Speech](https://arxiv.org/abs/1911.12798)

Authors: [Umut Sulubacak](https://arxiv.org/search/cs?searchtype=author&query=Sulubacak%2C+U), [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Stig-Arne Grönroos](https://arxiv.org/search/cs?searchtype=author&query=Grönroos%2C+S), [Aku Rouhe](https://arxiv.org/search/cs?searchtype=author&query=Rouhe%2C+A), [Desmond Elliott](https://arxiv.org/search/cs?searchtype=author&query=Elliott%2C+D), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

*(Submitted on 28 Nov 2019)*

> Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are distinguished from their monolingual counterparts of speech recognition, image captioning, and video captioning by the requirement of models to generate outputs in a different language. This survey reviews the major data resources for these tasks, the evaluation campaigns concentrated around them, the state of the art in end-to-end and pipeline approaches, and also the challenges in performance evaluation. The paper concludes with a discussion of directions for future research in these areas: the need for more expansive and challenging datasets, for targeted evaluations of model performance, and for multimodality in both the input and output space.

| Comments: | 34 pages, 4 tables, 8 figures. Submitted (Nov 2019) to the Machine Translation journal (Springer) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.12798](https://arxiv.org/abs/1911.12798) [cs.CL] |
|           | (or [arXiv:1911.12798v1](https://arxiv.org/abs/1911.12798v1) [cs.CL] for this version) |





<h2 id="2019-12-02-3">3. GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors</h2>
Title: [GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors](https://arxiv.org/abs/1911.12893)

Authors: [Masato Hagiwara](https://arxiv.org/search/cs?searchtype=author&query=Hagiwara%2C+M), [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M)

*(Submitted on 28 Nov 2019)*

> The lack of large-scale datasets has been a major hindrance to the development of NLP tasks such as spelling correction and grammatical error correction (GEC). As a complementary new resource for these tasks, we present the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, a large and popular platform for hosting and sharing git repositories. The dataset, which we have made publicly available, contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date. We also describe our process for filtering true typo edits based on learned classifiers on a small annotated subset, and demonstrate that typo edits can be identified with F1 ~ 0.9 using a very simple classifier with only three features. The detailed analyses of the dataset show that existing spelling correctors merely achieve an F-measure of approx. 0.5, suggesting that the dataset serves as a new, rich source of spelling errors that complement existing datasets.

| Comments: | Submitted at LREC 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.12893](https://arxiv.org/abs/1911.12893) [cs.CL] |
|           | (or [arXiv:1911.12893v1](https://arxiv.org/abs/1911.12893v1) [cs.CL] for this version) |





<h2 id="2019-12-02-4">4. Neural Chinese Word Segmentation as Sequence to Sequence Translation</h2>
Title: [Neural Chinese Word Segmentation as Sequence to Sequence Translation](https://arxiv.org/abs/1911.12982)

Authors: [Xuewen Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Ping Jian](https://arxiv.org/search/cs?searchtype=author&query=Jian%2C+P), [Yuhang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Xiaochi Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Yi-Kun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y)

*(Submitted on 29 Nov 2019)*

> Recently, Chinese word segmentation (CWS) methods using neural networks have made impressive progress. Most of them regard the CWS as a sequence labeling problem which construct models based on local features rather than considering global information of input sequence. In this paper, we cast the CWS as a sequence translation problem and propose a novel sequence-to-sequence CWS model with an attention-based encoder-decoder framework. The model captures the global information from the input and directly outputs the segmented sequence. It can also tackle other NLP tasks with CWS jointly in an end-to-end mode. Experiments on Weibo, PKU and MSRA benchmark datasets show that our approach has achieved competitive performances compared with state-of-the-art methods. Meanwhile, we successfully applied our proposed model to jointly learning CWS and Chinese spelling correction, which demonstrates its applicability of multi-task fusion.

| Comments: | In proceedings of SMP 2017 (Chinese National Conference on Social Media Processing) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1007/978-981-10-6805-8_8](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-981-10-6805-8_8&v=a5b3c37f) |
| Cite as:  | [arXiv:1911.12982](https://arxiv.org/abs/1911.12982) [cs.CL] |
|           | (or [arXiv:1911.12982v1](https://arxiv.org/abs/1911.12982v1) [cs.CL] for this version) |







