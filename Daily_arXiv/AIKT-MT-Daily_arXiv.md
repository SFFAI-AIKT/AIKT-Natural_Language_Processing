# Daily arXiv: Machine Translation - August, 2020

# Index


- [2020-08-05](#2020-08-05)

  - [1. A Survey of Orthographic Information in Machine Translation](#2020-08-05-1)
  - [2. Defining and Evaluating Fair Natural Language Generation](#2020-08-05-2)
- [2020-08-04](#2020-08-04)

  - [1. Audiovisual Speech Synthesis using Tacotron2](#2020-08-04-1)
  - [2. DeLighT: Very Deep and Light-weight Transformer](#2020-08-04-2)
  - [3. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](#2020-08-04-3)
  - [4. LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?](#2020-08-04-4)
- [2020-08-03](#2020-08-03)
  - [1. Neural Language Generation: Formulation, Methods, and Evaluation](#2020-08-03-1)
  - [2. On Learning Universal Representations Across Languages](#2020-08-03-2)
  - [3. Word Embeddings: Stability and Semantic Change](#2020-08-03-3)
  - [4. Exploring Swedish & English fastText Embeddings with the Transformer](#2020-08-03-4)
  - [5. Multi-task learning for natural language processing in the 2020s: where are we going?](#2020-08-03-5)
  - [6. Toward Givenness Hierarchy Theoretic Natural Language Generation](#2020-08-03-6)
  - [7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](#2020-08-03-7)
  - [8. Neural Machine Translation model for University Email Application](#2020-08-03-8)
  - [9. Neural Composition: Learning to Generate from Multiple Models](#2020-08-03-9)
  - [10. SimulEval: An Evaluation Toolkit for Simultaneous Translation](#2020-08-03-10)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-08-05

[Return to Index](#Index)



<h2 id="2020-08-05-1">1. A Survey of Orthographic Information in Machine Translation</h2>

Title: [A Survey of Orthographic Information in Machine Translation](https://arxiv.org/abs/2008.01391)

Authors: [Bharathi Raja Chakravarthi](https://arxiv.org/search/cs?searchtype=author&query=Chakravarthi%2C+B+R), [Priya Rani](https://arxiv.org/search/cs?searchtype=author&query=Rani%2C+P), [Mihael Arcan](https://arxiv.org/search/cs?searchtype=author&query=Arcan%2C+M), [John P. McCrae](https://arxiv.org/search/cs?searchtype=author&query=McCrae%2C+J+P)

> Machine translation is one of the applications of natural language processing which has been explored in different languages. Recently researchers started paying attention towards machine translation for resource-poor languages and closely related languages. A widespread and underlying problem for these machine translation systems is the variation in orthographic conventions which causes many issues to traditional approaches. Two languages written in two different orthographies are not easily comparable, but orthographic information can also be used to improve the machine translation system. This article offers a survey of research regarding orthography's influence on machine translation of under-resourced languages. It introduces under-resourced languages in terms of machine translation and how orthographic information can be utilised to improve machine translation. We describe previous work in this area, discussing what underlying assumptions were made, and showing how orthographic knowledge improves the performance of machine translation of under-resourced languages. We discuss different types of machine translation and demonstrate a recent trend that seeks to link orthographic information with well-established machine translation methods. Considerable attention is given to current efforts of cognates information at different levels of machine translation and the lessons that can be drawn from this. Additionally, multilingual neural machine translation of closely related languages is given a particular focus in this survey. This article ends with a discussion of the way forward in machine translation with orthographic information, focusing on multilingual settings and bilingual lexicon induction.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.01391](https://arxiv.org/abs/2008.01391) [cs.CL]** |
|           | (or **[arXiv:2008.01391v1](https://arxiv.org/abs/2008.01391v1) [cs.CL]** for this version) |





<h2 id="2020-08-05-2">2. Defining and Evaluating Fair Natural Language Generation</h2>

Title: [Defining and Evaluating Fair Natural Language Generation](https://arxiv.org/abs/2008.01548)

Authors: [Catherine Yeo](https://arxiv.org/search/cs?searchtype=author&query=Yeo%2C+C), [Alyssa Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+A)

> Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.

| Comments: | 7 pages, 2 figures, to be published in Proceedings of the The Fourth Widening Natural Language Processing Workshop at ACL |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2008.01548](https://arxiv.org/abs/2008.01548) [cs.CL]** |
|           | (or **[arXiv:2008.01548v1](https://arxiv.org/abs/2008.01548v1) [cs.CL]** for this version) |



# 2020-08-04

[Return to Index](#Index)



<h2 id="2020-08-04-1">1. Audiovisual Speech Synthesis using Tacotron2</h2>

Title: [Audiovisual Speech Synthesis using Tacotron2](https://arxiv.org/abs/2008.00620)

Authors: [Ahmed Hussen Abdelaziz](https://arxiv.org/search/eess?searchtype=author&query=Abdelaziz%2C+A+H), [Anushree Prasanna Kumar](https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+A+P), [Chloe Seivwright](https://arxiv.org/search/eess?searchtype=author&query=Seivwright%2C+C), [Gabriele Fanelli](https://arxiv.org/search/eess?searchtype=author&query=Fanelli%2C+G), [Justin Binder](https://arxiv.org/search/eess?searchtype=author&query=Binder%2C+J), [Yannis Stylianou](https://arxiv.org/search/eess?searchtype=author&query=Stylianou%2C+Y), [Sachin Kajarekar](https://arxiv.org/search/eess?searchtype=author&query=Kajarekar%2C+S)

> Audiovisual speech synthesis is the problem of synthesizing a talking face while maximizing the coherency of the acoustic and visual speech. In this paper, we propose and compare two audiovisual speech synthesis systems for 3D face models. The first system is the AVTacotron2, which is an end-to-end text-to-audiovisual speech synthesizer based on the Tacotron2 architecture. AVTacotron2 converts a sequence of phonemes representing the sentence to synthesize into a sequence of acoustic features and the corresponding controllers of a face model. The output acoustic features are used to condition a WaveRNN to reconstruct the speech waveform, and the output facial controllers are used to generate the corresponding video of the talking face. The second audiovisual speech synthesis system is modular, where acoustic speech is synthesized from text using the traditional Tacotron2. The reconstructed acoustic speech signal is then used to drive the facial controls of the face model using an independently trained audio-to-facial-animation neural network. We further condition both the end-to-end and modular approaches on emotion embeddings that encode the required prosody to generate emotional audiovisual speech. We analyze the performance of the two systems and compare them to the ground truth videos using subjective evaluation tests. The end-to-end and modular systems are able to synthesize close to human-like audiovisual speech with mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS of 4.1 for the ground truth generated from professionally recorded videos. While the end-to-end system gives a better overall quality, the modular approach is more flexible and the quality of acoustic speech and visual speech synthesis is almost independent of each other.

| Comments: | This work has been submitted to the IEEE transactions on Multimedia for possible publication |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2008.00620](https://arxiv.org/abs/2008.00620) [eess.AS]** |
|           | (or **[arXiv:2008.00620v1](https://arxiv.org/abs/2008.00620v1) [eess.AS]** for this version) |





<h2 id="2020-08-04-2">2. DeLighT: Very Deep and Light-weight Transformer</h2>

Title: [DeLighT: Very Deep and Light-weight Transformer](https://arxiv.org/abs/2008.00623)

Authors: [Sachin Mehta](https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+S), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Srinivasan Iyer](https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+S), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Hannaneh Hajishirzi](https://arxiv.org/search/cs?searchtype=author&query=Hajishirzi%2C+H)

> We introduce a very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using DExTra, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on machine translation and language modeling tasks show that DeLighT matches the performance of baseline Transformers with significantly fewer parameters. On the WMT'14 En-Fr high resource dataset, DeLighT requires 1.8 times fewer parameters and 2 times fewer operations and achieves better performance (+0.4 BLEU score) than baseline transformers. On the WMT'16 En-Ro low resource dataset, DeLighT delivers similar performance with 2.8 times fewer parameters than baseline transformers.

| Comments: | 16 pages including references and appendix                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2008.00623](https://arxiv.org/abs/2008.00623) [cs.LG]** |
|           | (or **[arXiv:2008.00623v1](https://arxiv.org/abs/2008.00623v1) [cs.LG]** for this version) |





<h2 id="2020-08-04-3">3. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning</h2>

Title: [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401)

Authors: [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Chau Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+C), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.

| Comments: | 10 pages (main) + 5 pages (appendices). 9 tables and 2 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.00401](https://arxiv.org/abs/2008.00401) [cs.CL]** |
|           | (or **[arXiv:2008.00401v1](https://arxiv.org/abs/2008.00401v1) [cs.CL]** for this version) |





<h2 id="2020-08-04-4">4. LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?</h2>

Title: [LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?](https://arxiv.org/abs/2008.00805)

Authors: [Marc Pàmies](https://arxiv.org/search/cs?searchtype=author&query=Pàmies%2C+M), [Emily Öhman](https://arxiv.org/search/cs?searchtype=author&query=Öhman%2C+E), [Kaisla Kajava](https://arxiv.org/search/cs?searchtype=author&query=Kajava%2C+K), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

> This paper presents the different models submitted by the LT@Helsinki team for the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and C; titled offensive language identification and offense target identification, respectively. In both cases we used the so-called Bidirectional Encoder Representation from Transformer (BERT), a model pre-trained by Google and fine-tuned by us on the OLID and SOLID datasets. The results show that offensive tweet classification is one of several language-based tasks where BERT can achieve state-of-the-art results.

| Comments: | Accepted at SemEval-2020 Task 12. Identical to camera-ready version except where adjustments to fit arXiv requirements were necessary |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.00805](https://arxiv.org/abs/2008.00805) [cs.CL]** |
|           | (or **[arXiv:2008.00805v1](https://arxiv.org/abs/2008.00805v1) [cs.CL]** for this version) |







# 2020-08-03

[Return to Index](#Index)



<h2 id="2020-08-03-1">1. Neural Language Generation: Formulation, Methods, and Evaluation</h2>

Title: [Neural Language Generation: Formulation, Methods, and Evaluation](https://arxiv.org/abs/2007.15780)

Authors: [Cristina Garbacea](https://arxiv.org/search/cs?searchtype=author&query=Garbacea%2C+C), [Qiaozhu Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+Q)

> Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.

| Comments: | 70 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.15780](https://arxiv.org/abs/2007.15780) [cs.CL]** |
|           | (or **[arXiv:2007.15780v1](https://arxiv.org/abs/2007.15780v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-2">2. On Learning Universal Representations Across Languages</h2>

Title: [On Learning Universal Representations Across Languages](https://arxiv.org/abs/2007.15960)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations, and show the effectiveness on cross-lingual understanding and generation. We propose Hierarchical Contrastive Learning (HiCTL) to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on three benchmarks: language understanding tasks (QQP, QNLI, SST-2, MRPC, STS-B and MNLI) in the GLUE benchmark, cross-lingual natural language inference (XNLI) and machine translation. Experimental results show that the HiCTL obtains an absolute gain of 1.0%/2.2% accuracy on GLUE/XNLI as well as achieves substantial improvements of +1.7-+3.6 BLEU on both the high-resource and low-resource English-to-X translation tasks over strong baselines. We will release the source codes as soon as possible.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.15960](https://arxiv.org/abs/2007.15960) [cs.CL]** |
|           | (or **[arXiv:2007.15960v1](https://arxiv.org/abs/2007.15960v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-3">3. Word Embeddings: Stability and Semantic Change</h2>

Title: [Word Embeddings: Stability and Semantic Change](https://arxiv.org/abs/2007.16006)

Authors: [Lucas Rettenmeier](https://arxiv.org/search/cs?searchtype=author&query=Rettenmeier%2C+L)

> Word embeddings are computed by a class of techniques within natural language processing (NLP), that create continuous vector representations of words in a language from a large text corpus. The stochastic nature of the training process of most embedding techniques can lead to surprisingly strong instability, i.e. subsequently applying the same technique to the same data twice, can produce entirely different results. In this work, we present an experimental study on the instability of the training process of three of the most influential embedding techniques of the last decade: word2vec, GloVe and fastText. Based on the experimental results, we propose a statistical model to describe the instability of embedding techniques and introduce a novel metric to measure the instability of the representation of an individual word. Finally, we propose a method to minimize the instability - by computing a modified average over multiple runs - and apply it to a specific linguistic problem: The detection and quantification of semantic change, i.e. measuring changes in the meaning and usage of words over time.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16006](https://arxiv.org/abs/2007.16006) [cs.CL]** |
|           | (or **[arXiv:2007.16006v1](https://arxiv.org/abs/2007.16006v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-4">4. Exploring Swedish & English fastText Embeddings with the Transformer</h2>

Title: [Exploring Swedish & English fastText Embeddings with the Transformer](https://arxiv.org/abs/2007.16007)

Authors: [Tosin P. Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T+P), [Foteini Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F), [Marcus Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M)

> In this paper, our main contributions are that embeddings from relatively smaller corpora can outperform ones from far larger corpora and we present the new Swedish analogy test set. To achieve a good network performance in natural language processing (NLP) downstream tasks, several factors play important roles: dataset size, the right hyper-parameters, and well-trained embedding. We show that, with the right set of hyper-parameters, good network performance can be reached even on smaller datasets. We evaluate the embeddings at the intrinsic level and extrinsic level, by deploying them on the Transformer in named entity recognition (NER) task and conduct significance tests.This is done for both Swedish and English. We obtain better performance in both languages on the downstream task with far smaller training data, compared to recently released, common crawl versions and character n-grams appear useful for Swedish, a morphologically rich language.

| Comments: | 10 pages, 2 figures, 8 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.16007](https://arxiv.org/abs/2007.16007) [cs.CL]** |
|           | (or **[arXiv:2007.16007v1](https://arxiv.org/abs/2007.16007v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-5">5. Multi-task learning for natural language processing in the 2020s: where are we going?</h2>

Title: [Multi-task learning for natural language processing in the 2020s: where are we going?](https://arxiv.org/abs/2007.16008)

Authors: [Joseph Worsham](https://arxiv.org/search/cs?searchtype=author&query=Worsham%2C+J), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.

| Comments:          | 12 pages, 2 figures. Published in Elsevier Pattern Recognition Letters Volume 136. Accepted manuscript published here |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes:       | I.2.6; I.2.7                                                 |
| Journal reference: | Pattern Recognition Letters 136 (2020) 120-126               |
| DOI:               | [10.1016/j.patrec.2020.05.031](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1016%2Fj.patrec.2020.05.031&v=27506e7c) |
| Cite as:           | **[arXiv:2007.16008](https://arxiv.org/abs/2007.16008) [cs.CL]** |
|                    | (or **[arXiv:2007.16008v1](https://arxiv.org/abs/2007.16008v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-6">6. Toward Givenness Hierarchy Theoretic Natural Language Generation</h2>

Title: [Toward Givenness Hierarchy Theoretic Natural Language Generation](https://arxiv.org/abs/2007.16009)

Authors: [Poulomi Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+P), [Tom Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+T)

> Language-capable interactive robots participating in dialogues with human interlocutors must be able to naturally and efficiently communicate about the entities in their environment. A key aspect of such communication is the use of anaphoric language. The linguistic theory of the Givenness Hierarchy(GH) suggests that humans use anaphora based on the cognitive statuses their referents have in the minds of their interlocutors. In previous work, researchers presented GH-theoretic approaches to robot anaphora understanding. In this paper we describe how the GH might need to be used quite differently to facilitate robot anaphora generation.

| Comments: | Extended Abstract accepted for (non-archival) presentation at Advances in Cognitive Systems 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2007.16009](https://arxiv.org/abs/2007.16009) [cs.CL]** |
|           | (or **[arXiv:2007.16009v1](https://arxiv.org/abs/2007.16009v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-7">7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs</h2>

Title: [Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](https://arxiv.org/abs/2007.16010)

Authors: [Subhadip Maji](https://arxiv.org/search/cs?searchtype=author&query=Maji%2C+S), [Arijit Ghosh Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+A+G), [Raghav Bali](https://arxiv.org/search/cs?searchtype=author&query=Bali%2C+R), [Vamsi M Bhandaru](https://arxiv.org/search/cs?searchtype=author&query=Bhandaru%2C+V+M)

> Deep Neural Networks in NLP have enabled systems to learn complex non-linear relationships. One of the major bottlenecks towards being able to use DNNs for real world applications is their characterization as black boxes. To solve this problem, we introduce a model agnostic algorithm which calculates phrase-wise importance of input features. We contend that our method is generalizable to a diverse set of tasks, by carrying out experiments for both Regression and Classification. We also observe that our approach is robust to outliers, implying that it only captures the essential aspects of the input.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2007.16010](https://arxiv.org/abs/2007.16010) [cs.CL]** |
|           | (or **[arXiv:2007.16010v1](https://arxiv.org/abs/2007.16010v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-8">8. Neural Machine Translation model for University Email Application</h2>

Title: [Neural Machine Translation model for University Email Application](https://arxiv.org/abs/2007.16011)

Authors: [Sandhya Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+S), [Siti Nur Afikah Bte Abdul Mazid](https://arxiv.org/search/cs?searchtype=author&query=Mazid%2C+S+N+A+B+A), [Nagender Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+N)

> Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation (NMT) model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of EN -> ML of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.

| Comments:          | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Journal reference: | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| Cite as:           | **[arXiv:2007.16011](https://arxiv.org/abs/2007.16011) [cs.CL]** |
|                    | (or **[arXiv:2007.16011v1](https://arxiv.org/abs/2007.16011v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-9">9. Neural Composition: Learning to Generate from Multiple Models</h2>

Title: [Neural Composition: Learning to Generate from Multiple Models](https://arxiv.org/abs/2007.16013)

Authors: [Denis Filimonov](https://arxiv.org/search/cs?searchtype=author&query=Filimonov%2C+D), [Ravi Teja Gadde](https://arxiv.org/search/cs?searchtype=author&query=Gadde%2C+R+T), [Ariya Rastrow](https://arxiv.org/search/cs?searchtype=author&query=Rastrow%2C+A)

> Decomposing models into multiple components is critically important in many applications such as language modeling (LM) as it enables adapting individual components separately and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, which requires class-annotated data, or through biasing to individual phrases which is limited in scale. In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component, and how to combine probability distributions from each component, directly from unlabeled text data.

| Comments:    | submitted to NeurIPS'20 (under review)                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ACM classes: | I.2.6; I.2.7                                                 |
| Cite as:     | **[arXiv:2007.16013](https://arxiv.org/abs/2007.16013) [cs.CL]** |
|              | (or **[arXiv:2007.16013v1](https://arxiv.org/abs/2007.16013v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-10">10. SimulEval: An Evaluation Toolkit for Simultaneous Translation</h2>

Title: [SimulEval: An Evaluation Toolkit for Simultaneous Translation](https://arxiv.org/abs/2007.16193)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16193](https://arxiv.org/abs/2007.16193) [cs.CL]** |
|           | (or **[arXiv:2007.16193v1](https://arxiv.org/abs/2007.16193v1) [cs.CL]** for this version) |



