# Daily arXiv: Machine Translation - Oct., 2019

# Index

- [2019-10-21](#2019-10-21)
  - [1. Controlling Utterance Length in NMT-based Word Segmentation with Attention](#2019-10-21-1)
- [2019-10-18](#2019-10-18)
  - [1. LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition](#2019-10-18-1)
- [2019-10-17](#2019-10-17)
  - [1. Root Mean Square Layer Normalization](#2019-10-17-1)
  - [2. Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models](#2019-10-17-2)
  - [3. Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings](#2019-10-17-3)
  - [4. Fine-grained evaluation of German-English Machine Translation based on a Test Suite](#2019-10-17-4)
  - [5. Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite](#2019-10-17-5)
  - [6. Using Whole Document Context in Neural Machine Translation](#2019-10-17-6)
- [2019-10-16](#2019-10-16)
  - [1. In-training Matrix Factorization for Parameter-frugal Neural Machine Translation](#2019-10-16-1)
  - [2. Mapping Supervised Bilingual Word Embeddings from English to low-resource languages](#2019-10-16-2)
  - [3. Detecting Machine-Translated Text using Back Translation](#2019-10-16-3)
  - [4. Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation](#2019-10-16-4)
  - [5. On the Importance of Word Boundaries in Character-level Neural Machine Translation](#2019-10-16-5)
  - [6. Facebook AI's WAT19 Myanmar-English Translation Task Submission](#2019-10-16-6)
- [2019-10-15](#2019-10-15)
  - [1. From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction](#2019-10-15-1)
  - [2. Transformers without Tears: Improving the Normalization of Self-Attention](#2019-10-15-2)
  - [3. Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality](#2019-10-15-3)
  - [4. Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment](#2019-10-15-4)
- [2019-10-14](#2019-10-14)
  - [1. How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages](#2019-10-14-1)
- [2019-10-11](#2019-10-11)
  - [1. Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](#2019-10-11-1)
  - [2. Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)](#2019-10-11-2)
- [2019-10-10](#2019-10-10)
  - [1. Novel Applications of Factored Neural Machine Translation](#2019-10-10-1)
- [2019-10-09](#2019-10-09)
  - [1. Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation](#2019-10-09-1)
  - [2. Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](#2019-10-09-2)
  - [3. One-To-Many Multilingual End-to-end Speech Translation](#2019-10-09-3)
  - [4. An Interactive Machine Translation Framework for Modernizing Historical Documents](#2019-10-09-4)
  - [5. Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation](#2019-10-09-5)
  - [6. Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents](#2019-10-09-6)
- [2019-10-08](#2019-10-08)
  - [1. How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems](#2019-10-08-1)
  - [2. Domain Differential Adaptation for Neural Machine Translation](#2019-10-08-2)
  - [3. On Leveraging the Visual Modality for Neural Machine Translation](#2019-10-08-3)
  - [4. Adversarial reconstruction for Multi-modal Machine Translation](#2019-10-08-4)
- [2019-10-07](#2019-10-07)
  - [1. Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data](#2019-10-07-1)
  - [2. Modeling Confidence in Sequence-to-Sequence Models](#2019-10-07-2)
  - [3. Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods](#2019-10-07-3)
- [2019-10-04](#2019-10-04)
  - [1. Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations](#2019-10-04-1)
  - [2. Linking artificial and human neural representations of language](#2019-10-04-2)
- [2019-10-03](#2019-10-03)
  - [1. Speech-to-speech Translation between Untranscribed Unknown Languages](2019-10-03-1)
  - [2. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](2019-10-03-2)
- [2019-10-02](#2019-10-02)
  - [1. Interrogating the Explanatory Power of Attention in Neural Machine Translation](#2019-10-02-1)
  - [2. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations](#2019-10-02-2)
  - [3. Multilingual End-to-End Speech Translation](#2019-10-02-3)
  - [4. When and Why is Document-level Context Useful in Neural Machine Translation?](#2019-10-02-4)
  - [5. Grammatical Error Correction in Low-Resource Scenarios](#2019-10-02-5)
  - [6. Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair](#2019-10-02-6)
  - [7. A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation](#2019-10-02-7)
  - [8. Machine Translation for Machines: the Sentiment Classification Use Case](#2019-10-02-8)
  - [9. Putting Machine Translation in Context with the Noisy Channel Model](#2019-10-02-9)
- [2019-10-01](#2019-10-01)
  - [1. Revisiting Self-Training for Neural Sequence Generation](2019-10-01-1)
  - [2. The Source-Target Domain Mismatch Problem in Machine Translation](2019-10-01-2)
  - [3. Controllable Data Synthesis Method for Grammatical Error Correction](2019-10-01-3)
  - [4. Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation](2019-10-01-4)
  - [5. Simple and Effective Paraphrastic Similarity from Parallel Translations](2019-10-01-5)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-10-21

[Return to Index](#Index)



<h2 id="2019-10-21-1">1. Controlling Utterance Length in NMT-based Word Segmentation with Attention</h2>

Title: [Controlling Utterance Length in NMT-based Word Segmentation with Attention]( https://arxiv.org/abs/1910.08418 )

Authors: [Pierre Godard](https://arxiv.org/search/cs?searchtype=author&query=Godard%2C+P), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Francois Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

*(Submitted on 18 Oct 2019)*

> One of the basic tasks of computational language documentation (CLD) is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, well-resourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.

| Comments: | Accepted to IWSLT 2019 (Hong-Kong)                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.08418](https://arxiv.org/abs/1910.08418) [cs.CL]** |
|           | (or **[arXiv:1910.08418v1](https://arxiv.org/abs/1910.08418v1) [cs.CL]** for this version) |



# 2019-10-18

[Return to Index](#Index)



<h2 id="2019-10-18-1">1. LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition</h2>
Title: [LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition]( https://arxiv.org/abs/1910.07924 )

Authors: [Benjamin Beilharz](https://arxiv.org/search/cs?searchtype=author&query=Beilharz%2C+B), [Xin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Sariya Karimova](https://arxiv.org/search/cs?searchtype=author&query=Karimova%2C+S), [Stefan Riezler](https://arxiv.org/search/cs?searchtype=author&query=Riezler%2C+S)

*(Submitted on 17 Oct 2019)*

> We present a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material and over 50k parallel sentences. The audio data is read speech and thus low in disfluencies. The quality of audio and sentence alignments has been checked by a manual evaluation, showing that speech alignment quality is in general very high. The sentence alignment quality is comparable to well-used parallel translation data and can be adjusted by cutoffs on the automatic alignment score. To our knowledge, this corpus is to date the largest resource for end-to-end speech translation for German.

| Comments: | LREC 2020 submission                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.07924](https://arxiv.org/abs/1910.07924) [cs.CL]** |
|           | (or **[arXiv:1910.07924v1](https://arxiv.org/abs/1910.07924v1) [cs.CL]** for this version) |



# 2019-10-17

[Return to Index](#Index)



<h2 id="2019-10-17-1">1. Root Mean Square Layer Normalization</h2>
Title: [Root Mean Square Layer Normalization]( https://arxiv.org/abs/1910.07467 )

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 16 Oct 2019)*

> Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at [this https URL](https://github.com/bzhangGo/rmsnorm).

| Comments: | NeurIPS 2019                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.07467](https://arxiv.org/abs/1910.07467) [cs.LG]** |
|           | (or **[arXiv:1910.07467v1](https://arxiv.org/abs/1910.07467v1) [cs.LG]** for this version) |





<h2 id="2019-10-17-2">2. Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models</h2>
Title: [Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models]( https://arxiv.org/abs/1910.07117 )

Authors: [Tianxing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Jun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Bing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J), [Fuchun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+F)

*(Submitted on 16 Oct 2019)*

> In this work, we study how the large-scale pretrain-finetune framework changes the behavior of a neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. We find that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training. We demonstrate the forgetting phenomenon through a detailed behavior analysis from the perspectives of context sensitivity and knowledge transfer. Adopting the concept of data mixing, we propose an intuitive fine-tuning strategy named "mix-review". We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.07117](https://arxiv.org/abs/1910.07117) [cs.CL]** |
|           | (or **[arXiv:1910.07117v1](https://arxiv.org/abs/1910.07117v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-3">3. Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings</h2>
Title: [Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings]( https://arxiv.org/abs/1910.07221 )

Authors: [Yerai Doval](https://arxiv.org/search/cs?searchtype=author&query=Doval%2C+Y), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J), [Luis Espinosa-Anke](https://arxiv.org/search/cs?searchtype=author&query=Espinosa-Anke%2C+L), [Steven Schockaert](https://arxiv.org/search/cs?searchtype=author&query=Schockaert%2C+S)

*(Submitted on 16 Oct 2019)*

> Word embeddings have become a standard resource in the toolset of any Natural Language Processing practitioner. While monolingual word embeddings encode information about words in the context of a particular language, cross-lingual embeddings define a multilingual space where word embeddings from two or more languages are integrated together. Current state-of-the-art approaches learn these embeddings by aligning two disjoint monolingual vector spaces through an orthogonal transformation which preserves the structure of the monolingual counterparts. In this work, we propose to apply an additional transformation after this initial alignment step, which aims to bring the vector representations of a given word and its translations closer to their average. Since this additional transformation is non-orthogonal, it also affects the structure of the monolingual spaces. We show that our approach both improves the integration of the monolingual spaces as well as the quality of the monolingual spaces themselves. Furthermore, because our transformation can be applied to an arbitrary number of languages, we are able to effectively obtain a truly multilingual space. The resulting (monolingual and multilingual) spaces show consistent gains over the current state-of-the-art in standard intrinsic tasks, namely dictionary induction and word similarity, as well as in extrinsic tasks such as cross-lingual hypernym discovery and cross-lingual natural language inference.

| Comments:    | 32 pages, 2 figures, 7 tables. Preprint submitted to Journal of Information Processing and Management |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| Cite as:     | **[arXiv:1910.07221](https://arxiv.org/abs/1910.07221) [cs.CL]** |
|              | (or **[arXiv:1910.07221v1](https://arxiv.org/abs/1910.07221v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-4">4. Fine-grained evaluation of German-English Machine Translation based on a Test Suite</h2>
Title: [Fine-grained evaluation of German-English Machine Translation based on a Test Suite]( https://arxiv.org/abs/1910.07460 )

Authors: [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Eleftherios Avramidis](https://arxiv.org/search/cs?searchtype=author&query=Avramidis%2C+E), [Aljoscha Burchardt](https://arxiv.org/search/cs?searchtype=author&query=Burchardt%2C+A), [Hans Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+H)

*(Submitted on 16 Oct 2019)*

> We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The test suite has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the Third Conference on Machine Translation (WMT-2018) |
| DOI:               | [10.18653/v1/W18-6436](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FW18-6436&v=5a63db80) |
| Cite as:           | **[arXiv:1910.07460](https://arxiv.org/abs/1910.07460) [cs.CL]** |
|                    | (or **[arXiv:1910.07460v1](https://arxiv.org/abs/1910.07460v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-5">5. Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite</h2>
Title: [Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite]( https://arxiv.org/abs/1910.07468 )

Authors: [Avramidis Eleftherios](https://arxiv.org/search/cs?searchtype=author&query=Eleftherios%2C+A), [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Arle Lommel](https://arxiv.org/search/cs?searchtype=author&query=Lommel%2C+A), [Hans Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+H)

*(Submitted on 16 Oct 2019)*

> We present an alternative method of evaluating Quality Estimation systems, which is based on a linguistically-motivated Test Suite. We create a test-set consisting of 14 linguistic error categories and we gather for each of them a set of samples with both correct and erroneous translations. Then, we measure the performance of 5 Quality Estimation systems by checking their ability to distinguish between the correct and the erroneous translations. The detailed results are much more informative about the ability of each system. The fact that different Quality Estimation systems perform differently at various phenomena confirms the usefulness of the Test Suite.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the First Workshop on Translation Quality Estimation and Automatic Post-Editing (QEAPE-2018) |
| Cite as:           | **[arXiv:1910.07468](https://arxiv.org/abs/1910.07468) [cs.CL]** |
|                    | (or **[arXiv:1910.07468v1](https://arxiv.org/abs/1910.07468v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-6">6. Using Whole Document Context in Neural Machine Translation</h2>
Title: [Using Whole Document Context in Neural Machine Translation]( https://arxiv.org/abs/1910.07481 )

Authors: [Valentin Macé](https://arxiv.org/search/cs?searchtype=author&query=Macé%2C+V), [Christophe Servan](https://arxiv.org/search/cs?searchtype=author&query=Servan%2C+C)

*(Submitted on 16 Oct 2019)*

> In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.

| Comments: | Accepted paper to IWSLT2019                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.07481](https://arxiv.org/abs/1910.07481) [cs.CL]** |
|           | (or **[arXiv:1910.07481v1](https://arxiv.org/abs/1910.07481v1) [cs.CL]** for this version) |



# 2019-10-16

[Return to Index](#Index)



<h2 id="2019-10-16-1">1. In-training Matrix Factorization for Parameter-frugal Neural Machine Translation</h2>
Title: [In-training Matrix Factorization for Parameter-frugal Neural Machine Translation]( https://arxiv.org/abs/1910.06393 )

Authors: [Zachary Kaden](https://arxiv.org/search/cs?searchtype=author&query=Kaden%2C+Z), [Teven Le Scao](https://arxiv.org/search/cs?searchtype=author&query=Scao%2C+T+L), [Raphael Olivier](https://arxiv.org/search/cs?searchtype=author&query=Olivier%2C+R)

*(Submitted on 27 Sep 2019)*

> In this paper, we propose the use of in-training matrix factorization to reduce the model size for neural machine translation. Using in-training matrix factorization, parameter matrices may be decomposed into the products of smaller matrices, which can compress large machine translation architectures by vastly reducing the number of learnable parameters. We apply in-training matrix factorization to different layers of standard neural architectures and show that in-training factorization is capable of reducing nearly 50% of learnable parameters without any associated loss in BLEU score. Further, we find that in-training matrix factorization is especially powerful on embedding layers, providing a simple and effective method to curtail the number of parameters with minimal impact on model performance, and, at times, an increase in performance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.06393](https://arxiv.org/abs/1910.06393) [cs.CL]** |
|           | (or **[arXiv:1910.06393v1](https://arxiv.org/abs/1910.06393v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-2">2. Mapping Supervised Bilingual Word Embeddings from English to low-resource languages</h2>
Title: [Mapping Supervised Bilingual Word Embeddings from English to low-resource languages]( https://arxiv.org/abs/1910.06411 )

Authors: [Sourav Dutta](https://arxiv.org/search/cs?searchtype=author&query=Dutta%2C+S) (1) ((1) Saarland University)

*(Submitted on 14 Oct 2019)*

> It is very challenging to work with low-resource languages due to the inadequate availability of data. Using a dictionary to map independently trained word embeddings into a shared vector space has proved to be very useful in learning bilingual embeddings in the past. Here we have tried to map individual embeddings of words in English and their corresponding translated words in low-resource languages like Estonian, Slovenian, Slovakian, and Hungarian. We have used a supervised learning approach. We report accuracy scores through various retrieval strategies which show that it is possible to approach challenging tasks in Natural Language Processing like machine translation for such languages, provided that we have at least some amount of proper bilingual data. We also conclude that we can follow an unsupervised learning path on monolingual text data as that is more suitable for low-resource languages.

| Comments: | 7 pages, 4 tables                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06411](https://arxiv.org/abs/1910.06411) [cs.CL]** |
|           | (or **[arXiv:1910.06411v1](https://arxiv.org/abs/1910.06411v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-3">3. Detecting Machine-Translated Text using Back Translation</h2>
Title: [Detecting Machine-Translated Text using Back Translation]( https://arxiv.org/abs/1910.06558 )

Authors: [Hoang-Quoc Nguyen-Son](https://arxiv.org/search/cs?searchtype=author&query=Nguyen-Son%2C+H), [Tran Phuong Thao](https://arxiv.org/search/cs?searchtype=author&query=Thao%2C+T+P), [Seira Hidano](https://arxiv.org/search/cs?searchtype=author&query=Hidano%2C+S), [Shinsaku Kiyomoto](https://arxiv.org/search/cs?searchtype=author&query=Kiyomoto%2C+S)

*(Submitted on 15 Oct 2019)*

> Machine-translated text plays a crucial role in the communication of people using different languages. However, adversaries can use such text for malicious purposes such as plagiarism and fake review. The existing methods detected a machine-translated text only using the text's intrinsic content, but they are unsuitable for classifying the machine-translated and human-written texts with the same meanings. We have proposed a method to extract features used to distinguish machine/human text based on the similarity between the intrinsic text and its back-translation. The evaluation of detecting translated sentences with French shows that our method achieves 75.0% of both accuracy and F-score. It outperforms the existing methods whose the best accuracy is 62.8% and the F-score is 62.7%. The proposed method even detects more efficiently the back-translated text with 83.4% of accuracy, which is higher than 66.7% of the best previous accuracy. We also achieve similar results not only with F-score but also with similar experiments related to Japanese. Moreover, we prove that our detector can recognize both machine-translated and machine-back-translated texts without the language information which is used to generate these machine texts. It demonstrates the persistence of our method in various applications in both low- and rich-resource languages.

| Comments: | INLG 2019, 9 pages                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06558](https://arxiv.org/abs/1910.06558) [cs.CL]** |
|           | (or **[arXiv:1910.06558v1](https://arxiv.org/abs/1910.06558v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-4">4. Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</h2>
Title: [Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation]( https://arxiv.org/abs/1910.06717 )

Authors: [Kenton Murray](https://arxiv.org/search/cs?searchtype=author&query=Murray%2C+K), [Jeffery Kinnison](https://arxiv.org/search/cs?searchtype=author&query=Kinnison%2C+J), [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Walter Scheirer](https://arxiv.org/search/cs?searchtype=author&query=Scheirer%2C+W), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

*(Submitted on 1 Oct 2019)*

> Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in machine translation. Yet these neural networks are very sensitive to architecture and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate architecture search into a single training run through auto-sizing, which uses regularization to delete neurons in a network over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the parameters from the model.

| Comments: | The 3rd Workshop on Neural Generation and Translation (WNGT 2019) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.06717](https://arxiv.org/abs/1910.06717) [cs.CL]** |
|           | (or **[arXiv:1910.06717v1](https://arxiv.org/abs/1910.06717v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-5">5. On the Importance of Word Boundaries in Character-level Neural Machine Translation</h2>
Title: [On the Importance of Word Boundaries in Character-level Neural Machine Translation]( https://arxiv.org/abs/1910.06753 )

Authors: [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Mattia A. Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

*(Submitted on 15 Oct 2019)*

> Neural Machine Translation (NMT) models generally perform translation using a fixed-size lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model.

| Comments: | To appear at the 3rd Workshop on Neural Generation and Translation (WNGT 2019) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06753](https://arxiv.org/abs/1910.06753) [cs.CL]** |
|           | (or **[arXiv:1910.06753v1](https://arxiv.org/abs/1910.06753v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-6">6. Facebook AI's WAT19 Myanmar-English Translation Task Submission</h2>
Title: [Facebook AI's WAT19 Myanmar-English Translation Task Submission]( https://arxiv.org/abs/1910.06848 )

Authors: [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Matt Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+M), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 15 Oct 2019)*

> This paper describes Facebook AI's submission to the WAT 2019 Myanmar-English translation task. Our baseline systems are BPE-based transformer models. We explore methods to leverage monolingual data to improve generalization, including self-training, back-translation and their combination. We further improve results by using noisy channel re-ranking and ensembling. We demonstrate that these techniques can significantly improve not only a system trained with additional monolingual data, but even the baseline system trained exclusively on the provided small parallel dataset. Our system ranks first in both directions according to human evaluation and BLEU, with a gain of over 8 BLEU points above the second best system.

| Comments: | The 6th Workshop on Asian Translation                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06848](https://arxiv.org/abs/1910.06848) [cs.CL]** |
|           | (or **[arXiv:1910.06848v1](https://arxiv.org/abs/1910.06848v1) [cs.CL]** for this version) |





# 2019-10-15

[Return to Index](#Index)



<h2 id="2019-10-15-1">1. From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction</h2>
Title: [From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction]( https://arxiv.org/abs/1910.05535 )

Authors: [Mika Hämäläinen](https://arxiv.org/search/cs?searchtype=author&query=Hämäläinen%2C+M), [Simon Hengchen](https://arxiv.org/search/cs?searchtype=author&query=Hengchen%2C+S)

*(Submitted on 12 Oct 2019)*

> A great deal of historical corpora suffer from errors introduced by the OCR (optical character recognition) methods used in the digitization process. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We present a fully automatic unsupervised way of extracting parallel data for training a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of Recent Advances in Natural Language Processing. Angelova, G., Mitkov, R., Nikolova, I. & Temnikova, I. (eds.). Shoumen: INCOMA, p. 432-437 6 p (2019) |
| Cite as:           | **[arXiv:1910.05535](https://arxiv.org/abs/1910.05535) [cs.CL]** |
|                    | (or **[arXiv:1910.05535v1](https://arxiv.org/abs/1910.05535v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-2">2. Transformers without Tears: Improving the Normalization of Self-Attention</h2>
Title: [Transformers without Tears: Improving the Normalization of Self-Attention]( https://arxiv.org/abs/1910.05895 )

Authors: [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J)

*(Submitted on 14 Oct 2019)*

> We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose ℓ2 normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.

| Comments: | Accepted to IWSLT'19                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.05895](https://arxiv.org/abs/1910.05895) [cs.CL]** |
|           | (or **[arXiv:1910.05895v1](https://arxiv.org/abs/1910.05895v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-3">3. Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality</h2>
Title: [Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality]( https://arxiv.org/abs/1910.06204 )

Authors: [Carolina Scarton](https://arxiv.org/search/cs?searchtype=author&query=Scarton%2C+C), [Mikel L. Forcada](https://arxiv.org/search/cs?searchtype=author&query=Forcada%2C+M+L), [Miquel Esplà-Gomis](https://arxiv.org/search/cs?searchtype=author&query=Esplà-Gomis%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 14 Oct 2019)*

> Devising metrics to assess translation quality has always been at the core of machine translation (MT) research. Traditional automatic reference-based metrics, such as BLEU, have shown correlations with human judgements of adequacy and fluency and have been paramount for the advancement of MT system development. Crowd-sourcing has popularised and enabled the scalability of metrics based on human judgements, such as subjective direct assessments (DA) of adequacy, that are believed to be more reliable than reference-based automatic metrics. Finally, task-based measurements, such as post-editing time, are expected to provide a more detailed evaluation of the usefulness of translations for a specific task. Therefore, while DA averages adequacy judgements to obtain an appraisal of (perceived) quality independently of the task, and reference-based automatic metrics try to objectively estimate quality also in a task-independent way, task-based metrics are measurements obtained either during or after performing a specific task. In this paper we argue that, although expensive, task-based measurements are the most reliable when estimating MT quality in a specific task; in our case, this task is post-editing. To that end, we report experiments on a dataset with newly-collected post-editing indicators and show their usefulness when estimating post-editing effort. Our results show that task-based metrics comparing machine-translated and post-edited versions are the best at tracking post-editing effort, as expected. These metrics are followed by DA, and then by metrics comparing the machine-translated version and independent references. We suggest that MT practitioners should be aware of these differences and acknowledge their implications when deciding how to evaluate MT for post-editing purposes.

| Comments: | IWSLT 2019, Hong Kong, November 2 and 3, 2019                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06204](https://arxiv.org/abs/1910.06204) [cs.CL]** |
|           | (or **[arXiv:1910.06204v1](https://arxiv.org/abs/1910.06204v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-4">4. Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment</h2>
Title: [Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment]( https://arxiv.org/abs/1910.06241 )

Authors: [Piotr Bojanowski](https://arxiv.org/search/cs?searchtype=author&query=Bojanowski%2C+P), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Tomas Mikolov](https://arxiv.org/search/cs?searchtype=author&query=Mikolov%2C+T), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A)

*(Submitted on 14 Oct 2019)*

> In this paper, we focus on the problem of adapting word vector-based models to new textual data. Given a model pre-trained on large reference data, how can we adapt it to a smaller piece of data with a slightly different language distribution? We frame the adaptation problem as a monolingual word vector alignment problem, and simply average models after alignment. We align vectors using the RCSLS criterion. Our formulation results in a simple and efficient algorithm that allows adapting general-purpose models to changing word distributions. In our evaluation, we consider applications to word embedding and text classification models. We show that the proposed approach yields good performance in all setups and outperforms a baseline consisting in fine-tuning the model on new data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.06241](https://arxiv.org/abs/1910.06241) [cs.CL]** |
|           | (or **[arXiv:1910.06241v1](https://arxiv.org/abs/1910.06241v1) [cs.CL]** for this version) |





# 2019-10-14

[Return to Index](#Index)

<h2 id="2019-10-14-1">1. How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages</h2>
Title: [How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages]( https://arxiv.org/abs/1910.05154 )

Authors: [Marcely Zanon Boito](https://arxiv.org/search/cs?searchtype=author&query=Boito%2C+M+Z), [Aline Villavicencio](https://arxiv.org/search/cs?searchtype=author&query=Villavicencio%2C+A), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 11 Oct 2019)*

> For language documentation initiatives, transcription is an expensive resource: one minute of audio is estimated to take one hour and a half on average of a linguist's work (Austin and Sallabank, 2013). Recently, collecting aligned translations in well-resourced languages became a popular solution for ensuring posterior interpretability of the recordings (Adda et al. 2016). In this paper we investigate language-related impact in automatic approaches for computational language documentation. We translate the bilingual Mboshi-French parallel corpus (Godard et al. 2017) into four other languages, and we perform bilingual-rooted unsupervised word discovery. Our results hint towards an impact of the well-resourced language in the quality of the output. However, by combining the information learned by different bilingual models, we are only able to marginally increase the quality of the segmentation.

| Comments: | 4 pages, workshop LIFT 2019                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.05154](https://arxiv.org/abs/1910.05154) [cs.CL]** |
|           | (or **[arXiv:1910.05154v1](https://arxiv.org/abs/1910.05154v1) [cs.CL]** for this version) |





# 2019-10-11

[Return to Index](#Index)



<h2 id="2019-10-11-1">1. Novel Applications of Factored Neural Machine Translation</h2>
Title: [Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](https://arxiv.org/abs/1910.04708)

Authors: [Zirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Jiateng Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Ruochen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jaime Carbonell](https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J)

*(Submitted on 10 Oct 2019)*

> Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these methods across diverse cross-lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task-dependent. Stemming from this analysis, we propose a simple and novel framework that combines these two previously mutually-exclusive approaches. Extensive experiments on various tasks demonstrate that our proposed framework alleviates limitations of both approaches, and outperforms existing methods on the MUSE bilingual lexicon induction (BLI) benchmark. We further show that our proposed framework can generalize to contextualized representations and achieves state-of-the-art results on the CoNLL cross-lingual NER benchmark.

| Comments: | First two authors contributted equally. Source code is available at [this https URL](https://github.com/thespectrewithin/joint-align) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.04708 [cs.CL]**                                 |
|           | (or **arXiv:1910.04708v1 [cs.CL]** for this version)         |





<h2 id="2019-10-11-2">2. Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)</h2>
Title: [Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)](https://arxiv.org/abs/1910.04731)

Authors: [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Karin Sevegnani](https://arxiv.org/search/cs?searchtype=author&query=Sevegnani%2C+K), [Ioannis Konstas](https://arxiv.org/search/cs?searchtype=author&query=Konstas%2C+I), [Verena Rieser](https://arxiv.org/search/cs?searchtype=author&query=Rieser%2C+V)

*(Submitted on 10 Oct 2019)*

> We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network.
> We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: we synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12% increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dušek et al., 2019), where synthetic data lead to a 4% accuracy increase over the base model.

| Comments:    | Accepted as a short paper at INLG 2019               |
| ------------ | ---------------------------------------------------- |
| Subjects:    | **Computation and Language (cs.CL)**                 |
| ACM classes: | I.2.7                                                |
| Cite as:     | **arXiv:1910.04731 [cs.CL]**                         |
|              | (or **arXiv:1910.04731v1 [cs.CL]** for this version) |





# 2019-10-10

[Return to Index](#Index)

<h2 id="2019-10-10-1">1. Novel Applications of Factored Neural Machine Translation</h2>
Title: [Novel Applications of Factored Neural Machine Translation](https://arxiv.org/abs/1910.03912)

Authors: [Patrick Wilken](https://arxiv.org/search/cs?searchtype=author&query=Wilken%2C+P), [Evgeny Matusov](https://arxiv.org/search/cs?searchtype=author&query=Matusov%2C+E)

*(Submitted on 9 Oct 2019)*

> In this work, we explore the usefulness of target factors in neural machine translation (NMT) beyond their original purpose of predicting word lemmas and their inflections, as proposed by Garcìa-Martìnez et al., 2016. For this, we introduce three novel applications of the factored output architecture: In the first one, we use a factor to explicitly predict the word case separately from the target word itself. This allows for information to be shared between different casing variants of a word. In a second task, we use a factor to predict when two consecutive subwords have to be joined, eliminating the need for target subword joining markers. The third task is the prediction of special tokens of the operation sequence NMT model (OSNMT) of Stahlberg et al., 2018. Automatic evaluation on English-to-German and English-to-Turkish tasks showed that integration of such auxiliary prediction tasks into NMT is at least as good as the standard NMT approach. For the OSNMT, we observed a significant improvement in BLEU over the baseline OSNMT implementation due to a reduced output sequence length that resulted from the introduction of the target factors.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.03912 [cs.CL]**                                 |
|           | (or **arXiv:1910.03912v1 [cs.CL]** for this version)         |



# 2019-10-09

[Return to Index](#Index)



<h2 id="2019-10-09-1">1. Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation</h2>
Title: [Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation](https://arxiv.org/abs/1910.03009)

Authors: [Zhenhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 7 Oct 2019)*

> Neural Machine Translation (NMT) models have been proved strong when translating clean texts, but they are very sensitive to noise in the input. Improving NMT models robustness can be seen as a form of "domain'' adaption to noise. The recently created Machine Translation on Noisy Text task corpus provides noisy-clean parallel data for a few language pairs, but this data is very limited in size and diversity. The state-of-the-art approaches are heavily dependent on large volumes of back-translated data. This paper has two main contributions: Firstly, we propose new data augmentation methods to extend limited noisy data and further improve NMT robustness to noise while keeping the models small. Secondly, we explore the effect of utilizing noise from external data in the form of speech transcripts and show that it could help robustness.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.03009 [cs.CL]**                         |
|           | (or **arXiv:1910.03009v1 [cs.CL]** for this version) |





<h2 id="2019-10-09-2">2. Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations</h2>
Title: [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://arxiv.org/abs/1910.03065)

Authors: [Camburu Oana-Maria](https://arxiv.org/search/cs?searchtype=author&query=Oana-Maria%2C+C), [Shillingford Brendan](https://arxiv.org/search/cs?searchtype=author&query=Brendan%2C+S), [Minervini Pasquale](https://arxiv.org/search/cs?searchtype=author&query=Pasquale%2C+M), [Lukasiewicz Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+L), [Blunsom Phil](https://arxiv.org/search/cs?searchtype=author&query=Phil%2C+B)

*(Submitted on 7 Oct 2019)*

> To increase trust in artificial intelligence systems, a growing amount of works are enhancing these systems with the capability of producing natural language explanations that support their predictions. In this work, we show that such appealing frameworks are nonetheless prone to generating inconsistent explanations, such as "A dog is an animal" and "A dog is not an animal", which are likely to decrease users' trust in these systems. To detect such inconsistencies, we introduce a simple but effective adversarial framework for generating a complete target sequence, a scenario that has not been addressed so far. Finally, we apply our framework to a state-of-the-art neural model that provides natural language explanations on SNLI, and we show that this model is capable of generating a significant amount of inconsistencies.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | NeurIPS 2019 Workshop on Safety and Robustness in Decision Making, Vancouver, Canada |
| Cite as:           | **arXiv:1910.03065 [cs.CL]**                                 |
|                    | (or **arXiv:1910.03065v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-3">3. One-To-Many Multilingual End-to-end Speech Translation</h2>
Title: [One-To-Many Multilingual End-to-end Speech Translation](https://arxiv.org/abs/1910.03320)

Authors: [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 8 Oct 2019)*

> Nowadays, training end-to-end neural models for spoken language translation (SLT) still has to confront with extreme data scarcity conditions. The existing SLT parallel corpora are indeed orders of magnitude smaller than those available for the closely related tasks of automatic speech recognition (ASR) and machine translation (MT), which usually comprise tens of millions of instances. To cope with data paucity, in this paper we explore the effectiveness of transfer learning in end-to-end SLT by presenting a multilingual approach to the task. Multilingual solutions are widely studied in MT and usually rely on ``\textit{target forcing}'', in which multilingual parallel data are combined to train a single model by prepending to the input sequences a language token that specifies the target language. However, when tested in speech translation, our experiments show that MT-like \textit{target forcing}, used as is, is not effective in discriminating among the target languages. Thus, we propose a variant that uses target-language embeddings to shift the input representations in different portions of the space according to the language, so to better support the production of output in the desired target language. Our experiments on end-to-end SLT from English into six languages show important improvements when translating into similar languages, especially when these are supported by scarce data. Further improvements are obtained when using English ASR data as an additional language (up to *[Math Processing Error]* BLEU points).

| Comments: | 8 pages, one figure, version accepted at ASRU 2019           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.03320 [cs.CL]**                                 |
|           | (or **arXiv:1910.03320v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-4">4. An Interactive Machine Translation Framework for Modernizing Historical Documents</h2>
Title: [An Interactive Machine Translation Framework for Modernizing Historical Documents](https://arxiv.org/abs/1910.03355)

Authors: [Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F)

*(Submitted on 8 Oct 2019)*

> Due to the nature of human language, historical documents are hard to comprehend by contemporary people. This limits their accessibility to scholars specialized in the time period in which the documents were written. Modernization aims at breaking this language barrier by generating a new version of a historical document, written in the modern version of the document's original language. However, while it is able to increase the document's comprehension, modernization is still far from producing an error-free version. In this work, we propose a collaborative framework in which a scholar can work together with the machine to generate the new version. We tested our approach on a simulated environment, achieving significant reductions of the human effort needed to produce the modernized version of the document.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.03355 [cs.CL]**                         |
|           | (or **arXiv:1910.03355v1 [cs.CL]** for this version) |





<h2 id="2019-10-09-5">5. Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation</h2>
Title: [Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation](https://arxiv.org/abs/1910.03467)

Authors: [Thi-Vinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+T), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Phuong-Thai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P), [Le-Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L)

*(Submitted on 7 Oct 2019)*

> Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017), rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two low-resource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved significant improvements of up to roughly +1.0 BLEU points in both language pairs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.03467 [cs.CL]**                                 |
|           | (or **arXiv:1910.03467v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-6">6. Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents</h2>
Title: [Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents](https://arxiv.org/abs/1910.03487)

Authors: [Nikolaos Malandrakis](https://arxiv.org/search/cs?searchtype=author&query=Malandrakis%2C+N), [Minmin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+M), [Anuj Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+A), [Shuyang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+S), [Abhishek Sethi](https://arxiv.org/search/cs?searchtype=author&query=Sethi%2C+A), [Angeliki Metallinou](https://arxiv.org/search/cs?searchtype=author&query=Metallinou%2C+A)

*(Submitted on 4 Oct 2019)*

> Data availability is a bottleneck during early stages of development of new capabilities for intelligent artificial agents. We investigate the use of text generation techniques to augment the training data of a popular commercial artificial agent across categories of functionality, with the goal of faster development of new functionality. We explore a variety of encoder-decoder generative models for synthetic training data generation and propose using conditional variational auto-encoders. Our approach requires only direct optimization, works well with limited data and significantly outperforms the previous controlled text generation techniques. Further, the generated data are used as additional training samples in an extrinsic intent classification task, leading to improved performance by up to 5\% absolute f-score in low-resource cases, validating the usefulness of our approach.

| Comments: | EMNLP WNGT workshop                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1910.03487 [cs.CL]**                                 |
|           | (or **arXiv:1910.03487v1 [cs.CL]** for this version)         |



# 2019-10-08

[Return to Index](#Index)



<h2 id="2019-10-08-1">1. How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems</h2>
Title: [How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems](https://arxiv.org/abs/1910.02238)

Authors:[Thi-Vinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+T), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Phuong-Thai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P), [Le-Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L)

*(Submitted on 5 Oct 2019)*

> While translating between Chinese-centric languages, many works have discovered clear advantages of using characters as the translation unit. Unfortunately, traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. They are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. In this paper, we demonstrate that the new transformer architecture can perform character-based translation better than the recurrent one. We conduct experiments on a low-resource language pair: Japanese-Vietnamese. Our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.02238 [cs.CL]**                         |
|           | (or **arXiv:1910.02238v1 [cs.CL]** for this version) |





<h2 id="2019-10-08-2">2. Domain Differential Adaptation for Neural Machine Translation</h2>
Title: [Domain Differential Adaptation for Neural Machine Translation](https://arxiv.org/abs/1910.02555)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 7 Oct 2019)*

> Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of {\it Domain Differential Adaptation (DDA)}, where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.

| Comments: | Workshop on Neural Generation and Translation (WNGT) at EMNLP 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.02555 [cs.CL]**                                 |
|           | (or **arXiv:1910.02555v1 [cs.CL]** for this version)         |





<h2 id="2019-10-08-3">3. On Leveraging the Visual Modality for Neural Machine Translation</h2>
Title: [On Leveraging the Visual Modality for Neural Machine Translation](https://arxiv.org/abs/1910.02754)

Authors:[Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Sang Keun Choe](https://arxiv.org/search/cs?searchtype=author&query=Choe%2C+S+K), [Quanyang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Q), [Yi Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F)

*(Submitted on 7 Oct 2019)*

> Leveraging the visual modality effectively for Neural Machine Translation (NMT) remains an open problem in computational linguistics. Recently, Caglayan et al. posit that the observed gains are limited mainly due to the very simple, short, repetitive sentences of the Multi30k dataset (the only multimodal MT dataset available at the time), which renders the source text sufficient for context. In this work, we further investigate this hypothesis on a new large scale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times longer mean sentence length than Multi30k and no repetition. We propose and evaluate three novel fusion techniques, each of which is designed to ensure the utilization of visual context at different stages of the Sequence-to-Sequence transduction pipeline, even under full linguistic context. However, we still obtain only marginal gains under full linguistic context and posit that visual embeddings extracted from deep vision models (ResNet for Multi30k, ResNext for How2) do not lend themselves to increasing the discriminativeness between the vocabulary elements at token level prediction in NMT. We demonstrate this qualitatively by analyzing attention distribution and quantitatively through Principal Component Analysis, arriving at the conclusion that it is the quality of the visual embeddings rather than the length of sentences, which need to be improved in existing MMT datasets.

| Comments: | Accepted to INLG 2019                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.02754 [cs.CL]**                                 |
|           | (or **arXiv:1910.02754v1 [cs.CL]** for this version)         |





<h2 id="2019-10-08-4">4. Adversarial reconstruction for Multi-modal Machine Translation</h2>
Title: [Adversarial reconstruction for Multi-modal Machine Translation](https://arxiv.org/abs/1910.02766)

Authors:[Jean-Benoit Delbrouck](https://arxiv.org/search/cs?searchtype=author&query=Delbrouck%2C+J), [Stéphane Dupont](https://arxiv.org/search/cs?searchtype=author&query=Dupont%2C+S)

*(Submitted on 7 Oct 2019)*

> Even with the growing interest in problems at the intersection of Computer Vision and Natural Language, grounding (i.e. identifying) the components of a structured description in an image still remains a challenging task. This contribution aims to propose a model which learns grounding by reconstructing the visual features for the Multi-modal translation task. Previous works have partially investigated standard approaches such as regression methods to approximate the reconstruction of a visual input. In this paper, we propose a different and novel approach which learns grounding by adversarial feedback. To do so, we modulate our network following the recent promising adversarial architectures and evaluate how the adversarial response from a visual reconstruction as an auxiliary task helps the model in its learning. We report the highest scores in term of BLEU and METEOR metrics on the different datasets.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.02766 [cs.CL]**                         |
|           | (or **arXiv:1910.02766v1 [cs.CL]** for this version) |







# 2019-10-07

[Return to Index](#Index)



<h2 id="2019-10-07-1">1. Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data</h2>
Title: [Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data](https://arxiv.org/abs/1910.01769)

Authors: [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

*(Submitted on 4 Oct 2019)*

> Recent advances in pre-training huge models on large amounts of text through self supervision have obtained state-of-the-art results in various natural language processing tasks. However, these huge and expensive models are difficult to use in practise for downstream tasks. Some recent efforts use knowledge distillation to compress these models. However, we see a gap between the performance of the smaller student models as compared to that of the large teacher. In this work, we leverage large amounts of in-domain unlabeled transfer data in addition to a limited amount of labeled training instances to bridge this gap. We show that simple RNN based student models even with hard distillation can perform at par with the huge teachers given the transfer set. The student performance can be further improved with soft distillation and leveraging teacher intermediate representations. We show that our student models can compress the huge teacher by up to 26x while still matching or even marginally exceeding the teacher performance in low-resource settings with small amount of labeled data.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.01769 [cs.CL]**                                 |
|           | (or **arXiv:1910.01769v1 [cs.CL]** for this version)         |





<h2 id="2019-10-07-2">2. Modeling Confidence in Sequence-to-Sequence Models</h2>
Title: [Modeling Confidence in Sequence-to-Sequence Models](https://arxiv.org/abs/1910.01859)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J), [Ngoc-Quan Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+N)

*(Submitted on 4 Oct 2019)*

> Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output.
> In this work, we propose to use the similarity between training and test conditions as a measure for models' confidence. We investigate methods solely using the similarity as well as methods combining it with the posterior probability. While traditionally only target tokens are annotated with confidence measures, we also investigate methods to annotate source tokens with confidence. By learning an internal alignment model, we can significantly improve confidence projection over using state-of-the-art external alignment tools. We evaluate the proposed methods on downstream confidence estimation for machine translation (MT). We show improvements on segment-level confidence estimation as well as on confidence estimation for source tokens. In addition, we show that the same methods can also be applied to other tasks using sequence-to-sequence models. On the automatic speech recognition (ASR) task, we are able to find 60% of the errors by looking at 20% of the data.

| Comments: | 8 pages; INLG 2019                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.01859 [cs.CL]**                         |
|           | (or **arXiv:1910.01859v1 [cs.CL]** for this version) |





<h2 id="2019-10-07-3">3. Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods</h2>
Title: [Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods](https://arxiv.org/abs/1910.02065)

Authors: [Camburu Oana-Maria](https://arxiv.org/search/cs?searchtype=author&query=Oana-Maria%2C+C), [Giunchiglia Eleonora](https://arxiv.org/search/cs?searchtype=author&query=Eleonora%2C+G), [Foerster Jakob](https://arxiv.org/search/cs?searchtype=author&query=Jakob%2C+F), [Lukasiewicz Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+L), [Blunsom Phil](https://arxiv.org/search/cs?searchtype=author&query=Phil%2C+B)

*(Submitted on 4 Oct 2019)*

> For AI systems to garner widespread public acceptance, we must develop methods capable of explaining the decisions of black-box models such as neural networks. In this work, we identify two issues of current explanatory methods. First, we show that two prevalent perspectives on explanations---feature-additivity and feature-selection---lead to fundamentally different instance-wise explanations. In the literature, explainers from different perspectives are currently being directly compared, despite their distinct explanation goals. The second issue is that current post-hoc explainers have only been thoroughly validated on simple models, such as linear regression, and, when applied to real-world neural networks, explainers are commonly evaluated under the assumption that the learned models behave reasonably. However, neural networks often rely on unreasonable correlations, even when producing correct decisions. We introduce a verification framework for explanatory methods under the feature-selection perspective. Our framework is based on a non-trivial neural network architecture trained on a real-world task, and for which we are able to provide guarantees on its inner workings. We validate the efficacy of our evaluation by showing the failure modes of current explainers. We aim for this framework to provide a publicly available, off-the-shelf evaluation when the feature-selection perspective on explanations is needed.

| Comments: | NeurIPS 2019 Workshop on Safety and Robustness in Decision Making, Vancouver, Canada |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.02065 [cs.CL]**                                 |
|           | (or **arXiv:1910.02065v1 [cs.CL]** for this version)         |





# 2019-10-04

[Return to Index](#Index)



<h2 id="2019-10-04-1">1. Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</h2>
Title: [Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations](https://arxiv.org/abs/1910.01157)

Authors: [Jeff Da](https://arxiv.org/search/cs?searchtype=author&query=Da%2C+J), [Jungo Kusai](https://arxiv.org/search/cs?searchtype=author&query=Kusai%2C+J)

*(Submitted on 2 Oct 2019)*

> Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT's commonsense representation abilities. First, we probe BERT's ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT's pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.

| Comments: | Accepted to EMNLP Commonsense (COIN)                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.01157 [cs.CL]**                         |
|           | (or **arXiv:1910.01157v1 [cs.CL]** for this version) |





<h2 id="2019-10-04-2">2. Linking artificial and human neural representations of language</h2>
Title: [Linking artificial and human neural representations of language](https://arxiv.org/abs/1910.01244)

Authors: [Jon Gauthier](https://arxiv.org/search/cs?searchtype=author&query=Gauthier%2C+J), [Roger Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+R)

*(Submitted on 2 Oct 2019)*

> What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance.
> We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neurons and Cognition (q-bio.NC) |
| Cite as:  | **arXiv:1910.01244 [cs.CL]**                                 |
|           | (or **arXiv:1910.01244v1 [cs.CL]** for this version)         |





# 2019-10-03

[Return to Index](#Index)



<h2 id="2019-10-03-1">1. Speech-to-speech Translation between Untranscribed Unknown Languages</h2>
Title: [Speech-to-speech Translation between Untranscribed Unknown Languages](https://arxiv.org/abs/1910.00795)

Authors: [Andros Tjandra](https://arxiv.org/search/cs?searchtype=author&query=Tjandra%2C+A), [Sakriani Sakti](https://arxiv.org/search/cs?searchtype=author&query=Sakti%2C+S), [Satoshi Nakamura](https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+S)

*(Submitted on 2 Oct 2019)*

> In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target language's discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.

| Comments: | Accepted in IEEE ASRU 2019. Web-page for more samples & details: [this https URL](https://sp2code-translation-v1.netlify.com/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.00795 [cs.CL]**                                 |
|           | (or **arXiv:1910.00795v1 [cs.CL]** for this version)         |





<h2 id="2019-10-03-2">2. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</h2>
Title: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

Authors: [Victor Sanh](https://arxiv.org/search/cs?searchtype=author&query=Sanh%2C+V), [Lysandre Debut](https://arxiv.org/search/cs?searchtype=author&query=Debut%2C+L), [Julien Chaumond](https://arxiv.org/search/cs?searchtype=author&query=Chaumond%2C+J), [Thomas Wolf](https://arxiv.org/search/cs?searchtype=author&query=Wolf%2C+T)

*(Submitted on 2 Oct 2019)*

> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remain challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.

| Comments: | 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.01108 [cs.CL]**                                 |
|           | (or **arXiv:1910.01108v1 [cs.CL]** for this version)         |



# 2019-10-02

[Return to Index](#Index)



<h2 id="2019-10-02-1">1. Interrogating the Explanatory Power of Attention in Neural Machine Translation</h2>
Title: [Interrogating the Explanatory Power of Attention in Neural Machine Translation](https://arxiv.org/abs/1910.00139)

Authors: [Pooya Moradi](https://arxiv.org/search/cs?searchtype=author&query=Moradi%2C+P), [Nishant Kambhatla](https://arxiv.org/search/cs?searchtype=author&query=Kambhatla%2C+N), [Anoop Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+A)

*(Submitted on 30 Sep 2019)*

> Attention models have become a crucial component in neural machine translation (NMT). They are often implicitly or explicitly used to justify the model's decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in NMT. To evaluate the explanatory power of attention for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained attention model. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68% of function words and 21% of content words in our German-English dataset. Our experiments demonstrate that attention models by themselves cannot reliably explain the decisions made by a NMT model.

| Comments: | Accepted at the 3rd Workshop on Neural Generation and Translation (WNGT 2019) held at EMNLP-IJCNLP 2019 (Camera ready) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.00139 [cs.CL]**                                 |
|           | (or **arXiv:1910.00139v1 [cs.CL]** for this version)         |





<h2 id="2019-10-02-2">2. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations</h2>
Title: [Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations](https://arxiv.org/abs/1910.00194)

Authors: [Christian Hadiwinoto](https://arxiv.org/search/cs?searchtype=author&query=Hadiwinoto%2C+C), [Hwee Tou Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+H+T), [Wee Chung Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+W+C)

*(Submitted on 1 Oct 2019)*

> Contextualized word representations are able to give different representations for the same word in different contexts, and they have been shown to be effective in downstream natural language processing tasks, such as question answering, named entity recognition, and sentiment analysis. However, evaluation on word sense disambiguation (WSD) in prior work shows that using contextualized word representations does not outperform the state-of-the-art approach that makes use of non-contextualized word embeddings. In this paper, we explore different strategies of integrating pre-trained contextualized word representations and our best strategy achieves accuracies exceeding the best prior published accuracies by significant margins on multiple benchmark WSD datasets.

| Comments: | 10 pages, 2 figures, EMNLP 2019                      |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00194 [cs.CL]**                         |
|           | (or **arXiv:1910.00194v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-3">3. Multilingual End-to-End Speech Translation</h2>
Title: [Multilingual End-to-End Speech Translation](https://arxiv.org/abs/1910.00254)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

*(Submitted on 1 Oct 2019)*

> In this paper, we propose a simple yet effective framework for multilingual end-to-end speech translation (ST), in which speech utterances in source languages are directly translated to the desired target languages with a universal sequence-to-sequence architecture. While multilingual models have shown to be useful for automatic speech recognition (ASR) and machine translation (MT), this is the first time they are applied to the end-to-end ST problem. We show the effectiveness of multilingual end-to-end ST in two scenarios: one-to-many and many-to-many translations with publicly available data. We experimentally confirm that multilingual end-to-end ST models significantly outperform bilingual ones in both scenarios. The generalization of multilingual training is also evaluated in a transfer learning scenario to a very low-resource language pair. All of our codes and the database are publicly available to encourage further research in this emergent multilingual ST topic.

| Comments: | Accepted to ASRU 2019                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.00254 [cs.CL]**                                 |
|           | (or **arXiv:1910.00254v1 [cs.CL]** for this version)         |





<h2 id="2019-10-02-4">4. When and Why is Document-level Context Useful in Neural Machine Translation?</h2>
Title: [When and Why is Document-level Context Useful in Neural Machine Translation?](https://arxiv.org/abs/1910.00294)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Duc Thanh Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+D+T), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 1 Oct 2019)*

> Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT.

| Comments: | DiscoMT 2019 camera-ready                            |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00294 [cs.CL]**                         |
|           | (or **arXiv:1910.00294v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-5">5. Grammatical Error Correction in Low-Resource Scenarios</h2>
Title: [Grammatical Error Correction in Low-Resource Scenarios](https://arxiv.org/abs/1910.00353)

Authors: [Jakub Náplava](https://arxiv.org/search/cs?searchtype=author&query=Náplava%2C+J), [Milan Straka](https://arxiv.org/search/cs?searchtype=author&query=Straka%2C+M)

*(Submitted on 1 Oct 2019 ([v1](https://arxiv.org/abs/1910.00353v1)), last revised 2 Oct 2019 (this version, v2))*

> Grammatical error correction in English is a long studied problem with many existing systems and datasets. However, there has been only a limited research on error correction of other languages. In this paper, we present a new dataset AKCES-GEC on grammatical error correction for Czech. We then make experiments on Czech, German and Russian and show that when utilizing synthetic parallel corpus, Transformer neural machine translation model can reach new state-of-the-art results on these datasets. AKCES-GEC is published under CC BY-NC-SA 4.0 license at [this https URL](https://hdl.handle.net/11234/1-3057) and the source code of the GEC model is available at [this https URL](https://github.com/ufal/low-resource-gec-wnut2019).

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.00353 [cs.CL]**                         |
|           | (or **arXiv:1910.00353v2 [cs.CL]** for this version) |





<h2 id="2019-10-02-6">6. Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair</h2>
Title: [Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair](https://arxiv.org/abs/1910.00368)

Authors: [Aidar Valeev](https://arxiv.org/search/cs?searchtype=author&query=Valeev%2C+A), [Ilshat Gibadullin](https://arxiv.org/search/cs?searchtype=author&query=Gibadullin%2C+I), [Albina Khusainova](https://arxiv.org/search/cs?searchtype=author&query=Khusainova%2C+A), [Adil Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A)

*(Submitted on 1 Oct 2019)*

> Neural machine translation is the current state-of-the-art in machine translation. Although it is successful in a resource-rich setting, its applicability for low-resource language pairs is still debatable. In this paper, we explore the effect of different techniques to improve machine translation quality when a parallel corpus is as small as 324 000 sentences, taking as an example previously unexplored Russian-Tatar language pair. We apply such techniques as transfer learning and semi-supervised learning to the base Transformer model, and empirically show that the resulting models improve Russian to Tatar and Tatar to Russian translation quality by +2.57 and +3.66 BLEU, respectively.

| Comments: | Presented on ICATHS'19                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00368 [cs.CL]**                         |
|           | (or **arXiv:1910.00368v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-7">7. A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation</h2>
Title: [A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation](https://arxiv.org/abs/1910.00373)

Authors: [Ilshat Gibadullin](https://arxiv.org/search/cs?searchtype=author&query=Gibadullin%2C+I), [Aidar Valeev](https://arxiv.org/search/cs?searchtype=author&query=Valeev%2C+A), [Albina Khusainova](https://arxiv.org/search/cs?searchtype=author&query=Khusainova%2C+A), [Adil Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A)

*(Submitted on 1 Oct 2019)*

> Neural machine translation has become the state-of-the-art for language pairs with large parallel corpora. However, the quality of machine translation for low-resource languages leaves much to be desired. There are several approaches to mitigate this problem, such as transfer learning, semi-supervised and unsupervised learning techniques. In this paper, we review the existing methods, where the main idea is to exploit the power of monolingual data, which, compared to parallel, is usually easier to obtain and significantly greater in amount.

| Comments: | Presented in ICATHS'19                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00373 [cs.CL]**                         |
|           | (or **arXiv:1910.00373v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-8">8. Machine Translation for Machines: the Sentiment Classification Use Case</h2>
Title: [Machine Translation for Machines: the Sentiment Classification Use Case](https://arxiv.org/abs/1910.00478)

Authors: [Amirhossein Tebbifakhr](https://arxiv.org/search/cs?searchtype=author&query=Tebbifakhr%2C+A), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 1 Oct 2019)*

> We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency ("human-oriented" quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a "machine-oriented" criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.00478 [cs.CL]**                         |
|           | (or **arXiv:1910.00478v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-9">9. Putting Machine Translation in Context with the Noisy Channel Model</h2>
Title: [Putting Machine Translation in Context with the Noisy Channel Model](https://arxiv.org/abs/1910.00553)

Authors: [Lei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Laurent Sartran](https://arxiv.org/search/cs?searchtype=author&query=Sartran%2C+L), [Wojciech Stokowiec](https://arxiv.org/search/cs?searchtype=author&query=Stokowiec%2C+W), [Wang Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+W), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Phil Blunsom](https://arxiv.org/search/cs?searchtype=author&query=Blunsom%2C+P), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C)

*(Submitted on 1 Oct 2019)*

> We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperforms comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.00553 [cs.CL]**                                 |
|           | (or **arXiv:1910.00553v1 [cs.CL]** for this version)         |





# 2019-10-01

[Return to Index](#Index)



<h2 id="2019-10-01-1">1. Revisiting Self-Training for Neural Sequence Generation</h2> 
Title: [Revisiting Self-Training for Neural Sequence Generation](https://arxiv.org/abs/1909.13788)

Authors:[Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 30 Sep 2019)*

> Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a "noisy" version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.13788 [cs.LG]**                                 |
|           | (or **arXiv:1909.13788v1 [cs.LG]** for this version)         |





<h2 id="2019-10-01-2">2. The Source-Target Domain Mismatch Problem in Machine Translation</h2> 
Title: [The Source-Target Domain Mismatch Problem in Machine Translation](https://arxiv.org/abs/1909.13151)

Authors:[Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Matt Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+M), [Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 28 Sep 2019)*

> While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that particularly in low resource settings this causes the domains of the source and target language to greatly mismatch, as the two languages are often spoken in further apart regions of the world with more distinctive cultural traits and unrelated local events. In this work we first propose a controlled setting to carefully analyze the source-target domain mismatch, and its dependence on the amount of parallel and monolingual data. Second, we test both a model trained with back-translation and one trained with self-training. The latter leverages in-domain source monolingual data but uses potentially incorrect target references. We found that these two approaches are often complementary to each other. For instance, on a low-resource Nepali-English dataset the combined approach improves upon the baseline using just parallel data by 2.5 BLEU points, and by 0.6 BLEU point when compared to back-translation.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.13151 [cs.CL]**                         |
|           | (or **arXiv:1909.13151v1 [cs.CL]** for this version) |





<h2 id="2019-10-01-3">3. Controllable Data Synthesis Method for Grammatical Error Correction</h2> 
Title: [Controllable Data Synthesis Method for Grammatical Error Correction](https://arxiv.org/abs/1909.13302)

Authors:[Chencheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yongping Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Erhong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+E)

*(Submitted on 29 Sep 2019 ([v1](https://arxiv.org/abs/1909.13302v1)), last revised 2 Oct 2019 (this version, v2))*

> Due to the lack of parallel data in current Grammatical Error Correction (GEC) task, models based on Sequence to Sequence framework cannot be adequately trained to obtain higher performance. We propose two data synthesis methods which can control the error rate and the ratio of error types on synthetic data. The first approach is to corrupt each word in the monolingual corpus with a fixed probability, including replacement, insertion and deletion. Another approach is to train error generation models and further filtering the decoding results of the models. The experiments on different synthetic data show that the error rate is 40% and the ratio of error types is the same can improve the model performance better. Finally, we synthesize about 100 million data and achieve comparable performance as the state of the art, which uses twice as much data as we use.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.13302 [cs.CL]**                         |
|           | (or **arXiv:1909.13302v2 [cs.CL]** for this version) |





<h2 id="2019-10-01-4">4. Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation</h2> 
Title: [Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation](https://arxiv.org/abs/1909.13466)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Ehsan Zare Borzeshi](https://arxiv.org/search/cs?searchtype=author&query=Borzeshi%2C+E+Z), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

*(Submitted on 30 Sep 2019)*

> In recent years, neural machine translation (NMT) has become the dominant approach in automated translation. However, like many other deep learning approaches, NMT suffers from overfitting when the amount of training data is limited. This is a serious issue for low-resource language pairs and many specialized translation domains that are inherently limited in the amount of available supervised data. For this reason, in this paper we propose regressing word (ReWE) and sentence (ReSE) embeddings at training time as a way to regularize NMT models and improve their generalization. During training, our models are trained to jointly predict categorical (words in the vocabulary) and continuous (word and sentence embeddings) outputs. An extensive set of experiments over four language pairs of variable training set size has showed that ReWE and ReSE can outperform strong state-of-the-art baseline models, with an improvement that is larger for smaller training sets (e.g., up to +5:15 BLEU points in Basque-English translation). Visualizations of the decoder's output space show that the proposed regularizers improve the clustering of unique words, facilitating correct predictions. In a final experiment on unsupervised NMT, we show that ReWE and ReSE are also able to improve the quality of machine translation when no parallel data are available.

| Comments: | \c{opyright} 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1909.13466 [cs.CL]**                                 |
|           | (or **arXiv:1909.13466v1 [cs.CL]** for this version)         |





<h2 id="2019-10-01-5">5. Simple and Effective Paraphrastic Similarity from Parallel Translations</h2> 
Title: [Simple and Effective Paraphrastic Similarity from Parallel Translations](https://arxiv.org/abs/1909.13872)

Authors: [John Wieting](https://arxiv.org/search/cs?searchtype=author&query=Wieting%2C+J), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

*(Submitted on 30 Sep 2019)*

> We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.

| Comments: | Published as a short paper at ACL 2019               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.13872 [cs.CL]**                         |
|           | (or **arXiv:1909.13872v1 [cs.CL]** for this version) |

