# Daily arXiv: Machine Translation - September, 2020

# Index


- [2020-09-07](#2020-09-07)

  - [1. Data Readiness for Natural Language Processing](#2020-09-07-1)
  - [2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation](#2020-09-07-2)
  - [3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search](#2020-09-07-3)
  - [4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](#2020-09-07-4)
- [2020-09-01](#2020-09-01)

  - [1. Knowledge Efficient Deep Learning for Natural Language Processing](#2020-09-01-1)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-09-07

[Return to Index](#Index)



<h2 id="2020-09-07-1">1. Data Readiness for Natural Language Processing</h2>

Title: [Data Readiness for Natural Language Processing](https://arxiv.org/abs/2009.02043)

Authors: [Fredrik Olsson](https://arxiv.org/search/cs?searchtype=author&query=Olsson%2C+F), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> This document concerns data readiness in the context of machine learning and Natural Language Processing. It describes how an organization may proceed to identify, make available, validate, and prepare data to facilitate automated analysis methods. The contents of the document is based on the practical challenges and frequently asked questions we have encountered in our work as an applied research institute with helping organizations and companies, both in the public and private sectors, to use data in their business processes.

| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02043](https://arxiv.org/abs/2009.02043) [cs.CY]** |
|           | (or **[arXiv:2009.02043v1](https://arxiv.org/abs/2009.02043v1) [cs.CY]** for this version) |





<h2 id="2020-09-07-2">2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation</h2>

Title: [Dynamic Context-guided Capsule Network for Multimodal Machine Translation](https://arxiv.org/abs/2009.02016)

Authors: [Huan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Zhengyuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yubin Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on [this https URL](https://github.com/DeepLearnXMU/MM-DCCN).

| Subjects: | **Computation and Language (cs.CL)**; Multimedia (cs.MM)     |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3394171.3413715](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3394171.3413715&v=711e661a) |
| Cite as:  | **[arXiv:2009.02016](https://arxiv.org/abs/2009.02016) [cs.CL]** |
|           | (or **[arXiv:2009.02016v1](https://arxiv.org/abs/2009.02016v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-3">3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search</h2>

Title: [AutoTrans: Automating Transformer Design via Reinforced Architecture Search](https://arxiv.org/abs/2009.02070)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xiaoling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Yuan Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+Y), [Guotong Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+G)

> Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc, so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02070](https://arxiv.org/abs/2009.02070) [cs.CL]** |
|           | (or **[arXiv:2009.02070v1](https://arxiv.org/abs/2009.02070v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-4">4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings</h2>

Title: [Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](https://arxiv.org/abs/2009.02113)

Authors: [Vincent D. Warmerdam](https://arxiv.org/search/cs?searchtype=author&query=Warmerdam%2C+V+D), [Thomas Kober](https://arxiv.org/search/cs?searchtype=author&query=Kober%2C+T), [Rachael Tatman](https://arxiv.org/search/cs?searchtype=author&query=Tatman%2C+R)

> We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from [this https URL](https://rasahq.github.io/whatlies/).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02113](https://arxiv.org/abs/2009.02113) [cs.CL]** |
|           | (or **[arXiv:2009.02113v1](https://arxiv.org/abs/2009.02113v1) [cs.CL]** for this version) |



# 2020-09-01

[Return to Index](#Index)



<h2 id="2020-09-01-1">1. Knowledge Efficient Deep Learning for Natural Language Processing</h2>

Title: [Knowledge Efficient Deep Learning for Natural Language Processing](https://arxiv.org/abs/2008.12878)

Authors: [Hai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Deep learning has become the workhorse for a wide range of natural language processing applications. But much of the success of deep learning relies on annotated examples. Annotation is time-consuming and expensive to produce at scale. Here we are interested in methods for reducing the required quantity of annotated data -- by making the learning methods more knowledge efficient so as to make them more applicable in low annotation (low resource) settings. There are various classical approaches to making the models more knowledge efficient such as multi-task learning, transfer learning, weakly supervised and unsupervised learning etc. This thesis focuses on adapting such classical methods to modern deep learning models and algorithms.
> This thesis describes four works aimed at making machine learning models more knowledge efficient. First, we propose a knowledge rich deep learning model (KRDL) as a unifying learning framework for incorporating prior knowledge into deep models. In particular, we apply KRDL built on Markov logic networks to denoise weak supervision. Second, we apply a KRDL model to assist the machine reading models to find the correct evidence sentences that can support their decision. Third, we investigate the knowledge transfer techniques in multilingual setting, where we proposed a method that can improve pre-trained multilingual BERT based on the bilingual dictionary. Fourth, we present an episodic memory network for language modelling, in which we encode the large external knowledge for the pre-trained GPT.

| Comments: | Ph.D thesis                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.12878](https://arxiv.org/abs/2008.12878) [cs.CL]** |
|           | (or **[arXiv:2008.12878v1](https://arxiv.org/abs/2008.12878v1) [cs.CL]** for this version) |

