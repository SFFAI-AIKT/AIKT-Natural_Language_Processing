# Daily arXiv: Machine Translation - Jun., 2019

### Index

- [2019-06-20](#2019-06-20)
  - [1. Adaptation of Machine Translation Models with Back-translated Data using Transductive Data Selection Methods](#2019-06-20-1)
  - [2. Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation](#2019-06-20-2)
  - [3. The Effect of Translationese in Machine Translation Test Sets](#2019-06-20-3)
  - [4. Pre-Training with Whole Word Masking for Chinese BERT](#2019-06-20-4)
  - [5. XLNet: Generalized Autoregressive Pretraining for Language Understanding](#2019-06-20-5)
- [2019-06-18](#2019-06-18)
  - [1. Fixing Gaussian Mixture VAEs for Interpretable Text Generation](#2019-06-18-1)
  - [2. Tagged Back-Translation](#2019-06-18-2)
  - [3. Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks](#2019-06-18-3)
  - [4. Context is Key: Grammatical Error Detection with Contextual Word Representations](#2019-06-18-4)
- [2019-06-17](#2019-06-17)
  - [1. A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning](#2019-06-17-1)
- [2019-06-14](#2019-06-14)
  - [1. UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles](#2019-06-14-1)
  - [2. A Focus on Neural Machine Translation for African Languages](#2019-06-14-2)
  - [3. Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation](#2019-06-14-3)
  - [4. Lattice Transformer for Speech Translation](#2019-06-14-4)
  - [5. Cued@wmt19:ewc&lms](#2019-06-14-5)
  - [6. Analyzing the Limitations of Cross-lingual Word Embedding Mappings](#2019-06-14-6)
  - [7. Compositional generalization through meta sequence-to-sequence learning](#2019-06-14-7)
  - [8. A Multiscale Visualization of Attention in the Transformer Model](#2019-06-14-8)
- [2019-06-13](#2019-06-13)
  - [1. Continual and Multi-Task Architecture Search](#2019-06-13-1)
  - [2. Monotonic Infinite Lookback Attention for Simultaneous Machine Translation](#2019-06-13-2)
- [2019-06-12](#2019-06-12)
  - [1. What Does BERT Look At? An Analysis of BERT's Attention](#2019-06-12-1)
  - [2. Parallel Scheduled Sampling](#2019-06-12-2)
  - [3. Analyzing the Structure of Attention in a Transformer Language Model](#2019-06-12-3)
- [2019-06-11](#2019-06-11)
  - [1. The University of Helsinki submissions to the WMT19 news translation task](#2019-06-11-1)
  - [2. Generalized Data Augmentation for Low-Resource Translation](#2019-06-11-2)
  - [3. Is Attention Interpretable?](#2019-06-11-3)
  - [4. Making Asynchronous Stochastic Gradient Descent Work for Transformers](#2019-06-11-4)
  - [5. Assessing incrementality in sequence-to-sequence models](#2019-06-11-5)
  - [6. Syntax-Infused Variational Autoencoder for Text Generation](#2019-06-11-6)
- [2019-06-10](#2019-06-10)
  - [1. Word-based Domain Adaptation for Neural Machine Translation](#2019-06-10-1)
  - [2. Shared-Private Bilingual Word Embeddings for Neural Machine Translation](#2019-06-10-2)
  - [3. Syntactically Supervised Transformers for Faster Neural Machine Translation](#2019-06-10-3)
- [2019-06-06](#2019-06-06)
  - [1. Imitation Learning for Non-Autoregressive Neural Machine Translation](#2019-06-06-1)
  - [2. The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction](#2019-06-06-2)
  - [3. Learning Deep Transformer Models for Machine Translation](#2019-06-06-3)
  - [4. Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron](#2019-06-06-4)
- [2019-06-05](#2019-06-05)
  - [1. Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations](#2019-06-05-1)
  - [2. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation](#2019-06-05-2)
  - [3. Exploiting Sentential Context for Neural Machine Translation](#2019-06-05-3)
  - [4. Lattice-Based Transformer Encoder for Neural Machine Translation](#2019-06-05-4)
- [2019-06-04](#2019-06-04)
  - [1. Thinking Slow about Latency Evaluation for Simultaneous Machine Translation](#2019-06-04-1)
  - [2. Domain Adaptation of Neural Machine Translation by Lexicon Induction](#2019-06-04-2)
  - [3. Domain Adaptive Inference for Neural Machine Translation](#2019-06-04-3)
  - [4. Fluent Translations from Disfluent Speech in End-to-End Speech Translation](#2019-06-04-4)
  - [5. Evaluating Gender Bias in Machine Translation](#2019-06-04-5)
  - [6. From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots](#2019-06-04-6)
- [2019-06-03](#2019-06-03)
  - [1. DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation](#2019-06-03)

* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)
* [2019-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-02.md)



# 2019-06-18
[Return to Index](#Index)

<h2 id="2019-06-18-1">1. 
Fixing Gaussian Mixture VAEs for Interpretable Text Generation</h2>

Title: [Fixing Gaussian Mixture VAEs for Interpretable Text Generation](https://arxiv.org/abs/1906.06719)
Authors: [Wenxian Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Ning Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+N), [Shenjian Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

*(Submitted on 16 Jun 2019)*

> Variational auto-encoder (VAE) with Gaussian priors is effective in text generation. To improve the controllability and interpretability, we propose to use Gaussian mixture distribution as the prior for VAE (GMVAE), since it includes an extra discrete latent variable in addition to the continuous one. Unfortunately, training GMVAE using standard variational approximation often leads to the mode-collapse problem. We theoretically analyze the root cause --- maximizing the evidence lower bound of GMVAE implicitly aggregates the means of multiple Gaussian priors. We propose Dispersed-GMVAE (DGMVAE), an improved model for text generation. It introduces two extra terms to alleviate mode-collapse and to induce a better structured latent space. Experimental results show that DGMVAE outperforms strong baselines in several language modeling and text generation benchmarks.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.06719 [cs.LG]**                                 |
|           | (or **arXiv:1906.06719v1 [cs.LG]** for this version)         |


<h2 id="2019-06-18-2">2. Tagged Back-Translation</h2>

Title: [Tagged Back-Translation](https://arxiv.org/abs/1906.06442)
Authors: [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

*(Submitted on 15 Jun 2019)*

> Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data. We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic. We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, re-defining state-of-the-art in the former.

| Comments: | Accepted as oral presentation in WMT 2019; 9 pages; 9 tables; 1 figure |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **arXiv:1906.06442 [cs.CL]**                                 |
|           | (or **arXiv:1906.06442v1 [cs.CL]** for this version)         |

<h2 id="2019-06-18-3">3. Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks
</h2>

Title: [Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks](https://arxiv.org/abs/1906.06550)
Authors: [Ahmad Aghaebrahimian](https://arxiv.org/search/cs?searchtype=author&query=Aghaebrahimian%2C+A), [Mark Cieliebak](https://arxiv.org/search/cs?searchtype=author&query=Cieliebak%2C+M)

*(Submitted on 15 Jun 2019)*

> We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin, and to obtain competitive results on all other ones.

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.06550 [cs.CL]**                         |
|           | (or **arXiv:1906.06550v1 [cs.CL]** for this version) |

<h2 id="2019-06-18-4">4. Context is Key: Grammatical Error Detection with Contextual Word Representations
</h2>

Title: [Context is Key: Grammatical Error Detection with Contextual Word Representations](https://arxiv.org/abs/1906.06593)
Authors: [Samuel Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+S), [Helen Yannakoudakis](https://arxiv.org/search/cs?searchtype=author&query=Yannakoudakis%2C+H), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M)

*(Submitted on 15 Jun 2019)*

> Grammatical error detection (GED) in non-native writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.06593 [cs.CL]**                                 |
|           | (or **arXiv:1906.06593v1 [cs.CL]** for this version)         |

# 2019-06-17
[Return to Index](#Index)
<h2 id="2019-06-17-1">1. A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning</h2>
Title: [A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning](https://arxiv.org/abs/1906.06253)
Authors: [Gonçalo M. Correia](https://arxiv.org/search/cs?searchtype=author&query=Correia%2C+G+M), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

*(Submitted on 14 Jun 2019)*

> Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training an MT system from scratch. In this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU, we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data, our method obtains state-of-the-art results.

| Comments: | In proceedings of ACL 2019                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.06253 [cs.CL]**                                 |
|           | (or **arXiv:1906.06253v1 [cs.CL]** for this version)         |



# 2019-06-14
[Return to Index](#Index)
<h2 id="2019-06-14-1">1. UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles</h2>

Title: [UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles](https://arxiv.org/abs/1906.05786)
Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 13 Jun 2019)*

> The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experiment with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.

| Comments: | To appear at WMT19                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05786 [cs.CL]**                         |
|           | (or **arXiv:1906.05786v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-2">2. A Focus on Neural Machine Translation for African Languages</h2>

Title: [A Focus on Neural Machine Translation for African Languages](https://arxiv.org/abs/1906.05685)
Authors:[Laura Martinus](https://arxiv.org/search/cs?searchtype=author&query=Martinus%2C+L), [Jade Z. Abbott](https://arxiv.org/search/cs?searchtype=author&query=Abbott%2C+J+Z)

*(Submitted on 11 Jun 2019)*

> African languages are numerous, complex and low-resourced. The datasets required for machine translation are difficult to discover, and existing research is hard to reproduce. Minimal attention has been given to machine translation for African languages so there is scant research regarding the problems that arise when using machine translation techniques. To begin addressing these problems, we trained models to translate English to five of the official South African languages (Afrikaans, isiZulu, Northern Sotho, Setswana, Xitsonga), making use of modern neural machine translation techniques. The results obtained show the promise of using neural machine translation techniques for African languages. By providing reproducible publicly-available data, code and results, this research aims to provide a starting point for other researchers in African machine translation to compare to and build upon.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.05685 [cs.CL]**                                 |
|           | (or **arXiv:1906.05685v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-3">3. Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation</h2>

Title: [Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation](https://arxiv.org/abs/1906.05683)
Authors: [Nima Pourdamghani](https://arxiv.org/search/cs?searchtype=author&query=Pourdamghani%2C+N), [Nada Aldarrab](https://arxiv.org/search/cs?searchtype=author&query=Aldarrab%2C+N), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

*(Submitted on 11 Jun 2019)*

> Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then `translating' the resulting pseudo-translation, or `Translationese' into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.

| Comments: | Accepted in ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05683 [cs.CL]**                         |
|           | (or **arXiv:1906.05683v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-4">4. Lattice Transformer for Speech Translation</h2>

Title: [Lattice Transformer for Speech Translation](https://arxiv.org/abs/1906.05551)
Authors: [Pei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Niyu Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+N), [Kai Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+K)

*(Submitted on 13 Jun 2019)*

> Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.

| Comments: | accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05551 [cs.CL]**                         |
|           | (or **arXiv:1906.05551v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-5">5. Cued@wmt19:ewc&lms</h2>

Title: [Cued@wmt19:ewc&lms](https://arxiv.org/abs/1906.05447)
Authors: [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Adria de Gispert](https://arxiv.org/search/cs?searchtype=author&query=de+Gispert%2C+A), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 11 Jun 2019)*

> Two techniques provide the fabric of the Cambridge University Engineering Department's (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.

| Comments: | WMT2019 system description (University of Cambridge) |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05447 [cs.CL]**                         |
|           | (or **arXiv:1906.05447v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-6">6. Analyzing the Limitations of Cross-lingual Word Embedding Mappings</h2>

Title: [Analyzing the Limitations of Cross-lingual Word Embedding Mappings](https://arxiv.org/abs/1906.05407)
Authors: [Aitor Ormazabal](https://arxiv.org/search/cs?searchtype=author&query=Ormazabal%2C+A), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Aitor Soroa](https://arxiv.org/search/cs?searchtype=author&query=Soroa%2C+A), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

*(Submitted on 12 Jun 2019)*

> Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.

| Comments: | ACL 2019                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05407 [cs.CL]**                                 |
|           | (or **arXiv:1906.05407v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-7">7. Compositional generalization through meta sequence-to-sequence learning</h2>

Title: [Compositional generalization through meta sequence-to-sequence learning](https://arxiv.org/abs/1906.05381)
Authors: [Brenden M. Lake](https://arxiv.org/search/cs?searchtype=author&query=Lake%2C+B+M)

*(Submitted on 12 Jun 2019)*

> People can learn a new concept and use it compositionally, understanding how to "blicket twice" after learning how to "blicket." In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show that neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply rules to variables.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.05381 [cs.CL]**                                 |
|           | (or **arXiv:1906.05381v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-8">8. A Multiscale Visualization of Attention in the Transformer Model</h2>

Title: [A Multiscale Visualization of Attention in the Transformer Model](https://arxiv.org/abs/1906.05714)
Authors: [Jesse Vig](https://arxiv.org/search/cs?searchtype=author&query=Vig%2C+J)

*(Submitted on 12 Jun 2019)*

> The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.

| Comments: | To appear in ACL 2019 (System Demonstrations). arXiv admin note: substantial text overlap with [arXiv:1904.02679](https://arxiv.org/abs/1904.02679) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05714 [cs.HC]**                                 |
|           | (or **arXiv:1906.05714v1 [cs.HC]** for this version)         |




# 2019-06-13
[Return to Index](#Index)
<h2 id="2019-06-13-1">1. Continual and Multi-Task Architecture Search</h2>

Title: [Continual and Multi-Task Architecture Search](https://arxiv.org/abs/1906.05226)
Authors: [Ramakanth Pasunuru](https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

*(Submitted on 12 Jun 2019)*

> Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.

| Comments: | ACL 2019 (12 pages)                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05226 [cs.CL]**                                 |
|           | (or **arXiv:1906.05226v1 [cs.CL]** for this version)         |

<h2 id="2019-06-13-2">2. Monotonic Infinite Lookback Attention for Simultaneous Machine Translation</h2>

Title: [Monotonic Infinite Lookback Attention for Simultaneous Machine Translation](https://arxiv.org/abs/1906.05218)
Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W), [Chung-Cheng Chiu](https://arxiv.org/search/cs?searchtype=author&query=Chiu%2C+C), [Semih Yavuz](https://arxiv.org/search/cs?searchtype=author&query=Yavuz%2C+S), [Ruoming Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R), [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C)

*(Submitted on 12 Jun 2019)*

> Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk's adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.

| Comments: | Accepted for publication at ACL 2019                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05218 [cs.CL]**                         |
|           | (or **arXiv:1906.05218v1 [cs.CL]** for this version) |



# 2019-06-12
[Return to Index](#Index)
<h2 id="2019-06-12-1">1. What Does BERT Look At? An Analysis of BERT's Attention</h2>
Title: [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)
Authors: [Kevin Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+K), [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

*(Submitted on 11 Jun 2019)*

> Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.

| Comments: | BlackBoxNLP 2019                                     |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.04341 [cs.CL]**                         |
|           | (or **arXiv:1906.04341v1 [cs.CL]** for this version) |

<h2 id="2019-06-12-2">2. Parallel Scheduled Sampling</h2>

Title: [Parallel Scheduled Sampling](https://arxiv.org/abs/1906.04331)
Authors:[Daniel Duckworth](https://arxiv.org/search/cs?searchtype=author&query=Duckworth%2C+D), [Arvind Neelakantan](https://arxiv.org/search/cs?searchtype=author&query=Neelakantan%2C+A), [Ben Goodrich](https://arxiv.org/search/cs?searchtype=author&query=Goodrich%2C+B), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [Samy Bengio](https://arxiv.org/search/cs?searchtype=author&query=Bengio%2C+S)

*(Submitted on 11 Jun 2019)*

> Auto-regressive models are widely used in sequence generation problems. The output sequence is typically generated in a predetermined order, one discrete unit (pixel or word or character) at a time. The models are trained by teacher-forcing where ground-truth history is fed to the model as input, which at test time is replaced by the model prediction. Scheduled Sampling aims to mitigate this discrepancy between train and test time by randomly replacing some discrete units in the history with the model's prediction. While teacher-forced training works well with ML accelerators as the computation can be parallelized across time, Scheduled Sampling involves undesirable sequential processing. In this paper, we introduce a simple technique to parallelize Scheduled Sampling across time. We find that in most cases our technique leads to better empirical performance on summarization and dialog generation tasks compared to teacher-forced training. Further, we discuss the effects of different hyper-parameters associated with Scheduled Sampling on the model performance.

| Comments: | Initial submission                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.04331 [cs.CL]**                                 |
|           | (or **arXiv:1906.04331v1 [cs.CL]** for this version)         |

<h2 id="2019-06-12-3">3. Analyzing the Structure of Attention in a Transformer Language Model</h2>

Title: [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284)
Authors: [Jesse Vig](https://arxiv.org/search/cs?searchtype=author&query=Vig%2C+J), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

*(Submitted on 7 Jun 2019)*

> The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.

| Comments: | To appear in ACL BlackboxNLP workshop                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1906.04284 [cs.CL]**                                 |
|           | (or **arXiv:1906.04284v1 [cs.CL]** for this version)         |




# 2019-06-11
[Return to Index](#Index)
<h2 id="2019-06-11-1">1. The University of Helsinki submissions to the WMT19 news translation task</h2>

Title: [The University of Helsinki submissions to the WMT19 news translation task](https://arxiv.org/abs/1906.04040)
Authors: [Aarne Talman](https://arxiv.org/search/cs?searchtype=author&query=Talman%2C+A), [Umut Sulubacak](https://arxiv.org/search/cs?searchtype=author&query=Sulubacak%2C+U), [Raúl Vázquez](https://arxiv.org/search/cs?searchtype=author&query=Vázquez%2C+R), [Yves Scherrer](https://arxiv.org/search/cs?searchtype=author&query=Scherrer%2C+Y), [Sami Virpioja](https://arxiv.org/search/cs?searchtype=author&query=Virpioja%2C+S), [Alessandro Raganato](https://arxiv.org/search/cs?searchtype=author&query=Raganato%2C+A), [Arvi Hurskainen](https://arxiv.org/search/cs?searchtype=author&query=Hurskainen%2C+A), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

*(Submitted on 10 Jun 2019)*

> In this paper, we present the University of Helsinki submissions to the WMT 2019 shared task on news translation in three language pairs: English-German, English-Finnish and Finnish-English. This year, we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German, we trained both sentence-level transformer models and compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches, and we also included a rule-based system for English-Finnish.

| Comments: | To appear in WMT19                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.04040 [cs.CL]**                         |
|           | (or **arXiv:1906.04040v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-2">2. Generalized Data Augmentation for Low-Resource Translation</h2>

Title: [Generalized Data Augmentation for Low-Resource Translation](https://arxiv.org/abs/1906.03785)
Authors: [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Xiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 10 Jun 2019)*

> Translation to or from low-resource languages LRLs poses challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing large amounts of monolingual data is regarded as an effective way to alleviate these problems. In this paper, we propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data, but also pivots through a related high-resource language HRL. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the LRL, making use of available resources to better approximate the true data distribution of the LRL. First, we inject LRL words into HRL sentences through an induced bilingual dictionary. Second, we further edit these modified sentences using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to~1.5 to~8 BLEU points compared to supervised back-translation baselines

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.03785 [cs.CL]**                         |
|           | (or **arXiv:1906.03785v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-3">3. Is Attention Interpretable?</h2>

Title: [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731)
Authors: [Sofia Serrano](https://arxiv.org/search/cs?searchtype=author&query=Serrano%2C+S), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 9 Jun 2019)*

> Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.

| Comments: | To appear at ACL 2019                                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.03731 [cs.CL]**                         |
|           | (or **arXiv:1906.03731v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-4">4. Making Asynchronous Stochastic Gradient Descent Work for Transformers</h2>

Title: [Making Asynchronous Stochastic Gradient Descent Work for Transformers](https://arxiv.org/abs/1906.03496)
Authors: [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

*(Submitted on 8 Jun 2019)*

> Asynchronous stochastic gradient descent (SGD) is attractive from a speed perspective because workers do not wait for synchronization. However, the Transformer model converges poorly with asynchronous SGD, resulting in substantially lower quality compared to synchronous SGD. To investigate why this is the case, we isolate differences between asynchronous and synchronous methods to investigate batch size and staleness effects. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this hybrid method, Transformer training for neural machine translation task reaches a near-convergence level 1.36x faster in single-node multi-GPU training with no impact on model quality.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.03496 [cs.CL]**                                 |
|           | (or **arXiv:1906.03496v1 [cs.CL]** for this version)         |

<h2 id="2019-06-11-5">5. Assessing incrementality in sequence-to-sequence models</h2>

Title: [Assessing incrementality in sequence-to-sequence models](https://arxiv.org/abs/1906.03293)
Authors: [Dennis Ulmer](https://arxiv.org/search/cs?searchtype=author&query=Ulmer%2C+D), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D), [Elia Bruni](https://arxiv.org/search/cs?searchtype=author&query=Bruni%2C+E)

*(Submitted on 7 Jun 2019)*

> Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.

| Comments: | Accepted at Repl4NLP, ACL                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.03293 [cs.CL]**                                 |
|           | (or **arXiv:1906.03293v1 [cs.CL]** for this version)         |

<h2 id="2019-06-11-6">6. Syntax-Infused Variational Autoencoder for Text Generation</h2>

Title: [Syntax-Infused Variational Autoencoder for Text Generation](https://arxiv.org/abs/1906.02181)
Authors: [Xinyuan Zhang](https://arxiv.org/search/stat?searchtype=author&query=Zhang%2C+X), [Yi Yang](https://arxiv.org/search/stat?searchtype=author&query=Yang%2C+Y), [Siyang Yuan](https://arxiv.org/search/stat?searchtype=author&query=Yuan%2C+S), [Dinghan Shen](https://arxiv.org/search/stat?searchtype=author&query=Shen%2C+D), [Lawrence Carin](https://arxiv.org/search/stat?searchtype=author&query=Carin%2C+L)

*(Submitted on 5 Jun 2019)*

> We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.

| Comments: | Accepted by ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.02181 [stat.ML]**                               |
|           | (or **arXiv:1906.02181v1 [stat.ML]** for this version)       |




# 2019-06-10

[Return to Index](#Index)
<h2 id="2019-06-10-1">1. Word-based Domain Adaptation for Neural Machine Translation</h2>

Title: [Word-based Domain Adaptation for Neural Machine Translation](https://arxiv.org/abs/1906.03129)
Authors: [Shen Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+S), [Leonard Dahlmann](https://arxiv.org/search/cs?searchtype=author&query=Dahlmann%2C+L), [Pavel Petrushkov](https://arxiv.org/search/cs?searchtype=author&query=Petrushkov%2C+P), [Sanjika Hewavitharana](https://arxiv.org/search/cs?searchtype=author&query=Hewavitharana%2C+S), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S)

*(Submitted on 7 Jun 2019)*

> In this paper, we empirically investigate applying word-level weights to adapt neural machine translation to e-commerce domains, where small e-commerce datasets and large out-of-domain datasets are available. In order to mine in-domain like words in the out-of-domain datasets, we compute word weights by using a domain-specific and a non-domain-specific language model followed by smoothing and binary quantization. The baseline model is trained on mixed in-domain and out-of-domain datasets. Experimental results on English to Chinese e-commerce domain translation show that compared to continuing training without word weights, it improves MT quality by up to 2.11% BLEU absolute and 1.59% TER. We have also trained models using fine-tuning on the in-domain data. Pre-training a model with word weights improves fine-tuning up to 1.24% BLEU absolute and 1.64% TER, respectively.

| Comments:          | Published on the proceedings of the International Workshop on Spoken Language Translation (IWSLT), 2018 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | Proceedings of the 15th International Workshop on Spoken Language Translation, Bruges, Belgium, October 29-30, 2018 |
| Cite as:           | **arXiv:1906.03129 [cs.CL]**                                 |
|                    | (or **arXiv:1906.03129v1 [cs.CL]** for this version)         |

<h2 id="2019-06-10-2">2. Shared-Private Bilingual Word Embeddings for Neural Machine Translation</h2>

Title: [Shared-Private Bilingual Word Embeddings for Neural Machine Translation](https://arxiv.org/abs/1906.03100)
Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 7 Jun 2019)*

> Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.

| Comments: | Accepted to ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **arXiv:1906.03100 [cs.CL]**                                 |
|           | (or **arXiv:1906.03100v1 [cs.CL]** for this version)         |

<h2 id="2019-06-10-3">3. Syntactically Supervised Transformers for Faster Neural Machine Translation</h2>

Title: [Syntactically Supervised Transformers for Faster Neural Machine Translation](https://arxiv.org/abs/1906.02780)
Authors: 	[Nader Akoury](https://arxiv.org/search/cs?searchtype=author&query=Akoury%2C+N), [Kalpesh Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+K), [Mohit Iyyer](https://arxiv.org/search/cs?searchtype=author&query=Iyyer%2C+M)

*(Submitted on 6 Jun 2019)*

> Standard decoders for neural machine translation autoregressively generate a single target token per time step, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi- autoregressive decoding produce multiple tokens per time step independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~ 5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.

| Comments: | 9 pages, 5 figures, accepted to ACL 2019             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.02780 [cs.CL]**                         |
|           | (or **arXiv:1906.02780v1 [cs.CL]** for this version) |

# 2019-06-06

[Return to Index](#Index)
<h2 id="2019-06-06-1">1. Imitation Learning for Non-Autoregressive Neural Machine Translation</h2>

Title: [Imitation Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/1906.02041)

Authors: [Bingzhen Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+B), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Junyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

*(Submitted on 5 Jun 2019)*

> Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.02041 [cs.CL]**                         |
|           | (or **arXiv:1906.02041v1 [cs.CL]** for this version) |

<h2 id="2019-06-06-2">2. The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction</h2>

Title: [The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction](https://arxiv.org/abs/1906.01733)

Authors: [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D), [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V)

*(Submitted on 4 Jun 2019)*

> Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.

| Comments: | 7 pages, 3 tables, accepted at the 14th Workshop on Innovative Use of NLP for Building Educational Applications |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **arXiv:1906.01733 [cs.CL]**                                 |
|           | (or **arXiv:1906.01733v1 [cs.CL]** for this version)         |

<h2 id="2019-06-06-3">3. Learning Deep Transformer Models for Machine Translation</h2>

Title: [Learning Deep Transformer Models for Machine Translation](https://arxiv.org/abs/1906.01787)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Changliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

*(Submitted on 5 Jun 2019)*

> Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.

| Comments: | Accepted by ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.01787 [cs.CL]**                                 |
|           | (or **arXiv:1906.01787v1 [cs.CL]** for this version)         |

<h2 id="2019-06-06-4">4. Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron</h2>

Title: [Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron](https://arxiv.org/abs/1906.01942)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Hendrik Rosendahl](https://arxiv.org/search/cs?searchtype=author&query=Rosendahl%2C+H), [Nick Rossenbach](https://arxiv.org/search/cs?searchtype=author&query=Rossenbach%2C+N), [Jan Rosendahl](https://arxiv.org/search/cs?searchtype=author&query=Rosendahl%2C+J), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 5 Jun 2019)*

> We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.

| Comments: | ACL 2019 Repl4NLP camera-ready                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.01942 [cs.CL]**                                 |
|           | (or **arXiv:1906.01942v1 [cs.CL]** for this version)         |


# 2019-06-05

[Return to Index](#Index)
<h2 id="2019-06-05-1">1. Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations</h2>

Title: [Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations](https://arxiv.org/abs/1906.01181)

Authors: [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O)

*(Submitted on 4 Jun 2019)*

> Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4~22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01181 [cs.CL]**                         |
|           | (or **arXiv:1906.01181v1 [cs.CL]** for this version) |

<h2 id="2019-06-05-2">2. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</h2>

Title: [Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation](https://arxiv.org/abs/1906.01199)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

*(Submitted on 4 Jun 2019)*

> Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.

| Comments: | Accepted to ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1906.01199 [cs.CL]**                                 |
|           | (or **arXiv:1906.01199v1 [cs.CL]** for this version)         |

<h2 id="2019-06-05-3">3. Exploiting Sentential Context for Neural Machine Translation</h2>

Title: [Exploiting Sentential Context for Neural Machine Translation](https://arxiv.org/abs/1906.01268)

Authors: [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 4 Jun 2019)*

> In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we first show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-to-German and English-to-French benchmarks show that our model consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01268 [cs.CL]**                         |
|           | (or **arXiv:1906.01268v1 [cs.CL]** for this version) |

<h2 id="2019-06-05-4">4. Lattice-Based Transformer Encoder for Neural Machine Translation</h2>

Title: [Lattice-Based Transformer Encoder for Neural Machine Translation](https://arxiv.org/abs/1906.01282)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Jiangtong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K)

*(Submitted on 4 Jun 2019)*

> Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01282 [cs.CL]**                         |
|           | (or **arXiv:1906.01282v1 [cs.CL]** for this version) |



# 2019-06-04

[Return to Index](#Index)
<h2 id="2019-06-04-1">1. Thinking Slow about Latency Evaluation for Simultaneous Machine Translation</h2>

Title: [Thinking Slow about Latency Evaluation for Simultaneous Machine Translation](https://arxiv.org/abs/1906.00048)

Authors: [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G)

*(Submitted on 31 May 2019)*

> Simultaneous machine translation attempts to translate a source sentence before it is finished being spoken, with applications to translation of spoken language for live streaming and conversation. Since simultaneous systems trade quality to reduce latency, having an effective and interpretable latency metric is crucial. We introduce a variant of the recently proposed Average Lagging (AL) metric, which we call Differentiable Average Lagging (DAL). It distinguishes itself by being differentiable and internally consistent to its underlying mathematical model.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.00048 [cs.CL]**                         |
|           | (or **arXiv:1906.00048v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-2">2. Domain Adaptation of Neural Machine Translation by Lexicon Induction</h2>

Title: [Domain Adaptation of Neural Machine Translation by Lexicon Induction](https://arxiv.org/abs/1906.00376)

Authors:[Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jaime Carbonell](https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J)

*(Submitted on 2 Jun 2019)*

> It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | published at the 57th Annual Meeting of the Association for Computational Linguistics (ACL). July 2019 |
| Cite as:           | **arXiv:1906.00376 [cs.CL]**                                 |
|                    | (or **arXiv:1906.00376v1 [cs.CL]** for this version)         |

 





<h2 id="2019-06-04-3">3. Domain Adaptive Inference for Neural Machine Translation</h2>

Title: [Domain Adaptive Inference for Neural Machine Translation](https://arxiv.org/abs/1906.00408)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Adria de Gispert](https://arxiv.org/search/cs?searchtype=author&query=de+Gispert%2C+A), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 2 Jun 2019)*

> We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and show strong improvements across test domains without access to the domain label.

| Comments: | To appear at ACL 2019                                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00408 [cs.CL]**                         |
|           | (or **arXiv:1906.00408v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-4">4. Fluent Translations from Disfluent Speech in End-to-End Speech Translation</h2>

Title: [Fluent Translations from Disfluent Speech in End-to-End Speech Translation](https://arxiv.org/abs/1906.00556)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

*(Submitted on 3 Jun 2019)*

> Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected `copy-edited' references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, the translation of conversational speech with joint removal of disfluencies.

| Comments: | Accepted at NAACL 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00556 [cs.CL]**                         |
|           | (or **arXiv:1906.00556v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-5">5. Evaluating Gender Bias in Machine Translation</h2>

Title: [Evaluating Gender Bias in Machine Translation](https://arxiv.org/abs/1906.00591)

Authors: [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 3 Jun 2019)*

> We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., "The doctor asked the nurse to help her in the operation"). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word "doctor"). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are made publicly available.

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00591 [cs.CL]**                         |
|           | (or **arXiv:1906.00591v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-6">6. From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots</h2>

Title: [From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots](https://arxiv.org/abs/1906.00872)

Authors: [Shizhe Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Qin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q), [Jianlong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J)

*(Submitted on 3 Jun 2019)*

> The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behavior, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image-pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach significantly outperforms other state-of-the-art methods.

| Comments: | Accepted by IJCAI 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **arXiv:1906.00872 [cs.CL]**                                 |
|           | (or **arXiv:1906.00872v1 [cs.CL]** for this version)         |



# 2019-06-03

[Return to Index](#Index)
<h2 id="2019-06-03-1">1. DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation</h2>

Title: [DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation](https://arxiv.org/abs/1905.13354)

Authors: [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Sophie Rosset](https://arxiv.org/search/cs?searchtype=author&query=Rosset%2C+S), [Thomas Lavergne](https://arxiv.org/search/cs?searchtype=author&query=Lavergne%2C+T), [Eric Bilinski](https://arxiv.org/search/cs?searchtype=author&query=Bilinski%2C+E)

*(Submitted on 30 May 2019)*

> We present a new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and reference translations produced a posteriori. The motivation for the corpus is two-fold: to provide (i) a unique resource for evaluating MT models, and (ii) a corpus for the analysis of MT-mediated communication. We provide a preliminary analysis of the corpus to confirm that the participants' judgments reveal perceptible differences in MT quality between the two MT systems used.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1905.13354 [cs.CL]**                         |
|           | (or **arXiv:1905.13354v1 [cs.CL]** for this version) |