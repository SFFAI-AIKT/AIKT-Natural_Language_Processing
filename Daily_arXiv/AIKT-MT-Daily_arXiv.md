# Daily arXiv: Machine Translation - July, 2020

# Index


- [2020-08-03](#2020-08-03)
  - [1. Neural Language Generation: Formulation, Methods, and Evaluation](#2020-08-03-1)
  - [2. On Learning Universal Representations Across Languages](#2020-08-03-2)
  - [3. Word Embeddings: Stability and Semantic Change](#2020-08-03-3)
  - [4. Exploring Swedish & English fastText Embeddings with the Transformer](#2020-08-03-4)
  - [5. Multi-task learning for natural language processing in the 2020s: where are we going?](#2020-08-03-5)
  - [6. Toward Givenness Hierarchy Theoretic Natural Language Generation](#2020-08-03-6)
  - [7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](#2020-08-03-7)
  - [8. Neural Machine Translation model for University Email Application](#2020-08-03-8)
  - [9. Neural Composition: Learning to Generate from Multiple Models](#2020-08-03-9)
  - [10. SimulEval: An Evaluation Toolkit for Simultaneous Translation](#2020-08-03-10)

- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-08-03

[Return to Index](#Index)



<h2 id="2020-08-03-1">1. Neural Language Generation: Formulation, Methods, and Evaluation</h2>

Title: [Neural Language Generation: Formulation, Methods, and Evaluation](https://arxiv.org/abs/2007.15780)

Authors: [Cristina Garbacea](https://arxiv.org/search/cs?searchtype=author&query=Garbacea%2C+C), [Qiaozhu Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+Q)

> Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.

| Comments: | 70 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.15780](https://arxiv.org/abs/2007.15780) [cs.CL]** |
|           | (or **[arXiv:2007.15780v1](https://arxiv.org/abs/2007.15780v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-2">2. On Learning Universal Representations Across Languages</h2>

Title: [On Learning Universal Representations Across Languages](https://arxiv.org/abs/2007.15960)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations, and show the effectiveness on cross-lingual understanding and generation. We propose Hierarchical Contrastive Learning (HiCTL) to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on three benchmarks: language understanding tasks (QQP, QNLI, SST-2, MRPC, STS-B and MNLI) in the GLUE benchmark, cross-lingual natural language inference (XNLI) and machine translation. Experimental results show that the HiCTL obtains an absolute gain of 1.0%/2.2% accuracy on GLUE/XNLI as well as achieves substantial improvements of +1.7-+3.6 BLEU on both the high-resource and low-resource English-to-X translation tasks over strong baselines. We will release the source codes as soon as possible.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.15960](https://arxiv.org/abs/2007.15960) [cs.CL]** |
|           | (or **[arXiv:2007.15960v1](https://arxiv.org/abs/2007.15960v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-3">3. Word Embeddings: Stability and Semantic Change</h2>

Title: [Word Embeddings: Stability and Semantic Change](https://arxiv.org/abs/2007.16006)

Authors: [Lucas Rettenmeier](https://arxiv.org/search/cs?searchtype=author&query=Rettenmeier%2C+L)

> Word embeddings are computed by a class of techniques within natural language processing (NLP), that create continuous vector representations of words in a language from a large text corpus. The stochastic nature of the training process of most embedding techniques can lead to surprisingly strong instability, i.e. subsequently applying the same technique to the same data twice, can produce entirely different results. In this work, we present an experimental study on the instability of the training process of three of the most influential embedding techniques of the last decade: word2vec, GloVe and fastText. Based on the experimental results, we propose a statistical model to describe the instability of embedding techniques and introduce a novel metric to measure the instability of the representation of an individual word. Finally, we propose a method to minimize the instability - by computing a modified average over multiple runs - and apply it to a specific linguistic problem: The detection and quantification of semantic change, i.e. measuring changes in the meaning and usage of words over time.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16006](https://arxiv.org/abs/2007.16006) [cs.CL]** |
|           | (or **[arXiv:2007.16006v1](https://arxiv.org/abs/2007.16006v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-4">4. Exploring Swedish & English fastText Embeddings with the Transformer</h2>

Title: [Exploring Swedish & English fastText Embeddings with the Transformer](https://arxiv.org/abs/2007.16007)

Authors: [Tosin P. Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T+P), [Foteini Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F), [Marcus Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M)

> In this paper, our main contributions are that embeddings from relatively smaller corpora can outperform ones from far larger corpora and we present the new Swedish analogy test set. To achieve a good network performance in natural language processing (NLP) downstream tasks, several factors play important roles: dataset size, the right hyper-parameters, and well-trained embedding. We show that, with the right set of hyper-parameters, good network performance can be reached even on smaller datasets. We evaluate the embeddings at the intrinsic level and extrinsic level, by deploying them on the Transformer in named entity recognition (NER) task and conduct significance tests.This is done for both Swedish and English. We obtain better performance in both languages on the downstream task with far smaller training data, compared to recently released, common crawl versions and character n-grams appear useful for Swedish, a morphologically rich language.

| Comments: | 10 pages, 2 figures, 8 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.16007](https://arxiv.org/abs/2007.16007) [cs.CL]** |
|           | (or **[arXiv:2007.16007v1](https://arxiv.org/abs/2007.16007v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-5">5. Multi-task learning for natural language processing in the 2020s: where are we going?</h2>

Title: [Multi-task learning for natural language processing in the 2020s: where are we going?](https://arxiv.org/abs/2007.16008)

Authors: [Joseph Worsham](https://arxiv.org/search/cs?searchtype=author&query=Worsham%2C+J), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.

| Comments:          | 12 pages, 2 figures. Published in Elsevier Pattern Recognition Letters Volume 136. Accepted manuscript published here |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes:       | I.2.6; I.2.7                                                 |
| Journal reference: | Pattern Recognition Letters 136 (2020) 120-126               |
| DOI:               | [10.1016/j.patrec.2020.05.031](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1016%2Fj.patrec.2020.05.031&v=27506e7c) |
| Cite as:           | **[arXiv:2007.16008](https://arxiv.org/abs/2007.16008) [cs.CL]** |
|                    | (or **[arXiv:2007.16008v1](https://arxiv.org/abs/2007.16008v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-6">6. Toward Givenness Hierarchy Theoretic Natural Language Generation</h2>

Title: [Toward Givenness Hierarchy Theoretic Natural Language Generation](https://arxiv.org/abs/2007.16009)

Authors: [Poulomi Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+P), [Tom Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+T)

> Language-capable interactive robots participating in dialogues with human interlocutors must be able to naturally and efficiently communicate about the entities in their environment. A key aspect of such communication is the use of anaphoric language. The linguistic theory of the Givenness Hierarchy(GH) suggests that humans use anaphora based on the cognitive statuses their referents have in the minds of their interlocutors. In previous work, researchers presented GH-theoretic approaches to robot anaphora understanding. In this paper we describe how the GH might need to be used quite differently to facilitate robot anaphora generation.

| Comments: | Extended Abstract accepted for (non-archival) presentation at Advances in Cognitive Systems 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2007.16009](https://arxiv.org/abs/2007.16009) [cs.CL]** |
|           | (or **[arXiv:2007.16009v1](https://arxiv.org/abs/2007.16009v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-7">7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs</h2>

Title: [Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](https://arxiv.org/abs/2007.16010)

Authors: [Subhadip Maji](https://arxiv.org/search/cs?searchtype=author&query=Maji%2C+S), [Arijit Ghosh Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+A+G), [Raghav Bali](https://arxiv.org/search/cs?searchtype=author&query=Bali%2C+R), [Vamsi M Bhandaru](https://arxiv.org/search/cs?searchtype=author&query=Bhandaru%2C+V+M)

> Deep Neural Networks in NLP have enabled systems to learn complex non-linear relationships. One of the major bottlenecks towards being able to use DNNs for real world applications is their characterization as black boxes. To solve this problem, we introduce a model agnostic algorithm which calculates phrase-wise importance of input features. We contend that our method is generalizable to a diverse set of tasks, by carrying out experiments for both Regression and Classification. We also observe that our approach is robust to outliers, implying that it only captures the essential aspects of the input.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2007.16010](https://arxiv.org/abs/2007.16010) [cs.CL]** |
|           | (or **[arXiv:2007.16010v1](https://arxiv.org/abs/2007.16010v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-8">8. Neural Machine Translation model for University Email Application</h2>

Title: [Neural Machine Translation model for University Email Application](https://arxiv.org/abs/2007.16011)

Authors: [Sandhya Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+S), [Siti Nur Afikah Bte Abdul Mazid](https://arxiv.org/search/cs?searchtype=author&query=Mazid%2C+S+N+A+B+A), [Nagender Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+N)

> Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation (NMT) model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of EN -> ML of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.

| Comments:          | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Journal reference: | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| Cite as:           | **[arXiv:2007.16011](https://arxiv.org/abs/2007.16011) [cs.CL]** |
|                    | (or **[arXiv:2007.16011v1](https://arxiv.org/abs/2007.16011v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-9">9. Neural Composition: Learning to Generate from Multiple Models</h2>

Title: [Neural Composition: Learning to Generate from Multiple Models](https://arxiv.org/abs/2007.16013)

Authors: [Denis Filimonov](https://arxiv.org/search/cs?searchtype=author&query=Filimonov%2C+D), [Ravi Teja Gadde](https://arxiv.org/search/cs?searchtype=author&query=Gadde%2C+R+T), [Ariya Rastrow](https://arxiv.org/search/cs?searchtype=author&query=Rastrow%2C+A)

> Decomposing models into multiple components is critically important in many applications such as language modeling (LM) as it enables adapting individual components separately and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, which requires class-annotated data, or through biasing to individual phrases which is limited in scale. In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component, and how to combine probability distributions from each component, directly from unlabeled text data.

| Comments:    | submitted to NeurIPS'20 (under review)                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ACM classes: | I.2.6; I.2.7                                                 |
| Cite as:     | **[arXiv:2007.16013](https://arxiv.org/abs/2007.16013) [cs.CL]** |
|              | (or **[arXiv:2007.16013v1](https://arxiv.org/abs/2007.16013v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-10">10. SimulEval: An Evaluation Toolkit for Simultaneous Translation</h2>

Title: [SimulEval: An Evaluation Toolkit for Simultaneous Translation](https://arxiv.org/abs/2007.16193)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16193](https://arxiv.org/abs/2007.16193) [cs.CL]** |
|           | (or **[arXiv:2007.16193v1](https://arxiv.org/abs/2007.16193v1) [cs.CL]** for this version) |



