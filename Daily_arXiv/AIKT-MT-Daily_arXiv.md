# Daily arXiv: Machine Translation - September, 2020

# Index


- [2020-09-25](#2020-09-25)

  - [1. Multi-Pass Transformer for Machine Translation](#2020-09-25-1)
  - [2. AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding and Generation](#2020-09-25-2)
  - [3. N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models](#2020-09-25-3)
- [2020-09-23](#2020-09-23)

  - [1. KoBE: Knowledge-Based Machine Translation Evaluation](#2020-09-23-1)
  - [2. Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages](#2020-09-23-2)
- [2020-09-22](#2020-09-22)

  - [1. Presenting Simultaneous Translation in Limited Space](#2020-09-22-1)
  - [2. COMET: A Neural Framework for MT Evaluation](#2020-09-22-2)
  - [3. Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing](#2020-09-22-3)
  - [4. Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation](#2020-09-22-4)
  - [5. Towards Computational Linguistics in Minangkabau Language: Studies on Sentiment Analysis and Machine Translation](#2020-09-22-5)
  - [6. Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation](#2020-09-22-6)
  - [7. Softmax Tempering for Training Neural Machine Translation Models](#2020-09-22-7)
  - [8. Improving Robustness and Generality of NLP Models Using Disentangled Representations](#2020-09-22-8)
  - [9. Generative Imagination Elevates Machine Translation](#2020-09-22-9)
  - [10. Alleviating the Inequality of Attention Heads for Neural Machine Translation](#2020-09-22-10)
  - [11. TED: Triple Supervision Decouples End-to-end Speech-to-text Translation](#2020-09-22-11)
  - [12. SDST: Successive Decoding for Speech-to-text Translation](#2020-09-22-12)
- [2020-09-21](#2020-09-21)

  - [1. A Study of Genetic Algorithms for Hyperparameter Optimization of Neural Networks in Machine Translation](#2020-09-21-1)
  - [2. fastHan: A BERT-based Joint Many-Task Toolkit for Chinese NLP](#2020-09-21-2)
  - [3. Hierarchical GPT with Congruent Transformers for Multi-Sentence Language Models](#2020-09-21-3)
  - [4. Document-level Neural Machine Translation with Document Embeddings](#2020-09-21-4)
- [2020-09-18](#2020-09-18)
- [1. Code-switching pre-training for neural machine translation](#2020-09-18-1)
  - [2. FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding](#2020-09-18-2)
- [2020-09-17](#2020-09-17)

  - [1. Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation](#2020-09-17-1)
  - [2. Simultaneous Machine Translation with Visual Context](#2020-09-17-2)
  - [3. Graph-to-Sequence Neural Machine Translation](#2020-09-17-3)
  - [4. Contextualized Perturbation for Textual Adversarial Attack](#2020-09-17-4)
  - [5. Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](#2020-09-17-5)
  - [6. Knowledge Graphs for Multilingual Language Translation and Generation](#2020-09-17-6)
  - [7. Text Generation by Learning from Off-Policy Demonstrations](#2020-09-17-7)
- [2020-09-16](#2020-09-16)

  - [1. Efficient Transformers: A Survey](#2020-09-16-1)
  - [2. Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models](#2020-09-16-2)
  - [3. Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](#2020-09-16-3)
  - [4. A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation](#2020-09-16-4)
  - [5. Autoregressive Knowledge Distillation through Imitation Learning](#2020-09-16-5)
- [2020-09-15](#2020-09-15)

  - [1. Unit Test Case Generation with Transformers](#2020-09-15-1)
  - [2. Improving Indonesian Text Classification Using Multilingual Language Model](#2020-09-15-2)
  - [3. Combining Word and Character Vector Representation on Neural Machine Translation](#2020-09-15-3)
  - [4. Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding](#2020-09-15-4)
  - [5. Contrastive Triple Extraction with Generative Transformer](#2020-09-15-5)
  - [6. Improving Language Generation with Sentence Coherence Objective](#2020-09-15-6)
  - [7. GeDi: Generative Discriminator Guided Sequence Generation](#2020-09-15-7)
- [2020-09-14](#2020-09-14)

  - [1. FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding](#2020-09-14-1)
  - [2. Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation](#2020-09-14-2)
- [2020-09-11](#2020-09-11)
- [1. Pay Attention when Required](#2020-09-11-1)
  - [2. Learning Universal Representations from Word to Sentence](#2020-09-11-2)
  - [3. Modern Methods for Text Generation](#2020-09-11-3)
  - [4. Investigating Gender Bias in BERT](#2020-09-11-4)
- [2020-09-10](#2020-09-10)

  - [1. Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages](#2020-09-10-1)
- [2020-09-08](#2020-09-08)

  - [1. Measuring Massive Multitask Language Understanding](#2020-09-08-1)
  - [2. Recent Trends in the Use of Deep Learning Models for Grammar Error Handling](#2020-09-08-2)
  - [3. Bio-inspired Structure Identification in Language Embeddings](#2020-09-08-3)
  - [4. Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity](#2020-09-08-4)
- [2020-09-07](#2020-09-07)
- [1. Data Readiness for Natural Language Processing](#2020-09-07-1)
  - [2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation](#2020-09-07-2)
  - [3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search](#2020-09-07-3)
  - [4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](#2020-09-07-4)
- [2020-09-01](#2020-09-01)

  - [1. Knowledge Efficient Deep Learning for Natural Language Processing](#2020-09-01-1)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-09-25

[Return to Index](#Index)



<h2 id="2020-09-25-1">1. Multi-Pass Transformer for Machine Translation</h2>

Title: [Multi-Pass Transformer for Machine Translation](https://arxiv.org/abs/2009.11382)

Authors: [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Chiori Hori](https://arxiv.org/search/cs?searchtype=author&query=Hori%2C+C), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Takaaki Hori](https://arxiv.org/search/cs?searchtype=author&query=Hori%2C+T), [Jonathan Le Roux](https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+J+L)

> In contrast with previous approaches where information flows only towards deeper layers of a stack, we consider a multi-pass transformer (MPT) architecture in which earlier layers are allowed to process information in light of the output of later layers. To maintain a directed acyclic graph structure, the encoder stack of a transformer is repeated along a new multi-pass dimension, keeping the parameters tied, and information is allowed to proceed unidirectionally both towards deeper layers within an encoder stack and towards any layer of subsequent stacks. We consider both soft (i.e., continuous) and hard (i.e., discrete) connections between parallel encoder stacks, relying on a neural architecture search to find the best connection pattern in the hard case. We perform an extensive ablation study of the proposed MPT architecture and compare it with other state-of-the-art transformer architectures. Surprisingly, Base Transformer equipped with MPT can surpass the performance of Large Transformer on the challenging machine translation En-De and En-Fr datasets. In the hard connection case, the optimal connection pattern found for En-De also leads to improved performance for En-Fr.

| Comments: | 10 pages, 5 figures and 2 tables                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.11382](https://arxiv.org/abs/2009.11382) [cs.CL]** |
|           | (or **[arXiv:2009.11382v1](https://arxiv.org/abs/2009.11382v1) [cs.CL]** for this version) |





<h2 id="2020-09-25-2">2. AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding and Generation</h2>

Title: [AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding and Generation](https://arxiv.org/abs/2009.11473)

Authors: [Huishuang Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Kexin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Jiancheng Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J)

> Ancient Chinese is the essence of Chinese culture. There are several natural language processing tasks of ancient Chinese domain, such as ancient-modern Chinese translation, poem generation, and couplet generation. Previous studies usually use the supervised models which deeply rely on parallel data. However, it is difficult to obtain large-scale parallel data of ancient Chinese. In order to make full use of the more easily available monolingual ancient Chinese corpora, we release AnchiBERT, a pre-trained language model based on the architecture of BERT, which is trained on large-scale ancient Chinese corpora. We evaluate AnchiBERT on both language understanding and generation tasks, including poem classification, ancient-modern Chinese translation, poem generation, and couplet generation. The experimental results show that AnchiBERT outperforms BERT as well as the non-pretrained models and achieves state-of-the-art results in all cases.

| Comments: | 10 pages with 3 figures                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.11473](https://arxiv.org/abs/2009.11473) [cs.CL]** |
|           | (or **[arXiv:2009.11473v1](https://arxiv.org/abs/2009.11473v1) [cs.CL]** for this version) |





<h2 id="2020-09-25-3">3. N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models</h2>

Title: [N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models](https://arxiv.org/abs/2009.11616)

Authors: [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Yunlong Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Libo Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+L), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> We introduce \texttt{N-LTP}, an open-source Python Chinese natural language processing toolkit supporting five basic tasks: Chinese word segmentation, part-of-speech tagging, named entity recognition, dependency parsing, and semantic dependency parsing. \texttt{N-LTP} adopts the multi-task framework with the pre-trained model to capture the shared knowledge across all Chinese relevant tasks. In addition, we propose to use knowledge distillation where single-task models teach a multi-task model, helping the multi-task model surpass its single-task teachers. Finally, we provide fundamental tasks API and a visualization tool to make users easier to use and view the processing results directly. To the best of our knowledge, this is the first toolkit to support all Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at [this https URL](https://github.com/HIT-SCIR/ltp).

| Comments: | Work in progress. Code is available at [GitHub]([this https URL](https://github.com/HIT-SCIR/ltp)) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.11616](https://arxiv.org/abs/2009.11616) [cs.CL]** |
|           | (or **[arXiv:2009.11616v1](https://arxiv.org/abs/2009.11616v1) [cs.CL]** for this version) |



# 2020-09-23

[Return to Index](#Index)



<h2 id="2020-09-23-1">1. KoBE: Knowledge-Based Machine Translation Evaluation</h2>

Title: [KoBE: Knowledge-Based Machine Translation Evaluation](https://arxiv.org/abs/2009.11027)

Authors: [Zorik Gekhman](https://arxiv.org/search/cs?searchtype=author&query=Gekhman%2C+Z), [Roee Aharoni](https://arxiv.org/search/cs?searchtype=author&query=Aharoni%2C+R), [Genady Beryozkin](https://arxiv.org/search/cs?searchtype=author&query=Beryozkin%2C+G), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

> We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.

| Comments: | Accepted as a short paper in Findings of EMNLP 2020          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.11027](https://arxiv.org/abs/2009.11027) [cs.CL]** |
|           | (or **[arXiv:2009.11027v1](https://arxiv.org/abs/2009.11027v1) [cs.CL]** for this version) |





<h2 id="2020-09-23-2">2. Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages</h2>

Title: [Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages](https://arxiv.org/abs/2009.11201)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

> Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform a large collection of supervised WMT submissions for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.11201](https://arxiv.org/abs/2009.11201) [cs.CL]** |
|           | (or **[arXiv:2009.11201v1](https://arxiv.org/abs/2009.11201v1) [cs.CL]** for this version) |



# 2020-09-22

[Return to Index](#Index)



<h2 id="2020-09-22-1">1. Presenting Simultaneous Translation in Limited Space</h2>

Title: [Presenting Simultaneous Translation in Limited Space](https://arxiv.org/abs/2009.09016)

Authors: [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Some methods of automatic simultaneous translation of a long-form speech allow revisions of outputs, trading accuracy for low latency. Deploying these systems for users faces the problem of presenting subtitles in a limited space, such as two lines on a television screen. The subtitles must be shown promptly, incrementally, and with adequate time for reading. We provide an algorithm for subtitling. Furthermore, we propose a way how to estimate the overall usability of the combination of automatic translation and subtitling by measuring the quality, latency, and stability on a test set, and propose an improved measure for translation latency.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ITAT WAFNL 2020                                              |
| Cite as:           | **[arXiv:2009.09016](https://arxiv.org/abs/2009.09016) [cs.CL]** |
|                    | (or **[arXiv:2009.09016v1](https://arxiv.org/abs/2009.09016v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-2">2. COMET: A Neural Framework for MT Evaluation</h2>

Title: [COMET: A Neural Framework for MT Evaluation](https://arxiv.org/abs/2009.09025)

Authors: [Ricardo Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+R), [Craig Stewart](https://arxiv.org/search/cs?searchtype=author&query=Stewart%2C+C), [Ana C Farinha](https://arxiv.org/search/cs?searchtype=author&query=Farinha%2C+A+C), [Alon Lavie](https://arxiv.org/search/cs?searchtype=author&query=Lavie%2C+A)

> We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.09025](https://arxiv.org/abs/2009.09025) [cs.CL]** |
|           | (or **[arXiv:2009.09025v1](https://arxiv.org/abs/2009.09025v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-3">3. Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing</h2>

Title: [Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing](https://arxiv.org/abs/2009.09126)

Authors: [Ke Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Jiayi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Niyu Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+N), [Yangbing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Yu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Kai Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+K)

> With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an end-to-end deep learning framework of the quality estimation and automatic post-editing of the machine translation output. Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model. To imitate the behavior of human translators, we design three efficient delegation modules -- quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them. We examine this approach with the English--German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance. We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09126](https://arxiv.org/abs/2009.09126) [cs.CL]** |
|           | (or **[arXiv:2009.09126v1](https://arxiv.org/abs/2009.09126v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-4">4. Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation</h2>

Title: [Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation](https://arxiv.org/abs/2009.09127)

Authors: [Pei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Niyu Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+N), [Kai Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+K)

> Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.

| Comments: | accepted to EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2009.09127](https://arxiv.org/abs/2009.09127) [cs.CL]** |
|           | (or **[arXiv:2009.09127v1](https://arxiv.org/abs/2009.09127v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-5">5. Towards Computational Linguistics in Minangkabau Language: Studies on Sentiment Analysis and Machine Translation</h2>

Title: [Towards Computational Linguistics in Minangkabau Language: Studies on Sentiment Analysis and Machine Translation](https://arxiv.org/abs/2009.09309)

Authors: [Fajri Koto](https://arxiv.org/search/cs?searchtype=author&query=Koto%2C+F), [Ikhwan Koto](https://arxiv.org/search/cs?searchtype=author&query=Koto%2C+I)

> Although some linguists (Rusmali et al., 1985; Crouch, 2009) have fairly attempted to define the morphology and syntax of Minangkabau, information processing in this language is still absent due to the scarcity of the annotated resource. In this work, we release two Minangkabau corpora: sentiment analysis and machine translation that are harvested and constructed from Twitter and Wikipedia. We conduct the first computational linguistics in Minangkabau language employing classic machine learning and sequence-to-sequence models such as LSTM and Transformer. Our first experiments show that the classification performance over Minangkabau text significantly drops when tested with the model trained in Indonesian. Whereas, in the machine translation experiment, a simple word-to-word translation using a bilingual dictionary outperforms LSTM and Transformer model in terms of BLEU score.

| Comments: | Accepted at PACLIC 2020 - The 34th Pacific Asia Conference on Language, Information and Computation |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.09309](https://arxiv.org/abs/2009.09309) [cs.CL]** |
|           | (or **[arXiv:2009.09309v1](https://arxiv.org/abs/2009.09309v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-6">6. Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation</h2>

Title: [Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation](https://arxiv.org/abs/2009.09359)

Authors: [Tahmid Hasan](https://arxiv.org/search/cs?searchtype=author&query=Hasan%2C+T), [Abhik Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+A), [Kazi Samin](https://arxiv.org/search/cs?searchtype=author&query=Samin%2C+K), [Md Hasan](https://arxiv.org/search/cs?searchtype=author&query=Hasan%2C+M), [Madhusudan Basak](https://arxiv.org/search/cs?searchtype=author&query=Basak%2C+M), [M. Sohel Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+M+S), [Rifat Shahriyar](https://arxiv.org/search/cs?searchtype=author&query=Shahriyar%2C+R)

> Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural machine translation models, we achieve an improvement of more than 9 BLEU over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 615 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set; thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.09359](https://arxiv.org/abs/2009.09359) [cs.CL]** |
|           | (or **[arXiv:2009.09359v1](https://arxiv.org/abs/2009.09359v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-7">7. Softmax Tempering for Training Neural Machine Translation Models</h2>

Title: [Softmax Tempering for Training Neural Machine Translation Models](https://arxiv.org/abs/2009.09372)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against smoothed gold labels. In low-resource scenarios, NMT models tend to over-fit because the softmax distribution quickly approaches the gold label distribution. To address this issue, we propose to divide the logits by a temperature coefficient, prior to applying softmax, during training. In our experiments on 11 language pairs in the Asian Language Treebank dataset and the WMT 2019 English-to-German translation task, we observed significant improvements in translation quality by up to 3.9 BLEU points. Furthermore, softmax tempering makes the greedy search to be as good as beam search decoding in terms of translation quality, enabling 1.5 to 3.5 times speed-up. We also study the impact of softmax tempering on multilingual NMT and recurrently stacked NMT, both of which aim to reduce the NMT model size by parameter sharing thereby verifying the utility of temperature in developing compact NMT models. Finally, an analysis of softmax entropies and gradients reveal the impact of our method on the internal behavior of NMT models.

| Comments: | The paper is about prediction smoothing for improving sequence to sequence performance. Related to but not the same as label smoothing. Work in progress. Updates with deeper analyses and comparisons to related methods to follow. Rejected from EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2009.09372](https://arxiv.org/abs/2009.09372) [cs.CL]** |
|           | (or **[arXiv:2009.09372v1](https://arxiv.org/abs/2009.09372v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-8">8. Improving Robustness and Generality of NLP Models Using Disentangled Representations</h2>

Title: [Improving Robustness and Generality of NLP Models Using Disentangled Representations](https://arxiv.org/abs/2009.09587)

Authors: [Jiawei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiang Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Supervised neural networks, which first map an input x to a single representation z, and then map z to the output label y, have achieved remarkable success in a wide range of natural language processing (NLP) tasks. Despite their success, neural models lack for both robustness and generality: small perturbations to inputs can result in absolutely different outputs; the performance of a model trained on one domain drops drastically when tested on another domain.
> In this paper, we present methods to improve robustness and generality of NLP models from the standpoint of disentangled representation learning. Instead of mapping x to a single representation z, the proposed strategy maps x to a set of representations {z1,z2,...,zK} while forcing them to be disentangled. These representations are then mapped to different logits ls, the ensemble of which is used to make the final prediction y. We propose different methods to incorporate this idea into currently widely-used models, including adding an L2 regularizer on zs or adding Total Correlation (TC) under the framework of variational information bottleneck (VIB). We show that models trained with the proposed criteria provide better robustness and domain adaptation ability in a wide range of supervised learning tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09587](https://arxiv.org/abs/2009.09587) [cs.CL]** |
|           | (or **[arXiv:2009.09587v1](https://arxiv.org/abs/2009.09587v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-9">9. Generative Imagination Elevates Machine Translation</h2>

Title: [Generative Imagination Elevates Machine Translation](https://arxiv.org/abs/2009.09654)

Authors: [Quanyu Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+Q), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> There are thousands of languages on earth, but visual perception is shared among peoples. Existing multimodal neural machine translation (MNMT) methods achieve knowledge transfer by enforcing one encoder to learn shared representation across textual and visual modalities. However, the training and inference process heavily relies on well-aligned bilingual sentence - image triplets as input, which are often limited in quantity. In this paper, we hypothesize that visual imagination via synthesizing visual representation from source text could help the neural model map two languages with different symbols, thus helps the translation task. Our proposed end-to-end imagination-based machine translation model (ImagiT) first learns to generate semantic-consistent visual representation from source sentence, and then generate target sentence based on both text representation and imagined visual representation. Experiments demonstrate that our translation model benefits from visual imagination and significantly outperforms the text-only neural machine translation (NMT) baseline. We also conduct analyzing experiments, and the results show that imagination can help fill in missing information when performing the degradation strategy.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09654](https://arxiv.org/abs/2009.09654) [cs.CL]** |
|           | (or **[arXiv:2009.09654v1](https://arxiv.org/abs/2009.09654v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-10">10. Alleviating the Inequality of Attention Heads for Neural Machine Translation</h2>

Title: [Alleviating the Inequality of Attention Heads for Neural Machine Translation](https://arxiv.org/abs/2009.09672)

Authors: [Zewei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Xinyu Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Recent studies show that the attention heads in Transformer are not equal. We relate this phenomenon to the imbalance training of multi-head attention and the model dependence on specific heads. To tackle this problem, we propose a simple masking method: HeadMask, in two specific ways. Experiments show that translation improvements are achieved on multiple language pairs. Subsequent empirical analyses also support our assumption and confirm the effectiveness of the method.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09672](https://arxiv.org/abs/2009.09672) [cs.CL]** |
|           | (or **[arXiv:2009.09672v1](https://arxiv.org/abs/2009.09672v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-11">11. TED: Triple Supervision Decouples End-to-end Speech-to-text Translation</h2>

Title: [TED: Triple Supervision Decouples End-to-end Speech-to-text Translation](https://arxiv.org/abs/2009.09704)

Authors: [Qianqian Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Shuang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Bo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+B), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> An end-to-end speech-to-text translation (ST) takes audio in a source language and outputs the text in a target language. Inspired by neuroscience, humans have perception systems and cognitive systems to process different information, we propose TED, \textbf{T}ransducer-\textbf{E}ncoder-\textbf{D}ecoder, a unified framework with triple supervision to decouple the end-to-end speech-to-text translation task. In addition to the target sentence translation loss, \method includes two auxiliary supervising signals to guide the acoustic transducer that extracts acoustic features from the input, and the semantic encoder to extract semantic features relevant to the source transcription text. Our method achieves state-of-the-art performance on both English-French and English-German speech translation benchmarks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09704](https://arxiv.org/abs/2009.09704) [cs.CL]** |
|           | (or **[arXiv:2009.09704v1](https://arxiv.org/abs/2009.09704v1) [cs.CL]** for this version) |





<h2 id="2020-09-22-12">12. SDST: Successive Decoding for Speech-to-text Translation</h2>

Title: [SDST: Successive Decoding for Speech-to-text Translation](https://arxiv.org/abs/2009.09737)

Authors: [Qianqian Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Shuang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Bo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+B), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> End-to-end speech-to-text translation (ST), which directly translates the source language speech to the target language text, has attracted intensive attention recently. However, the combination of speech recognition and machine translation in a single model poses a heavy burden on the direct cross-modal cross-lingual mapping. To reduce the learning difficulty, we propose SDST, an integral framework with \textbf{S}uccessive \textbf{D}ecoding for end-to-end \textbf{S}peech-to-text \textbf{T}ranslation task. This method is verified in two mainstream datasets. Experiments show that our proposed \method improves the previous state-of-the-art methods by big margins.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.09737](https://arxiv.org/abs/2009.09737) [cs.CL]** |
|           | (or **[arXiv:2009.09737v1](https://arxiv.org/abs/2009.09737v1) [cs.CL]** for this version) |



# 2020-09-21

[Return to Index](#Index)



<h2 id="2020-09-21-1">1. A Study of Genetic Algorithms for Hyperparameter Optimization of Neural Networks in Machine Translation</h2>

Title: [A Study of Genetic Algorithms for Hyperparameter Optimization of Neural Networks in Machine Translation](https://arxiv.org/abs/2009.08928)

Authors: [Keshav Ganapathy](https://arxiv.org/search/cs?searchtype=author&query=Ganapathy%2C+K)

> With neural networks having demonstrated their versatility and benefits, the need for their optimal performance is as prevalent as ever. A defining characteristic, hyperparameters, can greatly affect its performance. Thus engineers go through a process, tuning, to identify and implement optimal hyperparameters. That being said, excess amounts of manual effort are required for tuning network architectures, training configurations, and preprocessing settings such as Byte Pair Encoding (BPE). In this study, we propose an automatic tuning method modeled after Darwin's Survival of the Fittest Theory via a Genetic Algorithm (GA). Research results show that the proposed method, a GA, outperforms a random selection of hyperparameters.

| Subjects: | **Neural and Evolutionary Computing (cs.NE)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.08928](https://arxiv.org/abs/2009.08928) [cs.NE]** |
|           | (or **[arXiv:2009.08928v1](https://arxiv.org/abs/2009.08928v1) [cs.NE]** for this version) |





<h2 id="2020-09-21-2">2. fastHan: A BERT-based Joint Many-Task Toolkit for Chinese NLP</h2>

Title: [fastHan: A BERT-based Joint Many-Task Toolkit for Chinese NLP](https://arxiv.org/abs/2009.08633)

Authors: [Zhichao Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+Z), [Hang Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+H), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> We present fastHan, an open-source toolkit for four basic tasks in Chinese natural language processing: Chinese word segmentation, Part-of-Speech tagging, named entity recognition, and dependency parsing. The kernel of fastHan is a joint many-task model based on a pruned BERT, which uses the first 8 layers in BERT. We also provide a 4-layer base version of model compressed from the 8-layer model. The joint-model is trained and evaluated in 13 corpora of four tasks, yielding near state-of-the-art (SOTA) performance in the dependency parsing task and SOTA performance in the other three tasks. In addition to its small size and excellent performance, fastHan is also very user-friendly. Implemented as a python package, fastHan allows users to easily download and use it. Users can get what they want with one line of code, even if they have little knowledge of deep learning. The project is released on Github.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.08633](https://arxiv.org/abs/2009.08633) [cs.CL]** |
|           | (or **[arXiv:2009.08633v1](https://arxiv.org/abs/2009.08633v1) [cs.CL]** for this version) |





<h2 id="2020-09-21-3">3. Hierarchical GPT with Congruent Transformers for Multi-Sentence Language Models</h2>

Title: [Hierarchical GPT with Congruent Transformers for Multi-Sentence Language Models](https://arxiv.org/abs/2009.08636)

Authors: [Jihyeon Roh](https://arxiv.org/search/cs?searchtype=author&query=Roh%2C+J), [Huiseong Gim](https://arxiv.org/search/cs?searchtype=author&query=Gim%2C+H), [Soo-Young Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> We report a GPT-based multi-sentence language model for dialogue generation and document understanding. First, we propose a hierarchical GPT which consists of three blocks, i.e., a sentence encoding block, a sentence generating block, and a sentence decoding block. The sentence encoding and decoding blocks are basically the encoder-decoder blocks of the standard Transformers, which work on each sentence independently. The sentence generating block is inserted between the encoding and decoding blocks, and generates the next sentence embedding vector from the previous sentence embedding vectors. We believe it is the way human make conversation and understand paragraphs and documents. Since each sentence may consist of fewer words, the sentence encoding and decoding Transformers can use much smaller dimensional embedding vectors. Secondly, we note the attention in the Transformers utilizes the inner-product similarity measure. Therefore, to compare the two vectors in the same space, we set the transform matrices for queries and keys to be the same. Otherwise, the similarity concept is incongruent. We report experimental results to show that these two modifications increase the language model performance for tasks with multiple sentences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.08636](https://arxiv.org/abs/2009.08636) [cs.CL]** |
|           | (or **[arXiv:2009.08636v1](https://arxiv.org/abs/2009.08636v1) [cs.CL]** for this version) |





<h2 id="2020-09-21-4">4. Document-level Neural Machine Translation with Document Embeddings</h2>

Title: [Document-level Neural Machine Translation with Document Embeddings](https://arxiv.org/abs/2009.08775)

Authors: [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Bao-Liang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+B)

> Standard neural machine translation (NMT) is on the assumption of document-level context independent. Most existing document-level NMT methods are satisfied with a smattering sense of brief document-level information, while this work focuses on exploiting detailed document-level context in terms of multiple forms of document embeddings, which is capable of sufficiently modeling deeper and richer document-level context. The proposed document-aware NMT is implemented to enhance the Transformer baseline by introducing both global and local document-level clues on the source end. Experiments show that the proposed method significantly improves the translation performance over strong baselines and other related studies.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1910.14528](https://arxiv.org/abs/1910.14528) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.08775](https://arxiv.org/abs/2009.08775) [cs.CL]** |
|           | (or **[arXiv:2009.08775v1](https://arxiv.org/abs/2009.08775v1) [cs.CL]** for this version) |



# 2020-09-18

[Return to Index](#Index)



<h2 id="2020-09-18-1">1. Code-switching pre-training for neural machine translation</h2>

Title: [Code-switching pre-training for neural machine translation](https://arxiv.org/abs/2009.08088)

Authors: [Zhen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Bojie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Ambyera Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+A), [Shen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Qi Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+Q)

> This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.

| Comments: | 10 pages, EMNLP2020 main conference                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.08088](https://arxiv.org/abs/2009.08088) [cs.CL]** |
|           | (or **[arXiv:2009.08088v1](https://arxiv.org/abs/2009.08088v1) [cs.CL]** for this version) |





<h2 id="2020-09-18-2">2. FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding</h2>

Title: [FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding](https://arxiv.org/abs/2009.08138)

Authors: [Yutai Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Jiafeng Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+J), [Yongkui Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Y), [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Zhigang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Few-learn learning (FSL) is one of the key future steps in machine learning and has raised a lot of attention. However, in contrast to the rapid development in other domains, such as Computer Vision, the progress of FSL in Nature Language Processing (NLP) is much slower. One of the key reasons for this is the lacking of public benchmarks. NLP FSL researches always report new results on their own constructed few-shot datasets, which is pretty inefficient in results comparison and thus impedes cumulative progress. In this paper, we present FewJoint, a novel Few-Shot Learning benchmark for NLP. Different from most NLP FSL research that only focus on simple N-classification problems, our benchmark introduces few-shot joint dialogue language understanding, which additionally covers the structure prediction and multi-task reliance problems. This allows our benchmark to reflect the real-word NLP complexity beyond simple N-classification. Our benchmark is used in the few-shot learning contest of SMP2020-ECDT task-1. We also provide a compatible FSL platform to ease experiment set-up.

| Comments: | Code and dataset is available at: [this https URL](https://github.com/AtmaHou/MetaDialog) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2009.08138](https://arxiv.org/abs/2009.08138) [cs.CL]** |
|           | (or **[arXiv:2009.08138v1](https://arxiv.org/abs/2009.08138v1) [cs.CL]** for this version) |





# 2020-09-17

[Return to Index](#Index)



<h2 id="2020-09-17-1">1. Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation</h2>

Title: [Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation](https://arxiv.org/abs/2009.07453)

Authors: [Insoo Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+I), [Byeongwook Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+B), [Yoonjung Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Se Jung Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+S+J), [Yongkweon Jeon](https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+Y), [Baeseong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+B), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Dongsoo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D)

> Transformer is being widely used in Neural Machine Translation (NMT). Deploying Transformer models to mobile or edge devices with limited resources is challenging because of heavy computation and memory overhead during inference. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation accuracy and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights with lower bits (e.g. under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8x smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3x reduction in run-time memory footprints and 3.5x speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07453](https://arxiv.org/abs/2009.07453) [cs.LG]** |
|           | (or **[arXiv:2009.07453v1](https://arxiv.org/abs/2009.07453v1) [cs.LG]** for this version) |





<h2 id="2020-09-17-2">2. Simultaneous Machine Translation with Visual Context</h2>

Title: [Simultaneous Machine Translation with Visual Context](https://arxiv.org/abs/2009.07310)

Authors: [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Veneta Haralampieva](https://arxiv.org/search/cs?searchtype=author&query=Haralampieva%2C+V), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Loïc Barrault](https://arxiv.org/search/cs?searchtype=author&query=Barrault%2C+L), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus have to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order such as adjective-noun placement between English and French.

| Comments: | Long paper accepted to EMNLP 2020                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.07310](https://arxiv.org/abs/2009.07310) [cs.CL]** |
|           | (or **[arXiv:2009.07310v1](https://arxiv.org/abs/2009.07310v1) [cs.CL]** for this version) |





<h2 id="2020-09-17-3">3. Graph-to-Sequence Neural Machine Translation</h2>

Title: [Graph-to-Sequence Neural Machine Translation](https://arxiv.org/abs/2009.07489)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R)

> Neural machine translation (NMT) usually works in a seq2seq learning way by viewing either source or target sentence as a linear sequence of words, which can be regarded as a special case of graph, taking words in the sequence as nodes and relationships between words as edges. In the light of the current NMT models more or less capture graph information among the sequence in a latent way, we present a graph-to-sequence model facilitating explicit graph information capturing. In detail, we propose a graph-based SAN-based NMT model called Graph-Transformer by capturing information of subgraphs of different orders in every layers. Subgraphs are put into different groups according to their orders, and every group of subgraphs respectively reflect different levels of dependency between words. For fusing subgraph representations, we empirically explore three methods which weight different groups of subgraphs of different orders. Results of experiments on WMT14 English-German and IWSLT14 German-English show that our method can effectively boost the Transformer with an improvement of 1.1 BLEU points on WMT14 English-German dataset and 1.0 BLEU points on IWSLT14 German-English dataset.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07489](https://arxiv.org/abs/2009.07489) [cs.CL]** |
|           | (or **[arXiv:2009.07489v1](https://arxiv.org/abs/2009.07489v1) [cs.CL]** for this version) |





<h2 id="2020-09-17-4">4. Contextualized Perturbation for Textual Adversarial Attack</h2>

Title: [Contextualized Perturbation for Textual Adversarial Attack](https://arxiv.org/abs/2009.07502)

Authors: [Dianqi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Hao Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+H), [Liqun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L), [Chris Brockett](https://arxiv.org/search/cs?searchtype=author&query=Brockett%2C+C), [Ming-Ting Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Bill Dolan](https://arxiv.org/search/cs?searchtype=author&query=Dolan%2C+B)

> Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07502](https://arxiv.org/abs/2009.07502) [cs.CL]** |
|           | (or **[arXiv:2009.07502v1](https://arxiv.org/abs/2009.07502v1) [cs.CL]** for this version) |





<h2 id="2020-09-17-5">5. Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</h2>

Title: [Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](https://arxiv.org/abs/2009.07610)

Authors: [Alexandra Chronopoulou](https://arxiv.org/search/cs?searchtype=author&query=Chronopoulou%2C+A), [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.

| Comments: | EMNLP 2020, main conference                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.07610](https://arxiv.org/abs/2009.07610) [cs.CL]** |
|           | (or **[arXiv:2009.07610v1](https://arxiv.org/abs/2009.07610v1) [cs.CL]** for this version) |





<h2 id="2020-09-17-6">6. Knowledge Graphs for Multilingual Language Translation and Generation</h2>

Title: [Knowledge Graphs for Multilingual Language Translation and Generation](https://arxiv.org/abs/2009.07715)

Authors: [Diego Moussallem](https://arxiv.org/search/cs?searchtype=author&query=Moussallem%2C+D)

> The Natural Language Processing (NLP) community has recently seen outstanding progress, catalysed by the release of different Neural Network (NN) architectures. Neural-based approaches have proven effective by significantly increasing the output quality of a large number of automated solutions for NLP tasks (Belinkov and Glass, 2019). Despite these notable advancements, dealing with entities still poses a difficult challenge as they are rarely seen in training data. Entities can be classified into two groups, i.e., proper nouns and common nouns. Proper nouns are also known as Named Entities (NE) and correspond to the name of people, organizations, or locations, e.g., John, WHO, or Canada. Common nouns describe classes of objects, e.g., spoon or cancer. Both types of entities can be found in a Knowledge Graph (KG). Recent work has successfully exploited the contribution of KGs in NLP tasks, such as Natural Language Inference (NLI) (KM et al.,2018) and Question Answering (QA) (Sorokin and Gurevych, 2018). Only a few works had exploited the benefits of KGs in Neural Machine Translation (NMT) when the work presented herein began. Additionally, few works had studied the contribution of KGs to Natural Language Generation (NLG) tasks. Moreover, the multilinguality also remained an open research area in these respective tasks (Young et al., 2018). In this thesis, we focus on the use of KGs for machine translation and the generation of texts to deal with the problems caused by entities and consequently enhance the quality of automatically generated texts.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.17619/UNIPB/1-980](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.17619%2FUNIPB%2F1-980&v=918509fa) |
| Cite as:  | **[arXiv:2009.07715](https://arxiv.org/abs/2009.07715) [cs.CL]** |
|           | (or **[arXiv:2009.07715v1](https://arxiv.org/abs/2009.07715v1) [cs.CL]** for this version) |





<h2 id="2020-09-17-7">7. Text Generation by Learning from Off-Policy Demonstrations</h2>

Title: [Text Generation by Learning from Off-Policy Demonstrations](https://arxiv.org/abs/2009.07839)

Authors: [Richard Yuanzhe Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y), [He He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H)

> Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as a reinforcement learning (RL) problem with expert demonstrations (i.e., the training data), where the goal is to maximize quality given model-generated histories. Prior RL approaches to generation often face optimization issues due to the large action space and sparse reward. We propose GOLD (generation by off-policy learning from demonstrations): an algorithm that learns from the off-policy demonstrations by importance weighting and does not suffer from degenerative solutions. We find that GOLD outperforms the baselines according to automatic and human evaluation on summarization, question generation, and machine translation, including attaining state-of-the-art results for CNN/DailyMail summarization. Further, we show that models trained by GOLD are less sensitive to decoding algorithms and the generation quality does not degrade much as the length increases.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07839](https://arxiv.org/abs/2009.07839) [cs.CL]** |
|           | (or **[arXiv:2009.07839v1](https://arxiv.org/abs/2009.07839v1) [cs.CL]** for this version) |







# 2020-09-16

[Return to Index](#Index)



<h2 id="2020-09-16-1">1. Efficient Transformers: A Survey</h2>

Title: [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \emph{"X-former"} models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory \emph{efficiency}. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06732](https://arxiv.org/abs/2009.06732) [cs.LG]** |
|           | (or **[arXiv:2009.06732v1](https://arxiv.org/abs/2009.06732v1) [cs.LG]** for this version) |



<h2 id="2020-09-16-2">2. Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models</h2>

Title: [Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models](https://arxiv.org/abs/2009.07053)

Authors: [Joseph F DeRose](https://arxiv.org/search/cs?searchtype=author&query=DeRose%2C+J+F), [Jiayao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Matthew Berger](https://arxiv.org/search/cs?searchtype=author&query=Berger%2C+M)

> Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.

| Comments: | 11 pages, 12 figures, to be published in IEEE Transactions on Visualization and Computer Graphics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2009.07053](https://arxiv.org/abs/2009.07053) [cs.HC]** |
|           | (or **[arXiv:2009.07053v1](https://arxiv.org/abs/2009.07053v1) [cs.HC]** for this version) |



<h2 id="2020-09-16-3">3. Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation</h2>

Title: [Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2009.07177)

Authors: [Jason Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Raphael Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+R), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 En-De, WMT'16 Ro-En and IWSLT'16 De-En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 En-De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).

| Comments: | Accepted to EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.07177](https://arxiv.org/abs/2009.07177) [cs.CL]** |
|           | (or **[arXiv:2009.07177v1](https://arxiv.org/abs/2009.07177v1) [cs.CL]** for this version) |



<h2 id="2020-09-16-4">4. A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation</h2>

Title: [A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation](https://arxiv.org/abs/2009.07243)

Authors: [Moin Nadeem](https://arxiv.org/search/cs?searchtype=author&query=Nadeem%2C+M), [Tianxing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J)

> This work studies the widely adopted ancestral sampling algorithms for auto-regressive language models, which is not widely studied in the literature. We use the quality-diversity (Q-D) trade-off to investigate three popular sampling algorithms (top-k, nucleus and tempered sampling). We focus on the task of open-ended language generation. We first show that the existing sampling algorithms have similar performance. After carefully inspecting the transformations defined by different sampling algorithms, we identify three key properties that are shared among them: entropy reduction, order preservation, and slope preservation. To validate the importance of the identified properties, we design two sets of new sampling algorithms: one set in which each algorithm satisfies all three properties, and one set in which each algorithm violates at least one of the properties. We compare their performance with existing sampling algorithms, and find that violating the identified properties could lead to drastic performance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms. Our data and code are available at [this https URL](https://github.com/moinnadeem/characterizing-sampling-algorithms)

| Comments: | To appear at AACL 2020; 9 pages, 12 figures, 2 tables        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.07243](https://arxiv.org/abs/2009.07243) [cs.CL]** |
|           | (or **[arXiv:2009.07243v1](https://arxiv.org/abs/2009.07243v1) [cs.CL]** for this version) |



<h2 id="2020-09-16-5">5. Autoregressive Knowledge Distillation through Imitation Learning</h2>

Title: [Autoregressive Knowledge Distillation through Imitation Learning](https://arxiv.org/abs/2009.07253)

Authors: [Alexander Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+A), [Jeremy Wohlwend](https://arxiv.org/search/cs?searchtype=author&query=Wohlwend%2C+J), [Howard Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Tao Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+T)

> The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07253](https://arxiv.org/abs/2009.07253) [cs.CL]** |
|           | (or **[arXiv:2009.07253v1](https://arxiv.org/abs/2009.07253v1) [cs.CL]** for this version) |







# 2020-09-15

[Return to Index](#Index)



<h2 id="2020-09-15-1">1. Unit Test Case Generation with Transformers</h2>

Title: [Unit Test Case Generation with Transformers](https://arxiv.org/abs/2009.05617)

Authors: [Michele Tufano](https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M), [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Shao Kun Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S+K), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N)

> Automated Unit Test Case generation has been the focus of extensive literature within the research community. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult to read or understand for developers. In this paper we propose AthenaTest, an approach that aims at generating unit test cases by learning from real-world, developer-written test cases. Our approach relies on a state-of-the-art sequence-to-sequence transformer model which is able to write useful test cases for a given method under test (i.e., focal method). We also introduce methods2test - the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 630k test cases mined from 70k open-source repositories hosted on GitHub. We use this dataset to train a transformer model to translate focal methods into the corresponding test cases. We evaluate the ability of our model in generating test cases using natural language processing as well as code-specific criteria. First, we assess the quality of the translation compared to the target test case, then we analyze properties of the test case such as syntactic correctness and number and variety of testing APIs (e.g., asserts). We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated test cases.

| Subjects: | **Software Engineering (cs.SE)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.05617](https://arxiv.org/abs/2009.05617) [cs.SE]** |
|           | (or **[arXiv:2009.05617v1](https://arxiv.org/abs/2009.05617v1) [cs.SE]** for this version) |





<h2 id="2020-09-15-2">2. Improving Indonesian Text Classification Using Multilingual Language Model</h2>

Title: [Improving Indonesian Text Classification Using Multilingual Language Model](https://arxiv.org/abs/2009.05713)

Authors: [Ilham Firdausi Putra](https://arxiv.org/search/cs?searchtype=author&query=Putra%2C+I+F) (1), [Ayu Purwarianti](https://arxiv.org/search/cs?searchtype=author&query=Purwarianti%2C+A) (1 and 2) ((1) Institut Teknologi Bandung, (2) U-CoE AI-VLB)

> Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | 2020 International Conference on Advanced Informatics: Concept, Theory and Application (ICAICTA) |
| Cite as:           | **[arXiv:2009.05713](https://arxiv.org/abs/2009.05713) [cs.CL]** |
|                    | (or **[arXiv:2009.05713v1](https://arxiv.org/abs/2009.05713v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-3">3. Combining Word and Character Vector Representation on Neural Machine Translation</h2>

Title: [Combining Word and Character Vector Representation on Neural Machine Translation](https://arxiv.org/abs/2009.05935)

Authors: [K. M. Shahih](https://arxiv.org/search/cs?searchtype=author&query=Shahih%2C+K+M), [Ayu Purwarianti](https://arxiv.org/search/cs?searchtype=author&query=Purwarianti%2C+A)

> This paper describes combinations of word vector representation and character vector representation in English-Indonesian neural machine translation (NMT). Six configurations of NMT models were built with different input vector representations: word-based, combination of word and character representation using bidirectional LSTM(bi-LSTM), combination of word and character representation using CNN, combination of word and character representation by combining bi-LSTM and CNN by three different vector operations: addition, pointwise multiplication, and averaging. The experiment results showed that NMT models with concatenation of word and character representation obtained BLEU score higher than baseline model, ranging from 9.14 points to 11.65 points, for all models that combining both word and character representation, except the model that combining word and character representation using both bi-LSTM and CNN by addition operation. The highest BLEU score achieved was 42.48 compared to the 30.83 of the baseline model.

| Comments: | 5 pages, 5 figures. Published in 2019 Fourth International Conference on Informatics and Computing (ICIC) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05935](https://arxiv.org/abs/2009.05935) [cs.CL]** |
|           | (or **[arXiv:2009.05935v1](https://arxiv.org/abs/2009.05935v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-4">4. Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding</h2>

Title: [Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding](https://arxiv.org/abs/2009.06097)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Luowei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Our proposed method allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.06097](https://arxiv.org/abs/2009.06097) [cs.CL]** |
|           | (or **[arXiv:2009.06097v1](https://arxiv.org/abs/2009.06097v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-5">5. Contrastive Triple Extraction with Generative Transformer</h2>

Title: [Contrastive Triple Extraction with Generative Transformer](https://arxiv.org/abs/2009.06207)

Authors: [Hongbin Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Shumin Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S), [Mosha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Chuanqi Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, We introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines. Our code and datasets will be released after publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06207](https://arxiv.org/abs/2009.06207) [cs.CL]** |
|           | (or **[arXiv:2009.06207v1](https://arxiv.org/abs/2009.06207v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-6">6. Improving Language Generation with Sentence Coherence Objective</h2>

Title: [Improving Language Generation with Sentence Coherence Objective](https://arxiv.org/abs/2009.06358)

Authors: [Ruixiao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+R), [Jie Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Mehrdad Yousefzadeh](https://arxiv.org/search/cs?searchtype=author&query=Yousefzadeh%2C+M)

> Conditional story generation and contextual text continuation have become increasingly popular topics in NLP community. Existing models are often prone to output paragraphs of texts that gradually diverge from the given prompt. Although the generated text may have a reasonable perplexity and diversity, it could easily be identified by human as gibberish. The goal of our project is to improve the coherence and consistency across sentences in a language-generation model. We aim to solve this issue by first training a sentence pair coherence classifier with GPT-2 pretrained model, and then co-train the GPT-2 language model with this new coherence objective using a method analogous to the REINFORCE algorithm. This fine-tuned language model is able to generate lengthy paragraph conditioned on a given topic without diverging too much. The simplicity of this model allows it to be applicable to a variety of underlying language model architecture since it only modifies the final layer of the pre-trained model.

| Comments: | 11 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.06358](https://arxiv.org/abs/2009.06358) [cs.CL]** |
|           | (or **[arXiv:2009.06358v1](https://arxiv.org/abs/2009.06358v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-7">7. GeDi: Generative Discriminator Guided Sequence Generation</h2>

Title: [GeDi: Generative Discriminator Guided Sequence Generation](https://arxiv.org/abs/2009.06367)

Authors: [Ben Krause](https://arxiv.org/search/cs?searchtype=author&query=Krause%2C+B), [Akhilesh Deepak Gotmare](https://arxiv.org/search/cs?searchtype=author&query=Gotmare%2C+A+D), [Bryan McCann](https://arxiv.org/search/cs?searchtype=author&query=McCann%2C+B), [Nitish Shirish Keskar](https://arxiv.org/search/cs?searchtype=author&query=Keskar%2C+N+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Richard Socher](https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R), [Nazneen Fatema Rajani](https://arxiv.org/search/cs?searchtype=author&query=Rajani%2C+N+F)

> Class-conditional language models (CC-LMs) can be used to generate natural language with specific attributes, such as style or sentiment, by conditioning on an attribute label, or control code. However, we find that these models struggle to control generation when applied to out-of-domain prompts or unseen control codes. To overcome these limitations, we propose generative discriminator (GeDi) guided contrastive generation, which uses CC-LMs as generative discriminators (GeDis) to efficiently guide generation from a (potentially much larger) LM towards a desired attribute. In our human evaluation experiments, we show that GeDis trained for sentiment control on movie reviews are able to control the tone of book text. We also demonstrate that GeDis are able to detoxify generation and control topic while maintaining the same level of linguistic acceptability as direct generation from GPT-2 (1.5B parameters). Lastly, we show that a GeDi trained on only 4 topics can generalize to new control codes from word embeddings, allowing it to guide generation towards wide array of topics.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06367](https://arxiv.org/abs/2009.06367) [cs.CL]** |
|           | (or **[arXiv:2009.06367v1](https://arxiv.org/abs/2009.06367v1) [cs.CL]** for this version) |





# 2020-09-14

[Return to Index](#Index)



<h2 id="2020-09-14-1">1. FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding</h2>

Title: [FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding](https://arxiv.org/abs/2009.05166)

Authors: [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that is essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs cross-lingual fusion to extract multilingual knowledge in the intermediate layers, and finally performs further language-specific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. For better model scalability, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art (77.0 on average) on the challenging multilingual multi-task benchmark, XTREME.

| Comments: | Top-1 Performance on XTREME leaderboard (https://sites.research.google/xtreme) on September 8, 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05166](https://arxiv.org/abs/2009.05166) [cs.CL]** |
|           | (or **[arXiv:2009.05166v1](https://arxiv.org/abs/2009.05166v1) [cs.CL]** for this version) |





<h2 id="2020-09-14-2">2. Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation</h2>

Title: [Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation](https://arxiv.org/abs/2009.05460)

Authors: [Toms Bergmanis](https://arxiv.org/search/cs?searchtype=author&query=Bergmanis%2C+T), [Artūrs Stafanovičs](https://arxiv.org/search/cs?searchtype=author&query=Stafanovičs%2C+A), [Mārcis Pinnis](https://arxiv.org/search/cs?searchtype=author&query=Pinnis%2C+M)

> Neural machine translation systems typically are trained on curated corpora and break when faced with non-standard orthography or punctuation. Resilience to spelling mistakes and typos, however, is crucial as machine translation systems are used to translate texts of informal origins, such as chat conversations, social media posts and web pages. We propose a simple generative noise model to generate adversarial examples of ten different types. We use these to augment machine translation systems' training data and show that, when tested on noisy data, systems trained using adversarial examples perform almost as well as when translating clean data, while baseline systems' performance drops by 2-3 BLEU points. To measure the robustness and noise invariance of machine translation systems' outputs, we use the average translation edit rate between the translation of the original sentence and its noised variants. Using this measure, we show that systems trained on adversarial examples on average yield 50% consistency improvements when compared to baselines trained on clean data.

| Comments: | Accepted in BALTIC HLT 2020                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05460](https://arxiv.org/abs/2009.05460) [cs.CL]** |
|           | (or **[arXiv:2009.05460v1](https://arxiv.org/abs/2009.05460v1) [cs.CL]** for this version) |



# 2020-09-11

[Return to Index](#Index)



<h2 id="2020-09-11-1">1. Pay Attention when Required</h2>

Title: [Pay Attention when Required](https://arxiv.org/abs/2009.04534)

Authors: [Swetha Mandava](https://arxiv.org/search/cs?searchtype=author&query=Mandava%2C+S), [Szymon Migacz](https://arxiv.org/search/cs?searchtype=author&query=Migacz%2C+S), [Alex Fit Florea](https://arxiv.org/search/cs?searchtype=author&query=Florea%2C+A+F)

> Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.

| Comments: | 9 pages, 4 figures, 7 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2009.04534](https://arxiv.org/abs/2009.04534) [cs.LG]** |
|           | (or **[arXiv:2009.04534v1](https://arxiv.org/abs/2009.04534v1) [cs.LG]** for this version) |





<h2 id="2020-09-11-2">2. Learning Universal Representations from Word to Sentence</h2>

Title: [Learning Universal Representations from Word to Sentence](https://arxiv.org/abs/2009.04656)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific level of linguistic unit, which cause great inconvenience when being confronted with handling multiple layers of linguistic objects in a unified way. Thus this work introduces and explores the universal representation learning, i.e., embeddings of different levels of linguistic unit in a uniform vector space through a task-independent evaluation. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space. Then we empirically verify that well pre-trained Transformer models incorporated with appropriate training settings may effectively yield universal representation. Especially, our implementation of fine-tuning ALBERT on NLI and PPDB datasets achieves the highest accuracy on analogy tasks in different language levels. Further experiments on the insurance FAQ task show effectiveness of universal representation models in real-world applications.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04656](https://arxiv.org/abs/2009.04656) [cs.CL]** |
|           | (or **[arXiv:2009.04656v1](https://arxiv.org/abs/2009.04656v1) [cs.CL]** for this version) |



<h2 id="2020-09-11-3">3. Modern Methods for Text Generation</h2>

Title: [Modern Methods for Text Generation](https://arxiv.org/abs/2009.04968)

Authors: [Dimas Munoz Montesinos](https://arxiv.org/search/cs?searchtype=author&query=Montesinos%2C+D+M)

> Synthetic text generation is challenging and has limited success. Recently, a new architecture, called Transformers, allow machine learning models to understand better sequential data, such as translation or summarization. BERT and GPT-2, using Transformers in their cores, have shown a great performance in tasks such as text classification, translation and NLI tasks. In this article, we analyse both algorithms and compare their output quality in text generation tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04968](https://arxiv.org/abs/2009.04968) [cs.CL]** |
|           | (or **[arXiv:2009.04968v1](https://arxiv.org/abs/2009.04968v1) [cs.CL]** for this version) |





<h2 id="2020-09-11-4">4. Investigating Gender Bias in BERT</h2>

Title: [Investigating Gender Bias in BERT](https://arxiv.org/abs/2009.05021)

Authors: [Rishabh Bhardwaj](https://arxiv.org/search/cs?searchtype=author&query=Bhardwaj%2C+R), [Navonil Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+N), [Soujanya Poria](https://arxiv.org/search/cs?searchtype=author&query=Poria%2C+S)

> Contextual language models (CLMs) have pushed the NLP benchmarks to a new height. It has become a new norm to utilize CLM provided word embeddings in downstream tasks such as text classification. However, unless addressed, CLMs are prone to learn intrinsic gender-bias in the dataset. As a result, predictions of downstream NLP models can vary noticeably by varying gender words, such as replacing "he" to "she", or even gender-neutral words. In this paper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the gender-bias it induces in five downstream tasks related to emotion and sentiment intensity prediction. For each task, we train a simple regressor utilizing BERT's word embeddings. We then evaluate the gender-bias in regressors using an equity evaluation corpus. Ideally and from the specific design, the models should discard gender informative features from the input. However, the results show a significant dependence of the system's predictions on gender-particular words and phrases. We claim that such biases can be reduced by removing genderspecific features from word embedding. Hence, for each layer in BERT, we identify directions that primarily encode gender information. The space formed by such directions is referred to as the gender subspace in the semantic space of word embeddings. We propose an algorithm that finds fine-grained gender directions, i.e., one primary direction for each BERT layer. This obviates the need of realizing gender subspace in multiple dimensions and prevents other crucial information from being omitted. Experiments show that removing embedding components in such directions achieves great success in reducing BERT-induced bias in the downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.05021](https://arxiv.org/abs/2009.05021) [cs.CL]** |
|           | (or **[arXiv:2009.05021v1](https://arxiv.org/abs/2009.05021v1) [cs.CL]** for this version) |





# 2020-09-10

[Return to Index](#Index)



<h2 id="2020-09-10-1">1. Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages</h2>

Title: [Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages](https://arxiv.org/abs/2009.04087)

Authors: [Christopher Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Laura Dominé](https://arxiv.org/search/cs?searchtype=author&query=Dominé%2C+L), [Kevin Chavez](https://arxiv.org/search/cs?searchtype=author&query=Chavez%2C+K), [Richard Socher](https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R)

> Machine translation tools do not yet exist for the Yup'ik language, a polysynthetic language spoken by around 8,000 people who live primarily in Southwest Alaska. We compiled a parallel text corpus for Yup'ik and English and developed a morphological parser for Yup'ik based on grammar rules. We trained a seq2seq neural machine translation model with attention to translate Yup'ik input into English. We then compared the influence of different tokenization methods, namely rule-based, unsupervised (byte pair encoding), and unsupervised morphological (Morfessor) parsing, on BLEU score accuracy for Yup'ik to English translation. We find that using tokenized input increases the translation accuracy compared to that of unparsed input. Although overall Morfessor did best with a vocabulary size of 30k, our first experiments show that BPE performed best with a reduced vocabulary size.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04087](https://arxiv.org/abs/2009.04087) [cs.CL]** |
|           | (or **[arXiv:2009.04087v1](https://arxiv.org/abs/2009.04087v1) [cs.CL]** for this version) |







# 2020-09-08

[Return to Index](#Index)



<h2 id="2020-09-08-1">1. Measuring Massive Multitask Language Understanding</h2>

Title: [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)

Authors: [Dan Hendrycks](https://arxiv.org/search/cs?searchtype=author&query=Hendrycks%2C+D), [Collin Burns](https://arxiv.org/search/cs?searchtype=author&query=Burns%2C+C), [Steven Basart](https://arxiv.org/search/cs?searchtype=author&query=Basart%2C+S), [Andy Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+A), [Mantas Mazeika](https://arxiv.org/search/cs?searchtype=author&query=Mazeika%2C+M), [Dawn Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D), [Jacob Steinhardt](https://arxiv.org/search/cs?searchtype=author&query=Steinhardt%2C+J)

> We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach human-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.

| Comments: | The test and code is available at [this https URL](https://github.com/hendrycks/test) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.03300](https://arxiv.org/abs/2009.03300) [cs.CY]** |
|           | (or **[arXiv:2009.03300v1](https://arxiv.org/abs/2009.03300v1) [cs.CY]** for this version) |





<h2 id="2020-09-08-2">2. Recent Trends in the Use of Deep Learning Models for Grammar Error Handling</h2>

Title: [Recent Trends in the Use of Deep Learning Models for Grammar Error Handling](https://arxiv.org/abs/2009.02358)

Authors: [Mina Naghshnejad](https://arxiv.org/search/cs?searchtype=author&query=Naghshnejad%2C+M), [Tarun Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+T), [Vijayan N. Nair](https://arxiv.org/search/cs?searchtype=author&query=Nair%2C+V+N)

> Grammar error handling (GEH) is an important topic in natural language processing (NLP). GEH includes both grammar error detection and grammar error correction. Recent advances in computation systems have promoted the use of deep learning (DL) models for NLP problems such as GEH. In this survey we focus on two main DL approaches for GEH: neural machine translation models and editor models. We describe the three main stages of the pipeline for these models: data preparation, training, and inference. Additionally, we discuss different techniques to improve the performance of these models at each stage of the pipeline. We compare the performance of different models and conclude with proposed future directions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02358](https://arxiv.org/abs/2009.02358) [cs.CL]** |
|           | (or **[arXiv:2009.02358v1](https://arxiv.org/abs/2009.02358v1) [cs.CL]** for this version) |





<h2 id="2020-09-08-3">3. Bio-inspired Structure Identification in Language Embeddings</h2>

Title: [Bio-inspired Structure Identification in Language Embeddings](https://arxiv.org/abs/2009.02459)

Authors: [Hongwei](https://arxiv.org/search/cs?searchtype=author&query=Hongwei) (Henry)[Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou), [Oskar Elek](https://arxiv.org/search/cs?searchtype=author&query=Elek%2C+O), [Pranav Anand](https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+P), [Angus G. Forbes](https://arxiv.org/search/cs?searchtype=author&query=Forbes%2C+A+G)

> Word embeddings are a popular way to improve downstream per-formances in contemporary language modeling. However, the un-derlying geometric structure of the embedding space is not wellunderstood. We present a series of explorations using bio-inspiredmethodology to traverse and visualize word embeddings, demon-strating evidence of discernible structure. Moreover, our modelalso produces word similarity rankings that are plausible yet verydifferent from common similarity metrics, mainly cosine similarityand Euclidean distance. We show that our bio-inspired model canbe used to investigate how different word embedding techniquesresult in different semantic outputs, which can emphasize or obscureparticular interpretations in textual data.

| Comments: | 7 pages, 8 figures, 2 tables, Visualisation for the Digital Humanities 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2009.02459](https://arxiv.org/abs/2009.02459) [cs.CL]** |
|           | (or **[arXiv:2009.02459v1](https://arxiv.org/abs/2009.02459v1) [cs.CL]** for this version) |





<h2 id="2020-09-08-4">4. Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity</h2>

Title: [Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity](https://arxiv.org/abs/2009.03116)

Authors: [Tim Isbister](https://arxiv.org/search/cs?searchtype=author&query=Isbister%2C+T), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> This paper presents the first Swedish evaluation benchmark for textual semantic similarity. The benchmark is compiled by simply running the English STS-B dataset through the Google machine translation API. This paper discusses potential problems with using such a simple approach to compile a Swedish evaluation benchmark, including translation errors, vocabulary variation, and productive compounding. Despite some obvious problems with the resulting dataset, we use the benchmark to compare the majority of the currently existing Swedish text representations, demonstrating that native models outperform multilingual ones, and that simple bag of words performs remarkably well.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.03116](https://arxiv.org/abs/2009.03116) [cs.CL]** |
|           | (or **[arXiv:2009.03116v1](https://arxiv.org/abs/2009.03116v1) [cs.CL]** for this version) |





# 2020-09-07

[Return to Index](#Index)



<h2 id="2020-09-07-1">1. Data Readiness for Natural Language Processing</h2>

Title: [Data Readiness for Natural Language Processing](https://arxiv.org/abs/2009.02043)

Authors: [Fredrik Olsson](https://arxiv.org/search/cs?searchtype=author&query=Olsson%2C+F), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> This document concerns data readiness in the context of machine learning and Natural Language Processing. It describes how an organization may proceed to identify, make available, validate, and prepare data to facilitate automated analysis methods. The contents of the document is based on the practical challenges and frequently asked questions we have encountered in our work as an applied research institute with helping organizations and companies, both in the public and private sectors, to use data in their business processes.

| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02043](https://arxiv.org/abs/2009.02043) [cs.CY]** |
|           | (or **[arXiv:2009.02043v1](https://arxiv.org/abs/2009.02043v1) [cs.CY]** for this version) |





<h2 id="2020-09-07-2">2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation</h2>

Title: [Dynamic Context-guided Capsule Network for Multimodal Machine Translation](https://arxiv.org/abs/2009.02016)

Authors: [Huan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Zhengyuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yubin Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on [this https URL](https://github.com/DeepLearnXMU/MM-DCCN).

| Subjects: | **Computation and Language (cs.CL)**; Multimedia (cs.MM)     |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3394171.3413715](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3394171.3413715&v=711e661a) |
| Cite as:  | **[arXiv:2009.02016](https://arxiv.org/abs/2009.02016) [cs.CL]** |
|           | (or **[arXiv:2009.02016v1](https://arxiv.org/abs/2009.02016v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-3">3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search</h2>

Title: [AutoTrans: Automating Transformer Design via Reinforced Architecture Search](https://arxiv.org/abs/2009.02070)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xiaoling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Yuan Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+Y), [Guotong Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+G)

> Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc, so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02070](https://arxiv.org/abs/2009.02070) [cs.CL]** |
|           | (or **[arXiv:2009.02070v1](https://arxiv.org/abs/2009.02070v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-4">4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings</h2>

Title: [Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](https://arxiv.org/abs/2009.02113)

Authors: [Vincent D. Warmerdam](https://arxiv.org/search/cs?searchtype=author&query=Warmerdam%2C+V+D), [Thomas Kober](https://arxiv.org/search/cs?searchtype=author&query=Kober%2C+T), [Rachael Tatman](https://arxiv.org/search/cs?searchtype=author&query=Tatman%2C+R)

> We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from [this https URL](https://rasahq.github.io/whatlies/).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02113](https://arxiv.org/abs/2009.02113) [cs.CL]** |
|           | (or **[arXiv:2009.02113v1](https://arxiv.org/abs/2009.02113v1) [cs.CL]** for this version) |



# 2020-09-01

[Return to Index](#Index)



<h2 id="2020-09-01-1">1. Knowledge Efficient Deep Learning for Natural Language Processing</h2>

Title: [Knowledge Efficient Deep Learning for Natural Language Processing](https://arxiv.org/abs/2008.12878)

Authors: [Hai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Deep learning has become the workhorse for a wide range of natural language processing applications. But much of the success of deep learning relies on annotated examples. Annotation is time-consuming and expensive to produce at scale. Here we are interested in methods for reducing the required quantity of annotated data -- by making the learning methods more knowledge efficient so as to make them more applicable in low annotation (low resource) settings. There are various classical approaches to making the models more knowledge efficient such as multi-task learning, transfer learning, weakly supervised and unsupervised learning etc. This thesis focuses on adapting such classical methods to modern deep learning models and algorithms.
> This thesis describes four works aimed at making machine learning models more knowledge efficient. First, we propose a knowledge rich deep learning model (KRDL) as a unifying learning framework for incorporating prior knowledge into deep models. In particular, we apply KRDL built on Markov logic networks to denoise weak supervision. Second, we apply a KRDL model to assist the machine reading models to find the correct evidence sentences that can support their decision. Third, we investigate the knowledge transfer techniques in multilingual setting, where we proposed a method that can improve pre-trained multilingual BERT based on the bilingual dictionary. Fourth, we present an episodic memory network for language modelling, in which we encode the large external knowledge for the pre-trained GPT.

| Comments: | Ph.D thesis                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.12878](https://arxiv.org/abs/2008.12878) [cs.CL]** |
|           | (or **[arXiv:2008.12878v1](https://arxiv.org/abs/2008.12878v1) [cs.CL]** for this version) |

