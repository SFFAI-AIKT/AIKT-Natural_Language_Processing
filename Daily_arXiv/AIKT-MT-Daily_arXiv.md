# Daily arXiv: Machine Translation - Jan., 2020

# Index

- [2020-01-15](#2020-01-15)
  - [1. Faster Transformer Decoding: N-gram Masked Self-Attention](#2020-01-15-1)
  - [2. Faster Transformer Decoding: N-gram Masked Self-Attention](#2020-01-15-2)
  - [3. Bi-Decoder Augmented Network for Neural Machine Translation](#2020-01-15-3)

- [2020-01-14](#2020-01-14)
  - [1. Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training](#2020-01-14-1)
  - [2. Reformer: The Efficient Transformer](#2020-01-14-2)
  - [3. PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata](#2020-01-14-3)
- [2020-01-13](#2020-01-13)
  - [1. Learning to Multi-Task Learn for Better Neural Machine Translation](#2020-01-13-1)
  - [2. Towards Minimal Supervision BERT-based Grammar Error Correction](#2020-01-13-2)
- [2020-01-07](#2020-01-07)
  - [1. A Comprehensive Survey of Multilingual Neural Machine Translation](#2020-01-07-1)
  - [2. Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation](#2020-01-07-2)
  - [3. Exploring Benefits of Transfer Learning in Neural Machine Translation](#2020-01-07-3)
- [2020-01-06](#2020-01-06)
  - [1. Learning Accurate Integer Transformer Machine-Translation Models](#2020-01-06-1)
- [2020-01-03](#2020-01-03)
  - [1. A Voice Interactive Multilingual Student Support System using IBM Watson](#2020-01-03-1)
- [2020-01-01](2020-01-01)
  - [1. TextScanner: Reading Characters in Order for Robust Scene Text Recognition](#2020-01-01-1)
  - [2. Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning](#2020-01-01-2)
  - [3. Robust Cross-lingual Embeddings from Parallel Sentences](#2020-01-01-3)
  - [4. "Hinglish" Language -- Modeling a Messy Code-Mixed Language](#2020-01-01-4)
  - [5. Amharic-Arabic Neural Machine Translation](#2020-01-01-5)
  - [6. LayoutLM: Pre-training of Text and Layout for Document Image Understanding](#2020-01-01-6)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-01-15

[Return to Index](#Index)



<h2 id="2020-01-15-1">1. Faster Transformer Decoding: N-gram Masked Self-Attention</h2>

Title: [Faster Transformer Decoding: N-gram Masked Self-Attention](https://arxiv.org/abs/2001.04589)

Authors: [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Mia Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)

*(Submitted on 14 Jan 2020)*

> Motivated by the fact that most of the information relevant to the prediction of target tokens is drawn from the source sentence S=s1,…,sS, we propose truncating the target-side window used for computing self-attention by making an N-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the N-gram masked self-attention model loses very little in BLEU score for N values in the range 4,…,8, depending on the task.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04589](https://arxiv.org/abs/2001.04589) [cs.LG] |
|           | (or [arXiv:2001.04589v1](https://arxiv.org/abs/2001.04589v1) [cs.LG] for this version) |





<h2 id="2020-01-15-2">2. Improved Robust ASR for Social Robots in Public Spaces</h2>

Title: [Improved Robust ASR for Social Robots in Public Spaces](https://arxiv.org/abs/2001.04619)

Authors: [Charles Jankowski](https://arxiv.org/search/eess?searchtype=author&query=Jankowski%2C+C), [Vishwas Mruthyunjaya](https://arxiv.org/search/eess?searchtype=author&query=Mruthyunjaya%2C+V), [Ruixi Lin](https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+R)

*(Submitted on 14 Jan 2020)*

> Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04619](https://arxiv.org/abs/2001.04619) [eess.AS] |
|           | (or [arXiv:2001.04619v1](https://arxiv.org/abs/2001.04619v1) [eess.AS] for this version) |





<h2 id="2020-01-15-3">3. Bi-Decoder Augmented Network for Neural Machine Translation</h2>

Title: [Bi-Decoder Augmented Network for Neural Machine Translation](https://arxiv.org/abs/2001.04586)

Authors: [Boyuan Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+B), [Yazheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Yueting Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D)

*(Submitted on 14 Jan 2020)*

> Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It's obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.04586](https://arxiv.org/abs/2001.04586) [cs.CL] |
|           | (or [arXiv:2001.04586v1](https://arxiv.org/abs/2001.04586v1) [cs.CL] for this version) |





# 2020-01-14

[Return to Index](#Index)



<h2 id="2020-01-14-1">1. Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training</h2>
Title: [Improving Dysarthric Speech Intelligibility Using Cycle-consistent Adversarial Training](https://arxiv.org/abs/2001.04260)

Authors: [Seung Hee Yang](https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+S+H), [Minhwa Chung](https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+M)

*(Submitted on 10 Jan 2020)*

> Dysarthria is a motor speech impairment affecting millions of people. Dysarthric speech can be far less intelligible than those of non-dysarthric speakers, causing significant communication difficulties. The goal of our work is to develop a model for dysarthric to healthy speech conversion using Cycle-consistent GAN. Using 18,700 dysarthric and 8,610 healthy control Korean utterances that were recorded for the purpose of automatic recognition of voice keyboard in a previous study, the generator is trained to transform dysarthric to healthy speech in the spectral domain, which is then converted back to speech. Objective evaluation using automatic speech recognition of the generated utterance on a held-out test set shows that the recognition performance is improved compared with the original dysarthic speech after performing adversarial training, as the absolute WER has been lowered by 33.4%. It demonstrates that the proposed GAN-based conversion method is useful for improving dysarthric speech intelligibility.

| Comments: | To be Published on the 24th February in BIOSIGNALS 2020. arXiv admin note: text overlap with [arXiv:1904.09407](https://arxiv.org/abs/1904.09407) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | [arXiv:2001.04260](https://arxiv.org/abs/2001.04260) [eess.AS] |
|           | (or [arXiv:2001.04260v1](https://arxiv.org/abs/2001.04260v1) [eess.AS] for this version) |





<h2 id="2020-01-14-2">2. Reformer: The Efficient Transformer</h2>
Title: [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

Authors: [Nikita Kitaev](https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N), [Łukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+Ł), [Anselm Levskaya](https://arxiv.org/search/cs?searchtype=author&query=Levskaya%2C+A)

*(Submitted on 13 Jan 2020)*

> Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.

| Comments: | ICLR 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2001.04451](https://arxiv.org/abs/2001.04451) [cs.LG] |
|           | (or [arXiv:2001.04451v1](https://arxiv.org/abs/2001.04451v1) [cs.LG] for this version) |





<h2 id="2020-01-14-3">3. PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata</h2>
Title: [PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata](https://arxiv.org/abs/2001.03708)

Authors: [Jieh-Sheng Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Jieh Hsiang](https://arxiv.org/search/cs?searchtype=author&query=Hsiang%2C+J)

*(Submitted on 11 Jan 2020)*

> PatentTransformer is our codename for patent text generation based on Transformer-based models. Our goal is "Augmented Inventing." In this second version, we leverage more of the structural metadata in patents. The structural metadata includes patent title, abstract, and dependent claim, in addition to independent claim previously. Metadata controls what kind of patent text for the model to generate. Also, we leverage the relation between metadata to build a text-to-text generation flow, for example, from a few words to a title, the title to an abstract, the abstract to an independent claim, and the independent claim to multiple dependent claims. The text flow can go backward because the relation is trained bidirectionally. We release our GPT-2 models trained from scratch and our code for inference so that readers can verify and generate patent text on their own. As for generation quality, we measure it by both ROUGE and Google Universal Sentence Encoder.

| Comments: | demo paper                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.03708](https://arxiv.org/abs/2001.03708) [cs.CL] |
|           | (or [arXiv:2001.03708v1](https://arxiv.org/abs/2001.03708v1) [cs.CL] for this version) |





# 2020-01-13

[Return to Index](#Index)



<h2 id="2020-01-13-1">1. Learning to Multi-Task Learn for Better Neural Machine Translation</h2>
Title: [Learning to Multi-Task Learn for Better Neural Machine Translation](https://arxiv.org/abs/2001.03294)

Authors: [Poorya Zaremoodi](https://arxiv.org/search/cs?searchtype=author&query=Zaremoodi%2C+P), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 10 Jan 2020)*

> Scarcity of parallel sentence pairs is a major challenge for training high quality neural machine translation (NMT) models in bilingually low-resource scenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to inject linguistic-related inductive biases into NMT, using auxiliary syntactic and semantic tasks, to improve generalisation. The challenge, however, is to devise effective training schedules, prescribing when to make use of the auxiliary tasks during the training process to fill the knowledge gaps of the main translation task, a setting referred to as biased-MTL. Current approaches for the training schedule are based on hand-engineering heuristics, whose effectiveness vary in different MTL settings. We propose a novel framework for learning the training schedule, ie learning to multi-task learn, for the MTL setting of interest. We formulate the training schedule as a Markov decision process which paves the way to employ policy learning methods to learn the scheduling policy. We effectively and efficiently learn the training schedule policy within the imitation learning framework using an oracle policy algorithm that dynamically sets the importance weights of auxiliary tasks based on their contributions to the generalisability of the main NMT task. Experiments on low-resource NMT settings show the resulting automatically learned training schedulers are competitive with the best heuristics, and lead to up to +1.1 BLEU score improvements.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.03294](https://arxiv.org/abs/2001.03294) [cs.CL] |
|           | (or [arXiv:2001.03294v1](https://arxiv.org/abs/2001.03294v1) [cs.CL] for this version) |





<h2 id="2020-01-13-2">2. Towards Minimal Supervision BERT-based Grammar Error Correction</h2>
Title: [Towards Minimal Supervision BERT-based Grammar Error Correction](https://arxiv.org/abs/2001.03521)

Authors: [Yiyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

*(Submitted on 10 Jan 2020)*

> Current grammatical error correction (GEC) models typically consider the task as sequence generation, which requires large amounts of annotated data and limit the applications in data-limited settings. We try to incorporate contextual information from pre-trained language model to leverage annotation and benefit multilingual scenarios. Results show strong potential of Bidirectional Encoder Representations from Transformers (BERT) in grammatical error correction task.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.03521](https://arxiv.org/abs/2001.03521) [cs.CL] |
|           | (or [arXiv:2001.03521v1](https://arxiv.org/abs/2001.03521v1) [cs.CL] for this version) |









# 2020-01-07

[Return to Index](#Index)



<h2 id="2020-01-07-1">1. A Comprehensive Survey of Multilingual Neural Machine Translation</h2>
Title: [A Comprehensive Survey of Multilingual Neural Machine Translation](https://arxiv.org/abs/2001.01115)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A)

*(Submitted on 4 Jan 2020)*

> We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues and challenges. Wherever possible we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions that MNMT research might take. This paper is aimed towards both, beginners and experts in NMT. We hope this paper will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.

| Comments: | This is an extended version of our survey paper on multilingual NMT. The previous version [[arXiv:1905.05395](https://arxiv.org/abs/1905.05395)] is rather condensed and is useful for speed-reading whereas this version is more beginner friendly. Under review at the computing surveys journal. We have intentionally decided to maintain both short and long versions of our survey paper for different reader groups |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.01115](https://arxiv.org/abs/2001.01115) [cs.CL] |
|           | (or [arXiv:2001.01115v1](https://arxiv.org/abs/2001.01115v1) [cs.CL] for this version) |





<h2 id="2020-01-07-2">2. Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation</h2>
Title: [Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation](https://arxiv.org/abs/2001.01589)

Authors: [Yirong Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Xiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yating Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Rui Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+R)

*(Submitted on 2 Jan 2020)*

> Neural machine translation (NMT) has achieved impressive performance on machine translation task in recent years. However, in consideration of efficiency, a limited-size vocabulary that only contains the top-N highest frequency words are employed for model training, which leads to many rare and unknown words. It is rather difficult when translating from the low-resource and morphologically-rich agglutinative languages, which have complex morphology and large vocabulary. In this paper, we propose a morphological word segmentation method on the source-side for NMT that incorporates morphology knowledge to preserve the linguistic and semantic information in the word structure while reducing the vocabulary size at training time. It can be utilized as a preprocessing tool to segment the words in agglutinative languages for other natural language processing (NLP) tasks. Experimental results show that our morphologically motivated word segmentation method is better suitable for the NMT model, which achieves significant improvements on Turkish-English and Uyghur-Chinese machine translation tasks on account of reducing data sparseness and language complexity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.01589](https://arxiv.org/abs/2001.01589) [cs.CL] |
|           | (or [arXiv:2001.01589v1](https://arxiv.org/abs/2001.01589v1) [cs.CL] for this version) |





<h2 id="2020-01-07-3">3. Exploring Benefits of Transfer Learning in Neural Machine Translation</h2>
Title: [Exploring Benefits of Transfer Learning in Neural Machine Translation](https://arxiv.org/abs/2001.01622)

Authors: [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T)

*(Submitted on 6 Jan 2020)*

> Neural machine translation is known to require large numbers of parallel training sentences, which generally prevent it from excelling on low-resource language pairs. This thesis explores the use of cross-lingual transfer learning on neural networks as a way of solving the problem with the lack of resources. We propose several transfer learning approaches to reuse a model pretrained on a high-resource language pair. We pay particular attention to the simplicity of the techniques. We study two scenarios: (a) when we reuse the high-resource model without any prior modifications to its training process and (b) when we can prepare the first-stage high-resource model for transfer learning in advance. For the former scenario, we present a proof-of-concept method by reusing a model trained by other researchers. In the latter scenario, we present a method which reaches even larger improvements in translation performance. Apart from proposed techniques, we focus on an in-depth analysis of transfer learning techniques and try to shed some light on transfer learning improvements. We show how our techniques address specific problems of low-resource languages and are suitable even in high-resource transfer learning. We evaluate the potential drawbacks and behavior by studying transfer learning in various situations, for example, under artificially damaged training corpora, or with fixed various model parts.

| Comments: | Defended PhD thesis                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | [arXiv:2001.01622](https://arxiv.org/abs/2001.01622) [cs.CL] |
|           | (or [arXiv:2001.01622v1](https://arxiv.org/abs/2001.01622v1) [cs.CL] for this version) |





# 2020-01-06

[Return to Index](#Index)



<h2 id="2020-01-06-1">1. Learning Accurate Integer Transformer Machine-Translation Models</h2>
Title: [Learning Accurate Integer Transformer Machine-Translation Models](https://arxiv.org/abs/2001.00926)

Authors: [Ephrem Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+E)

*(Submitted on 3 Jan 2020)*

> We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3% to 100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2001.00926](https://arxiv.org/abs/2001.00926) [cs.LG] |
|           | (or [arXiv:2001.00926v1](https://arxiv.org/abs/2001.00926v1) [cs.LG] for this version) |





# 2020-01-03

[Return to Index](#Index)




<h2 id="2020-01-03-1">1. A Voice Interactive Multilingual Student Support System using IBM Watson</h2>
Title: [A Voice Interactive Multilingual Student Support System using IBM Watson](https://arxiv.org/abs/2001.00471)

Authors: [Kennedy Ralston](https://arxiv.org/search/cs?searchtype=author&query=Ralston%2C+K), [Yuhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Haruna Isah](https://arxiv.org/search/cs?searchtype=author&query=Isah%2C+H), [Farhana Zulkernine](https://arxiv.org/search/cs?searchtype=author&query=Zulkernine%2C+F)

(Submitted on 20 Dec 2019)

> Systems powered by artificial intelligence are being developed to be more user-friendly by communicating with users in a progressively human-like conversational way. Chatbots, also known as dialogue systems, interactive conversational agents, or virtual agents are an example of such systems used in a wide variety of applications ranging from customer support in the business domain to companionship in the healthcare sector. It is becoming increasingly important to develop chatbots that can best respond to the personalized needs of their users so that they can be as helpful to the user as possible in a real human way. This paper investigates and compares three popular existing chatbots API offerings and then propose and develop a voice interactive and multilingual chatbot that can effectively respond to users mood, tone, and language using IBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was evaluated using a use case that was targeted at responding to users needs regarding exam stress based on university students survey data generated using Google Forms. The results of measuring the chatbot effectiveness at analyzing responses regarding exam stress indicate that the chatbot responding appropriately to the user queries regarding how they are feeling about exams 76.5%. The chatbot could also be adapted for use in other application areas such as student info-centers, government kiosks, and mental health support systems.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| Cite as:  | [arXiv:2001.00471](https://arxiv.org/abs/2001.00471) [cs.HC] |
|           | (or [arXiv:2001.00471v1](https://arxiv.org/abs/2001.00471v1) [cs.HC] for this version) |



# 2020-01-01

[Return to Index](#Index)



<h2 id="2020-01-01-1">1. TextScanner: Reading Characters in Order for Robust Scene Text Recognition</h2>
Title: [TextScanner: Reading Characters in Order for Robust Scene Text Recognition](https://arxiv.org/abs/1912.12422)

Authors: [Zhaoyi Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Z), [Mingling He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+M), [Haoran Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Xiang Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X), [Cong Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+C)

*(Submitted on 28 Dec 2019)*

> Driven by deep learning and the large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention based methods have dominated this field, but suffer from the problem of \textit{attention drift} in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such Chinese transcripts and aligning with target characters.

| Comments: | Accepted by AAAI-2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.12422](https://arxiv.org/abs/1912.12422) [cs.CV] |
|           | (or [arXiv:1912.12422v1](https://arxiv.org/abs/1912.12422v1) [cs.CV] for this version) |





<h2 id="2020-01-01-2">2. Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning</h2>
Title: [Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning](https://arxiv.org/abs/1912.13080)

Authors: [Sean MacAvaney](https://arxiv.org/search/cs?searchtype=author&query=MacAvaney%2C+S), [Luca Soldaini](https://arxiv.org/search/cs?searchtype=author&query=Soldaini%2C+L), [Nazli Goharian](https://arxiv.org/search/cs?searchtype=author&query=Goharian%2C+N)

*(Submitted on 30 Dec 2019)*

> While billions of non-English speaking users rely on search engines every day, the problem of ad-hoc information retrieval is rarely studied for non-English languages. This is primarily due to a lack of data set that are suitable to train ranking algorithms. In this paper, we tackle the lack of data by leveraging pre-trained multilingual language models to transfer a retrieval system trained on English collections to non-English queries and documents. Our model is evaluated in a zero-shot setting, meaning that we use them to predict relevance scores for query-document pairs in languages never seen during training. Our results show that the proposed approach can significantly outperform unsupervised retrieval techniques for Arabic, Chinese Mandarin, and Spanish. We also show that augmenting the English training collection with some examples from the target language can sometimes improve performance.

| Comments: | ECIR 2020 (short)                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.13080](https://arxiv.org/abs/1912.13080) [cs.IR] |
|           | (or [arXiv:1912.13080v1](https://arxiv.org/abs/1912.13080v1) [cs.IR] for this version) |





<h2 id="2020-01-01-3">3. Robust Cross-lingual Embeddings from Parallel Sentences</h2>
Title: [Robust Cross-lingual Embeddings from Parallel Sentences](https://arxiv.org/abs/1912.12481)

Authors: [Ali Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+A), [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Jean-Baptiste Cordonnier](https://arxiv.org/search/cs?searchtype=author&query=Cordonnier%2C+J), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R), [Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&query=Jaggi%2C+M)

*(Submitted on 28 Dec 2019)*

> Recent advances in cross-lingual word embeddings have primarily relied on mapping-based methods, which project pretrained word embeddings from different languages into a shared space through a linear transformation. However, these approaches assume word embedding spaces are isomorphic between different languages, which has been shown not to hold in practice (Søgaard et al., 2018), and fundamentally limits their performance. This motivates investigating joint learning methods which can overcome this impediment, by simultaneously learning embeddings across languages via a cross-lingual term in the training objective. Given the abundance of parallel data available (Tiedemann, 2012), we propose a bilingual extension of the CBOW method which leverages sentence-aligned corpora to obtain robust cross-lingual word and sentence representations. Our approach significantly improves cross-lingual sentence retrieval performance over all other approaches, as well as convincingly outscores mapping methods while maintaining parity with jointly trained methods on word-translation. It also achieves parity with a deep RNN method on a zero-shot cross-lingual document classification task, requiring far fewer computational resources for training and inference. As an additional advantage, our bilingual method also improves the quality of monolingual word vectors despite training on much smaller datasets. We make our code and models publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.12481](https://arxiv.org/abs/1912.12481) [cs.CL] |
|           | (or [arXiv:1912.12481v1](https://arxiv.org/abs/1912.12481v1) [cs.CL] for this version) |





<h2 id="2020-01-01-4">4. "Hinglish" Language -- Modeling a Messy Code-Mixed Language</h2>
Title: ["Hinglish" Language -- Modeling a Messy Code-Mixed Language](https://arxiv.org/abs/1912.13109)

Authors: [Vivek Kumar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+V+K)

*(Submitted on 30 Dec 2019)*

> With a sharp rise in fluency and users of "Hinglish" in linguistically diverse country, India, it has increasingly become important to analyze social content written in this language in platforms such as Twitter, Reddit, Facebook. This project focuses on using deep learning techniques to tackle a classification problem in categorizing social content written in Hindi-English into Abusive, Hate-Inducing and Not offensive categories. We utilize bi-directional sequence models with easy text augmentation techniques such as synonym replacement, random insertion, random swap, and random deletion to produce a state of the art classifier that outperforms the previous work done on analyzing this dataset.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1912.13109](https://arxiv.org/abs/1912.13109) [cs.CL] |
|           | (or [arXiv:1912.13109v1](https://arxiv.org/abs/1912.13109v1) [cs.CL] for this version) |





<h2 id="2020-01-01-5">5. Amharic-Arabic Neural Machine Translation</h2>
Title: [Amharic-Arabic Neural Machine Translation](https://arxiv.org/abs/1912.13161)

Authors: [Ibrahim Gashaw](https://arxiv.org/search/cs?searchtype=author&query=Gashaw%2C+I), [H L Shashirekha](https://arxiv.org/search/cs?searchtype=author&query=Shashirekha%2C+H+L)

*(Submitted on 26 Dec 2019)*

> Many automatic translation works have been addressed between major European language pairs, by taking advantage of large scale parallel corpora, but very few research works are conducted on the Amharic-Arabic language pair due to its parallel data scarcity. Two Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) based Neural Machine Translation (NMT) models are developed using Attention-based Encoder-Decoder architecture which is adapted from the open-source OpenNMT system. In order to perform the experiment, a small parallel Quranic text corpus is constructed by modifying the existing monolingual Arabic text and its equivalent translation of Amharic language text corpora available on Tanzile. LSTM and GRU based NMT models and Google Translation system are compared and found that LSTM based OpenNMT outperforms GRU based OpenNMT and Google Translation system, with a BLEU score of 12%, 11%, and 6% respectively.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1912.13161](https://arxiv.org/abs/1912.13161) [cs.CL] |
|           | (or [arXiv:1912.13161v1](https://arxiv.org/abs/1912.13161v1) [cs.CL] for this version) |





<h2 id="2020-01-01-6">6. LayoutLM: Pre-training of Text and Layout for Document Image Understanding</h2>
Title: [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)

Authors: [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Minghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 31 Dec 2019)*

> Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the wide spread of pre-training models for NLP applications, they almost focused on text-level manipulation, while neglecting the layout and style information that is vital for document image understanding. In this paper, we propose \textbf{LayoutLM} to jointly model the interaction between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. We also leverage the image features to incorporate the style information of words in LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training, leading to significant performance improvement in downstream tasks for document image understanding.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1912.13318](https://arxiv.org/abs/1912.13318) [cs.CL] |
|           | (or [arXiv:1912.13318v1](https://arxiv.org/abs/1912.13318v1) [cs.CL] for this version) |



