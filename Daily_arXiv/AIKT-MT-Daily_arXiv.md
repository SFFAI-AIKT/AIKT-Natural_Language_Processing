# Daily arXiv: Machine Translation - December, 2020

# Index


- [2020-12-29](#2020-12-29)

  - [1. Spatial Reasoning from Natural Language Instructions for Robot Manipulation](#2020-12-29-1)
  - [2. Translating Natural Language Instructions to Computer Programs for Robot Manipulation](#2020-12-29-2)
  - [3. Why Neural Machine Translation Prefers Empty Outputs](#2020-12-29-3)
  - [4. Contextual Temperature for Language Modeling](#2020-12-29-4)
  - [5. Learning Light-Weight Translation Models from Deep Transformer](#2020-12-29-5)
  - [6. Syntax-Enhanced Pre-trained Model](#2020-12-29-6)
  - [7. Towards Fully Automated Manga Translation](#2020-12-29-7)
  - [8. BURT: BERT-inspired Universal Representation from Learning Meaningful Segment](#2020-12-29-8)
  - [9. Universal Sentence Representation Learning with Conditional Masked Language Model](#2020-12-29-9)
- [2020-12-25](#2020-12-25)
- [1. Disentangling semantics in language throughs VAEs and a certain architectural choice](#2020-12-25-1)
  - [2. SubICap: Towards Subword-informed Image Captioning](#2020-12-25-2)
  - [3. Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters](#2020-12-25-3)
  - [4. Sentence-Based Model Agnostic NLP Interpretability](#2020-12-25-4)
  - [5. A Context Aware Approach for Generating Natural Language Attacks](#2020-12-25-5)
  - [6. To what extent do human explanations of model behavior align with actual model behavior?](#2020-12-25-6)
- [2020-12-24](#2020-12-24)

  - [1. Seeing past words: Testing the cross-modal capabilities of pretrained V&L models](#2020-12-24-1)
  - [2. Multi-Head Self-Attention with Role-Guided Masks](#2020-12-24-2)
  - [3. Future-Guided Incremental Transformer for Simultaneous Translation](#2020-12-24-3)
  - [4. Code Switching Language Model Using Monolingual Training Data](#2020-12-24-4)
  - [5. Learning Dense Representations of Phrases at Scale](#2020-12-24-5)
- [2020-12-23](#2020-12-23)

  - [1. A Distributional Approach to Controlled Text Generation](#2020-12-23-1)
  - [2. Subword Sampling for Low Resource Word Alignment](#2020-12-23-2)
  - [3. Undivided Attention: Are Intermediate Layers Necessary for BERT?](#2020-12-23-3)
  - [4. Pre-Training a Language Model Without Human Language](#2020-12-23-4)
  - [5. Domain Adaptation of NMT models for English-Hindi Machine Translation Task at AdapMT ICON 2020](#2020-12-23-5)
- [2020-12-22](#2020-12-22)

  - [1. Transductive Visual Verb Sense Disambiguation](#2020-12-22-1)
  - [2. Self-Supervised Learning for Visual Summary Identification in Scientific Publications](#2020-12-22-2)
  - [3. Finding Sparse Structure for Domain Specific Neural Machine Translation](#2020-12-22-3)
  - [4. Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection](#2020-12-22-4)
  - [5. Narrative Incoherence Detection](#2020-12-22-5)
- [2020-12-21](#2020-12-21)
- [1. On Modality Bias in the TVQA Dataset](#2020-12-21-1)
  - [2. NeurST: Neural Speech Translation Toolkit](#2020-12-21-2)
  - [3. Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning](#2020-12-21-3)
  - [4. AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text](#2020-12-21-4)
  - [5. Understood in Translation, Transformers for Domain Understanding](#2020-12-21-5)
- [2020-12-18](#2020-12-18)

  - [1. The effectiveness of unsupervised subword modeling with autoregressive and cross-lingual phone-aware networks](#2020-12-18-1)
  - [2. MIX : a Multi-task Learning Approach to Solve Open-Domain Question Answering](#2020-12-18-2)
  - [3. Continual Lifelong Learning in Natural Language Processing: A Survey](#2020-12-18-3)
- [2020-12-17](#2020-12-17)

  - [1. A Closer Look at the Robustness of Vision-and-Language Pre-trained Models](#2020-12-17-1)
  - [2. Improving Multilingual Neural Machine Translation For Low-Resource Languages: French-, English- Vietnamese](#2020-12-17-2)
- [2020-12-16](#2020-12-16)

  - [1. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks](#2020-12-16-1)
  - [2. Model Choices Influence Attributive Word Associations: A Semi-supervised Analysis of Static Word Embeddings](#2020-12-16-2)
  - [3. Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution](#2020-12-16-3)
  - [4. Modeling Homophone Noise for Robust Neural Machine Translation](#2020-12-16-4)
- [2020-12-15](#2020-12-15)

  - [1. Learning Contextual Causality from Time-consecutive Images](#2020-12-15-1)
  - [2. LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding](#2020-12-15-2)
  - [3. A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings](#2020-12-15-3)
  - [4. Ensemble Distillation Approaches for Grammatical Error Correction](#2020-12-15-4)
  - [5. Sentiment analysis in Bengali via transfer learning using multi-lingual BERT](#2020-12-15-5)
  - [6. Vartani Spellcheck -- Automatic Context-Sensitive Spelling Correction of OCR-generated Hindi Text Using BERT and Levenshtein Distance](#2020-12-15-6)
- [2020-12-14](#2020-12-14)

  - [1. Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer](#2020-12-14-1)
  - [2. Comprehension and Knowledge](#2020-12-14-2)
  - [3. Reinforced Multi-Teacher Selection for Knowledge Distillation](#2020-12-14-3)
  - [4. Improving Task-Agnostic BERT Distillation with Layer Mapping Search](#2020-12-14-4)
- [2020-12-11](#2020-12-11)

  - [1. Rewriter-Evaluator Framework for Neural Machine Translation](#2020-12-11-1)
  - [2. As good as new. How to successfully recycle English GPT-2 to make models for other languages](#2020-12-11-2)
  - [3. Direct multimodal few-shot learning of speech and images](#2020-12-11-3)
  - [4. Exploring Pair-Wise NMT for Indian Languages](#2020-12-11-4)
- [2020-12-10](#2020-12-10)

  - [1. SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint](#2020-12-10-1)
  - [2. Breeding Gender-aware Direct Speech Translation Systems](#2020-12-10-2)
  - [3. On Knowledge Distillation for Direct Speech Translation](#2020-12-10-3)
  - [4. Towards Zero-shot Cross-lingual Image Retrieval](#2020-12-10-4)
- [2020-12-09](#2020-12-09)

  - [1. Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization](#2020-12-09-1)
  - [2. Globetrotter: Unsupervised Multilingual Translation from Visual Alignment](#2020-12-09-2)
- [2020-12-08](#2020-12-08)

  - [1. Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment](#2020-12-08-1)
  - [2. MLS: A Large-Scale Multilingual Dataset for Speech Research](#2020-12-08-2)
  - [3. Reciprocal Supervised Learning Improves Neural Machine Translation](#2020-12-08-3)
  - [4. Document Graph for Neural Machine Translation](#2020-12-08-4)
  - [5. KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning](#2020-12-08-5)
  - [6. PPKE: Knowledge Representation Learning by Path-based Pre-training](#2020-12-08-6)
- [2020-12-07](#2020-12-07)

  - [1. A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings](#2020-12-07-1)
  - [2. Self-Supervised VQA: Answering Visual Questions using Images and Captions](#2020-12-07-2)
  - [3. Accurate and Scalable Matching of Translators to Displaced Persons for Overcoming Language Barriers](#2020-12-07-3)
  - [4. A Benchmark Dataset for Understandable Medical Language Translation](#2020-12-07-4)
  - [5. Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning](#2020-12-07-5)
- [2020-12-04](#2020-12-04)

  - [1. SemMT: A Semantic-based Testing Approach for Machine Translation Systems](#2020-12-04-1)
  - [2. Self-Explaining Structures Improve NLP Models](#2020-12-04-2)
  - [3. On Extending NLP Techniques from the Categorical to the Latent Space: KL Divergence, Zipf's Law, and Similarity Search](#2020-12-04-3)
- [2020-12-03](#2020-12-03)

  - [1. Evaluating Explanations: How much do explanations from the teacher aid students?](#2020-12-03-1)
  - [2. How Can We Know When Language Models Know?](#2020-12-03-2)
  - [3. Interactive Teaching for Conversational AI](#2020-12-03-3)
- [2020-12-02](#2020-12-02)

  - [1. Modifying Memories in Transformer Models](#2020-12-02-1)
  - [2. An Enhanced Knowledge Injection Model for Commonsense Generation](#2020-12-02-2)
  - [3. CPM: A Large-scale Generative Chinese Pre-trained Language Model](#2020-12-02-3)
  - [4. Extracting Synonyms from Bilingual Dictionaries](#2020-12-02-4)
  - [5. Intrinsic analysis for dual word embedding space models](#2020-12-02-5)
- [2020-12-01](#2020-12-01)
  - [1. EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP](#2020-12-01-1)
  - [2. Understanding How BERT Learns to Identify Edits](#2020-12-01-2)
  - [3. Using Multiple Subwords to Improve English-Esperanto Automated Literary Translation Quality](#2020-12-01-3)
  - [4. Intrinsic Knowledge Evaluation on Chinese Language Models](#2020-12-01-4)
  - [5. Dynamic Curriculum Learning for Low-Resource Neural Machine Translation](#2020-12-01-5)
  - [6. A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction](#2020-12-01-6)
  - [7. Machine Translation of Novels in the Age of Transformer](#2020-12-01-7)
  - [8. Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs](#2020-12-01-8)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2020-12-29

[Return to Index](#Index)



<h2 id="2020-12-29-1">1. Spatial Reasoning from Natural Language Instructions for Robot Manipulation</h2>

Title: [Spatial Reasoning from Natural Language Instructions for Robot Manipulation](https://arxiv.org/abs/2012.13693)

Authors: [Sagar Gubbi Venkatesh](https://arxiv.org/search/cs?searchtype=author&query=Venkatesh%2C+S+G), [Anirban Biswas](https://arxiv.org/search/cs?searchtype=author&query=Biswas%2C+A), [Raviteja Upadrashta](https://arxiv.org/search/cs?searchtype=author&query=Upadrashta%2C+R), [Vikram Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+V), [Partha Talukdar](https://arxiv.org/search/cs?searchtype=author&query=Talukdar%2C+P), [Bharadwaj Amrutur](https://arxiv.org/search/cs?searchtype=author&query=Amrutur%2C+B)

> Robots that can manipulate objects in unstructured environments and collaborate with humans can benefit immensely by understanding natural language. We propose a pipelined architecture of two stages to perform spatial reasoning on the text input. All the objects in the scene are first localized, and then the instruction for the robot in natural language and the localized co-ordinates are mapped to the start and end co-ordinates corresponding to the locations where the robot must pick up and place the object respectively. We show that representing the localized objects by quantizing their positions to a binary grid is preferable to representing them as a list of 2D co-ordinates. We also show that attention improves generalization and can overcome biases in the dataset. The proposed method is used to pick-and-place playing cards using a robot arm.

| Comments: | Under review for ICRA 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Robotics (cs.RO)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.13693](https://arxiv.org/abs/2012.13693) [cs.RO]** |
|           | (or **[arXiv:2012.13693v1](https://arxiv.org/abs/2012.13693v1) [cs.RO]** for this version) |





<h2 id="2020-12-29-2">2. Translating Natural Language Instructions to Computer Programs for Robot Manipulation</h2>

Title: [Translating Natural Language Instructions to Computer Programs for Robot Manipulation](https://arxiv.org/abs/2012.13695)

Authors: [Sagar Gubbi Venkatesh](https://arxiv.org/search/cs?searchtype=author&query=Venkatesh%2C+S+G), [Raviteja Upadrashta](https://arxiv.org/search/cs?searchtype=author&query=Upadrashta%2C+R), [Bharadwaj Amrutur](https://arxiv.org/search/cs?searchtype=author&query=Amrutur%2C+B)

> It is highly desirable for robots that work alongside humans to be able to understand instructions in natural language. Existing language conditioned imitation learning methods predict the actuator commands from the image observation and the instruction text. Rather than directly predicting actuator commands, we propose translating the natural language instruction to a Python function which when executed queries the scene by accessing the output of the object detector and controls the robot to perform the specified task. This enables the use of non-differentiable modules such as a constraint solver when computing commands to the robot. Moreover, the labels in this setup are significantly more descriptive computer programs rather than teleoperated demonstrations. We show that the proposed method performs better than training a neural network to directly predict the robot actions.

| Comments: | Under review for ICRA 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Robotics (cs.RO)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.13695](https://arxiv.org/abs/2012.13695) [cs.RO]** |
|           | (or **[arXiv:2012.13695v1](https://arxiv.org/abs/2012.13695v1) [cs.RO]** for this version) |





<h2 id="2020-12-29-3">3. Why Neural Machine Translation Prefers Empty Outputs</h2>

Title: [Why Neural Machine Translation Prefers Empty Outputs](https://arxiv.org/abs/2012.13454)

Authors: [Xing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X), [Yijun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K)

> We investigate why neural machine translation (NMT) systems assign high probability to empty translations. We find two explanations. First, label smoothing makes correct-length translations less confident, making it easier for the empty translation to finally outscore them. Second, NMT systems use the same, high-frequency EoS word to end all target sentences, regardless of length. This creates an implicit smoothing that increases zero-length translations. Using different EoS types in target sentences of different lengths exposes and eliminates this implicit smoothing.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.13454](https://arxiv.org/abs/2012.13454) [cs.CL]** |
|           | (or **[arXiv:2012.13454v1](https://arxiv.org/abs/2012.13454v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-4">4. Contextual Temperature for Language Modeling</h2>

Title: [Contextual Temperature for Language Modeling](https://arxiv.org/abs/2012.13575)

Authors: [Pei-Hsin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Sheng-Iou Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+S), [Shih-Chieh Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Yu-Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jia-Yu Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+J), [Wei Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+W), [Da-Chang Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+D)

> Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.13575](https://arxiv.org/abs/2012.13575) [cs.CL]** |
|           | (or **[arXiv:2012.13575v1](https://arxiv.org/abs/2012.13575v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-5">5. Learning Light-Weight Translation Models from Deep Transformer</h2>

Title: [Learning Light-Weight Translation Models from Deep Transformer](https://arxiv.org/abs/2012.13866)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Chunliang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Recently, deep models have shown tremendous improvements in neural machine translation (NMT). However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is 8X shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at [this https URL](https://github.com/libeineu/GPKD).

| Comments: | Accepted by AAAI2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.13866](https://arxiv.org/abs/2012.13866) [cs.CL]** |
|           | (or **[arXiv:2012.13866v1](https://arxiv.org/abs/2012.13866v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-6">6. Syntax-Enhanced Pre-trained Model</h2>

Title: [Syntax-Enhanced Pre-trained Model](https://arxiv.org/abs/2012.14116)

Authors: [Zenan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Z), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Qinliang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Q), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Wanjun Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+W), [Xiaojun Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+X), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D)

> We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14116](https://arxiv.org/abs/2012.14116) [cs.CL]** |
|           | (or **[arXiv:2012.14116v1](https://arxiv.org/abs/2012.14116v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-7">7. Towards Fully Automated Manga Translation</h2>

Title: [Towards Fully Automated Manga Translation](https://arxiv.org/abs/2012.14271)

Authors: [Ryota Hinami](https://arxiv.org/search/cs?searchtype=author&query=Hinami%2C+R), [Shonosuke Ishiwatari](https://arxiv.org/search/cs?searchtype=author&query=Ishiwatari%2C+S), [Kazuhiko Yasuda](https://arxiv.org/search/cs?searchtype=author&query=Yasuda%2C+K), [Yusuke Matsui](https://arxiv.org/search/cs?searchtype=author&query=Matsui%2C+Y)

> We tackle the problem of machine translation of manga, Japanese comics. Manga translation involves two important problems in machine translation: context-aware and multimodal translation. Since text and images are mixed up in an unstructured fashion in Manga, obtaining context from the image is essential for manga translation. However, it is still an open problem how to extract context from image and integrate into MT models. In addition, corpus and benchmarks to train and evaluate such model is currently unavailable. In this paper, we make the following four contributions that establishes the foundation of manga translation research. First, we propose multimodal context-aware translation framework. We are the first to incorporate context information obtained from manga image. It enables us to translate texts in speech bubbles that cannot be translated without using context information (e.g., texts in other speech bubbles, gender of speakers, etc.). Second, for training the model, we propose the approach to automatic corpus construction from pairs of original manga and their translations, by which large parallel corpus can be constructed without any manual labeling. Third, we created a new benchmark to evaluate manga translation. Finally, on top of our proposed methods, we devised a first comprehensive system for fully automated manga translation.

| Comments: | Accepted to AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.14271](https://arxiv.org/abs/2012.14271) [cs.CL]** |
|           | (or **[arXiv:2012.14271v1](https://arxiv.org/abs/2012.14271v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-8">8. BURT: BERT-inspired Universal Representation from Learning Meaningful Segment</h2>

Title: [BURT: BERT-inspired Universal Representation from Learning Meaningful Segment](https://arxiv.org/abs/2012.14320)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Although pre-trained contextualized language models such as BERT achieve significant performance on various downstream tasks, current language representation still only focuses on linguistic objective at a specific granularity, which may not applicable when multiple levels of linguistic units are involved at the same time. Thus this work introduces and explores the universal representation learning, i.e., embeddings of different levels of linguistic unit in a uniform vector space. We present a universal representation model, BURT (BERT-inspired Universal Representation from learning meaningful segmenT), to encode different levels of linguistic unit into the same vector space. Specifically, we extract and mask meaningful segments based on point-wise mutual information (PMI) to incorporate different granular objectives into the pre-training stage. We conduct experiments on datasets for English and Chinese including the GLUE and CLUE benchmarks, where our model surpasses its baselines and alternatives on a wide range of downstream tasks. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space through a task-independent evaluation. Finally, we verify the effectiveness of our unified pre-training strategy in two real-world text matching scenarios. As a result, our model significantly outperforms existing information retrieval (IR) methods and yields universal representations that can be directly applied to retrieval-based question-answering and natural language generation tasks.

| Comments: | arXiv admin note: text overlap with [arXiv:2009.04656](https://arxiv.org/abs/2009.04656) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.14320](https://arxiv.org/abs/2012.14320) [cs.CL]** |
|           | (or **[arXiv:2012.14320v1](https://arxiv.org/abs/2012.14320v1) [cs.CL]** for this version) |





<h2 id="2020-12-29-9">9. Universal Sentence Representation Learning with Conditional Masked Language Model</h2>

Title: [Universal Sentence Representation Learning with Conditional Masked Language Model](https://arxiv.org/abs/2012.14388)

Authors: [Ziyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Daniel Cer](https://arxiv.org/search/cs?searchtype=author&query=Cer%2C+D), [Jax Law](https://arxiv.org/search/cs?searchtype=author&query=Law%2C+J), [Eric Darve](https://arxiv.org/search/cs?searchtype=author&query=Darve%2C+E)

> This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using (semi-)supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval~(BR) and natural language inference~(NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin. We explore the same language bias of the learned representations, and propose a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics.

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.14388](https://arxiv.org/abs/2012.14388) [cs.CL]** |
|           | (or **[arXiv:2012.14388v1](https://arxiv.org/abs/2012.14388v1) [cs.CL]** for this version) |



# 2020-12-25

[Return to Index](#Index)



<h2 id="2020-12-25-1">1. Disentangling semantics in language throughs VAEs and a certain architectural choice</h2>

Title: [Disentangling semantics in language throughs VAEs and a certain architectural choice](https://arxiv.org/abs/2012.13031)

Authors: [Ghazi Felhi](https://arxiv.org/search/cs?searchtype=author&query=Felhi%2C+G), [Joseph Le Roux](https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+J+L), [Djamé Seddah](https://arxiv.org/search/cs?searchtype=author&query=Seddah%2C+D)

> We present an unsupervised method to obtain disentangled representations of sentences that single out semantic content. Using modified Transformers as building blocks, we train a Variational Autoencoder to \emph{translate} the sentence to a fixed number of hierarchically structured latent variables. We study the influence of each latent variable in generation on the dependency structure of sentences, and on the predicate structure it yields when passed through an Open Information Extraction model. Our model could separate verbs, subjects, direct objects, and prepositional objects into latent variables we identified. We show that varying the corresponding latent variables results in varying these elements in sentences, and that swapping them between couples of sentences leads to the expected partial semantic swap.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.13031](https://arxiv.org/abs/2012.13031) [cs.CL]** |
|           | (or **[arXiv:2012.13031v1](https://arxiv.org/abs/2012.13031v1) [cs.CL]** for this version) |





<h2 id="2020-12-25-2">2. SubICap: Towards Subword-informed Image Captioning</h2>

Title: [SubICap: Towards Subword-informed Image Captioning](https://arxiv.org/abs/2012.13122)

Authors: [Naeha Sharif](https://arxiv.org/search/cs?searchtype=author&query=Sharif%2C+N), [Mohammed Bennamoun](https://arxiv.org/search/cs?searchtype=author&query=Bennamoun%2C+M), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Syed Afaq Ali Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+S+A+A)

> Existing Image Captioning (IC) systems model words as atomic units in captions and are unable to exploit the structural information in the words. This makes representation of rare words very difficult and out-of-vocabulary words impossible. Moreover, to avoid computational complexity, existing IC models operate over a modest sized vocabulary of frequent words, such that the identity of rare words is lost. In this work we address this common limitation of IC systems in dealing with rare words in the corpora. We decompose words into smaller constituent units 'subwords' and represent captions as a sequence of subwords instead of words. This helps represent all words in the corpora using a significantly lower subword vocabulary, leading to better parameter learning. Using subword language modeling, our captioning system improves various metric scores, with a training vocabulary size approximately 90% less than the baseline and various state-of-the-art word-level models. Our quantitative and qualitative results and analysis signify the efficacy of our proposed approach.

| Comments:          | 8 pages                                                      |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Journal reference: | Workshop on Applications of Computer Vision (WACV), 2021     |
| Cite as:           | **[arXiv:2012.13122](https://arxiv.org/abs/2012.13122) [cs.CL]** |
|                    | (or **[arXiv:2012.13122v1](https://arxiv.org/abs/2012.13122v1) [cs.CL]** for this version) |





<h2 id="2020-12-25-3">3. Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters</h2>

Title: [Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters](https://arxiv.org/abs/2012.13176)

Authors: [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Christine Basta](https://arxiv.org/search/cs?searchtype=author&query=Basta%2C+C), [Javier Ferrando](https://arxiv.org/search/cs?searchtype=author&query=Ferrando%2C+J), [Roser Batlle](https://arxiv.org/search/cs?searchtype=author&query=Batlle%2C+R), [Ksenia Kharitonova](https://arxiv.org/search/cs?searchtype=author&query=Kharitonova%2C+K)

> Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias.

| Comments:    | 12 pages, 5 figures, 3 tables                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2012.13176](https://arxiv.org/abs/2012.13176) [cs.CL]** |
|              | (or **[arXiv:2012.13176v1](https://arxiv.org/abs/2012.13176v1) [cs.CL]** for this version) |





<h2 id="2020-12-25-4">4. Sentence-Based Model Agnostic NLP Interpretability</h2>

Title: [Sentence-Based Model Agnostic NLP Interpretability](https://arxiv.org/abs/2012.13189)

Authors: [Yves Rychener](https://arxiv.org/search/cs?searchtype=author&query=Rychener%2C+Y), [Xavier Renard](https://arxiv.org/search/cs?searchtype=author&query=Renard%2C+X), [Djamé Seddah](https://arxiv.org/search/cs?searchtype=author&query=Seddah%2C+D), [Pascal Frossard](https://arxiv.org/search/cs?searchtype=author&query=Frossard%2C+P), [Marcin Detyniecki](https://arxiv.org/search/cs?searchtype=author&query=Detyniecki%2C+M)

> Today, interpretability of Black-Box Natural Language Processing (NLP) models based on surrogates, like LIME or SHAP, uses word-based sampling to build the explanations. In this paper we explore the use of sentences to tackle NLP interpretability. While this choice may seem straight forward, we show that, when using complex classifiers like BERT, the word-based approach raises issues not only of computational complexity, but also of an out of distribution sampling, eventually leading to non founded explanations. By using sentences, the altered text remains in-distribution and the dimensionality of the problem is reduced for better fidelity to the black-box at comparable computational complexity.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.13189](https://arxiv.org/abs/2012.13189) [cs.CL]** |
|           | (or **[arXiv:2012.13189v1](https://arxiv.org/abs/2012.13189v1) [cs.CL]** for this version) |





<h2 id="2020-12-25-5">5. A Context Aware Approach for Generating Natural Language Attacks</h2>

Title: [A Context Aware Approach for Generating Natural Language Attacks](https://arxiv.org/abs/2012.13339)

Authors: [Rishabh Maheshwary](https://arxiv.org/search/cs?searchtype=author&query=Maheshwary%2C+R), [Saket Maheshwary](https://arxiv.org/search/cs?searchtype=author&query=Maheshwary%2C+S), [Vikram Pudi](https://arxiv.org/search/cs?searchtype=author&query=Pudi%2C+V)

> We study an important task of attacking natural language processing models in a black box setting. We propose an attack strategy that crafts semantically similar adversarial examples on text classification and entailment tasks. Our proposed attack finds candidate words by considering the information of both the original word and its surrounding context. It jointly leverages masked language modelling and next sentence prediction for context understanding. In comparison to attacks proposed in prior literature, we are able to generate high quality adversarial examples that do significantly better both in terms of success rate and word perturbation percentage.

| Comments: | Accepted as Student Poster at AAAI 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.13339](https://arxiv.org/abs/2012.13339) [cs.CL]** |
|           | (or **[arXiv:2012.13339v1](https://arxiv.org/abs/2012.13339v1) [cs.CL]** for this version) |





<h2 id="2020-12-25-6">6. To what extent do human explanations of model behavior align with actual model behavior?</h2>

Title: [To what extent do human explanations of model behavior align with actual model behavior?](https://arxiv.org/abs/2012.13354)

Authors: [Grusha Prasad](https://arxiv.org/search/cs?searchtype=author&query=Prasad%2C+G), [Yixin Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Robin Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> Given the increasingly prominent role NLP models (will) play in our lives, it is important to evaluate models on their alignment with human expectations of how models behave. Using Natural Language Inference (NLI) as a case study, we investigated the extent to which human-generated explanations of models' inference decisions align with how models actually make these decisions. More specifically, we defined two alignment metrics that quantify how well natural language human explanations align with model sensitivity to input words, as measured by integrated gradients. Then, we evaluated six different transformer models (the base and large versions of BERT, RoBERTa and ELECTRA), and found that the BERT-base model has the highest alignment with human-generated explanations, for both alignment metrics. Additionally, the base versions of the models we surveyed tended to have higher alignment with human-generated explanations than their larger counterparts, suggesting that increasing the number model parameters could result in worse alignment with human explanations. Finally, we find that a model's alignment with human explanations is not predicted by the model's accuracy on NLI, suggesting that accuracy and alignment are orthogonal, and both are important ways to evaluate models.

| Comments: | 10 pages, 3 figures, 2 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.13354](https://arxiv.org/abs/2012.13354) [cs.CL]** |
|           | (or **[arXiv:2012.13354v1](https://arxiv.org/abs/2012.13354v1) [cs.CL]** for this version) |







# 2020-12-24

[Return to Index](#Index)



<h2 id="2020-12-24-1">1. Seeing past words: Testing the cross-modal capabilities of pretrained V&L models</h2>

Title: [Seeing past words: Testing the cross-modal capabilities of pretrained V&L models](https://arxiv.org/abs/2012.12352)

Authors: [Letitia Parcalabescu](https://arxiv.org/search/cs?searchtype=author&query=Parcalabescu%2C+L), [Albert Gatt](https://arxiv.org/search/cs?searchtype=author&query=Gatt%2C+A), [Anette Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+A), [Iacer Calixto](https://arxiv.org/search/cs?searchtype=author&query=Calixto%2C+I)

> We investigate the ability of general-purpose pretrained vision and language V&L models to perform reasoning in two tasks that require multimodal integration: (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V&L models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that models solve task (1) very well, as expected, since all models use task (1) for pretraining. However, none of the pretrained V&L models are able to adequately solve task (2), our counting probe, and they cannot generalise to out-of-distribution quantities. Our investigations suggest that pretrained V&L representations are less successful than expected at integrating the two modalities. We propose a number of explanations for these findings: LXMERT's results on the image-sentence alignment task (and to a lesser extent those obtained by ViLBERT 12-in-1) indicate that the model may exhibit catastrophic forgetting. As for our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input.

| Comments:    | 13 pages, 3 figures, 7 Tables                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| MSC classes: | 68Txx                                                        |
| ACM classes: | I.2.7; I.2.10                                                |
| Cite as:     | **[arXiv:2012.12352](https://arxiv.org/abs/2012.12352) [cs.CV]** |
|              | (or **[arXiv:2012.12352v1](https://arxiv.org/abs/2012.12352v1) [cs.CV]** for this version) |





<h2 id="2020-12-24-2">2. Multi-Head Self-Attention with Role-Guided Masks</h2>

Title: [Multi-Head Self-Attention with Role-Guided Masks](https://arxiv.org/abs/2012.12366)

Authors: [Dongsheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Casper Hansen](https://arxiv.org/search/cs?searchtype=author&query=Hansen%2C+C), [Lucas Chaves Lima](https://arxiv.org/search/cs?searchtype=author&query=Lima%2C+L+C), [Christian Hansen](https://arxiv.org/search/cs?searchtype=author&query=Hansen%2C+C), [Maria Maistro](https://arxiv.org/search/cs?searchtype=author&query=Maistro%2C+M), [Jakob Grue Simonsen](https://arxiv.org/search/cs?searchtype=author&query=Simonsen%2C+J+G), [Christina Lioma](https://arxiv.org/search/cs?searchtype=author&query=Lioma%2C+C)

> The state of the art in learning meaningful semantic representations of words is the Transformer model and its attention mechanisms. Simply put, the attention mechanisms learn to attend to specific parts of the input dispensing recurrence and convolutions. While some of the learned attention heads have been found to play linguistically interpretable roles, they can be redundant or prone to errors. We propose a method to guide the attention heads towards roles identified in prior work as important. We do this by defining role-specific masks to constrain the heads to attend to specific parts of the input, such that different heads are designed to play different roles. Experiments on text classification and machine translation using 7 different datasets show that our method outperforms competitive attention-based, CNN, and RNN baselines.

| Comments: | Accepted at ECIR@2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.12366](https://arxiv.org/abs/2012.12366) [cs.CL]** |
|           | (or **[arXiv:2012.12366v1](https://arxiv.org/abs/2012.12366v1) [cs.CL]** for this version) |







<h2 id="2020-12-24-3">3. Future-Guided Incremental Transformer for Simultaneous Translation</h2>

Title: [Future-Guided Incremental Transformer for Simultaneous Translation](https://arxiv.org/abs/2012.12465)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Simultaneous translation (ST) starts translations synchronously while reading source sentences, and is used in many online scenarios. The previous wait-k policy is concise and achieved good results in ST. However, wait-k policy faces two weaknesses: low training speed caused by the recalculation of hidden states and lack of future source information to guide training. For the low training speed, we propose an incremental Transformer with an average embedding layer (AEL) to accelerate the speed of calculation of the hidden states during training. For future-guided training, we propose a conventional Transformer as the teacher of the incremental Transformer, and try to invisibly embed some future information in the model through knowledge distillation. We conducted experiments on Chinese-English and German-English simultaneous translation tasks and compared with the wait-k policy to evaluate the proposed method. Our method can effectively increase the training speed by about 28 times on average at different k and implicitly embed some predictive abilities in the model, achieving better translation quality than wait-k baseline.

| Comments: | Accepted by AAAI 2021. 9 pages, 5 figures                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.12465](https://arxiv.org/abs/2012.12465) [cs.CL]** |
|           | (or **[arXiv:2012.12465v1](https://arxiv.org/abs/2012.12465v1) [cs.CL]** for this version) |







<h2 id="2020-12-24-4">4. Code Switching Language Model Using Monolingual Training Data</h2>

Title: [Code Switching Language Model Using Monolingual Training Data](https://arxiv.org/abs/2012.12543)

Authors: [Asad Ullah](https://arxiv.org/search/cs?searchtype=author&query=Ullah%2C+A), [Tauseef Ahmed](https://arxiv.org/search/cs?searchtype=author&query=Ahmed%2C+T)

> Training a code-switching (CS) language model using only monolingual data is still an ongoing research problem. In this paper, a CS language model is trained using only monolingual training data. As recurrent neural network (RNN) models are best suited for predicting sequential data. In this work, an RNN language model is trained using alternate batches from only monolingual English and Spanish data and the perplexity of the language model is computed. From the results, it is concluded that using alternate batches of monolingual data in training reduced the perplexity of a CS language model. The results were consistently improved using mean square error (MSE) in the output embeddings of RNN based language model. By combining both methods, perplexity is reduced from 299.63 to 80.38. The proposed methods were comparable to the language model fine tune with code-switch training data.

| Comments: | submitted to ICASSP2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2012.12543](https://arxiv.org/abs/2012.12543) [cs.CL]** |
|           | (or **[arXiv:2012.12543v1](https://arxiv.org/abs/2012.12543v1) [cs.CL]** for this version) |







<h2 id="2020-12-24-5">5. Learning Dense Representations of Phrases at Scale</h2>

Title: [Learning Dense Representations of Phrases at Scale](https://arxiv.org/abs/2012.12624)

Authors: [Jinhyuk Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Mujeen Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+M), [Jaewoo Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on their sparse representations while still underperforming retriever-reader approaches. In this work, we show for the first time that we can learn dense phrase representations alone that achieve much stronger performance in open-domain QA. Our approach includes (1) learning query-agnostic phrase representations via question generation and distillation; (2) novel negative-sampling methods for global normalization; (3) query-side fine-tuning for transfer learning. On five popular QA datasets, our model DensePhrases improves previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.12624](https://arxiv.org/abs/2012.12624) [cs.CL]** |
|           | (or **[arXiv:2012.12624v1](https://arxiv.org/abs/2012.12624v1) [cs.CL]** for this version) |







# 2020-12-23

[Return to Index](#Index)



<h2 id="2020-12-23-1">1. A Distributional Approach to Controlled Text Generation</h2>

Title: [A Distributional Approach to Controlled Text Generation](https://arxiv.org/abs/2012.11635)

Authors:[Muhammad Khalifa](https://arxiv.org/search/cs?searchtype=author&query=Khalifa%2C+M), [Hady Elsahar](https://arxiv.org/search/cs?searchtype=author&query=Elsahar%2C+H), [Marc Dymetman](https://arxiv.org/search/cs?searchtype=author&query=Dymetman%2C+M)

> We propose a Distributional Approach to address Controlled Text Generation from pre-trained Language Models (LMs). This view permits to define, in a single formal framework, "pointwise" and "distributional" constraints over the target LM -- to our knowledge, this is the first approach with such generality -- while minimizing KL divergence with the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train the target controlled autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM (GPT-2). We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study we show the effectiveness of our adaptive technique for obtaining faster convergence.

| Comments: | Under review at ICLR 2021                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.11635](https://arxiv.org/abs/2012.11635) [cs.CL]** |
|           | (or **[arXiv:2012.11635v1](https://arxiv.org/abs/2012.11635v1) [cs.CL]** for this version) |





<h2 id="2020-12-23-2">2. Subword Sampling for Low Resource Word Alignment</h2>

Title: [Subword Sampling for Low Resource Word Alignment](https://arxiv.org/abs/2012.11657)

Authors:[Ehsaneddin Asgari](https://arxiv.org/search/cs?searchtype=author&query=Asgari%2C+E), [Masoud Jalili Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+M+J), [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Christopher Ringlstetter](https://arxiv.org/search/cs?searchtype=author&query=Ringlstetter%2C+C), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Annotation projection is an important area in NLP that can greatly contribute to creating language resources for low-resource languages. Word alignment plays a key role in this setting. However, most of the existing word alignment methods are designed for a high resource setting in machine translation where millions of parallel sentences are available. This amount reduces to a few thousands of sentences when dealing with low-resource languages failing the existing established IBM models. In this paper, we propose subword sampling-based alignment of text units. This method's hypothesis is that the aggregation of different granularities of text for certain language pairs can help word-level alignment. For certain languages for which gold-standard alignments exist, we propose an iterative Bayesian optimization framework to optimize selecting possible subwords from the space of possible subword representations of the source and target sentences. We show that the subword sampling method consistently outperforms word-level alignment on six language pairs: English-German, English-French, English-Romanian, English-Persian, English-Hindi, and English-Inuktitut. In addition, we show that the hyperparameters learned for certain language pairs can be applied to other languages at no supervision and consistently improve the alignment results. We observe that using 5K parallel sentences together with our proposed subword sampling approach, we obtain similar F1 scores to the use of 100K's of parallel sentences in existing word-level fast-align/eflomal alignment methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.11657](https://arxiv.org/abs/2012.11657) [cs.CL]** |
|           | (or **[arXiv:2012.11657v1](https://arxiv.org/abs/2012.11657v1) [cs.CL]** for this version) |





<h2 id="2020-12-23-3">3. Undivided Attention: Are Intermediate Layers Necessary for BERT?</h2>

Title: [Undivided Attention: Are Intermediate Layers Necessary for BERT?](https://arxiv.org/abs/2012.11881)

Authors:[Sharath Nittur Sridhar](https://arxiv.org/search/cs?searchtype=author&query=Sridhar%2C+S+N), [Anthony Sarah](https://arxiv.org/search/cs?searchtype=author&query=Sarah%2C+A)

> In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-Base results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use the central kernel alignment (CKA) similarity metric and probing classifiers to demonstrate that removing intermediate layers has little impact on the learned self-attention representations.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.11881](https://arxiv.org/abs/2012.11881) [cs.CL]** |
|           | (or **[arXiv:2012.11881v1](https://arxiv.org/abs/2012.11881v1) [cs.CL]** for this version) |





<h2 id="2020-12-23-4">4. Pre-Training a Language Model Without Human Language</h2>

Title: [Pre-Training a Language Model Without Human Language](https://arxiv.org/abs/2012.11995)

Authors:[Cheng-Han Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+C), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> In this paper, we study how the intrinsic nature of pre-training data contributes to the fine-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we fine-tune those language models on GLUE benchmarks. We find that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another non-English language.

| Comments: | 9 pages, work in progress                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.11995](https://arxiv.org/abs/2012.11995) [cs.CL]** |
|           | (or **[arXiv:2012.11995v1](https://arxiv.org/abs/2012.11995v1) [cs.CL]** for this version) |





<h2 id="2020-12-23-5">5. Domain Adaptation of NMT models for English-Hindi Machine Translation Task at AdapMT ICON 2020</h2>

Title: [Domain Adaptation of NMT models for English-Hindi Machine Translation Task at AdapMT ICON 2020](https://arxiv.org/abs/2012.12112)

Authors:[Ramchandra Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+R), [Rushabh Karnavat](https://arxiv.org/search/cs?searchtype=author&query=Karnavat%2C+R), [Kaustubh Jirapure](https://arxiv.org/search/cs?searchtype=author&query=Jirapure%2C+K), [Raviraj Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+R)

> Recent advancements in Neural Machine Translation (NMT) models have proved to produce a state of the art results on machine translation for low resource Indian languages. This paper describes the neural machine translation systems for the English-Hindi language presented in AdapMT Shared Task ICON 2020. The shared task aims to build a translation system for Indian languages in specific domains like Artificial Intelligence (AI) and Chemistry using a small in-domain parallel corpus. We evaluated the effectiveness of two popular NMT models i.e, LSTM, and Transformer architectures for the English-Hindi machine translation task based on BLEU scores. We train these models primarily using the out of domain data and employ simple domain adaptation techniques based on the characteristics of the in-domain dataset. The fine-tuning and mixed-domain data approaches are used for domain adaptation. Our team was ranked first in the chemistry and general domain En-Hi translation task and second in the AI domain En-Hi translation task.

| Comments: | Accepted at ICON 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.12112](https://arxiv.org/abs/2012.12112) [cs.CL]** |
|           | (or **[arXiv:2012.12112v1](https://arxiv.org/abs/2012.12112v1) [cs.CL]** for this version) |







# 2020-12-22

[Return to Index](#Index)



<h2 id="2020-12-22-1">1. Transductive Visual Verb Sense Disambiguation</h2>

Title: [Transductive Visual Verb Sense Disambiguation](https://arxiv.org/abs/2012.10821)

Authors: [Sebastiano Vascon](https://arxiv.org/search/cs?searchtype=author&query=Vascon%2C+S), [Sinem Aslan](https://arxiv.org/search/cs?searchtype=author&query=Aslan%2C+S), [Gianluca Bigaglia](https://arxiv.org/search/cs?searchtype=author&query=Bigaglia%2C+G), [Lorenzo Giudice](https://arxiv.org/search/cs?searchtype=author&query=Giudice%2C+L), [Marcello Pelillo](https://arxiv.org/search/cs?searchtype=author&query=Pelillo%2C+M)

> Verb Sense Disambiguation is a well-known task in NLP, the aim is to find the correct sense of a verb in a sentence. Recently, this problem has been extended in a multimodal scenario, by exploiting both textual and visual features of ambiguous verbs leading to a new problem, the Visual Verb Sense Disambiguation (VVSD). Here, the sense of a verb is assigned considering the content of an image paired with it rather than a sentence in which the verb appears. Annotating a dataset for this task is more complex than textual disambiguation, because assigning the correct sense to a pair of <image, verb> requires both non-trivial linguistic and visual skills. In this work, differently from the literature, the VVSD task will be performed in a transductive semi-supervised learning (SSL) setting, in which only a small amount of labeled information is required, reducing tremendously the need for annotated data. The disambiguation process is based on a graph-based label propagation method which takes into account mono or multimodal representations for <image, verb> pairs. Experiments have been carried out on the recently published dataset VerSe, the only available dataset for this task. The achieved results outperform the current state-of-the-art by a large margin while using only a small fraction of labeled samples per sense. Code available: [this https URL](https://github.com/GiBg1aN/TVVSD).

| Comments: | Accepted at the IEEE Workshop on Application of Computer Vision 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2012.10821](https://arxiv.org/abs/2012.10821) [cs.CV]** |
|           | (or **[arXiv:2012.10821v1](https://arxiv.org/abs/2012.10821v1) [cs.CV]** for this version) |





<h2 id="2020-12-22-2">2. Self-Supervised Learning for Visual Summary Identification in Scientific Publications</h2>

Title: [Self-Supervised Learning for Visual Summary Identification in Scientific Publications](https://arxiv.org/abs/2012.11213)

Authors: [Shintaro Yamamoto](https://arxiv.org/search/cs?searchtype=author&query=Yamamoto%2C+S), [Anne Lauscher](https://arxiv.org/search/cs?searchtype=author&query=Lauscher%2C+A), [Simone Paolo Ponzetto](https://arxiv.org/search/cs?searchtype=author&query=Ponzetto%2C+S+P), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Shigeo Morishima](https://arxiv.org/search/cs?searchtype=author&query=Morishima%2C+S)

> Providing visual summaries of scientific publications can increase information access for readers and thereby help deal with the exponential growth in the number of scientific publications. Nonetheless, efforts in providing visual publication summaries have been few and fart apart, primarily focusing on the biomedical domain. This is primarily because of the limited availability of annotated gold standards, which hampers the application of robust and high-performing supervised learning techniques. To address these problems we create a new benchmark dataset for selecting figures to serve as visual summaries of publications based on their abstracts, covering several domains in computer science. Moreover, we develop a self-supervised learning approach, based on heuristic matching of inline references to figures with figure captions. Experiments in both biomedical and computer science domains show that our model is able to outperform the state of the art despite being self-supervised and therefore not relying on any annotated training data.

| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.11213](https://arxiv.org/abs/2012.11213) [cs.IR]** |
|           | (or **[arXiv:2012.11213v1](https://arxiv.org/abs/2012.11213v1) [cs.IR]** for this version) |





<h2 id="2020-12-22-3">3. Finding Sparse Structure for Domain Specific Neural Machine Translation</h2>

Title: [Finding Sparse Structure for Domain Specific Neural Machine Translation]()

Authors: [Jianze Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Fine-tuning is a major approach for domain adaptation in Neural Machine Translation (NMT). However, unconstrained fine-tuning requires very careful hyper-parameter tuning otherwise it is easy to fall into over-fitting on the target domain and degradation on the general domain. To mitigate it, we propose PRUNE-TUNE, a novel domain adaptation method via gradual pruning. It learns tiny domain-specific subnetworks for tuning. During adaptation to a new domain, we only tune its corresponding subnetwork. PRUNE-TUNE alleviates the over-fitting and the degradation problem without model modification. Additionally, with no overlapping between domain-specific subnetworks, PRUNE-TUNE is also capable of sequential multi-domain learning. Empirical experiment results show that PRUNE-TUNE outperforms several strong competitors in the target domain test set without the quality degradation of the general domain in both single and multiple domain settings.

| Comments: | Accepted to AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.10586](https://arxiv.org/abs/2012.10586) [cs.CL]** |
|           | (or **[arXiv:2012.10586v1](https://arxiv.org/abs/2012.10586v1) [cs.CL]** for this version) |







<h2 id="2020-12-22-4">4. Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection</h2>

Title: [Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection](https://arxiv.org/abs/2012.10813)

Authors: [Yikang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Pulkit Goel](https://arxiv.org/search/cs?searchtype=author&query=Goel%2C+P), [Varsha Kuppur Rajendra](https://arxiv.org/search/cs?searchtype=author&query=Rajendra%2C+V+K), [Har Simrat Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+H+S), [Jonathan Francis](https://arxiv.org/search/cs?searchtype=author&query=Francis%2C+J), [Kaixin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+K), [Eric Nyberg](https://arxiv.org/search/cs?searchtype=author&query=Nyberg%2C+E), [Alessandro Oltramari](https://arxiv.org/search/cs?searchtype=author&query=Oltramari%2C+A)

> Conditional text generation has been a challenging task that is yet to see human-level performance from state-of-the-art models. In this work, we specifically focus on the Commongen benchmark, wherein the aim is to generate a plausible sentence for a given set of input concepts. Despite advances in other tasks, large pre-trained language models that are fine-tuned on this dataset often produce sentences that are syntactically correct but qualitatively deviate from a human understanding of common sense. Furthermore, generated sequences are unable to fulfill such lexical requirements as matching part-of-speech and full concept coverage. In this paper, we explore how commonsense knowledge graphs can enhance model performance, with respect to commonsense reasoning and lexically-constrained decoding. We propose strategies for enhancing the semantic correctness of the generated text, which we accomplish through: extracting commonsense relations from Conceptnet, injecting these relations into the Unified Language Model (UniLM) through attention mechanisms, and enforcing the aforementioned lexical requirements through output constraints. By performing several ablations, we find that commonsense injection enables the generation of sentences that are more aligned with human understanding, while remaining compliant with lexical requirements.

| Comments: | AAAI-CSKG 2021                                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.10813](https://arxiv.org/abs/2012.10813) [cs.CL]** |
|           | (or **[arXiv:2012.10813v1](https://arxiv.org/abs/2012.10813v1) [cs.CL]** for this version) |





<h2 id="2020-12-22-5">5. Narrative Incoherence Detection</h2>

Title: [Narrative Incoherence Detection](https://arxiv.org/abs/2012.11157)

Authors: [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Yichen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Wai Lam](https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+W), [Bill Dolan](https://arxiv.org/search/cs?searchtype=author&query=Dolan%2C+B)

> Motivated by the increasing popularity of intelligent editing assistant, we introduce and investigate the task of narrative incoherence detection: Given a (corrupted) long-form narrative, decide whether there exists some semantic discrepancy in the narrative flow. Specifically, we focus on the missing sentence and incoherent sentence detection. Despite its simple setup, this task is challenging as the model needs to understand and analyze a multi-sentence narrative text, and make decisions at the sentence level. As an initial step towards this task, we implement several baselines either directly analyzing the raw text (\textit{token-level}) or analyzing learned sentence representations (\textit{sentence-level}). We observe that while token-level modeling enjoys greater expressive power and hence better performance, sentence-level modeling possesses an advantage in efficiency and flexibility. With pre-training on large-scale data and cycle-consistent sentence embedding, our extended sentence-level model can achieve comparable detection accuracy to the token-level model. As a by-product, such a strategy enables simultaneous incoherence detection and infilling/modification suggestions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.11157](https://arxiv.org/abs/2012.11157) [cs.CL]** |
|           | (or **[arXiv:2012.11157v1](https://arxiv.org/abs/2012.11157v1) [cs.CL]** for this version) |





# 2020-12-21

[Return to Index](#Index)



<h2 id="2020-12-21-1">1. On Modality Bias in the TVQA Dataset</h2>

Title: [On Modality Bias in the TVQA Dataset](https://arxiv.org/abs/2012.10210)

Authors: [Thomas Winterbottom](https://arxiv.org/search/cs?searchtype=author&query=Winterbottom%2C+T), [Sarah Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+S), [Alistair McLean](https://arxiv.org/search/cs?searchtype=author&query=McLean%2C+A), [Noura Al Moubayed](https://arxiv.org/search/cs?searchtype=author&query=Moubayed%2C+N+A)

> TVQA is a large scale video question answering (video-QA) dataset based on popular TV shows. The questions were specifically designed to require "both vision and language understanding to answer". In this work, we demonstrate an inherent bias in the dataset towards the textual subtitle modality. We infer said bias both directly and indirectly, notably finding that models trained with subtitles learn, on-average, to suppress video feature contribution. Our results demonstrate that models trained on only the visual information can answer ~45% of the questions, while using only the subtitles achieves ~68%. We find that a bilinear pooling based joint representation of modalities damages model performance by 9% implying a reliance on modality specific information. We also show that TVQA fails to benefit from the RUBi modality bias reduction technique popularised in VQA. By simply improving text processing using BERT embeddings with the simple model first proposed for TVQA, we achieve state-of-the-art results (72.13%) compared to the highly complex STAGE model (70.50%). We recommend a multimodal evaluation framework that can highlight biases in models and isolate visual and textual reliant subsets of data. Using this framework we propose subsets of TVQA that respond exclusively to either or both modalities in order to facilitate multimodal modelling as TVQA originally intended.

| Comments:    | 10 pages, 4 Figures, 2 Tables, +Supp Mats, BMVC 2020         |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| MSC classes: | 68T99                                                        |
| ACM classes: | I.2.10; I.2.7; I.2.4                                         |
| Cite as:     | **[arXiv:2012.10210](https://arxiv.org/abs/2012.10210) [cs.CV]** |
|              | (or **[arXiv:2012.10210v1](https://arxiv.org/abs/2012.10210v1) [cs.CV]** for this version) |





<h2 id="2020-12-21-2">2. NeurST: Neural Speech Translation Toolkit</h2>

Title: [NeurST: Neural Speech Translation Toolkit](https://arxiv.org/abs/2012.10018)

Authors: [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> NeurST is an open-source toolkit for neural speech translation developed by ByteDance AI Lab. The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and provides a complete setup for speech translation benchmarks, including feature extraction, data preprocessing, distributed training, and evaluation. Moreover, The toolkit implements several major architectures for end-to-end speech translation. It shows experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at [this https URL](https://github.com/bytedance/neurst).

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.10018](https://arxiv.org/abs/2012.10018) [cs.CL]** |
|           | (or **[arXiv:2012.10018v1](https://arxiv.org/abs/2012.10018v1) [cs.CL]** for this version) |





<h2 id="2020-12-21-3">3. Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning</h2>

Title: [Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning](https://arxiv.org/abs/2012.10033)

Authors: [Jerry Zikun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J+Z), [Shi Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+S), [Haoran Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Query reformulation aims to alter potentially noisy or ambiguous text sequences into coherent ones closer to natural language questions. In this process, it is also crucial to maintain and even enhance performance in a downstream environments like question answering when rephrased queries are given as input. We explore methods to generate these query reformulations by training reformulators using text-to-text transformers and apply policy-based reinforcement learning algorithms to further encourage reward learning. Query fluency is numerically evaluated by the same class of model fine-tuned on a human-evaluated well-formedness dataset. The reformulator leverages linguistic knowledge obtained from transfer learning and generates more well-formed reformulations than a translation-based model in qualitative and quantitative analysis. During reinforcement learning, it better retains fluency while optimizing the RL objective to acquire question answering rewards and can generalize to out-of-sample textual data in qualitative evaluations. Our RL framework is demonstrated to be flexible, allowing reward signals to be sourced from different downstream environments such as intent classification.

| Comments: | Workshop on the 9th Dialog System TechnologyChallenge (DSTC-9), AAAI 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.10033](https://arxiv.org/abs/2012.10033) [cs.CL]** |
|           | (or **[arXiv:2012.10033v1](https://arxiv.org/abs/2012.10033v1) [cs.CL]** for this version) |





<h2 id="2020-12-21-4">4. AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text</h2>

Title: [AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text](https://arxiv.org/abs/2012.10235)

Authors: [Zhihong Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Z), [Zitao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jiyong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Zhongqin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M)

> Adversarial examples are vital to expose the vulnerability of machine learning models. Despite the success of the most popular substitution-based methods which substitutes some characters or words in the original examples, only substitution is insufficient to uncover all robustness issues of models. In this paper, we present AdvExpander, a method that crafts new adversarial examples by expanding text, which is complementary to previous substitution-based methods. We first utilize linguistic rules to determine which constituents to expand and what types of modifiers to expand with. We then expand each constituent by inserting an adversarial modifier searched from a CVAE-based generative model which is pre-trained on a large scale corpus. To search adversarial modifiers, we directly search adversarial latent codes in the latent space without tuning the pre-trained parameters. To ensure that our adversarial examples are label-preserving for text matching, we also constrain the modifications with a heuristic rule. Experiments on three classification tasks verify the effectiveness of AdvExpander and the validity of our adversarial examples. AdvExpander crafts a new type of adversarial examples by text expansion, thereby promising to reveal new robustness issues.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.10235](https://arxiv.org/abs/2012.10235) [cs.CL]** |
|           | (or **[arXiv:2012.10235v1](https://arxiv.org/abs/2012.10235v1) [cs.CL]** for this version) |





<h2 id="2020-12-21-5">5. Understood in Translation, Transformers for Domain Understanding</h2>

Title: [Understood in Translation, Transformers for Domain Understanding](https://arxiv.org/abs/2012.10271)

Authors: [Dimitrios Christofidellis](https://arxiv.org/search/cs?searchtype=author&query=Christofidellis%2C+D), [Matteo Manica](https://arxiv.org/search/cs?searchtype=author&query=Manica%2C+M), [Leonidas Georgopoulos](https://arxiv.org/search/cs?searchtype=author&query=Georgopoulos%2C+L), [Hans Vandierendonck](https://arxiv.org/search/cs?searchtype=author&query=Vandierendonck%2C+H)

> Knowledge acquisition is the essential first step of any Knowledge Graph (KG) application. This knowledge can be extracted from a given corpus (KG generation process) or specified from an existing KG (KG specification process). Focusing on domain specific solutions, knowledge acquisition is a labor intensive task usually orchestrated and supervised by subject matter experts. Specifically, the domain of interest is usually manually defined and then the needed generation or extraction tools are utilized to produce the KG. Herein, we propose a supervised machine learning method, based on Transformers, for domain definition of a corpus. We argue why such automated definition of the domain's structure is beneficial both in terms of construction time and quality of the generated graph. The proposed method is extensively validated on three public datasets (WebNLG, NYT and DocRED) by comparing it with two reference methods based on CNNs and RNNs models. The evaluation shows the efficiency of our model in this task. Focusing on scientific document understanding, we present a new health domain dataset based on publications extracted from PubMed and we successfully utilize our method on this. Lastly, we demonstrate how this work lays the foundation for fully automated and unsupervised KG generation.

| Comments: | 4 figures, 7 tables, main text pages 8, appendix pages 6     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.10271](https://arxiv.org/abs/2012.10271) [cs.CL]** |
|           | (or **[arXiv:2012.10271v1](https://arxiv.org/abs/2012.10271v1) [cs.CL]** for this version) |







# 2020-12-18

[Return to Index](#Index)



<h2 id="2020-12-18-1">1. The effectiveness of unsupervised subword modeling with autoregressive and cross-lingual phone-aware networks</h2>

Title: [The effectiveness of unsupervised subword modeling with autoregressive and cross-lingual phone-aware networks](https://arxiv.org/abs/2012.09544)

Authors: [Siyuan Feng](https://arxiv.org/search/eess?searchtype=author&query=Feng%2C+S), [Odette Scharenborg](https://arxiv.org/search/eess?searchtype=author&query=Scharenborg%2C+O)

> This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding (APC) as the front-end and a cross-lingual deep neural network (DNN) as the back-end. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies. Comprehensive and systematic analyses at the phoneme- and articulatory feature (AF)-level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information. Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.

| Comments: | 18 pages (including 1 page for supplementary material), 13 figures. Submitted to IEEE Open Journal of Signal Processing (OJ-SP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2012.09544](https://arxiv.org/abs/2012.09544) [eess.AS]** |
|           | (or **[arXiv:2012.09544v1](https://arxiv.org/abs/2012.09544v1) [eess.AS]** for this version) |





<h2 id="2020-12-18-2">2. MIX : a Multi-task Learning Approach to Solve Open-Domain Question Answering</h2>

Title: [MIX : a Multi-task Learning Approach to Solve Open-Domain Question Answering](https://arxiv.org/abs/2012.09766)

Authors: [Sofian Chaybouti](https://arxiv.org/search/cs?searchtype=author&query=Chaybouti%2C+S), [Achraf Saghe](https://arxiv.org/search/cs?searchtype=author&query=Saghe%2C+A), [Aymen Shabou](https://arxiv.org/search/cs?searchtype=author&query=Shabou%2C+A)

> In this paper, we introduce MIX : a multi-task deep learning approach to solve Open-Domain Question Answering. First, we design our system as a multi-stage pipeline made of 3 building blocks : a BM25-based Retriever, to reduce the search space; RoBERTa based Scorer and Extractor, to rank retrieved documents and extract relevant spans of text respectively. Eventually, we further improve computational efficiency of our system to deal with the scalability challenge : thanks to multi-task learning, we parallelize the close tasks solved by the Scorer and the Extractor. Our system outperforms previous state-of-the-art by 12 points in both f1-score and exact-match on the squad-open benchmark.

| Comments:    | 7 pages, 6 figures, 4 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2012.09766](https://arxiv.org/abs/2012.09766) [cs.CL]** |
|              | (or **[arXiv:2012.09766v1](https://arxiv.org/abs/2012.09766v1) [cs.CL]** for this version) |





<h2 id="2020-12-18-3">3. Continual Lifelong Learning in Natural Language Processing: A Survey</h2>

Title: [Continual Lifelong Learning in Natural Language Processing: A Survey](https://arxiv.org/abs/2012.09823)

Authors: [Magdalena Biesialska](https://arxiv.org/search/cs?searchtype=author&query=Biesialska%2C+M), [Katarzyna Biesialska](https://arxiv.org/search/cs?searchtype=author&query=Biesialska%2C+K), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.

| Comments:          | COLING 2020                                                  |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Journal reference: | Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020), Barcelona, Spain (Online), pp. 6523--6541 |
| Cite as:           | **[arXiv:2012.09823](https://arxiv.org/abs/2012.09823) [cs.CL]** |
|                    | (or **[arXiv:2012.09823v1](https://arxiv.org/abs/2012.09823v1) [cs.CL]** for this version) |





# 2020-12-17

[Return to Index](#Index)



<h2 id="2020-12-17-1">1. A Closer Look at the Robustness of Vision-and-Language Pre-trained Models</h2>

Title: [A Closer Look at the Robustness of Vision-and-Language Pre-trained Models](https://arxiv.org/abs/2012.08673)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER, have propelled the state of the art in vision-and-language (V+L) research to a new level. Although achieving impressive performance on standard tasks, to date, it still remains unclear how robust these pre-trained models are. To investigate, we conduct a host of thorough evaluations on existing pre-trained models over 4 different types of V+L specific model robustness: (i) Linguistic Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv) Answer Distribution Shift. Interestingly, by standard model finetuning, pre-trained V+L models already exhibit better robustness than many task-specific state-of-the-art methods. To further enhance model robustness, we propose Mango, a generic and efficient approach that learns a Multimodal Adversarial Noise GeneratOr in the embedding space to fool pre-trained V+L models. Differing from previous studies focused on one specific type of robustness, Mango is task-agnostic, and enables universal performance lift for pre-trained models over diverse tasks designed to evaluate broad aspects of robustness. Comprehensive experiments demonstrate that Mango achieves new state of the art on 7 out of 9 robustness benchmarks, surpassing existing methods by a significant margin. As the first comprehensive study on V+L robustness, this work puts robustness of pre-trained models into sharper focus, pointing new directions for future study.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.08673](https://arxiv.org/abs/2012.08673) [cs.CV]** |
|           | (or **[arXiv:2012.08673v1](https://arxiv.org/abs/2012.08673v1) [cs.CV]** for this version) |





<h2 id="2020-12-17-2">2. Improving Multilingual Neural Machine Translation For Low-Resource Languages: French-, English- Vietnamese</h2>

Title: [Improving Multilingual Neural Machine Translation For Low-Resource Languages: French-, English- Vietnamese](https://arxiv.org/abs/2012.08743)

Authors: [Thi-Vinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+T), [Phuong-Thai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Khac-Quy Dinh](https://arxiv.org/search/cs?searchtype=author&query=Dinh%2C+K), [Le-Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L)

> Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation (MT) systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | The 3rd Workshop on Technologies for MT of Low Resource Languages (LoResMT 2020) |
| Cite as:           | **[arXiv:2012.08743](https://arxiv.org/abs/2012.08743) [cs.CL]** |
|                    | (or **[arXiv:2012.08743v1](https://arxiv.org/abs/2012.08743v1) [cs.CL]** for this version) |





# 2020-12-16

[Return to Index](#Index)



<h2 id="2020-12-16-1">1. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks</h2>

Title: [Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks](https://arxiv.org/abs/2012.08492)

Authors: [Cunchao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Muhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Changjun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Guangquan Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+G), [Yan Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+Y)

> Large knowledge graphs often grow to store temporal facts that model the dynamic relations or interactions of entities along the timeline. Since such temporal knowledge graphs often suffer from incompleteness, it is important to develop time-aware representation learning models that help to infer the missing temporal facts. While the temporal facts are typically evolving, it is observed that many facts often show a repeated pattern along the timeline, such as economic crises and diplomatic activities. This observation indicates that a model could potentially learn much from the known facts appeared in history. To this end, we propose a new representation learning model for temporal knowledge graphs, namely CyGNet, based on a novel timeaware copy-generation mechanism. CyGNet is not only able to predict future facts from the whole entity vocabulary, but also capable of identifying facts with repetition and accordingly predicting such future facts with reference to the known facts in the past. We evaluate the proposed method on the knowledge graph completion task using five benchmark datasets. Extensive experiments demonstrate the effectiveness of CyGNet for predicting future facts with repetition as well as de novo fact prediction.

| Comments: | AAAI 2021; preprint with Appendices                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.08492](https://arxiv.org/abs/2012.08492) [cs.AI]** |
|           | (or **[arXiv:2012.08492v1](https://arxiv.org/abs/2012.08492v1) [cs.AI]** for this version) |





<h2 id="2020-12-16-2">2. Model Choices Influence Attributive Word Associations: A Semi-supervised Analysis of Static Word Embeddings</h2>

Title: [Model Choices Influence Attributive Word Associations: A Semi-supervised Analysis of Static Word Embeddings](https://arxiv.org/abs/2012.07978)

Authors: [Geetanjali Bihani](https://arxiv.org/search/cs?searchtype=author&query=Bihani%2C+G), [Julia Taylor Rayz](https://arxiv.org/search/cs?searchtype=author&query=Rayz%2C+J+T)

> Static word embeddings encode word associations, extensively utilized in downstream NLP tasks. Although prior studies have discussed the nature of such word associations in terms of biases and lexical regularities captured, the variation in word associations based on the embedding training procedure remains in obscurity. This work aims to address this gap by assessing attributive word associations across five different static word embedding architectures, analyzing the impact of the choice of the model architecture, context learning flavor and training corpora. Our approach utilizes a semi-supervised clustering method to cluster annotated proper nouns and adjectives, based on their word embedding features, revealing underlying attributive word associations formed in the embedding space, without introducing any confirmation bias. Our results reveal that the choice of the context learning flavor during embedding training (CBOW vs skip-gram) impacts the word association distinguishability and word embeddings' sensitivity to deviations in the training corpora. Moreover, it is empirically shown that even when trained over the same corpora, there is significant inter-model disparity and intra-model similarity in the encoded word associations across different word embedding models, portraying specific patterns in the way the embedding space is created for each embedding architecture.

| Comments: | 2020 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT'20) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.07978](https://arxiv.org/abs/2012.07978) [cs.CL]** |
|           | (or **[arXiv:2012.07978v1](https://arxiv.org/abs/2012.07978v1) [cs.CL]** for this version) |





<h2 id="2020-12-16-3">3. Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution</h2>

Title: [Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution](https://arxiv.org/abs/2012.08290)

Authors: [Ron Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+R)

> Hateful meme detection is a new research area recently brought out that requires both visual, linguistic understanding of the meme and some background knowledge to performing well on the task. This technical report summarises the first place solution of the Hateful Meme Detection Challenge 2020, which extending state-of-the-art visual-linguistic transformers to tackle this problem. At the end of the report, we also point out the shortcomings and possible directions for improving the current methodology.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.08290](https://arxiv.org/abs/2012.08290) [cs.CL]** |
|           | (or **[arXiv:2012.08290v1](https://arxiv.org/abs/2012.08290v1) [cs.CL]** for this version) |





<h2 id="2020-12-16-4">4. Modeling Homophone Noise for Robust Neural Machine Translation</h2>

Title: [Modeling Homophone Noise for Robust Neural Machine Translation](https://arxiv.org/abs/2012.08396)

Authors: [Wenjie Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+W), [Xiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuhui Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D), [Jianwei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+J), [Bin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B)

> In this paper, we propose a robust neural machine translation (NMT) framework. The framework consists of a homophone noise detector and a syllable-aware NMT model to homophone errors. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese->English translation demonstrate that our proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves a substantial improvement on clean text.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.08396](https://arxiv.org/abs/2012.08396) [cs.CL]** |
|           | (or **[arXiv:2012.08396v1](https://arxiv.org/abs/2012.08396v1) [cs.CL]** for this version) |





# 2020-12-15

[Return to Index](#Index)



<h2 id="2020-12-15-1">1. Learning Contextual Causality from Time-consecutive Images</h2>

Title: [Learning Contextual Causality from Time-consecutive Images](https://arxiv.org/abs/2012.07138)

Authors: [Hongming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yintong Huo](https://arxiv.org/search/cs?searchtype=author&query=Huo%2C+Y), [Xinran Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X), [Yangqiu Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records (e.g., ConceptNet) typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages: (1) Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos; (2) Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from; (3) All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.07138](https://arxiv.org/abs/2012.07138) [cs.AI]** |
|           | (or **[arXiv:2012.07138v1](https://arxiv.org/abs/2012.07138v1) [cs.AI]** for this version) |





<h2 id="2020-12-15-2">2. LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding</h2>

Title: [LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding](https://arxiv.org/abs/2012.07335)

Authors: [Hao Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H), [Shaojun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S), [Qihong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Q), [Junjie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J), [Guiquan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G), [Kaikui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K), [Xiaolong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> The pre-training models such as BERT have achieved great results in various natural language processing problems. However, a large number of parameters need significant amounts of memory and the consumption of inference time, which makes it difficult to deploy them on edge devices. In this work, we propose a knowledge distillation method LRC-BERT based on contrastive learning to fit the output of the intermediate layer from the angular distance aspect, which is not considered by the existing distillation methods. Furthermore, we introduce a gradient perturbation-based training architecture in the training phase to increase the robustness of LRC-BERT, which is the first attempt in knowledge distillation. Additionally, in order to better capture the distribution characteristics of the intermediate layer, we design a two-stage training method for the total distillation loss. Finally, by verifying 8 datasets on the General Language Understanding Evaluation (GLUE) benchmark, the performance of the proposed LRC-BERT exceeds the existing state-of-the-art methods, which proves the effectiveness of our method.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.07335](https://arxiv.org/abs/2012.07335) [cs.CL]** |
|           | (or **[arXiv:2012.07335v1](https://arxiv.org/abs/2012.07335v1) [cs.CL]** for this version) |





<h2 id="2020-12-15-3">3. A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings</h2>

Title: [A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings](https://arxiv.org/abs/2012.07387)

Authors: [Lisa van Staden](https://arxiv.org/search/cs?searchtype=author&query=van+Staden%2C+L), [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H)

> Many speech processing tasks involve measuring the acoustic similarity between speech segments. Acoustic word embeddings (AWE) allow for efficient comparisons by mapping speech segments of arbitrary duration to fixed-dimensional vectors. For zero-resource speech processing, where unlabelled speech is the only available resource, some of the best AWE approaches rely on weak top-down constraints in the form of automatically discovered word-like segments. Rather than learning embeddings at the segment level, another line of zero-resource research has looked at representation learning at the short-time frame level. Recent approaches include self-supervised predictive coding and correspondence autoencoder (CAE) models. In this paper we consider whether these frame-level features are beneficial when used as inputs for training to an unsupervised AWE model. We compare frame-level features from contrastive predictive coding (CPC), autoregressive predictive coding and a CAE to conventional MFCCs. These are used as inputs to a recurrent CAE-based AWE model. In a word discrimination task on English and Xitsonga data, all three representation learning approaches outperform MFCCs, with CPC consistently showing the biggest improvement. In cross-lingual experiments we find that CPC features trained on English can also be transferred to Xitsonga.

| Comments: | Accepted to SLT 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2012.07387](https://arxiv.org/abs/2012.07387) [cs.CL]** |
|           | (or **[arXiv:2012.07387v1](https://arxiv.org/abs/2012.07387v1) [cs.CL]** for this version) |





<h2 id="2020-12-15-4">4. Ensemble Distillation Approaches for Grammatical Error Correction</h2>

Title: [Ensemble Distillation Approaches for Grammatical Error Correction](https://arxiv.org/abs/2012.07535)

Authors: [Yassir Fathullah](https://arxiv.org/search/cs?searchtype=author&query=Fathullah%2C+Y), [Mark Gales](https://arxiv.org/search/cs?searchtype=author&query=Gales%2C+M), [Andrey Malinin](https://arxiv.org/search/cs?searchtype=author&query=Malinin%2C+A)

> Ensemble approaches are commonly used techniques to improving a system by combining multiple model predictions. Additionally these schemes allow the uncertainty, as well as the source of the uncertainty, to be derived for the prediction. Unfortunately these benefits come at a computational and memory cost. To address this problem ensemble distillation (EnD) and more recently ensemble distribution distillation (EnDD) have been proposed that compress the ensemble into a single model, representing either the ensemble average prediction or prediction distribution respectively. This paper examines the application of both these distillation approaches to a sequence prediction task, grammatical error correction (GEC). This is an important application area for language learning tasks as it can yield highly useful feedback to the learner. It is, however, more challenging than the standard tasks investigated for distillation as the prediction of any grammatical correction to a word will be highly dependent on both the input sequence and the generated output history for the word. The performance of both EnD and EnDD are evaluated on both publicly available GEC tasks as well as a spoken language task.

| Comments: | Submitted to ICASSP 2020                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.07535](https://arxiv.org/abs/2012.07535) [cs.CL]** |
|           | (or **[arXiv:2012.07535v1](https://arxiv.org/abs/2012.07535v1) [cs.CL]** for this version) |





<h2 id="2020-12-15-5">5. Sentiment analysis in Bengali via transfer learning using multi-lingual BERT</h2>

Title: [Sentiment analysis in Bengali via transfer learning using multi-lingual BERT](https://arxiv.org/abs/2012.07538)

Authors: [Khondoker Ittehadul Islam](https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+K+I), [Md. Saiful Islam](https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+M+S), [Md Ruhul Amin](https://arxiv.org/search/cs?searchtype=author&query=Amin%2C+M+R)

> Sentiment analysis (SA) in Bengali is challenging due to this Indo-Aryan language's highly inflected properties with more than 160 different inflected forms for verbs and 36 different forms for noun and 24 different forms for pronouns. The lack of standard labeled datasets in the Bengali domain makes the task of SA even harder. In this paper, we present manually tagged 2-class and 3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT model with relevant extensions can be trained via the approach of transfer learning over those novel datasets to improve the state-of-the-art performance in sentiment classification tasks. This deep learning model achieves an accuracy of 71\% for 2-class sentiment classification compared to the current state-of-the-art accuracy of 68\%. We also present the very first Bengali SA classifier for the 3-class manually tagged dataset, and our proposed model achieves an accuracy of 60\%. We further use this model to analyze the sentiment of public comments in the online daily newspaper. Our analysis shows that people post negative comments for political or sports news more often, while the religious article comments represent positive sentiment. The dataset and code is publicly available at [this https URL](https://github.com/KhondokerIslam/Bengali)\_Sentiment.

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.07538](https://arxiv.org/abs/2012.07538) [cs.CL]** |
|           | (or **[arXiv:2012.07538v1](https://arxiv.org/abs/2012.07538v1) [cs.CL]** for this version) |





<h2 id="2020-12-15-6">6. Vartani Spellcheck -- Automatic Context-Sensitive Spelling Correction of OCR-generated Hindi Text Using BERT and Levenshtein Distance</h2>

Title: [Vartani Spellcheck -- Automatic Context-Sensitive Spelling Correction of OCR-generated Hindi Text Using BERT and Levenshtein Distance](https://arxiv.org/abs/2012.07652)

Authors: [Aditya Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+A), [Abhijit Mustafi](https://arxiv.org/search/cs?searchtype=author&query=Mustafi%2C+A)

> Traditional Optical Character Recognition (OCR) systems that generate text of highly inflectional Indic languages like Hindi tend to suffer from poor accuracy due to a wide alphabet set, compound characters and difficulty in segmenting characters in a word. Automatic spelling error detection and context-sensitive error correction can be used to improve accuracy by post-processing the text generated by these OCR systems. A majority of previously developed language models for error correction of Hindi spelling have been context-free. In this paper, we present Vartani Spellcheck - a context-sensitive approach for spelling correction of Hindi text using a state-of-the-art transformer - BERT in conjunction with the Levenshtein distance algorithm, popularly known as Edit Distance. We use a lookup dictionary and context-based named entity recognition (NER) for detection of possible spelling errors in the text. Our proposed technique has been tested on a large corpus of text generated by the widely used Tesseract OCR on the Hindi epic Ramayana. With an accuracy of 81%, the results show a significant improvement over some of the previously established context-sensitive error correction mechanisms for Hindi. We also explain how Vartani Spellcheck may be used for on-the-fly autocorrect suggestion during continuous typing in a text editor environment.

| Comments: | 5 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.07652](https://arxiv.org/abs/2012.07652) [cs.CL]** |
|           | (or **[arXiv:2012.07652v1](https://arxiv.org/abs/2012.07652v1) [cs.CL]** for this version) |











# 2020-12-14

[Return to Index](#Index)



<h2 id="2020-12-14-1">1. Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer</h2>

Title: [Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2012.06460)

Authors: [Marko Vidoni](https://arxiv.org/search/cs?searchtype=author&query=Vidoni%2C+M), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G)

> Adapter modules, additional trainable parameters that enable efficient fine-tuning of pretrained transformers, have recently been used for language specialization of multilingual transformers, improving downstream zero-shot cross-lingual transfer. In this work, we propose orthogonal language and task adapters (dubbed orthoadapters) for cross-lingual transfer. They are trained to encode language- and task-specific information that is complementary (i.e., orthogonal) to the knowledge already stored in the pretrained transformer's parameters. Our zero-shot cross-lingual transfer experiments, involving three tasks (POS-tagging, NER, NLI) and a set of 10 diverse languages, 1) point to the usefulness of orthoadapters in cross-lingual transfer, especially for the most complex NLI task, but also 2) indicate that the optimal adapter configuration highly depends on the task and the target language. We hope that our work will motivate a wider investigation of usefulness of orthogonality constraints in language- and task-specific fine-tuning of pretrained transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.06460](https://arxiv.org/abs/2012.06460) [cs.CL]** |
|           | (or **[arXiv:2012.06460v1](https://arxiv.org/abs/2012.06460v1) [cs.CL]** for this version) |





<h2 id="2020-12-14-2">2. Comprehension and Knowledge</h2>

Title: [Comprehension and Knowledge](https://arxiv.org/abs/2012.06561)

Authors: [Pavel Naumov](https://arxiv.org/search/cs?searchtype=author&query=Naumov%2C+P), [Kevin Ros](https://arxiv.org/search/cs?searchtype=author&query=Ros%2C+K)

> The ability of an agent to comprehend a sentence is tightly connected to the agent's prior experiences and background knowledge. The paper suggests to interpret comprehension as a modality and proposes a complete bimodal logical system that describes an interplay between comprehension and knowledge modalities.

| Comments: | To appear in Proceedings 35th AAAI Conference on Artificial Intelligence (AAAI 21), February 2-9, 2021 (without the technical appendix) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Logic in Computer Science (cs.LO) |
| Cite as:  | **[arXiv:2012.06561](https://arxiv.org/abs/2012.06561) [cs.AI]** |
|           | (or **[arXiv:2012.06561v1](https://arxiv.org/abs/2012.06561v1) [cs.AI]** for this version) |





<h2 id="2020-12-14-3">3. Reinforced Multi-Teacher Selection for Knowledge Distillation</h2>

Title: [Reinforced Multi-Teacher Selection for Knowledge Distillation](https://arxiv.org/abs/2012.06048)

Authors: [Fei Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+F), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Jian Pei](https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+J), [Wutao Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Yan Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D)

> In natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.

| Comments: | AAAI 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.06048](https://arxiv.org/abs/2012.06048) [cs.CL]** |
|           | (or **[arXiv:2012.06048v1](https://arxiv.org/abs/2012.06048v1) [cs.CL]** for this version) |





<h2 id="2020-12-14-4">4. Improving Task-Agnostic BERT Distillation with Layer Mapping Search</h2>

Title: [Improving Task-Agnostic BERT Distillation with Layer Mapping Search](https://arxiv.org/abs/2012.06153)

Authors:[Xiaoqi Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+X), [Huating Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+H), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Linlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Fang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Knowledge distillation (KD) which transfers the knowledge from a large teacher model to a small student model, has been widely used to compress the BERT model recently. Besides the supervision in the output in the original KD, recent works show that layer-level supervision is crucial to the performance of the student BERT model. However, previous works designed the layer mapping strategy heuristically (e.g., uniform or last-layer), which can lead to inferior performance. In this paper, we propose to use the genetic algorithm (GA) to search for the optimal layer mapping automatically. To accelerate the search process, we further propose a proxy setting where a small portion of the training corpus are sampled for distillation, and three representative tasks are chosen for evaluation. After obtaining the optimal layer mapping, we perform the task-agnostic BERT distillation with it on the whole corpus to build a compact student model, which can be directly fine-tuned on downstream tasks. Comprehensive experiments on the evaluation benchmarks demonstrate that 1) layer mapping strategy has a significant effect on task-agnostic BERT distillation and different layer mappings can result in quite different performances; 2) the optimal layer mapping strategy from the proposed search process consistently outperforms the other heuristic ones; 3) with the optimal layer mapping, our student model achieves state-of-the-art performance on the GLUE tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.06153](https://arxiv.org/abs/2012.06153) [cs.CL]** |
|           | (or **[arXiv:2012.06153v1](https://arxiv.org/abs/2012.06153v1) [cs.CL]** for this version) |





# 2020-12-11

[Return to Index](#Index)



<h2 id="2020-12-11-1">1. Rewriter-Evaluator Framework for Neural Machine Translation</h2>

Title: [Rewriter-Evaluator Framework for Neural Machine Translation](https://arxiv.org/abs/2012.05414)

Authors: [Yangming Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Kaisheng Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+K)

> Encoder-decoder architecture has been widely used in neural machine translation (NMT). A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent (PGD) method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models (e.g., Transformer). We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.05414](https://arxiv.org/abs/2012.05414) [cs.CL]** |
|           | (or **[arXiv:2012.05414v1](https://arxiv.org/abs/2012.05414v1) [cs.CL]** for this version) |





<h2 id="2020-12-11-2">2. As good as new. How to successfully recycle English GPT-2 to make models for other languages</h2>

Title: [As good as new. How to successfully recycle English GPT-2 to make models for other languages](https://arxiv.org/abs/2012.05628)

Authors: [Wietse de Vries](https://arxiv.org/search/cs?searchtype=author&query=de+Vries%2C+W), [Malvina Nissim](https://arxiv.org/search/cs?searchtype=author&query=Nissim%2C+M)

> Large generative language models have been very successful for English, but other languages lag behind due to data and computational limitations. We propose a method that may overcome these problems by adapting existing pre-trained language models to new languages. Specifically, we describe the adaptation of English GPT-2 to Italian and Dutch by retraining lexical embeddings without tuning the Transformer layers. As a result, we obtain lexical embeddings for Italian and Dutch that are aligned with the original English lexical embeddings and induce a bilingual lexicon from this alignment. Additionally, we show how to scale up complexity by transforming relearned lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch, but on average these sentences are still identifiable as artificial by humans. Based on perplexity scores and human judgements, we find that generated sentences become more realistic with some additional full model finetuning, especially for Dutch. For Italian, we see that they are evaluated on par with sentences generated by a GPT-2 model fully trained from scratch. Our work can be conceived as a blueprint for training GPT-2s for other languages, and we provide a 'recipe' to do so.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.05628](https://arxiv.org/abs/2012.05628) [cs.CL]** |
|           | (or **[arXiv:2012.05628v1](https://arxiv.org/abs/2012.05628v1) [cs.CL]** for this version) |





<h2 id="2020-12-11-3">3. Direct multimodal few-shot learning of speech and images</h2>

Title: [Direct multimodal few-shot learning of speech and images](https://arxiv.org/abs/2012.05680)

Authors: [Leanne Nortje](https://arxiv.org/search/cs?searchtype=author&query=Nortje%2C+L), [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H)

> We propose direct multimodal few-shot models that learn a shared embedding space of spoken words and images from only a few paired examples. Imagine an agent is shown an image along with a spoken word describing the object in the picture, e.g. pen, book and eraser. After observing a few paired examples of each class, the model is asked to identify the "book" in a set of unseen pictures. Previous work used a two-step indirect approach relying on learned unimodal representations: speech-speech and image-image comparisons are performed across the support set of given speech-image pairs. We propose two direct models which instead learn a single multimodal space where inputs from different modalities are directly comparable: a multimodal triplet network (MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these direct models, we mine speech-image pairs: the support set is used to pair up unlabelled in-domain speech and images. In a speech-to-image digit matching task, direct models outperform indirect models, with the MTriplet achieving the best multimodal five-shot accuracy. We show that the improvements are due to the combination of unsupervised and transfer learning in the direct models, and the absence of two-step compounding errors.

| Comments: | 3 figures, 2 tables                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2012.05680](https://arxiv.org/abs/2012.05680) [cs.CL]** |
|           | (or **[arXiv:2012.05680v1](https://arxiv.org/abs/2012.05680v1) [cs.CL]** for this version) |





<h2 id="2020-12-11-4">4. Exploring Pair-Wise NMT for Indian Languages</h2>

Title: [Exploring Pair-Wise NMT for Indian Languages](https://arxiv.org/abs/2012.05786)

Authors: [Kartheek Akella](https://arxiv.org/search/cs?searchtype=author&query=Akella%2C+K), [Sai Himal Allu](https://arxiv.org/search/cs?searchtype=author&query=Allu%2C+S+H), [Sridhar Suresh Ragupathi](https://arxiv.org/search/cs?searchtype=author&query=Ragupathi%2C+S+S), [Aman Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+A), [Zeeshan Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+Z), [Vinay P. Namboodiri](https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P), [C V Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C+V)

> In this paper, we address the task of improving pair-wise machine translation for specific low resource Indian languages. Multilingual NMT models have demonstrated a reasonable amount of effectiveness on resource-poor languages. In this work, we show that the performance of these models can be significantly improved upon by using back-translation through a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora. The analysis in this paper suggests that this method can significantly improve a multilingual model's performance over its baseline, yielding state-of-the-art results for various Indian languages.

| Comments: | ICON 2020 Short paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.05786](https://arxiv.org/abs/2012.05786) [cs.CL]** |
|           | (or **[arXiv:2012.05786v1](https://arxiv.org/abs/2012.05786v1) [cs.CL]** for this version) |





# 2020-12-10

[Return to Index](#Index)



<h2 id="2020-12-10-1">1. SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint</h2>

Title: [SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint](https://arxiv.org/abs/2012.05168)

Authors: [Zhonghao Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+Z), [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Yi Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y), [Wei Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+W), [Shikun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T)

> Automatic song writing aims to compose a song (lyric and/or melody) by machine, which is an interesting topic in both academia and industry. In automatic song writing, lyric-to-melody generation and melody-to-lyric generation are two important tasks, both of which usually suffer from the following challenges: 1) the paired lyric and melody data are limited, which affects the generation quality of the two tasks, considering a lot of paired training data are needed due to the weak correlation between lyric and melody; 2) Strict alignments are required between lyric and melody, which relies on specific alignment modeling. In this paper, we propose SongMASS to address the above challenges, which leverages masked sequence to sequence (MASS) pre-training and attention based alignment modeling for lyric-to-melody and melody-to-lyric generation. Specifically, 1) we extend the original sentence-level MASS pre-training to song level to better capture long contextual information in music, and use a separate encoder and decoder for each modality (lyric or melody); 2) we leverage sentence-level attention mask and token-level attention constraint during training to enhance the alignment between lyric and melody. During inference, we use a dynamic programming strategy to obtain the alignment between each word/syllable in lyric and note in melody. We pre-train SongMASS on unpaired lyric and melody datasets, and both objective and subjective evaluations demonstrate that SongMASS generates lyric and melody with significantly better quality than the baseline method without pre-training or alignment constraint.

| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.05168](https://arxiv.org/abs/2012.05168) [cs.SD]** |
|           | (or **[arXiv:2012.05168v1](https://arxiv.org/abs/2012.05168v1) [cs.SD]** for this version) |





<h2 id="2020-12-10-2">2. Breeding Gender-aware Direct Speech Translation Systems</h2>

Title: [Breeding Gender-aware Direct Speech Translation Systems](https://arxiv.org/abs/2012.04955)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> In automatic speech translation (ST), traditional cascade approaches involving separate transcription and translation steps are giving ground to increasingly competitive and more robust direct solutions. In particular, by translating speech audio data without intermediate transcription, direct ST models are able to leverage and preserve essential information present in the input (e.g. speaker's vocal characteristics) that is otherwise lost in the cascade framework. Although such ability proved to be useful for gender translation, direct ST is nonetheless affected by gender bias just like its cascade counterpart, as well as machine translation and numerous other natural language processing applications. Moreover, direct ST systems that exclusively rely on vocal biometric features as a gender cue can be unsuitable and potentially harmful for certain users. Going beyond speech signals, in this paper we compare different approaches to inform direct ST models about the speaker's gender and test their ability to handle gender translation from English into Italian and French. To this aim, we manually annotated large datasets with speakers' gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong - but gender-unaware - direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality.

| Comments:          | Outstanding paper at COLING 2020                             |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | In Proceedings of the 28th International Conference on Computational Linguistics, Dec 2020, 3951-3964. Online |
| Cite as:           | **[arXiv:2012.04955](https://arxiv.org/abs/2012.04955) [cs.CL]** |
|                    | (or **[arXiv:2012.04955v1](https://arxiv.org/abs/2012.04955v1) [cs.CL]** for this version) |







<h2 id="2020-12-10-3">3. On Knowledge Distillation for Direct Speech Translation</h2>

Title: [On Knowledge Distillation for Direct Speech Translation](https://arxiv.org/abs/2012.04964)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Mattia A. Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Direct speech translation (ST) has shown to be a complex task requiring knowledge transfer from its sub-tasks: automatic speech recognition (ASR) and machine translation (MT). For MT, one of the most promising techniques to transfer knowledge is knowledge distillation. In this paper, we compare the different solutions to distill knowledge in a sequence-to-sequence task like ST. Moreover, we analyze eventual drawbacks of this approach and how to alleviate them maintaining the benefits in terms of translation quality.

| Comments: | Accepted at CLiC-IT 2020                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.04964](https://arxiv.org/abs/2012.04964) [cs.CL]** |
|           | (or **[arXiv:2012.04964v1](https://arxiv.org/abs/2012.04964v1) [cs.CL]** for this version) |







<h2 id="2020-12-10-4">4. Towards Zero-shot Cross-lingual Image Retrieval</h2>

Title: [Towards Zero-shot Cross-lingual Image Retrieval](https://arxiv.org/abs/2012.05107)

Authors: [Pranav Aggarwal](https://arxiv.org/search/cs?searchtype=author&query=Aggarwal%2C+P), [Ajinkya Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+A)

> There has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts from each other. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for evaluating zero-shot model performance across languages. XTD10 dataset is made publicly available here: [this https URL](https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10)

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.05107](https://arxiv.org/abs/2012.05107) [cs.CL]** |
|           | (or **[arXiv:2012.05107v1](https://arxiv.org/abs/2012.05107v1) [cs.CL]** for this version) |







# 2020-12-09

[Return to Index](#Index)



<h2 id="2020-12-09-1">1. Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization</h2>

Title: [Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization](https://arxiv.org/abs/2012.04276)

Authors: [Yinuo Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Hualei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Zeqi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Bei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Jian-Guang Lou](https://arxiv.org/search/cs?searchtype=author&query=Lou%2C+J), [Dongmei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D)

> Human intelligence exhibits compositional generalization (i.e., the capacity to understand and produce unseen combinations of seen components), but current neural seq2seq models lack such ability. In this paper, we revisit iterative back-translation, a simple yet effective semi-supervised method, to investigate whether and how it can improve compositional generalization. In this work: (1) We first empirically show that iterative back-translation substantially improves the performance on compositional generalization benchmarks (CFQ and SCAN). (2) To understand why iterative back-translation is useful, we carefully examine the performance gains and find that iterative back-translation can increasingly correct errors in pseudo-parallel data. (3) To further encourage this mechanism, we propose curriculum iterative back-translation, which better improves the quality of pseudo-parallel data, thus further improving the performance.

| Comments: | accepted in AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.04276](https://arxiv.org/abs/2012.04276) [cs.CL]** |
|           | (or **[arXiv:2012.04276v1](https://arxiv.org/abs/2012.04276v1) [cs.CL]** for this version) |





<h2 id="2020-12-09-2">2. Globetrotter: Unsupervised Multilingual Translation from Visual Alignment</h2>

Title: [Globetrotter: Unsupervised Multilingual Translation from Visual Alignment](https://arxiv.org/abs/2012.04631)

Authors: [Dídac Surís](https://arxiv.org/search/cs?searchtype=author&query=Surís%2C+D), [Dave Epstein](https://arxiv.org/search/cs?searchtype=author&query=Epstein%2C+D), [Carl Vondrick](https://arxiv.org/search/cs?searchtype=author&query=Vondrick%2C+C)

> Multi-language machine translation without parallel corpora is challenging because there is no explicit supervision between languages. Existing unsupervised methods typically rely on topological properties of the language representations. We introduce a framework that instead uses the visual modality to align multiple languages, using images as the bridge between them. We estimate the cross-modal alignment between language and images, and use this estimate to guide the learning of cross-lingual representations. Our language representations are trained jointly in one model with a single stage. Experiments with fifty-two languages show that our method outperforms baselines on unsupervised word-level and sentence-level translation using retrieval.

| Comments: | 19 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.04631](https://arxiv.org/abs/2012.04631) [cs.CL]** |
|           | (or **[arXiv:2012.04631v1](https://arxiv.org/abs/2012.04631v1) [cs.CL]** for this version) |





# 2020-12-08

[Return to Index](#Index)



<h2 id="2020-12-08-1">1. Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</h2>

Title: [Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment](https://arxiv.org/abs/2012.02813)

Authors: [Paul Pu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P), [Peter Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+P), [Liu Ziyin](https://arxiv.org/search/cs?searchtype=author&query=Ziyin%2C+L), [Louis-Philippe Morency](https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R)

> The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can (1) quickly perform new tasks in a target modality (i.e. meta-learning) and (2) doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few (1-10) labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.02813](https://arxiv.org/abs/2012.02813) [cs.LG]** |
|           | (or **[arXiv:2012.02813v1](https://arxiv.org/abs/2012.02813v1) [cs.LG]** for this version) |





<h2 id="2020-12-08-2">2. MLS: A Large-Scale Multilingual Dataset for Speech Research</h2>

Title: [MLS: A Large-Scale Multilingual Dataset for Speech Research](https://arxiv.org/abs/2012.03411)

Authors: [Vineel Pratap](https://arxiv.org/search/eess?searchtype=author&query=Pratap%2C+V), [Qiantong Xu](https://arxiv.org/search/eess?searchtype=author&query=Xu%2C+Q), [Anuroop Sriram](https://arxiv.org/search/eess?searchtype=author&query=Sriram%2C+A), [Gabriel Synnaeve](https://arxiv.org/search/eess?searchtype=author&query=Synnaeve%2C+G), [Ronan Collobert](https://arxiv.org/search/eess?searchtype=author&query=Collobert%2C+R)

> This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at [this http URL](http://www.openslr.org/).

| Subjects:          | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Interspeech 2020                                             |
| DOI:               | [10.21437/Interspeech.2020-2826](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.21437%2FInterspeech.2020-2826&v=ec9c71bb) |
| Cite as:           | **[arXiv:2012.03411](https://arxiv.org/abs/2012.03411) [eess.AS]** |
|                    | (or **[arXiv:2012.03411v1](https://arxiv.org/abs/2012.03411v1) [eess.AS]** for this version) |







<h2 id="2020-12-08-3">3. Reciprocal Supervised Learning Improves Neural Machine Translation</h2>

Title: [Reciprocal Supervised Learning Improves Neural Machine Translation](https://arxiv.org/abs/2012.02975)

Authors: [Minkai Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Zhouhan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Weinan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Despite the recent success on image classification, self-training has only achieved limited gains on structured prediction tasks such as neural machine translation (NMT). This is mainly due to the compositionality of the target space, where the far-away prediction hypotheses lead to the notorious reinforced mistake problem. In this paper, we revisit the utilization of multiple diverse models and present a simple yet effective approach named Reciprocal-Supervised Learning (RSL). RSL first exploits individual models to generate pseudo parallel data, and then cooperatively trains each model on the combined synthetic corpus. RSL leverages the fact that different parameterized models have different inductive biases, and better predictions can be made by jointly exploiting the agreement among each other. Unlike the previous knowledge distillation methods built upon a much stronger teacher, RSL is capable of boosting the accuracy of one model by introducing other comparable or even weaker models. RSL can also be viewed as a more efficient alternative to ensemble. Extensive experiments demonstrate the superior performance of RSL on several benchmarks with significant margins.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.02975](https://arxiv.org/abs/2012.02975) [cs.CL]** |
|           | (or **[arXiv:2012.02975v1](https://arxiv.org/abs/2012.02975v1) [cs.CL]** for this version) |







<h2 id="2020-12-08-4">4. Document Graph for Neural Machine Translation</h2>

Title: [Document Graph for Neural Machine Translation](https://arxiv.org/abs/2012.03477)

Authors: [Mingzhou Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Derek. F. Wai](https://arxiv.org/search/cs?searchtype=author&query=Wai%2C+D+F), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English-French, Chinese-English, WMT English-German and Opensubtitle English-Russian, demonstrate that using document graphs can significantly improve the translation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.03477](https://arxiv.org/abs/2012.03477) [cs.CL]** |
|           | (or **[arXiv:2012.03477v1](https://arxiv.org/abs/2012.03477v1) [cs.CL]** for this version) |







<h2 id="2020-12-08-5">5. KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning</h2>

Title: [KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning](https://arxiv.org/abs/2012.03551)

Authors: [Bin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+B), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Jinghui Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA (+1.26 F1) and TriviaQA (+1.56 F1) over RoBERTa.

| Comments: | 10 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.03551](https://arxiv.org/abs/2012.03551) [cs.CL]** |
|           | (or **[arXiv:2012.03551v1](https://arxiv.org/abs/2012.03551v1) [cs.CL]** for this version) |





<h2 id="2020-12-08-6">6. PPKE: Knowledge Representation Learning by Path-based Pre-training</h2>

Title: [PPKE: Knowledge Representation Learning by Path-based Pre-training](https://arxiv.org/abs/2012.03573)

Authors: [Bin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+B), [Di Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+D), [Jing Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Jinghui Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Entities may have complex interactions in a knowledge graph (KG), such as multi-step relationships, which can be viewed as graph contextual information of the entities. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.03573](https://arxiv.org/abs/2012.03573) [cs.CL]** |
|           | (or **[arXiv:2012.03573v1](https://arxiv.org/abs/2012.03573v1) [cs.CL]** for this version) |







# 2020-12-07

[Return to Index](#Index)



<h2 id="2020-12-07-1">1. A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings</h2>

Title: [A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings](https://arxiv.org/abs/2012.02221)

Authors: [Puyuan Peng](https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+P), [Herman Kamper](https://arxiv.org/search/eess?searchtype=author&query=Kamper%2C+H), [Karen Livescu](https://arxiv.org/search/eess?searchtype=author&query=Livescu%2C+K)

> We propose a new unsupervised model for mapping a variable-duration speech segment to a fixed-dimensional representation. The resulting acoustic word embeddings can form the basis of search, discovery, and indexing systems for low- and zero-resource languages. Our model, which we refer to as a maximal sampling correspondence variational autoencoder (MCVAE), is a recurrent neural network (RNN) trained with a novel self-supervised correspondence loss that encourages consistency between embeddings of different instances of the same word. Our training scheme improves on previous correspondence training approaches through the use and comparison of multiple samples from the approximate posterior distribution. In the zero-resource setting, the MCVAE can be trained in an unsupervised way, without any ground-truth word pairs, by using the word-like segments discovered via an unsupervised term discovery system. In both this setting and a semi-supervised low-resource setting (with a limited set of ground-truth word pairs), the MCVAE outperforms previous state-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.

| Comments: | 10 pages, 6 figures, NeurIPS 2020 Workshop Self-Supervised Learning for Speech and Audio Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2012.02221](https://arxiv.org/abs/2012.02221) [eess.AS]** |
|           | (or **[arXiv:2012.02221v1](https://arxiv.org/abs/2012.02221v1) [eess.AS]** for this version) |





<h2 id="2020-12-07-2">2. Self-Supervised VQA: Answering Visual Questions using Images and Captions
</h2>

Title: [Self-Supervised VQA: Answering Visual Questions using Images and Captions](https://arxiv.org/abs/2012.02356)

Authors: [Pratyay Banerjee](https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+P), [Tejas Gokhale](https://arxiv.org/search/cs?searchtype=author&query=Gokhale%2C+T), [Yezhou Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Chitta Baral](https://arxiv.org/search/cs?searchtype=author&query=Baral%2C+C)

> Methodologies for training VQA models assume the availability of datasets with human-annotated Image-Question-Answer(I-Q-A) triplets for training. This has led to a heavy reliance and overfitting on datasets and a lack of generalization to new types of questions and scenes. Moreover, these datasets exhibit annotator subjectivity, biases, and errors, along with linguistic priors, which percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions which are descriptive and less subjective. We present a method to train models with procedurally generated Q-A pairs from captions using techniques, such as templates and annotation frameworks like QASRL. As most VQA models rely on dense and costly object annotations extracted from object detectors, we propose spatial-pyramid image patches as a simple but effective alternative to object bounding boxes, and demonstrate that our method uses fewer human annotations. We benchmark on VQA-v2, GQA, and on VQA-CP which contains a softer version of label shift. Our methods surpass prior supervised methods on VQA-CP and are competitive with methods without object features in fully supervised setting.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.02356](https://arxiv.org/abs/2012.02356) [cs.CV]** |
|           | (or **[arXiv:2012.02356v1](https://arxiv.org/abs/2012.02356v1) [cs.CV]** for this version) |





<h2 id="2020-12-07-3">3. Accurate and Scalable Matching of Translators to Displaced Persons for Overcoming Language Barriers</h2>

Title: [Accurate and Scalable Matching of Translators to Displaced Persons for Overcoming Language Barriers](https://arxiv.org/abs/2012.02595)

Authors: [Divyansh Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+D), [Yuta Baba](https://arxiv.org/search/cs?searchtype=author&query=Baba%2C+Y), [Pratik Sachdeva](https://arxiv.org/search/cs?searchtype=author&query=Sachdeva%2C+P), [Tanya Tandon](https://arxiv.org/search/cs?searchtype=author&query=Tandon%2C+T), [Thomas Vetterli](https://arxiv.org/search/cs?searchtype=author&query=Vetterli%2C+T), [Aziz Alghunaim](https://arxiv.org/search/cs?searchtype=author&query=Alghunaim%2C+A)

> Residents of developing countries are disproportionately susceptible to displacement as a result of humanitarian crises. During such crises, language barriers impede aid workers in providing services to those displaced. To build resilience, such services must be flexible and robust to a host of possible languages. \textit{Tarjimly} aims to overcome the barriers by providing a platform capable of matching bilingual volunteers to displaced persons or aid workers in need of translating. However, Tarjimly's large pool of translators comes with the challenge of selecting the right translator per request. In this paper, we describe a machine learning system that matches translator requests to volunteers at scale. We demonstrate that a simple logistic regression, operating on easily computable features, can accurately predict and rank translator response. In deployment, this lightweight system matches 82\% of requests with a median response time of 59 seconds, allowing aid workers to accelerate their services supporting displaced persons.

| Comments: | Presented at NeurIPS 2020 Workshop on Machine Learning for the Developing World |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.02595](https://arxiv.org/abs/2012.02595) [cs.CY]** |
|           | (or **[arXiv:2012.02595v1](https://arxiv.org/abs/2012.02595v1) [cs.CY]** for this version) |





<h2 id="2020-12-07-4">4. A Benchmark Dataset for Understandable Medical Language Translation</h2>

Title: [A Benchmark Dataset for Understandable Medical Language Translation](https://arxiv.org/abs/2012.02420)

Authors: [Junyu Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Zifei Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Hanzhong Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Muchao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+M), [Yaqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Quanzeng You](https://arxiv.org/search/cs?searchtype=author&query=You%2C+Q), [Cao Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Fenglong Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+F)

> In this paper, we introduce MedLane -- a new human-annotated Medical Language translation dataset, to align professional medical sentences with layperson-understandable expressions. The dataset contains 12,801 training samples, 1,015 validation samples, and 1,016 testing samples. We then evaluate one naive and six deep learning-based approaches on the MedLane dataset, including directly copying, a statistical machine translation approach Moses, four neural machine translation approaches (i.e., the proposed PMBERT-MT model, Seq2Seq and its two variants), and a modified text summarization model PointerNet. To compare the results, we utilize eleven metrics, including three new measures specifically designed for this task. Finally, we discuss the limitations of MedLane and baselines, and point out possible research directions for this task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.02420](https://arxiv.org/abs/2012.02420) [cs.CL]** |
|           | (or **[arXiv:2012.02420v1](https://arxiv.org/abs/2012.02420v1) [cs.CL]** for this version) |





<h2 id="2020-12-07-5">5. Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning</h2>

Title: [Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning](https://arxiv.org/abs/2012.02462)

Authors: [Daniel Grießhaber](https://arxiv.org/search/cs?searchtype=author&query=Grießhaber%2C+D), [Johannes Maucher](https://arxiv.org/search/cs?searchtype=author&query=Maucher%2C+J), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT -- a pre-trained Transformer based language model -- by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.

| Comments: | COLING'2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.02462](https://arxiv.org/abs/2012.02462) [cs.CL]** |
|           | (or **[arXiv:2012.02462v1](https://arxiv.org/abs/2012.02462v1) [cs.CL]** for this version) |





# 2020-12-04

[Return to Index](#Index)



<h2 id="2020-12-04-1">1. SemMT: A Semantic-based Testing Approach for Machine Translation Systems</h2>

Title: [SemMT: A Semantic-based Testing Approach for Machine Translation Systems](https://arxiv.org/abs/2012.01815)

Authors: [Jialun Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Meiziniu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Yeting Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Ming Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+M), [Shing-Chi Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+S)

> Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., the distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and translated sentences have the same meaning (i.e., Semantic similarity). Therefore, in this paper, we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and translated sentences. Our insight is that the semantics expressed by the logic and numeric constraint in sentences can be captured using regular expressions (or deterministic finite automata) where efficient equivalence/similarity checking algorithms are available. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. The experiment result reveals SemMT can achieve higher effectiveness compared with state-of-the-art works, achieving an increase of 21% and 23% on accuracy and F-Score, respectively. We also explore potential improvements that can be achieved when proper combinations of metrics are adopted. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which may shed lights on further exploration.

| Subjects: | **Software Engineering (cs.SE)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.01815](https://arxiv.org/abs/2012.01815) [cs.SE]** |
|           | (or **[arXiv:2012.01815v1](https://arxiv.org/abs/2012.01815v1) [cs.SE]** for this version) |





<h2 id="2020-12-04-2">2. Self-Explaining Structures Improve NLP Models</h2>

Title: [Self-Explaining Structures Improve NLP Models](https://arxiv.org/abs/2012.01786)

Authors: [Zijun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Chun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Qinghong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Existing approaches to explaining deep learning models in NLP usually suffer from two major drawbacks: (1) the main model and the explaining model are decoupled: an additional probing or surrogate model is used to interpret an existing model, and thus existing explaining tools are not self-explainable; (2) the probing model is only able to explain a model's predictions by operating on low-level features by computing saliency scores for individual words but are clumsy at high-level text units such as phrases, sentences, or paragraphs. To deal with these two issues, in this paper, we propose a simple yet general and effective self-explaining framework for deep learning models in NLP. The key point of the proposed framework is to put an additional layer, as is called by the interpretation layer, on top of any existing NLP model. This layer aggregates the information for each text span, which is then associated with a specific weight, and their weighted combination is fed to the softmax function for the final prediction. The proposed model comes with the following merits: (1) span weights make the model self-explainable and do not require an additional probing model for interpretation; (2) the proposed model is general and can be adapted to any existing deep learning structures in NLP; (3) the weight associated with each text span provides direct importance scores for higher-level text units such as phrases and sentences. We for the first time show that interpretability does not come at the cost of performance: a neural model of self-explaining features obtains better performances than its counterpart without the self-explaining nature, achieving a new SOTA performance of 59.1 on SST-5 and a new SOTA performance of 92.3 on SNLI.

| Comments: | Code is available at [this https URL](https://github.com/ShannonAI/Self_Explaining_Structures_Improve_NLP_Models) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.01786](https://arxiv.org/abs/2012.01786) [cs.CL]** |
|           | (or **[arXiv:2012.01786v1](https://arxiv.org/abs/2012.01786v1) [cs.CL]** for this version) |





<h2 id="2020-12-04-3">3. On Extending NLP Techniques from the Categorical to the Latent Space: KL Divergence, Zipf's Law, and Similarity Search</h2>

Title: [On Extending NLP Techniques from the Categorical to the Latent Space: KL Divergence, Zipf's Law, and Similarity Search](https://arxiv.org/abs/2012.01941)

Authors: [Adam Hare](https://arxiv.org/search/cs?searchtype=author&query=Hare%2C+A), [Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yinan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Zhenming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Christopher G. Brinton](https://arxiv.org/search/cs?searchtype=author&query=Brinton%2C+C+G)

> Despite the recent successes of deep learning in natural language processing (NLP), there remains widespread usage of and demand for techniques that do not rely on machine learning. The advantage of these techniques is their interpretability and low cost when compared to frequently opaque and expensive machine learning models. Although they may not be be as performant in all cases, they are often sufficient for common and relatively simple problems. In this paper, we aim to modernize these older methods while retaining their advantages by extending approaches from categorical or bag-of-words representations to word embeddings representations in the latent space. First, we show that entropy and Kullback-Leibler divergence can be efficiently estimated using word embeddings and use this estimation to compare text across several categories. Next, we recast the heavy-tailed distribution known as Zipf's law that is frequently observed in the categorical space to the latent space. Finally, we look to improve the Jaccard similarity measure for sentence suggestion by introducing a new method of identifying similar sentences based on the set cover problem. We compare the performance of this algorithm against several baselines including Word Mover's Distance and the Levenshtein distance.

| Comments: | 13 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.01941](https://arxiv.org/abs/2012.01941) [cs.CL]** |
|           | (or **[arXiv:2012.01941v1](https://arxiv.org/abs/2012.01941v1) [cs.CL]** for this version) |









# 2020-12-03

[Return to Index](#Index)



<h2 id="2020-12-03-1">1. Evaluating Explanations: How much do explanations from the teacher aid students?</h2>

Title: [Evaluating Explanations: How much do explanations from the teacher aid students?](https://arxiv.org/abs/2012.00893)

Authors: [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Bhuwan Dhingra](https://arxiv.org/search/cs?searchtype=author&query=Dhingra%2C+B), [Livio Baldini Soares](https://arxiv.org/search/cs?searchtype=author&query=Soares%2C+L+B), [Michael Collins](https://arxiv.org/search/cs?searchtype=author&query=Collins%2C+M), [Zachary C. Lipton](https://arxiv.org/search/cs?searchtype=author&query=Lipton%2C+Z+C), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [William W. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W)

> While many methods purport to explain predictions by highlighting salient features, what precise aims these explanations serve and how to evaluate their utility are often unstated. In this work, we formalize the value of explanations using a student-teacher paradigm that measures the extent to which explanations improve student models in learning to simulate the teacher model on unseen examples for which explanations are unavailable. Student models incorporate explanations in training (but not prediction) procedures. Unlike many prior proposals to evaluate explanations, our approach cannot be easily gamed, enabling principled, scalable, and automatic evaluation of attributions. Using our framework, we compare multiple attribution methods and observe consistent and quantitative differences amongst them across multiple learning strategies.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.00893](https://arxiv.org/abs/2012.00893) [cs.CL]** |
|           | (or **[arXiv:2012.00893v1](https://arxiv.org/abs/2012.00893v1) [cs.CL]** for this version) |





<h2 id="2020-12-03-2">2. How Can We Know When Language Models Know?</h2>

Title: [How Can We Know When Language Models Know?](https://arxiv.org/abs/2012.00955)

Authors: [Zhengbao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Jun Araki](https://arxiv.org/search/cs?searchtype=author&query=Araki%2C+J), [Haibo Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+H), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question "how can we know when language models know, with confidence, the answer to a particular query?" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probability of correctness. We first examine a state-of-the-art generative QA model, T5, and examine whether its probabilities are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.00955](https://arxiv.org/abs/2012.00955) [cs.CL]** |
|           | (or **[arXiv:2012.00955v1](https://arxiv.org/abs/2012.00955v1) [cs.CL]** for this version) |





<h2 id="2020-12-03-3">3. Interactive Teaching for Conversational AI</h2>

Title: [Interactive Teaching for Conversational AI](https://arxiv.org/abs/2012.00958)

Authors: [Qing Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+Q), [Feiyang Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+F), [Govind Thattai](https://arxiv.org/search/cs?searchtype=author&query=Thattai%2C+G), [Joel Chengottusseriyil](https://arxiv.org/search/cs?searchtype=author&query=Chengottusseriyil%2C+J), [Qiaozi Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Q), [Aishwarya Reganti](https://arxiv.org/search/cs?searchtype=author&query=Reganti%2C+A), [Prashanth Rajagopal](https://arxiv.org/search/cs?searchtype=author&query=Rajagopal%2C+P), [Gokhan Tur](https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G), [Dilek Hakkani-Tur](https://arxiv.org/search/cs?searchtype=author&query=Hakkani-Tur%2C+D), [Prem Nataraja](https://arxiv.org/search/cs?searchtype=author&query=Nataraja%2C+P)

> Current conversational AI systems aim to understand a set of pre-designed requests and execute related actions, which limits them to evolve naturally and adapt based on human interactions. Motivated by how children learn their first language interacting with adults, this paper describes a new Teachable AI system that is capable of learning new language nuggets called concepts, directly from end users using live interactive teaching sessions. The proposed setup uses three models to: a) Identify gaps in understanding automatically during live conversational interactions, b) Learn the respective interpretations of such unknown concepts from live interactions with users, and c) Manage a classroom sub-dialogue specifically tailored for interactive teaching sessions. We propose state-of-the-art transformer based neural architectures of models, fine-tuned on top of pre-trained models, and show accuracy improvements on the respective components. We demonstrate that this method is very promising in leading way to build more adaptive and personalized language understanding models.

| Comments: | Accepted at Human in the Loop Dialogue Systems Workshop @NeurIPS 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.00958](https://arxiv.org/abs/2012.00958) [cs.CL]** |
|           | (or **[arXiv:2012.00958v1](https://arxiv.org/abs/2012.00958v1) [cs.CL]** for this version) |







# 2020-12-02

[Return to Index](#Index)



<h2 id="2020-12-02-1">1. Modifying Memories in Transformer Models</h2>

Title: [Modifying Memories in Transformer Models](https://arxiv.org/abs/2012.00363)

Authors: [Chen Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Ankit Singh Rawat](https://arxiv.org/search/cs?searchtype=author&query=Rawat%2C+A+S), [Manzil Zaheer](https://arxiv.org/search/cs?searchtype=author&query=Zaheer%2C+M), [Srinadh Bhojanapalli](https://arxiv.org/search/cs?searchtype=author&query=Bhojanapalli%2C+S), [Daliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Felix Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+F), [Sanjiv Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S)

> Large Transformer models have achieved impressive performance in many natural language tasks. In particular, Transformer based language models have been shown to have great capabilities in encoding factual knowledge in their vast amount of parameters. While the tasks of improving the memorization and generalization of Transformers have been widely studied, it is not well known how to make transformers forget specific old facts and memorize new ones. In this paper, we propose a new task of \emph{explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts}. This task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and eliminating unintended biases stored in the models. We benchmarked several approaches that provide natural baseline performances on this task. This leads to the discovery of key components of a Transformer model that are especially effective for knowledge modifications. The work also provides insights into the role that different training phases (such as pretraining and fine-tuning) play towards memorization and knowledge modification.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.00363](https://arxiv.org/abs/2012.00363) [cs.CL]** |
|           | (or **[arXiv:2012.00363v1](https://arxiv.org/abs/2012.00363v1) [cs.CL]** for this version) |





<h2 id="2020-12-02-2">2. An Enhanced Knowledge Injection Model for Commonsense Generation</h2>

Title: [An Enhanced Knowledge Injection Model for Commonsense Generation](https://arxiv.org/abs/2012.00366)

Authors: [Zhihao Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Z), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Zhongyu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Siyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yameng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Jian Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Ruofei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R)

> Commonsense generation aims at generating plausible everyday scenario description based on a set of provided concepts. Digging the relationship of concepts from scratch is non-trivial, therefore, we retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation. We integrate two additional modules, namely position indicator and scaling module, into the pretrained encoder-decoder model for prototype modeling to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, and experimental results show that our method significantly improves the performance on all the metrics.

| Comments: | Accepted to COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.00366](https://arxiv.org/abs/2012.00366) [cs.CL]** |
|           | (or **[arXiv:2012.00366v1](https://arxiv.org/abs/2012.00366v1) [cs.CL]** for this version) |





<h2 id="2020-12-02-3">3. CPM: A Large-scale Generative Chinese Pre-trained Language Model</h2>

Title: [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)

Authors: [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Pei Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+P), [Yuxian Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y), [Deming Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+D), [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Haozhe Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Jian Guan](https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+J), [Fanchao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yanan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Guoyang Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+G), [Huanqi Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+H), [Shengqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Daixuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Zhenbo Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M), [Wentao Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+W), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J), [Juanzi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xiaoyan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at [this https URL](https://github.com/TsinghuaAI/CPM-Generate).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.00413](https://arxiv.org/abs/2012.00413) [cs.CL]** |
|           | (or **[arXiv:2012.00413v1](https://arxiv.org/abs/2012.00413v1) [cs.CL]** for this version) |





<h2 id="2020-12-02-4">4. Extracting Synonyms from Bilingual Dictionaries</h2>

Title: [Extracting Synonyms from Bilingual Dictionaries](https://arxiv.org/abs/2012.00600)

Authors: [Mustafa Jarrar](https://arxiv.org/search/cs?searchtype=author&query=Jarrar%2C+M), [Eman Karajah](https://arxiv.org/search/cs?searchtype=author&query=Karajah%2C+E), [Muhammad Khalifa](https://arxiv.org/search/cs?searchtype=author&query=Khalifa%2C+M), [Khaled Shaalan](https://arxiv.org/search/cs?searchtype=author&query=Shaalan%2C+K)

> We present our progress in developing a novel algorithm to extract synonyms from bilingual dictionaries. Identification and usage of synonyms play a significant role in improving the performance of information access applications. The idea is to construct a translation graph from translation pairs, then to extract and consolidate cyclic paths to form bilingual sets of synonyms. The initial evaluation of this algorithm illustrates promising results in extracting Arabic-English bilingual synonyms. In the evaluation, we first converted the synsets in the Arabic WordNet into translation pairs (i.e., losing word-sense memberships). Next, we applied our algorithm to rebuild these synsets. We compared the original and extracted synsets obtaining an F-Measure of 82.3% and 82.1% for Arabic and English synsets extraction, respectively.

| Comments: | In Proceedings - 11th International Global Wordnet Conference (GWC2021). Global Wordnet Association (2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2012.00600](https://arxiv.org/abs/2012.00600) [cs.CL]** |
|           | (or **[arXiv:2012.00600v1](https://arxiv.org/abs/2012.00600v1) [cs.CL]** for this version) |





<h2 id="2020-12-02-5">5. Intrinsic analysis for dual word embedding space models</h2>

Title: [Intrinsic analysis for dual word embedding space models](https://arxiv.org/abs/2012.00728)

Authors: [Mohit Mayank](https://arxiv.org/search/cs?searchtype=author&query=Mayank%2C+M)

> Recent word embeddings techniques represent words in a continuous vector space, moving away from the atomic and sparse representations of the past. Each such technique can further create multiple varieties of embeddings based on different settings of hyper-parameters like embedding dimension size, context window size and training method. One additional variety appears when we especially consider the Dual embedding space techniques which generate not one but two-word embeddings as output. This gives rise to an interesting question - "is there one or a combination of the two word embeddings variety, which works better for a specific task?". This paper tries to answer this question by considering all of these variations. Herein, we compare two classical embedding methods belonging to two different methodologies - Word2Vec from window-based and Glove from count-based. For an extensive evaluation after considering all variations, a total of 84 different models were compared against semantic, association and analogy evaluations tasks which are made up of 9 open-source linguistics datasets. The final Word2vec reports showcase the preference of non-default model for 2 out of 3 tasks. In case of Glove, non-default models outperform in all 3 evaluation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.00728](https://arxiv.org/abs/2012.00728) [cs.CL]** |
|           | (or **[arXiv:2012.00728v1](https://arxiv.org/abs/2012.00728v1) [cs.CL]** for this version) |





# 2020-12-01

[Return to Index](#Index)



<h2 id="2020-12-01-1">1. EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP</h2>

Title: [EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP](https://arxiv.org/abs/2011.14203)

Authors: [Thierry Tambe](https://arxiv.org/search/cs?searchtype=author&query=Tambe%2C+T), [Coleman Hooper](https://arxiv.org/search/cs?searchtype=author&query=Hooper%2C+C), [Lillian Pentecost](https://arxiv.org/search/cs?searchtype=author&query=Pentecost%2C+L), [En-Yu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+E), [Marco Donato](https://arxiv.org/search/cs?searchtype=author&query=Donato%2C+M), [Victor Sanh](https://arxiv.org/search/cs?searchtype=author&query=Sanh%2C+V), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M), [David Brooks](https://arxiv.org/search/cs?searchtype=author&query=Brooks%2C+D), [Gu-Yeon Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+G)

> Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements.
> We present EdgeBERT an in-depth and principled algorithm and hardware design methodology to achieve minimal latency and energy consumption on multi-task NLP inference. Compared to the ALBERT baseline, we achieve up to 2.4x and 13.4x inference latency and memory savings, respectively, with less than 1%-pt drop in accuracy on several GLUE benchmarks by employing a calibrated combination of 1) entropy-based early stopping, 2) adaptive attention span, 3) movement and magnitude pruning, and 4) floating-point quantization.
> Furthermore, in order to maximize the benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a scalable hardware architecture wherein floating-point bit encodings of the shareable multi-task embedding parameters are stored in high-density non-volatile memory. Altogether, EdgeBERT enables fully on-chip inference acceleration of NLP workloads with 5.2x, and 157x lower energy than that of an un-optimized accelerator and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.

| Comments: | 11 pages plus references                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Hardware Architecture (cs.AR)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2011.14203](https://arxiv.org/abs/2011.14203) [cs.AR]** |
|           | (or **[arXiv:2011.14203v1](https://arxiv.org/abs/2011.14203v1) [cs.AR]** for this version) |





<h2 id="2020-12-01-2">2. Understanding How BERT Learns to Identify Edits</h2>

Title: [Understanding How BERT Learns to Identify Edits](https://arxiv.org/abs/2011.14039)

Authors: [Samuel Stevens](https://arxiv.org/search/cs?searchtype=author&query=Stevens%2C+S), [Yu Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y)

> Pre-trained transformer language models such as BERT are ubiquitous in NLP research, leading to work on understanding how and why these models work. Attention mechanisms have been proposed as a means of interpretability with varying conclusions. We propose applying BERT-based models to a sequence classification task and using the data set's labeling schema to measure each model's interpretability. We find that classification performance scores do not always correlate with interpretability. Despite this, BERT's attention weights are interpretable for over 70% of examples.

| Comments: | 8 pages, 11 figures. A work in progress                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.14039](https://arxiv.org/abs/2011.14039) [cs.CL]** |
|           | (or **[arXiv:2011.14039v1](https://arxiv.org/abs/2011.14039v1) [cs.CL]** for this version) |







<h2 id="2020-12-01-3">3. Using Multiple Subwords to Improve English-Esperanto Automated Literary Translation Quality</h2>

Title: [Using Multiple Subwords to Improve English-Esperanto Automated Literary Translation Quality](https://arxiv.org/abs/2011.14190)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Jan Buts](https://arxiv.org/search/cs?searchtype=author&query=Buts%2C+J), [James Hadley](https://arxiv.org/search/cs?searchtype=author&query=Hadley%2C+J), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

> Building Machine Translation (MT) systems for low-resource languages remains challenging. For many language pairs, parallel data are not widely available, and in such cases MT models do not achieve results comparable to those seen with high-resource languages.
> When data are scarce, it is of paramount importance to make optimal use of the limited material available. To that end, in this paper we propose employing the same parallel sentences multiple times, only changing the way the words are split each time. For this purpose we use several Byte Pair Encoding models, with various merge operations used in their configuration.
> In our experiments, we use this technique to expand the available data and improve an MT system involving a low-resource language pair, namely English-Esperanto.
> As an additional contribution, we made available a set of English-Esperanto parallel data in the literary domain.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | The 3rd Workshop on Technologies for MT of Low Resource Languages (LoResMT 2020) |
| Cite as:           | **[arXiv:2011.14190](https://arxiv.org/abs/2011.14190) [cs.CL]** |
|                    | (or **[arXiv:2011.14190v1](https://arxiv.org/abs/2011.14190v1) [cs.CL]** for this version) |







<h2 id="2020-12-01-4">4. Intrinsic Knowledge Evaluation on Chinese Language Models</h2>

Title: [Intrinsic Knowledge Evaluation on Chinese Language Models](https://arxiv.org/abs/2011.14277)

Authors: [Zhiruo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Renfen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+R)

> Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of 39,308 questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at [this https URL](https://github.com/ZhiruoWang/ChnEval).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.14277](https://arxiv.org/abs/2011.14277) [cs.CL]** |
|           | (or **[arXiv:2011.14277v1](https://arxiv.org/abs/2011.14277v1) [cs.CL]** for this version) |







<h2 id="2020-12-01-5">5. Dynamic Curriculum Learning for Low-Resource Neural Machine Translation</h2>

Title: [Dynamic Curriculum Learning for Low-Resource Neural Machine Translation](https://arxiv.org/abs/2011.14608)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Bojie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Kai Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+K), [Zeyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Shen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Qi Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Large amounts of data has made neural machine translation (NMT) a big success in recent years. But it is still a challenge if we train these models on small-scale corpora. In this case, the way of using data appears to be more important. Here, we investigate the effective use of training data for low-resource NMT. In particular, we propose a dynamic curriculum learning (DCL) method to reorder training samples in training. Unlike previous work, we do not use a static scoring function for reordering. Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence. This eases training by highlighting easy samples that the current model has enough competence to learn. We test our DCL method in a Transformer-based system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT' 16 En-De.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.14608](https://arxiv.org/abs/2011.14608) [cs.CL]** |
|           | (or **[arXiv:2011.14608v1](https://arxiv.org/abs/2011.14608v1) [cs.CL]** for this version) |







<h2 id="2020-12-01-6">6. A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction</h2>

Title: [A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction](https://arxiv.org/abs/2011.14874)

Authors: [Yanyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yingfeng Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y), [Ye Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Huizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Unsupervised Bilingual Dictionary Induction methods based on the initialization and the self-learning have achieved great success in similar language pairs, e.g., English-Spanish. But they still fail and have an accuracy of 0% in many distant language pairs, e.g., English-Japanese. In this work, we show that this failure results from the gap between the actual initialization performance and the minimum initialization performance for the self-learning to succeed. We propose Iterative Dimension Reduction to bridge this gap. Our experiments show that this simple method does not hamper the performance of similar language pairs and achieves an accuracy of 13.64~55.53% between English and four distant languages, i.e., Chinese, Japanese, Vietnamese and Thai.

| Comments: | Accepted by COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.14874](https://arxiv.org/abs/2011.14874) [cs.CL]** |
|           | (or **[arXiv:2011.14874v1](https://arxiv.org/abs/2011.14874v1) [cs.CL]** for this version) |







<h2 id="2020-12-01-7">7. Machine Translation of Novels in the Age of Transformer</h2>

Title: [Machine Translation of Novels in the Age of Transformer](https://arxiv.org/abs/2011.14979)

Authors: [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A), [Antoni Oliver](https://arxiv.org/search/cs?searchtype=author&query=Oliver%2C+A), [Pau Ribas Ballestín](https://arxiv.org/search/cs?searchtype=author&query=Ballestín%2C+P+R)

> In this chapter we build a machine translation (MT) system tailored to the literary domain, specifically to novels, based on the state-of-the-art architecture in neural MT (NMT), the Transformer (Vaswani et al., 2017), for the translation direction English-to-Catalan. Subsequently, we assess to what extent such a system can be useful by evaluating its translations, by comparing this MT system against three other systems (two domain-specific systems under the recurrent and phrase-based paradigms and a popular generic on-line system) on three evaluations. The first evaluation is automatic and uses the most-widely used automatic evaluation metric, BLEU. The two remaining evaluations are manual and they assess, respectively, preference and amount of post-editing required to make the translation error-free. As expected, the domain-specific Transformer-based system outperformed the three other systems in all the three evaluations conducted, in all cases by a large margin.

| Comments: | Chapter published in the book Maschinelle Übersetzung für Übersetzungsprofis (pp. 276-295). Jörg Porsiel (Ed.), BDÜ Fachverlag, 2020. ISBN 978-3-946702-09-2 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.14979](https://arxiv.org/abs/2011.14979) [cs.CL]** |
|           | (or **[arXiv:2011.14979v1](https://arxiv.org/abs/2011.14979v1) [cs.CL]** for this version) |





<h2 id="2020-12-01-8">8. Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs</h2>

Title: [Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs](https://arxiv.org/abs/2011.15124)

Authors: [Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N), [Desmond Elliott](https://arxiv.org/search/cs?searchtype=author&query=Elliott%2C+D)

> Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorized into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five V&L BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.15124](https://arxiv.org/abs/2011.15124) [cs.CL]** |
|           | (or **[arXiv:2011.15124v1](https://arxiv.org/abs/2011.15124v1) [cs.CL]** for this version) |



