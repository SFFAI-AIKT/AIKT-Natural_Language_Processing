# Daily arXiv: Machine Translation - October, 2020

# Index


- [2020-11-27](#2020-11-27)

  - [1. Enhancing deep neural networks with morphological information](#2020-11-27-1)
- [2020-11-25](#2020-11-25)

  - [1. Multimodal Pretraining for Dense Video Captioning](#2020-11-25-1)
  - [2. GLGE: A New General Language Generation Evaluation Benchmark](#2020-11-25-2)
  - [3. Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional Translation Modeling using a Two-Dimensional Grid](#2020-11-25-3)
  - [4. Tight Integrated End-to-End Training for Cascaded Speech Translation](#2020-11-25-4)
- [2020-11-24](#2020-11-24)
- [1. Hierachical Delta-Attention Method for Multimodal Fusion](#2020-11-24-1)
  - [2. Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model](#2020-11-24-2)
- [2020-11-20](#2020-11-20)

  - [1. Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video](#2020-11-20-1)
- [2020-11-19](#2020-11-19)

  - [1. The Ubiqus English-Inuktitut System for WMT20](#2020-11-19-1)
  - [2. Master Thesis: Neural Sign Language Translation by Learning Tokenization](#2020-11-19-2)
  - [3. EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications](#2020-11-19-3)
- [2020-11-18](#2020-11-18)

  - [1. Facebook AI's WMT20 News Translation Task Submission](#2020-11-18-1)
  - [2. MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining](#2020-11-18-2)
- [2020-11-17](#2020-11-17)

  - [1. Iterative Self-Learning for Enhanced Back-Translation in Low Resource Neural Machine Translation](#2020-11-17-1)
  - [2. Morphologically Aware Word-Level Translation](#2020-11-17-2)
  - [3. Learning from Task Descriptions](#2020-11-17-3)
- [2020-11-16](#2020-11-16)

  - [1. Context-aware Stand-alone Neural Spelling Correction](#2020-11-16-1)
  - [2. RethinkCWS: Is Chinese Word Segmentation a Solved Task?](#2020-11-16-2)
  - [3. EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints](#2020-11-16-3)
  - [4. Deconstructing word embedding algorithms](#2020-11-16-4)
- [2020-11-13](#2020-11-13)

  - [1. Incorporating a Local Translation Mechanism into Non-autoregressive Translation](#2020-11-13-1)
- [2020-11-12](#2020-11-12)

  - [1. OCR Post Correction for Endangered Language Texts](#2020-11-12-1)
  - [2. From Unsupervised Machine Translation To Adversarial Text Generation](#2020-11-12-2)
  - [3.On the Sentence Embeddings from Pre-trained Language Models](#2020-11-12-3)
  - [4. The Impact of Text Presentation on Translator Performance](#2020-11-12-4)
- [2020-11-11](#2020-11-11)

  - [1. Simultaneous Speech-to-Speech Translation System with Neural Incremental ASR, MT, and TTS](#2020-11-11-1)
  - [2. When Do You Need Billions of Words of Pretraining Data?](#2020-11-11-2)
  - [3. To What Degree Can Language Borders Be Blurred In BERT-based Multilingual Spoken Language Understanding?](#2020-11-11-3)
  - [4. Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers](#2020-11-11-4)
  - [5. Multi-Task Sequence Prediction For Tunisian Arabizi Multi-Level Annotation](#2020-11-11-5)
  - [6. UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations](#2020-11-11-6)
  - [7. Towards Interpretable Natural Language Understanding with Explanations as Latent Variables](#2020-11-11-7)
- [2020-11-10](#2020-11-10)

  - [1. Long Range Arena: A Benchmark for Efficient Transformers](#2020-11-10-1)
  - [2. CxGBERT: BERT meets Construction Grammar](#2020-11-10-2)
  - [3. BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention](#2020-11-10-3)
  - [4. Hierarchical Multitask Learning Approach for BERT](#2020-11-10-4)
  - [5. VisBERT: Hidden-State Visualizations for Transformers](#2020-11-10-5)
- [2020-11-09](#2020-11-09)

  - [1. Improving RNN Transducer Based ASR with Auxiliary Tasks](#2020-11-09-1)
  - [2. Semi-Supervised Low-Resource Style Transfer of Indonesian Informal to Formal Language with Iterative Forward-Translation](#2020-11-09-2)
  - [3. Understanding Pure Character-Based Neural Machine Translation: The Case of Translating Finnish into English](#2020-11-09-3)
  - [4. An Unsupervised method for OCR Post-Correction and Spelling Normalisation for Finnish](#2020-11-09-4)
- [2020-11-06](#2020-11-06)

  - [1. Data Augmentation and Terminology Integration for Domain-Specific Sinhala-English-Tamil Statistical Machine Translation](#2020-11-06-1)
- [2020-11-05](#2020-11-05)

  - [1. SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation](#2020-11-05-1)
  - [2. Probing Multilingual BERT for Genetic and Typological Signals](#2020-11-05-2)
  - [3. Chinese Grammatical Correction Using BERT-based Pre-trained Model](#2020-11-05-3)
  - [4. PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents](#2020-11-05-4)
  - [5. Optimizing Transformer for Low-Resource Neural Machine Translation](#2020-11-05-5)
- [2020-11-04](#2020-11-04)

  - [1. Layer-Wise Multi-View Learning for Neural Machine Translation](#2020-11-04-1)
  - [2. CharBERT: Character-aware Pre-trained Language Model](#2020-11-04-2)
  - [3. TransQuest: Translation Quality Estimation with Cross-lingual Transformers](#2020-11-04-3)
  - [4. Cross-lingual Word Embeddings beyond Zero-shot Machine Translation](#2020-11-04-4)
  - [5. Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation](#2020-11-04-5)
- [2020-11-03](#2020-11-03)

  - [1. COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning](#2020-11-03-1)
  - [2. The 2020s Political Economy of Machine Translation](#2020-11-03-2)
  - [3. Streaming Simultaneous Speech Translation with Augmented Memory Transformer](#2020-11-03-3)
  - [4. Joint Masked CPC and CTC Training for ASR](#2020-11-03-4)
  - [5. Evaluating Bias In Dutch Word Embeddings](#2020-11-03-5)
  - [6. Targeted Poisoning Attacks on Black-Box Neural Machine Translation](#2020-11-03-6)
  - [7. Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation](#2020-11-03-7)
  - [8. Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation](#2020-11-03-8)
  - [9. Context-Aware Cross-Attention for Non-Autoregressive Translation](#2020-11-03-9)
  - [10. Emergent Communication Pretraining for Few-Shot Machine Translation](#2020-11-03-10)
  - [11. How Far Does BERT Look At:Distance-based Clustering and Analysis of BERTś Attention](#2020-11-03-11)
  - [12. Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders](#2020-11-03-12)
  - [13. The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks](#2020-11-03-13)
- [2020-11-02](#2020-11-02)

  - [1. VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation](#2020-11-02-1)
  - [2. Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents](#2020-11-02-2)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2020-11-27

[Return to Index](#Index)



<h2 id="2020-11-27-1">1. Enhancing deep neural networks with morphological information</h2>

Title: [Enhancing deep neural networks with morphological information](https://arxiv.org/abs/2011.12432)

Authors:[Matej Klemen](https://arxiv.org/search/cs?searchtype=author&query=Klemen%2C+M), [Luka Krsnik](https://arxiv.org/search/cs?searchtype=author&query=Krsnik%2C+L), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> Currently, deep learning approaches are superior in natural language processing due to their ability to extract informative features and patterns from languages. Two most successful neural architectures are LSTM and transformers, the latter mostly used in the form of large pretrained language models such as BERT. While cross-lingual approaches are on the rise, a vast majority of current natural language processing techniques is designed and applied to English, and less-resourced languages are lagging behind. In morphologically rich languages, plenty of information is conveyed through changes in morphology, e.g., through different prefixes and suffixes modifying stems of words. The existing neural approaches do not explicitly use the information on word morphology. We analyze the effect of adding morphological features to LSTM and BERT models. We use three tasks available in many less-resourced languages: named entity recognition (NER), dependency parsing (DP), and comment filtering (CF). We construct sensible baselines involving LSTM and BERT models, which we adjust by adding additional input in the form of part of speech (POS) tags and universal features. We compare the obtained models across subsets of eight languages. Our results suggest that adding morphological features has mixed effects depending on the quality of features and the task. The features improve the performance of LSTM-based models on the NER and DP tasks, while they do not benefit the performance on the CF task. For BERT-based models, the added morphological features only improve the performance on DP when they are of high quality, while they do not show any practical improvement when they are predicted. As in NER and CF datasets manually checked features are not available, we only experiment with the predicted morphological features and find that they do not cause any practical improvement in performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.12432](https://arxiv.org/abs/2011.12432) [cs.CL]** |
|           | (or **[arXiv:2011.12432v1](https://arxiv.org/abs/2011.12432v1) [cs.CL]** for this version) |







# 2020-11-25

[Return to Index](#Index)



<h2 id="2020-11-25-1">1. Multimodal Pretraining for Dense Video Captioning</h2>

Title: [Multimodal Pretraining for Dense Video Captioning](https://arxiv.org/abs/2011.11760)

Authors:[Gabriel Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Bo Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+B), [Zhenhai Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Z), [Clara Rivera](https://arxiv.org/search/cs?searchtype=author&query=Rivera%2C+C), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> Learning specific hands-on skills such as cooking, car maintenance, and home repairs increasingly happens via instructional videos. The user experience with such videos is known to be improved by meta-information such as time-stamped annotations for the main steps involved. Generating such annotations automatically is challenging, and we describe here two relevant contributions. First, we construct and release a new dense video captioning dataset, Video Timeline Tags (ViTT), featuring a variety of instructional videos together with time-stamped annotations. Second, we explore several multimodal sequence-to-sequence pretraining strategies that leverage large unsupervised datasets of videos and caption-like texts. We pretrain and subsequently finetune dense video captioning models using both YouCook2 and ViTT. We show that such models generalize well and are robust over a wide variety of instructional videos.

| Comments: | AACL-IJCNLP 2020                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.11760](https://arxiv.org/abs/2011.11760) [cs.CV]** |
|           | (or **[arXiv:2011.11760v1](https://arxiv.org/abs/2011.11760v1) [cs.CV]** for this version) |





<h2 id="2020-11-25-2">2. GLGE: A New General Language Generation Evaluation Benchmark</h2>

Title: [GLGE: A New General Language Generation Evaluation Benchmark](https://arxiv.org/abs/2011.11928)

Authors:[Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Yu Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Weizhen Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W), [Hang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Jian Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jie Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Pengcheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Jiusheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Jiancheng Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J), [Ruofei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Winnie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N)

> Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress of pretraining and transfer learning in Natural Language Processing (NLP). These benchmarks mostly focus on a range of Natural Language Understanding (NLU) tasks, without considering the Natural Language Generation (NLG) models. In this paper, we present the General Language Generation Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and ProphetNet\footnote{The source code and dataset will be publicly available at [this https URL](https://github.com/microsoft/glge).

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.11928](https://arxiv.org/abs/2011.11928) [cs.CL]** |
|           | (or **[arXiv:2011.11928v1](https://arxiv.org/abs/2011.11928v1) [cs.CL]** for this version) |





<h2 id="2020-11-25-3">3. Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional Translation Modeling using a Two-Dimensional Grid</h2>

Title: [Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional Translation Modeling using a Two-Dimensional Grid](https://arxiv.org/abs/2011.12165)

Authors:[Parnia Bahar](https://arxiv.org/search/cs?searchtype=author&query=Bahar%2C+P), [Christopher Brix](https://arxiv.org/search/cs?searchtype=author&query=Brix%2C+C), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> Neural translation models have proven to be effective in capturing sufficient information from a source sentence and generating a high-quality target sentence. However, it is not easy to get the best effect for bidirectional translation, i.e., both source-to-target and target-to-source translation using a single model. If we exclude some pioneering attempts, such as multilingual systems, all other bidirectional translation approaches are required to train two individual models. This paper proposes to build a single end-to-end bidirectional translation model using a two-dimensional grid, where the left-to-right decoding generates source-to-target, and the bottom-to-up decoding creates target-to-source output. Instead of training two models independently, our approach encourages a single network to jointly learn to translate in both directions. Experiments on the WMT 2018 German↔English and Turkish↔English translation tasks show that the proposed model is capable of generating a good translation quality and has sufficient potential to direct the research.

| Comments: | 6 pages, accepted at SLT2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.12165](https://arxiv.org/abs/2011.12165) [cs.CL]** |
|           | (or **[arXiv:2011.12165v1](https://arxiv.org/abs/2011.12165v1) [cs.CL]** for this version) |





<h2 id="2020-11-25-4">4. Tight Integrated End-to-End Training for Cascaded Speech Translation</h2>

Title: [Tight Integrated End-to-End Training for Cascaded Speech Translation](https://arxiv.org/abs/2011.12167)

Authors:[Parnia Bahar](https://arxiv.org/search/cs?searchtype=author&query=Bahar%2C+P), [Tobias Bieschke](https://arxiv.org/search/cs?searchtype=author&query=Bieschke%2C+T), [Ralf Schlüter](https://arxiv.org/search/cs?searchtype=author&query=Schlüter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> A cascaded speech translation model relies on discrete and non-differentiable transcription, which provides a supervision signal from the source side and helps the transformation between source speech and target text. Such modeling suffers from error propagation between ASR and MT models. Direct speech translation is an alternative method to avoid error propagation; however, its performance is often behind the cascade system. To use an intermediate representation and preserve the end-to-end trainability, previous studies have proposed using two-stage models by passing the hidden vectors of the recognizer into the decoder of the MT model and ignoring the MT encoder. This work explores the feasibility of collapsing the entire cascade components into a single end-to-end trainable model by optimizing all parameters of ASR and MT models jointly without ignoring any learned parameters. It is a tightly integrated method that passes renormalized source word posterior distributions as a soft decision instead of one-hot vectors and enables backpropagation. Therefore, it provides both transcriptions and translations and achieves strong consistency between them. Our experiments on four tasks with different data scenarios show that the model outperforms cascade models up to 1.8% in BLEU and 2.0% in TER and is superior compared to direct models.

| Comments: | 8 pages, accepted at SLT2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.12167](https://arxiv.org/abs/2011.12167) [cs.CL]** |
|           | (or **[arXiv:2011.12167v1](https://arxiv.org/abs/2011.12167v1) [cs.CL]** for this version) |





# 2020-11-24

[Return to Index](#Index)



<h2 id="2020-11-24-1">1. Hierachical Delta-Attention Method for Multimodal Fusion</h2>

Title: [Hierachical Delta-Attention Method for Multimodal Fusion](https://arxiv.org/abs/2011.10916)

Authors: [Kunjal Panchal](https://arxiv.org/search/cs?searchtype=author&query=Panchal%2C+K)

> In vision and linguistics; the main input modalities are facial expressions, speech patterns, and the words uttered. The issue with analysis of any one mode of expression (Visual, Verbal or Vocal) is that lot of contextual information can get lost. This asks researchers to inspect multiple modalities to get a thorough understanding of the cross-modal dependencies and temporal context of the situation to analyze the expression. This work attempts at preserving the long-range dependencies within and across different modalities, which would be bottle-necked by the use of recurrent networks and adds the concept of delta-attention to focus on local differences per modality to capture the idiosyncrasy of different people. We explore a cross-attention fusion technique to get the global view of the emotion expressed through these delta-self-attended modalities, in order to fuse all the local nuances and global context together. The addition of attention is new to the multi-modal fusion field and currently being scrutinized for on what stage the attention mechanism should be used, this work achieves competitive accuracy for overall and per-class classification which is close to the current state-of-the-art with almost half number of parameters.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.10916](https://arxiv.org/abs/2011.10916) [cs.CV]** |
|           | (or **[arXiv:2011.10916v1](https://arxiv.org/abs/2011.10916v1) [cs.CV]** for this version) |





<h2 id="2020-11-24-2">2. Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model</h2>

Title: [Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model](https://arxiv.org/abs/2011.11499)

Authors:[Juntao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Ruidan He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+R), [Hai Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Hwee Tou Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+H+T), [Lidong Bing](https://arxiv.org/search/cs?searchtype=author&query=Bing%2C+L), [Rui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+R)

> Recent research indicates that pretraining cross-lingual language models on large-scale unlabeled texts yields significant performance improvements over various cross-lingual and low-resource tasks. Through training on one hundred languages and terabytes of texts, cross-lingual language models have proven to be effective in leveraging high-resource languages to enhance low-resource language processing and outperform monolingual models. In this paper, we further investigate the cross-lingual and cross-domain (CLCD) setting when a pretrained cross-lingual language model needs to adapt to new domains. Specifically, we propose a novel unsupervised feature decomposition method that can automatically extract domain-specific features and domain-invariant features from the entangled pretrained cross-lingual representations, given unlabeled raw texts in the source language. Our proposed model leverages mutual information estimation to decompose the representations computed by a cross-lingual model into domain-invariant and domain-specific parts. Experimental results show that our proposed method achieves significant performance improvements over the state-of-the-art pretrained cross-lingual language model in the CLCD setting. The source code of this paper is publicly available at [this https URL](https://github.com/lijuntaopku/UFD).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | IJCAI-PRICAI2020                                             |
| Cite as:           | **[arXiv:2011.11499](https://arxiv.org/abs/2011.11499) [cs.CL]** |
|                    | (or **[arXiv:2011.11499v1](https://arxiv.org/abs/2011.11499v1) [cs.CL]** for this version) |





# 2020-11-20

[Return to Index](#Index)



<h2 id="2020-11-20-1">1. Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video</h2>

Title: [Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video](https://arxiv.org/abs/2011.09846)

Authors: [Ben Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+B), [Necati Cihan Camgoz](https://arxiv.org/search/cs?searchtype=author&query=Camgoz%2C+N+C), [Richard Bowden](https://arxiv.org/search/cs?searchtype=author&query=Bowden%2C+R)

> To be truly understandable and accepted by Deaf communities, an automatic Sign Language Production (SLP) system must generate a photo-realistic signer. Prior approaches based on graphical avatars have proven unpopular, whereas recent neural SLP works that produce skeleton pose sequences have been shown to be not understandable to Deaf viewers.
> In this paper, we propose SignGAN, the first SLP model to produce photo-realistic continuous sign language videos directly from spoken language. We employ a transformer architecture with a Mixture Density Network (MDN) formulation to handle the translation from spoken language to skeletal pose. A pose-conditioned human synthesis model is then introduced to generate a photo-realistic sign language video from the skeletal pose sequence. This allows the photo-realistic production of sign videos directly translated from written text.
> We further propose a novel keypoint-based loss function, which significantly improves the quality of synthesized hand images, operating in the keypoint space to avoid issues caused by motion blur. In addition, we introduce a method for controllable video generation, enabling training on large, diverse sign language datasets and providing the ability to control the signer appearance at inference.
> Using a dataset of eight different sign language interpreters extracted from broadcast footage, we show that SignGAN significantly outperforms all baseline methods for quantitative metrics and human perceptual studies.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.09846](https://arxiv.org/abs/2011.09846) [cs.CV]** |
|           | (or **[arXiv:2011.09846v1](https://arxiv.org/abs/2011.09846v1) [cs.CV]** for this version) |







# 2020-11-19

[Return to Index](#Index)



<h2 id="2020-11-19-1">1. The Ubiqus English-Inuktitut System for WMT20</h2>

Title: [The Ubiqus English-Inuktitut System for WMT20](https://arxiv.org/abs/2011.09249)

Authors: [François Hernandez](https://arxiv.org/search/cs?searchtype=author&query=Hernandez%2C+F), [Vincent Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+V)

> This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.

| Comments: | System Description paper for WMT 2020 English-Inuktitut News Translation Task |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.09249](https://arxiv.org/abs/2011.09249) [cs.CL]** |
|           | (or **[arXiv:2011.09249v1](https://arxiv.org/abs/2011.09249v1) [cs.CL]** for this version) |





<h2 id="2020-11-19-2">2. Master Thesis: Neural Sign Language Translation by Learning Tokenization</h2>

Title: [Master Thesis: Neural Sign Language Translation by Learning Tokenization](https://arxiv.org/abs/2011.09289)

Authors: [Alptekin Orbay](https://arxiv.org/search/cs?searchtype=author&query=Orbay%2C+A)

> In this thesis, we propose a multitask learning based method to improve Neural Sign Language Translation (NSLT) consisting of two parts, a tokenization layer and Neural Machine Translation (NMT). The tokenization part focuses on how Sign Language (SL) videos should be represented to be fed into the other part. It has not been studied elaborately whereas NMT research has attracted several researchers contributing enormous advancements. Up to now, there are two main input tokenization levels, namely frame-level and gloss-level tokenization. Glosses are world-like intermediate presentation and unique to SLs. Therefore, we aim to develop a generic sign-level tokenization layer so that it is applicable to other domains without further effort. We begin with investigating current tokenization approaches and explain their weaknesses with several experiments. To provide a solution, we adapt Transfer Learning, Multitask Learning and Unsupervised Domain Adaptation into this research to leverage additional supervision. We succeed in enabling knowledge transfer between SLs and improve translation quality by 5 points in BLEU-4 and 8 points in ROUGE scores. Secondly, we show the effects of body parts by extensive experiments in all the tokenization approaches. Apart from these, we adopt 3D-CNNs to improve efficiency in terms of time and space. Lastly, we discuss the advantages of sign-level tokenization over gloss-level tokenization. To sum up, our proposed method eliminates the need for gloss level annotation to obtain higher scores by providing additional supervision by utilizing weak supervision sources.

| Comments: | MS Thesis Extension of the Paper with the same name          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.09289](https://arxiv.org/abs/2011.09289) [cs.CL]** |
|           | (or **[arXiv:2011.09289v1](https://arxiv.org/abs/2011.09289v1) [cs.CL]** for this version) |







<h2 id="2020-11-19-3">3. EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications</h2>

Title: [EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications](https://arxiv.org/abs/2011.09463)

Authors: [Minghui Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Hanjie Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+H), [Chengyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Cen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Yaliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Dehong Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+D), [Jun Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Jun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Wei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W)

> The literature has witnessed the success of applying deep Transfer Learning (TL) algorithms to many NLP applications, yet it is not easy to build a simple and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to make it easy to develop deep TL algorithms for NLP applications. It is built with rich API abstractions, a scalable architecture and comprehensive deep TL algorithms, to make the development of NLP applications easier. To be specific, the build-in data and model parallelism strategy shows to be 4x faster than the default distribution strategy of Tensorflow. EasyTransfer supports the mainstream pre-trained ModelZoo, including Pre-trained Language Models (PLMs) and multi-modality models. It also integrates various SOTA models for mainstream NLP applications in AppZoo, and supports mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, offline prediction, and online deployment. This system is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, and conversational question answering. Extensive experiments on real-world datasets show that EasyTransfer is suitable for online production with cutting-edge performance. The source code of EasyTransfer is released at Github ([this https URL](https://github.com/alibaba/EasyTransfer)).

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.09463](https://arxiv.org/abs/2011.09463) [cs.CL]** |
|           | (or **[arXiv:2011.09463v1](https://arxiv.org/abs/2011.09463v1) [cs.CL]** for this version) |







# 2020-11-18

[Return to Index](#Index)



<h2 id="2020-11-18-1">1. Facebook AI's WMT20 News Translation Task Submission</h2>

Title: [Facebook AI's WMT20 News Translation Task Submission](https://arxiv.org/abs/2011.08298)

Authors: [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Ann Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Mary Williamson](https://arxiv.org/search/cs?searchtype=author&query=Williamson%2C+M), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

> This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En->Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta->En and En->Ta respectively, and 27.9 and 13.0 for Iu->En and En->Iu respectively.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.08298](https://arxiv.org/abs/2011.08298) [cs.CL]** |
|           | (or **[arXiv:2011.08298v1](https://arxiv.org/abs/2011.08298v1) [cs.CL]** for this version) |





<h2 id="2020-11-18-2">2. MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining</h2>

Title: [MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining](https://arxiv.org/abs/2011.08539)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W)

> Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert \cite{devlin2018bert}, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, \emph{seg\_tok}, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that: (a) compared with char based vocabulary, \emph{seg\_tok} does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve \emph{seg\_tok}'s performances on sequence labeling tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.08539](https://arxiv.org/abs/2011.08539) [cs.CL]** |
|           | (or **[arXiv:2011.08539v1](https://arxiv.org/abs/2011.08539v1) [cs.CL]** for this version) |







# 2020-11-17

[Return to Index](#Index)



<h2 id="2020-11-17-1">1. Iterative Self-Learning for Enhanced Back-Translation in Low Resource Neural Machine Translation</h2>

Title: [Iterative Self-Learning for Enhanced Back-Translation in Low Resource Neural Machine Translation](https://arxiv.org/abs/2011.07403)

Authors: [Idris Abdulmumin](https://arxiv.org/search/cs?searchtype=author&query=Abdulmumin%2C+I), [Bashir Shehu Galadanci](https://arxiv.org/search/cs?searchtype=author&query=Galadanci%2C+B+S), [Ismaila Idris Sinan](https://arxiv.org/search/cs?searchtype=author&query=Sinan%2C+I+I)

> Many language pairs are low resource - the amount and/or quality of parallel data is not sufficient to train a neural machine translation (NMT) model which can reach an acceptable standard of accuracy. Many works have explored the use of the easier-to-get monolingual data to improve the performance of translation models in this category of languages - and even high resource languages. The most successful of such works is the back-translation - using the translations of the target language monolingual data to increase the amount of the training data. The quality of the backward model - trained on the available parallel data - has been shown to determine the performance of the back-translation approach. Many approaches have been explored to improve the performance of this model especially in low resource languages where the amount of parallel data is not sufficient to train an acceptable backward model. Among such works are the use of self-learning and the iterative back-translation. These methods were shown to perform better than the standard back-translation. This work presents the iterative self-training approach as an improvement over the self-learning approach to further enhance the performance of the backward model. Over several iterations, the synthetic data generated by the backward model is used to improve its performance through forward translation. Experiments have shown that the method outperforms both the standard back-translation and self-learning approach on IWSLT'14 English German low resource NMT. While the method also outperforms the iterative back-translation, though slightly, the number of models required to be trained is reduced exactly by the number of iterations.

| Comments: | 9 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.07403](https://arxiv.org/abs/2011.07403) [cs.CL]** |
|           | (or **[arXiv:2011.07403v1](https://arxiv.org/abs/2011.07403v1) [cs.CL]** for this version) |





<h2 id="2020-11-17-2">2. Morphologically Aware Word-Level Translation</h2>

Title: [Morphologically Aware Word-Level Translation](https://arxiv.org/abs/2011.07593)

Authors: [Paula Czarnowska](https://arxiv.org/search/cs?searchtype=author&query=Czarnowska%2C+P), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Ann Copestake](https://arxiv.org/search/cs?searchtype=author&query=Copestake%2C+A)

> We propose a novel morphologically aware probability model for bilingual lexicon induction, which jointly models lexeme translation and inflectional morphology in a structured way. Our model exploits the basic linguistic intuition that the lexeme is the key lexical unit of meaning, while inflectional morphology provides additional syntactic information. This approach leads to substantial performance improvements - 19% average improvement in accuracy across 6 language pairs over the state of the art in the supervised setting and 16% in the weakly supervised setting. As another contribution, we highlight issues associated with modern BLI that stem from ignoring inflectional morphology, and propose three suggestions for improving the task.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.07593](https://arxiv.org/abs/2011.07593) [cs.CL]** |
|           | (or **[arXiv:2011.07593v1](https://arxiv.org/abs/2011.07593v1) [cs.CL]** for this version) |





<h2 id="2020-11-17-3">3. Learning from Task Descriptions</h2>

Title: [Learning from Task Descriptions](https://arxiv.org/abs/2011.08115)

Authors: [Orion Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+O), [Nicholas Lourie](https://arxiv.org/search/cs?searchtype=author&query=Lourie%2C+N), [Matt Gardner](https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+M), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E)

> Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.08115](https://arxiv.org/abs/2011.08115) [cs.CL]** |
|           | (or **[arXiv:2011.08115v1](https://arxiv.org/abs/2011.08115v1) [cs.CL]** for this version) |







# 2020-11-16

[Return to Index](#Index)



<h2 id="2020-11-16-1">1. Context-aware Stand-alone Neural Spelling Correction</h2>

Title: [Context-aware Stand-alone Neural Spelling Correction](https://arxiv.org/abs/2011.06642)

Authors: [Xiangci Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Hairong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings. On the contrary, humans can easily infer the corresponding correct words from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem, which only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperforms the previous state-of-the-art result by 12.8% absolute F0.5 score.

| Comments: | 8 pages, 5 tables, 1 figure. Findings of the Association for Computational Linguistics: EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.06642](https://arxiv.org/abs/2011.06642) [cs.CL]** |
|           | (or **[arXiv:2011.06642v1](https://arxiv.org/abs/2011.06642v1) [cs.CL]** for this version) |





<h2 id="2020-11-16-2">2. RethinkCWS: Is Chinese Word Segmentation a Solved Task?</h2>

Title: [RethinkCWS: Is Chinese Word Segmentation a Solved Task?](https://arxiv.org/abs/2011.06858)

Authors: [Jinlan Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Pengfei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Qi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: [this https URL](https://github.com/neulab/InterpretEval).

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.06858](https://arxiv.org/abs/2011.06858) [cs.CL]** |
|           | (or **[arXiv:2011.06858v1](https://arxiv.org/abs/2011.06858v1) [cs.CL]** for this version) |





<h2 id="2020-11-16-3">3. EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</h2>

Title: [EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints](https://arxiv.org/abs/2011.06868)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> We introduce an Edit-Based Transformer with Repositioning (EDITOR), which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation (Gu et al., 2019), EDITOR generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer (Gu et al., 2019) while speeding up decoding dramatically compared to constrained beam search (Post and Vilar, 2018). EDITOR also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.

| Comments: | Accepted at TACL (pre-MIT Press publication version)         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.06868](https://arxiv.org/abs/2011.06868) [cs.CL]** |
|           | (or **[arXiv:2011.06868v1](https://arxiv.org/abs/2011.06868v1) [cs.CL]** for this version) |





<h2 id="2020-11-16-4">4. Deconstructing word embedding algorithms</h2>

Title: [Deconstructing word embedding algorithms](https://arxiv.org/abs/2011.07013)

Authors: [Kian Kenyon-Dean](https://arxiv.org/search/cs?searchtype=author&query=Kenyon-Dean%2C+K), [Edward Newell](https://arxiv.org/search/cs?searchtype=author&query=Newell%2C+E), [Jackie Chi Kit Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+J+C+K)

> Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.

| Comments:    | EMNLP 2020, 6 pages. arXiv admin note: substantial text overlap with [arXiv:1911.13280](https://arxiv.org/abs/1911.13280) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| MSC classes: | 68T50                                                        |
| Cite as:     | **[arXiv:2011.07013](https://arxiv.org/abs/2011.07013) [cs.CL]** |
|              | (or **[arXiv:2011.07013v1](https://arxiv.org/abs/2011.07013v1) [cs.CL]** for this version) |







# 2020-11-13

[Return to Index](#Index)



<h2 id="2020-11-13-1">1. Incorporating a Local Translation Mechanism into Non-autoregressive Translation</h2>

Title: [Incorporating a Local Translation Mechanism into Non-autoregressive Translation](https://arxiv.org/abs/2011.06132)

Authors: [Xiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X), [Zhisong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among tar-get outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the out-put pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM; Ghazvininejad et al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5xspeedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.06132](https://arxiv.org/abs/2011.06132) [cs.CL]** |
|           | (or **[arXiv:2011.06132v1](https://arxiv.org/abs/2011.06132v1) [cs.CL]** for this version) |







# 2020-11-12

[Return to Index](#Index)



<h2 id="2020-11-12-1">1. OCR Post Correction for Endangered Language Texts</h2>

Title: [OCR Post Correction for Endangered Language Texts](https://arxiv.org/abs/2011.05402)

Authors: [Shruti Rijhwani](https://arxiv.org/search/cs?searchtype=author&query=Rijhwani%2C+S), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.

| Comments: | Accepted to EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.05402](https://arxiv.org/abs/2011.05402) [cs.CL]** |
|           | (or **[arXiv:2011.05402v1](https://arxiv.org/abs/2011.05402v1) [cs.CL]** for this version) |





<h2 id="2020-11-12-2">2. From Unsupervised Machine Translation To Adversarial Text Generation</h2>

Title: [From Unsupervised Machine Translation To Adversarial Text Generation](https://arxiv.org/abs/2011.05449)

Authors: [Ahmad Rashid](https://arxiv.org/search/cs?searchtype=author&query=Rashid%2C+A), [Alan Do-Omri](https://arxiv.org/search/cs?searchtype=author&query=Do-Omri%2C+A), [Md. Akmal Haidar](https://arxiv.org/search/cs?searchtype=author&query=Haidar%2C+M+A), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Mehdi Rezagholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Rezagholizadeh%2C+M)

> We present a self-attention based bilingual adversarial text generator (B-GAN) which can learn to generate text from the encoder representation of an unsupervised neural machine translation system. B-GAN is able to generate a distributed latent space representation which can be paired with an attention based decoder to generate fluent sentences. When trained on an encoder shared between two languages and paired with the appropriate decoder, it can generate sentences in either language. B-GAN is trained using a combination of reconstruction loss for auto-encoder, a cross domain loss for translation and a GAN based adversarial loss for text generation. We demonstrate that B-GAN, trained on monolingual corpora only using multiple losses, generates more fluent sentences compared to monolingual baselines while effectively using half the number of parameters.

| Comments: | Accepted at ICASSP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.05449](https://arxiv.org/abs/2011.05449) [cs.CL]** |
|           | (or **[arXiv:2011.05449v1](https://arxiv.org/abs/2011.05449v1) [cs.CL]** for this version) |





<h2 id="2020-11-12-3">3. On the Sentence Embeddings from Pre-trained Language Models</h2>

Title: [On the Sentence Embeddings from Pre-trained Language Models](https://arxiv.org/abs/2011.05864)

Authors: [Bohan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at [this https URL](https://github.com/bohanli/BERT-flow).

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.05864](https://arxiv.org/abs/2011.05864) [cs.CL]** |
|           | (or **[arXiv:2011.05864v1](https://arxiv.org/abs/2011.05864v1) [cs.CL]** for this version) |





<h2 id="2020-11-12-4">4. The Impact of Text Presentation on Translator Performance</h2>

Title: [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)

Authors: [Samuel Läubli](https://arxiv.org/search/cs?searchtype=author&query=Läubli%2C+S), [Patrick Simianer](https://arxiv.org/search/cs?searchtype=author&query=Simianer%2C+P), [Joern Wuebker](https://arxiv.org/search/cs?searchtype=author&query=Wuebker%2C+J), [Geza Kovacs](https://arxiv.org/search/cs?searchtype=author&query=Kovacs%2C+G), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Spence Green](https://arxiv.org/search/cs?searchtype=author&query=Green%2C+S)

> Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.

| Comments: | Accepted for publication in Target                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2011.05978](https://arxiv.org/abs/2011.05978) [cs.CL]** |
|           | (or **[arXiv:2011.05978v1](https://arxiv.org/abs/2011.05978v1) [cs.CL]** for this version) |







# 2020-11-11

[Return to Index](#Index)



<h2 id="2020-11-11-1">1. Simultaneous Speech-to-Speech Translation System with Neural Incremental ASR, MT, and TTS</h2>

Title: [Simultaneous Speech-to-Speech Translation System with Neural Incremental ASR, MT, and TTS](https://arxiv.org/abs/2011.04845)

Authors: [Katsuhito Sudoh](https://arxiv.org/search/cs?searchtype=author&query=Sudoh%2C+K), [Takatomo Kano](https://arxiv.org/search/cs?searchtype=author&query=Kano%2C+T), [Sashi Novitasari](https://arxiv.org/search/cs?searchtype=author&query=Novitasari%2C+S), [Tomoya Yanagita](https://arxiv.org/search/cs?searchtype=author&query=Yanagita%2C+T), [Sakriani Sakti](https://arxiv.org/search/cs?searchtype=author&query=Sakti%2C+S), [Satoshi Nakamura](https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+S)

> This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS). We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.04845](https://arxiv.org/abs/2011.04845) [cs.CL]** |
|           | (or **[arXiv:2011.04845v1](https://arxiv.org/abs/2011.04845v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-2">2. When Do You Need Billions of Words of Pretraining Data?</h2>

Title: [When Do You Need Billions of Words of Pretraining Data?](https://arxiv.org/abs/2011.04946)

Authors: [Yian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Alex Warstadt](https://arxiv.org/search/cs?searchtype=author&query=Warstadt%2C+A), [Haau-Sing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Samuel R. Bowman](https://arxiv.org/search/cs?searchtype=author&query=Bowman%2C+S+R)

> NLP is currently dominated by general-purpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? We adopt four probing methods---classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks---and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.

| Comments: | 10 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.04946](https://arxiv.org/abs/2011.04946) [cs.CL]** |
|           | (or **[arXiv:2011.04946v1](https://arxiv.org/abs/2011.04946v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-3">3. To What Degree Can Language Borders Be Blurred In BERT-based Multilingual Spoken Language Understanding?</h2>

Title: [To What Degree Can Language Borders Be Blurred In BERT-based Multilingual Spoken Language Understanding?](https://arxiv.org/abs/2011.05007)

Authors: [Quynh Do](https://arxiv.org/search/cs?searchtype=author&query=Do%2C+Q), [Judith Gaspers](https://arxiv.org/search/cs?searchtype=author&query=Gaspers%2C+J), [Tobias Roding](https://arxiv.org/search/cs?searchtype=author&query=Roding%2C+T), [Melanie Bradford](https://arxiv.org/search/cs?searchtype=author&query=Bradford%2C+M)

> This paper addresses the question as to what degree a BERT-based multilingual Spoken Language Understanding (SLU) model can transfer knowledge across languages. Through experiments we will show that, although it works substantially well even on distant language groups, there is still a gap to the ideal multilingual performance. In addition, we propose a novel BERT-based adversarial model architecture to learn language-shared and language-specific representations for multilingual SLU. Our experimental results prove that the proposed model is capable of narrowing the gap to the ideal multilingual performance.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2011.05007](https://arxiv.org/abs/2011.05007) [cs.CL]** |
|           | (or **[arXiv:2011.05007v1](https://arxiv.org/abs/2011.05007v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-4">4. Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers</h2>

Title: [Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers](https://arxiv.org/abs/2011.05037)

Authors: [Ife Adebara](https://arxiv.org/search/cs?searchtype=author&query=Adebara%2C+I), [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Muhammad Abdul Mageed](https://arxiv.org/search/cs?searchtype=author&query=Mageed%2C+M+A)

> We investigate different approaches to translate between similar languages under low resource conditions, as part of our contribution to the WMT 2020 Similar Languages Translation Shared Task. We submitted Transformer-based bilingual and multilingual systems for all language pairs, in the two directions. We also leverage back-translation for one of the language pairs, acquiring an improvement of more than 3 BLEU points. We interpret our results in light of the degree of mutual intelligibility (based on Jaccard similarity) between each pair, finding a positive correlation between mutual intelligibility and model performance. Our Spanish-Catalan model has the best performance of all the five language pairs. Except for the case of Hindi-Marathi, our bilingual models achieve better performance than the multilingual models on all pairs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.05037](https://arxiv.org/abs/2011.05037) [cs.CL]** |
|           | (or **[arXiv:2011.05037v1](https://arxiv.org/abs/2011.05037v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-5">5. Multi-Task Sequence Prediction For Tunisian Arabizi Multi-Level Annotation</h2>

Title: [Multi-Task Sequence Prediction For Tunisian Arabizi Multi-Level Annotation](https://arxiv.org/abs/2011.05152)

Authors: [Elisa Gugliotta](https://arxiv.org/search/cs?searchtype=author&query=Gugliotta%2C+E) (1,2,3), [Marco Dinarelli](https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M) (2), [Olivier Kraif](https://arxiv.org/search/cs?searchtype=author&query=Kraif%2C+O) (3) ((1) Sapienza University of Rome, (2) Université Grenoble Alpes - Laboratoire LIG, (Getalp group), (3) Université Grenoble Alpes- Laboratoire LIDILEM)

> In this paper we propose a multi-task sequence prediction system, based on recurrent neural networks and used to annotate on multiple levels an Arabizi Tunisian corpus. The annotation performed are text classification, tokenization, PoS tagging and encoding of Tunisian Arabizi into CODA* Arabic orthography. The system is learned to predict all the annotation levels in cascade, starting from Arabizi input. We evaluate the system on the TIGER German corpus, suitably converting data to have a multi-task problem, in order to show the effectiveness of our neural architecture. We show also how we used the system in order to annotate a Tunisian Arabizi corpus, which has been afterwards manually corrected and used to further evaluate sequence models on Tunisian data. Our system is developed for the Fairseq framework, which allows for a fast and easy use for any other sequence prediction problem.

| Comments: | Paper accepted at the Fifth Arabic Natural Language Processing Workshop (WANLP) 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.05152](https://arxiv.org/abs/2011.05152) [cs.CL]** |
|           | (or **[arXiv:2011.05152v1](https://arxiv.org/abs/2011.05152v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-6">6. UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations</h2>

Title: [UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations](https://arxiv.org/abs/2011.05197)

Authors: [Gabriele Sarti](https://arxiv.org/search/cs?searchtype=author&query=Sarti%2C+G)

> This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available. Multiple copies of the original model are initially trained on the downstream task. Their predictions are then used to annotate a large set of unlabeled examples. Finally, multi-task training is performed on the parallel annotations of the resulting training set, and final scores are obtained by averaging annotator-specific head predictions. Neural language models are fine-tuned using this procedure in the context of the AcCompl-it shared task at EVALITA 2020, obtaining considerable improvements in prediction quality.

| Comments:    | 5 pages, Best system award for the AcCompl-It shared task at the EVALITA 2020 workshop |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7; J.5                                                   |
| Cite as:     | **[arXiv:2011.05197](https://arxiv.org/abs/2011.05197) [cs.CL]** |
|              | (or **[arXiv:2011.05197v1](https://arxiv.org/abs/2011.05197v1) [cs.CL]** for this version) |





<h2 id="2020-11-11-7">7. Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</h2>

Title: [Towards Interpretable Natural Language Understanding with Explanations as Latent Variables](https://arxiv.org/abs/2011.05268)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Jinyi Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Hanlin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Xiaodan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Chenyan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C), [Jian Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.05268](https://arxiv.org/abs/2011.05268) [cs.CL]** |
|           | (or **[arXiv:2011.05268v1](https://arxiv.org/abs/2011.05268v1) [cs.CL]** for this version) |







# 2020-11-10

[Return to Index](#Index)



<h2 id="2020-11-10-1">1. Long Range Arena: A Benchmark for Efficient Transformers</h2>

Title: [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [Samira Abnar](https://arxiv.org/search/cs?searchtype=author&query=Abnar%2C+S), [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Philip Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+P), [Jinfeng Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+J), [Liu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at [this https URL](https://github.com/google-research/long-range-arena).

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.04006](https://arxiv.org/abs/2011.04006) [cs.LG]** |
|           | (or **[arXiv:2011.04006v1](https://arxiv.org/abs/2011.04006v1) [cs.LG]** for this version) |





<h2 id="2020-11-10-2">2. CxGBERT: BERT meets Construction Grammar</h2>

Title: [CxGBERT: BERT meets Construction Grammar](https://arxiv.org/abs/2011.04134)

Authors: [Harish Tayyar Madabushi](https://arxiv.org/search/cs?searchtype=author&query=Madabushi%2C+H+T), [Laurence Romain](https://arxiv.org/search/cs?searchtype=author&query=Romain%2C+L), [Dagmar Divjak](https://arxiv.org/search/cs?searchtype=author&query=Divjak%2C+D), [Petar Milin](https://arxiv.org/search/cs?searchtype=author&query=Milin%2C+P)

> While lexico-semantic elements no doubt capture a large amount of linguistic information, it has been argued that they do not capture all information contained in text. This assumption is central to constructionist approaches to language which argue that language consists of constructions, learned pairings of a form and a function or meaning that are either frequent or have a meaning that cannot be predicted from its component parts. BERT's training objectives give it access to a tremendous amount of lexico-semantic information, and while BERTology has shown that BERT captures certain important linguistic dimensions, there have been no studies exploring the extent to which BERT might have access to constructional information. In this work we design several probes and conduct extensive experiments to answer this question. Our results allow us to conclude that BERT does indeed have access to a significant amount of information, much of which linguists typically call constructional information. The impact of this observation is potentially far-reaching as it provides insights into what deep learning methods learn from text, while also showing that information contained in constructions is redundantly encoded in lexico-semantics.

| Comments: | 28th International Conference on Computational Linguistics (COLING 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.04134](https://arxiv.org/abs/2011.04134) [cs.CL]** |
|           | (or **[arXiv:2011.04134v1](https://arxiv.org/abs/2011.04134v1) [cs.CL]** for this version) |





<h2 id="2020-11-10-3">3. BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention</h2>

Title: [BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention](https://arxiv.org/abs/2011.04266)

Authors: [Zhebin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Sai Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Dawei Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Gang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G)

> BERT-enhanced neural machine translation (NMT) aims at leveraging BERT-encoded representations for translation tasks. A recently proposed approach uses attention mechanisms to fuse Transformer's encoder and decoder layers with BERT's last-layer representation and shows enhanced performance. However, their method doesn't allow for the flexible distribution of attention between the BERT representation and the encoder/decoder representation. In this work, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves upon existing models from two aspects: 1) BERT-JAM uses joint-attention modules to allow the encoder/decoder layers to dynamically allocate attention between different representations, and 2) BERT-JAM allows the encoder/decoder layers to make use of BERT's intermediate representations by composing them using a gated linear unit (GLU). We train BERT-JAM with a novel three-phase optimization strategy that progressively unfreezes different components of BERT-JAM. Our experiments show that BERT-JAM achieves SOTA BLEU scores on multiple translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.04266](https://arxiv.org/abs/2011.04266) [cs.CL]** |
|           | (or **[arXiv:2011.04266v1](https://arxiv.org/abs/2011.04266v1) [cs.CL]** for this version) |





<h2 id="2020-11-10-4">4. Hierarchical Multitask Learning Approach for BERT</h2>

Title: [Hierarchical Multitask Learning Approach for BERT](https://arxiv.org/abs/2011.04451)

Authors: [Çağla Aksoy](https://arxiv.org/search/cs?searchtype=author&query=Aksoy%2C+Ç), [Alper Ahmetoğlu](https://arxiv.org/search/cs?searchtype=author&query=Ahmetoğlu%2C+A), [Tunga Güngör](https://arxiv.org/search/cs?searchtype=author&query=Güngör%2C+T)

> Recent works show that learning contextualized embeddings for words is beneficial for downstream tasks. BERT is one successful example of this approach. It learns embeddings by solving two tasks, which are masked language model (masked LM) and the next sentence prediction (NSP). The pre-training of BERT can also be framed as a multitask learning problem. In this work, we adopt hierarchical multitask learning approaches for BERT pre-training. Pre-training tasks are solved at different layers instead of the last layer, and information from the NSP task is transferred to the masked LM task. Also, we propose a new pre-training task bigram shift to encode word order information. We choose two downstream tasks, one of which requires sentence-level embeddings (textual entailment), and the other requires contextualized embeddings of words (question answering). Due to computational restrictions, we use the downstream task data instead of a large dataset for the pre-training to see the performance of proposed models when given a restricted dataset. We test their performance on several probing tasks to analyze learned embeddings. Our results show that imposing a task hierarchy in pre-training improves the performance of embeddings.

| Comments: | 9 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.04451](https://arxiv.org/abs/2011.04451) [cs.CL]** |
|           | (or **[arXiv:2011.04451v1](https://arxiv.org/abs/2011.04451v1) [cs.CL]** for this version) |





<h2 id="2020-11-10-5">5. VisBERT: Hidden-State Visualizations for Transformers</h2>

Title: [VisBERT: Hidden-State Visualizations for Transformers](https://arxiv.org/abs/2011.04507)

Authors: [Betty van Aken](https://arxiv.org/search/cs?searchtype=author&query=van+Aken%2C+B), [Benjamin Winter](https://arxiv.org/search/cs?searchtype=author&query=Winter%2C+B), [Alexander Löser](https://arxiv.org/search/cs?searchtype=author&query=Löser%2C+A), [Felix A. Gers](https://arxiv.org/search/cs?searchtype=author&query=Gers%2C+F+A)

> Explainability and interpretability are two important concepts, the absence of which can and should impede the application of well-performing neural networks to real-world problems. At the same time, they are difficult to incorporate into the large, black-box models that achieve state-of-the-art results in a multitude of NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one such black-box model. It has become a staple architecture to solve many different NLP tasks and has inspired a number of related Transformer models. Understanding how these models draw conclusions is crucial for both their improvement and application. We contribute to this challenge by presenting VisBERT, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering. Instead of analyzing attention weights, we focus on the hidden states resulting from each encoder block within the BERT model. This way we can observe how the semantic representations are transformed throughout the layers of the model. VisBERT enables users to get insights about the model's internal state and to explore its inference steps or potential shortcomings. The tool allows us to identify distinct phases in BERT's transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.

| Comments:          | Published in WWW '20: Companion Proceedings of the Web Conference 2020 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Companion Proceedings of the Web Conference 2020             |
| Cite as:           | **[arXiv:2011.04507](https://arxiv.org/abs/2011.04507) [cs.CL]** |
|                    | (or **[arXiv:2011.04507v1](https://arxiv.org/abs/2011.04507v1) [cs.CL]** for this version) |







# 2020-11-09

[Return to Index](#Index)



<h2 id="2020-11-09-1">1. Improving RNN Transducer Based ASR with Auxiliary Tasks</h2>

Title: [Improving RNN Transducer Based ASR with Auxiliary Tasks](https://arxiv.org/abs/2011.03109)

Authors:[Chunxi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Frank Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F), [Duc Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+D), [Suyoun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Yatharth Saraf](https://arxiv.org/search/cs?searchtype=author&query=Saraf%2C+Y), [Geoffrey Zweig](https://arxiv.org/search/cs?searchtype=author&query=Zweig%2C+G)

> End-to-end automatic speech recognition (ASR) models with a single neural network have recently demonstrated state-of-the-art results compared to conventional hybrid speech recognizers. Specifically, recurrent neural network transducer (RNN-T) has shown competitive ASR performance on various benchmarks. In this work, we examine ways in which RNN-T can achieve better ASR accuracy via performing auxiliary tasks. We propose (i) using the same auxiliary task as primary RNN-T ASR task, and (ii) performing context-dependent graphemic state prediction as in conventional hybrid modeling. In transcribing social media videos with varying training data size, we first evaluate the streaming ASR performance on three languages: Romanian, Turkish and German. We find that both proposed methods provide consistent improvements. Next, we observe that both auxiliary tasks demonstrate efficacy in learning deep transformer encoders for RNN-T criterion, thus achieving competitive results - 2.0%/4.2% WER on LibriSpeech test-clean/other - as compared to prior top performing models.

| Comments: | Accepted for publication at IEEE Spoken Language Technology Workshop (SLT), 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.03109](https://arxiv.org/abs/2011.03109) [cs.CL]** |
|           | (or **[arXiv:2011.03109v1](https://arxiv.org/abs/2011.03109v1) [cs.CL]** for this version) |





<h2 id="2020-11-09-2">2. Semi-Supervised Low-Resource Style Transfer of Indonesian Informal to Formal Language with Iterative Forward-Translation</h2>

Title: [Semi-Supervised Low-Resource Style Transfer of Indonesian Informal to Formal Language with Iterative Forward-Translation](https://arxiv.org/abs/2011.03286)

Authors:[Haryo Akbarianto Wibowo](https://arxiv.org/search/cs?searchtype=author&query=Wibowo%2C+H+A), [Tatag Aziz Prawiro](https://arxiv.org/search/cs?searchtype=author&query=Prawiro%2C+T+A), [Muhammad Ihsan](https://arxiv.org/search/cs?searchtype=author&query=Ihsan%2C+M), [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Radityo Eko Prasojo](https://arxiv.org/search/cs?searchtype=author&query=Prasojo%2C+R+E), [Rahmad Mahendra](https://arxiv.org/search/cs?searchtype=author&query=Mahendra%2C+R)

> In its daily use, the Indonesian language is riddled with informality, that is, deviations from the standard in terms of vocabulary, spelling, and word order. On the other hand, current available Indonesian NLP models are typically developed with the standard Indonesian in mind. In this work, we address a style-transfer from informal to formal Indonesian as a low-resource machine translation problem. We build a new dataset of parallel sentences of informal Indonesian and its formal counterpart. We benchmark several strategies to perform style transfer from informal to formal Indonesian. We also explore augmenting the training set with artificial forward-translated data. Since we are dealing with an extremely low-resource setting, we find that a phrase-based machine translation approach outperforms the Transformer-based approach. Alternatively, a pre-trained GPT-2 fined-tuned to this task performed equally well but costs more computational resource. Our findings show a promising step towards leveraging machine translation models for style transfer. Our code and data are available in [this https URL](https://github.com/haryoa/stif-indonesia)

| Comments: | 6 pages, Camera ready to be presented at IALP 2020           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.03286](https://arxiv.org/abs/2011.03286) [cs.CL]** |
|           | (or **[arXiv:2011.03286v1](https://arxiv.org/abs/2011.03286v1) [cs.CL]** for this version) |





<h2 id="2020-11-09-3">3. Understanding Pure Character-Based Neural Machine Translation: The Case of Translating Finnish into English</h2>

Title: [Understanding Pure Character-Based Neural Machine Translation: The Case of Translating Finnish into English](https://arxiv.org/abs/2011.03469)

Authors:[Gongbo Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+G), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Joakim Nivre](https://arxiv.org/search/cs?searchtype=author&query=Nivre%2C+J)

> Recent work has shown that deeper character-based neural machine translation (NMT) models can outperform subword-based models. However, it is still unclear what makes deeper character-based models successful. In this paper, we conduct an investigation into pure character-based models in the case of translating Finnish into English, including exploring the ability to learn word senses and morphological inflections and the attention mechanism. We demonstrate that word-level information is distributed over the entire character sequence rather than over a single character, and characters at different positions play different roles in learning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop.

| Comments: | accepted by COLING 2020, camera-ready version                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.03469](https://arxiv.org/abs/2011.03469) [cs.CL]** |
|           | (or **[arXiv:2011.03469v1](https://arxiv.org/abs/2011.03469v1) [cs.CL]** for this version) |





<h2 id="2020-11-09-4">4. An Unsupervised method for OCR Post-Correction and Spelling Normalisation for Finnish</h2>

Title: [An Unsupervised method for OCR Post-Correction and Spelling Normalisation for Finnish](https://arxiv.org/abs/2011.03502)

Authors:[Quan Duong](https://arxiv.org/search/cs?searchtype=author&query=Duong%2C+Q), [Mika Hämäläinen](https://arxiv.org/search/cs?searchtype=author&query=Hämäläinen%2C+M), [Simon Hengchen](https://arxiv.org/search/cs?searchtype=author&query=Hengchen%2C+S)

> Historical corpora are known to contain errors introduced by OCR (optical character recognition) methods used in the digitization process, often said to be degrading the performance of NLP systems. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We build on previous work on fully automatic unsupervised extraction of parallel data to train a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction designed for English, and adapt it to Finnish by proposing solutions that take the rich morphology of the language into account. Our new method shows increased performance while remaining fully unsupervised, with the added benefit of spelling normalisation. The source code and models are available on GitHub and Zenodo.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.03502](https://arxiv.org/abs/2011.03502) [cs.CL]** |
|           | (or **[arXiv:2011.03502v1](https://arxiv.org/abs/2011.03502v1) [cs.CL]** for this version) |







# 2020-11-06

[Return to Index](#Index)



<h2 id="2020-11-06-1">1. Data Augmentation and Terminology Integration for Domain-Specific Sinhala-English-Tamil Statistical Machine Translation</h2>

Title: [Data Augmentation and Terminology Integration for Domain-Specific Sinhala-English-Tamil Statistical Machine Translation](https://arxiv.org/abs/2011.02821)

Authors: [Aloka Fernando](https://arxiv.org/search/cs?searchtype=author&query=Fernando%2C+A), [Surangika Ranathunga](https://arxiv.org/search/cs?searchtype=author&query=Ranathunga%2C+S), [Gihan Dias](https://arxiv.org/search/cs?searchtype=author&query=Dias%2C+G)

> Out of vocabulary (OOV) is a problem in the context of Machine Translation (MT) in low-resourced languages. When source and/or target languages are morphologically rich, it becomes even worse. Bilingual list integration is an approach to address the OOV problem. This allows more words to be translated than are in the training data. However, since bilingual lists contain words in the base form, it will not translate inflected forms for morphologically rich languages such as Sinhala and Tamil. This paper focuses on data augmentation techniques where bilingual lexicon terms are expanded based on case-markers with the objective of generating new words, to be used in Statistical machine Translation (SMT). This data augmentation technique for dictionary terms shows improved BLEU scores for Sinhala-English SMT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.02821](https://arxiv.org/abs/2011.02821) [cs.CL]** |
|           | (or **[arXiv:2011.02821v1](https://arxiv.org/abs/2011.02821v1) [cs.CL]** for this version) |









# 2020-11-05

[Return to Index](#Index)



<h2 id="2020-11-05-1">1. SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation</h2>

Title: [SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation](https://arxiv.org/abs/2011.02048)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Simultaneous text translation and end-to-end speech translation have recently made great progress but little work has combined these tasks together. We investigate how to adapt simultaneous text translation methods such as wait-k and monotonic multihead attention to end-to-end simultaneous speech translation by introducing a pre-decision module. A detailed analysis is provided on the latency-quality trade-offs of combining fixed and flexible pre-decision with fixed and flexible policies. We also design a novel computation-aware latency metric, adapted from Average Lagging.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.02048](https://arxiv.org/abs/2011.02048) [cs.CL]** |
|           | (or **[arXiv:2011.02048v1](https://arxiv.org/abs/2011.02048v1) [cs.CL]** for this version) |





<h2 id="2020-11-05-2">2. Probing Multilingual BERT for Genetic and Typological Signals</h2>

Title: [Probing Multilingual BERT for Genetic and Typological Signals](https://arxiv.org/abs/2011.02070)

Authors: [Taraka Rama](https://arxiv.org/search/cs?searchtype=author&query=Rama%2C+T), [Lisa Beinborn](https://arxiv.org/search/cs?searchtype=author&query=Beinborn%2C+L), [Steffen Eger](https://arxiv.org/search/cs?searchtype=author&query=Eger%2C+S)

> We probe the layers in multilingual BERT (mBERT) for phylogenetic and geographic language signals across 100 languages and compute language distances based on the mBERT representations. We 1) employ the language distances to infer and evaluate language trees, finding that they are close to the reference family tree in terms of quartet tree distance, 2) perform distance matrix regression analysis, finding that the language distances can be best explained by phylogenetic and worst by structural factors and 3) present a novel measure for measuring diachronic meaning stability (based on cross-lingual representation variability) which correlates significantly with published ranked lists based on linguistic approaches. Our results contribute to the nascent field of typological interpretability of cross-lingual text representations.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.02070](https://arxiv.org/abs/2011.02070) [cs.CL]** |
|           | (or **[arXiv:2011.02070v1](https://arxiv.org/abs/2011.02070v1) [cs.CL]** for this version) |





<h2 id="2020-11-05-3">3. Chinese Grammatical Correction Using BERT-based Pre-trained Model</h2>

Title: [Chinese Grammatical Correction Using BERT-based Pre-trained Model](https://arxiv.org/abs/2011.02093)

Authors: [Hongfei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Michiki Kurosawa](https://arxiv.org/search/cs?searchtype=author&query=Kurosawa%2C+M), [Satoru Katsumata](https://arxiv.org/search/cs?searchtype=author&query=Katsumata%2C+S), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

> In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we verify the effectiveness of two methods that incorporate a BERT-based pre-trained model developed by Cui et al. (2020) into an encoder-decoder model on Chinese grammatical error correction tasks. We also analyze the error type and conclude that sentence-level errors are yet to be addressed.

| Comments: | 6 pages; AACL-IJCNLP 2020                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.02093](https://arxiv.org/abs/2011.02093) [cs.CL]** |
|           | (or **[arXiv:2011.02093v1](https://arxiv.org/abs/2011.02093v1) [cs.CL]** for this version) |





<h2 id="2020-11-05-4">4. PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents</h2>

Title: [PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents](https://arxiv.org/abs/2011.02121)

Authors: [Ryo Fujii](https://arxiv.org/search/cs?searchtype=author&query=Fujii%2C+R), [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Kaori Abe](https://arxiv.org/search/cs?searchtype=author&query=Abe%2C+K), [Kazuaki Hanawa](https://arxiv.org/search/cs?searchtype=author&query=Hanawa%2C+K), [Makoto Morishita](https://arxiv.org/search/cs?searchtype=author&query=Morishita%2C+M), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> Neural Machine Translation (NMT) has shown drastic improvement in its quality when translating clean input, such as text from the news domain. However, existing studies suggest that NMT still struggles with certain kinds of input with considerable noise, such as User-Generated Contents (UGC) on the Internet. To make better use of NMT for cross-cultural communication, one of the most promising directions is to develop a model that correctly handles these expressions. Though its importance has been recognized, it is still not clear as to what creates the great gap in performance between the translation of clean input and that of UGC. To answer the question, we present a new dataset, PheMT, for evaluating the robustness of MT systems against specific linguistic phenomena in Japanese-English translation. Our experiments with the created dataset revealed that not only our in-house models but even widely used off-the-shelf systems are greatly disturbed by the presence of certain phenomena.

| Comments: | 15 pages, 4 figures, accepted at COLING 2020                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.02121](https://arxiv.org/abs/2011.02121) [cs.CL]** |
|           | (or **[arXiv:2011.02121v1](https://arxiv.org/abs/2011.02121v1) [cs.CL]** for this version) |





<h2 id="2020-11-05-5">5. Optimizing Transformer for Low-Resource Neural Machine Translation</h2>

Title: [Optimizing Transformer for Low-Resource Neural Machine Translation](https://arxiv.org/abs/2011.02266)

Authors: [Ali Araabi](https://arxiv.org/search/cs?searchtype=author&query=Araabi%2C+A), [Christof Monz](https://arxiv.org/search/cs?searchtype=author&query=Monz%2C+C)

> Language pairs with limited amounts of parallel data, also known as low-resource languages, remain a challenge for neural machine translation. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of Transformer under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.

| Comments:    | To be published in COLING 2020                               |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2011.02266](https://arxiv.org/abs/2011.02266) [cs.CL]** |
|              | (or **[arXiv:2011.02266v1](https://arxiv.org/abs/2011.02266v1) [cs.CL]** for this version) |





# 2020-11-04

[Return to Index](#Index)



<h2 id="2020-11-04-1">1. Layer-Wise Multi-View Learning for Neural Machine Translation</h2>

Title: [Layer-Wise Multi-View Learning for Neural Machine Translation](https://arxiv.org/abs/2011.01482)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Changliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure. We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence. In this way, in addition to the topmost encoder layer (referred to as the primary view), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder to maintain independent prediction. Consistency regularization based on KL divergence is used to encourage the two views to learn from each other. Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.01482](https://arxiv.org/abs/2011.01482) [cs.CL]** |
|           | (or **[arXiv:2011.01482v1](https://arxiv.org/abs/2011.01482v1) [cs.CL]** for this version) |





<h2 id="2020-11-04-2">2. CharBERT: Character-aware Pre-trained Language Model</h2>

Title: [CharBERT: Character-aware Pre-trained Language Model](https://arxiv.org/abs/2011.01513)

Authors: [Wentao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Chenglei Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+C), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Guoping Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+G)

> Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on question answering, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our method can significantly improve the performance and robustness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at [this https URL](https://github.com/wtma/CharBERT)

| Comments: | 12 pages, to appear at COLING 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.01513](https://arxiv.org/abs/2011.01513) [cs.CL]** |
|           | (or **[arXiv:2011.01513v1](https://arxiv.org/abs/2011.01513v1) [cs.CL]** for this version) |







<h2 id="2020-11-04-3">3. TransQuest: Translation Quality Estimation with Cross-lingual Transformers</h2>

Title: [TransQuest: Translation Quality Estimation with Cross-lingual Transformers](https://arxiv.org/abs/2011.01536)

Authors: [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Ruslan Mitkov](https://arxiv.org/search/cs?searchtype=author&query=Mitkov%2C+R)

> Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.

| Comments: | Accepted to COLING 2020. arXiv admin note: text overlap with [arXiv:2010.05318](https://arxiv.org/abs/2010.05318) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2011.01536](https://arxiv.org/abs/2011.01536) [cs.CL]** |
|           | (or **[arXiv:2011.01536v1](https://arxiv.org/abs/2011.01536v1) [cs.CL]** for this version) |







<h2 id="2020-11-04-4">4. Cross-lingual Word Embeddings beyond Zero-shot Machine Translation</h2>

Title: [Cross-lingual Word Embeddings beyond Zero-shot Machine Translation](https://arxiv.org/abs/2011.01682)

Authors: [Shifei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Ali Basirat](https://arxiv.org/search/cs?searchtype=author&query=Basirat%2C+A)

> We explore the transferability of a multilingual neural machine translation model to unseen languages when the transfer is grounded solely on the cross-lingual word embeddings. Our experimental results show that the translation knowledge can transfer weakly to other languages and that the degree of transferability depends on the languages' relatedness. We also discuss the limiting aspects of the multilingual architectures that cause weak translation transfer and suggest how to mitigate the limitations.

| Comments: | Accepted at the 8th Swedish Language Technology Conference (SLTC-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.01682](https://arxiv.org/abs/2011.01682) [cs.CL]** |
|           | (or **[arXiv:2011.01682v1](https://arxiv.org/abs/2011.01682v1) [cs.CL]** for this version) |







<h2 id="2020-11-04-5">5. Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation</h2>

Title: [Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation](https://arxiv.org/abs/2011.01703)

Authors: [Annette Rios](https://arxiv.org/search/cs?searchtype=author&query=Rios%2C+A), [Mathias Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+M), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN↔{FR,CS,DE,FI} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.01703](https://arxiv.org/abs/2011.01703) [cs.CL]** |
|           | (or **[arXiv:2011.01703v1](https://arxiv.org/abs/2011.01703v1) [cs.CL]** for this version) |







# 2020-11-03

[Return to Index](#Index)



<h2 id="2020-11-03-1">1. COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning</h2>

Title: [COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning](https://arxiv.org/abs/2011.00597)

Authors: [Simon Ging](https://arxiv.org/search/cs?searchtype=author&query=Ging%2C+S) (1), [Mohammadreza Zolfaghari](https://arxiv.org/search/cs?searchtype=author&query=Zolfaghari%2C+M) (1), [Hamed Pirsiavash](https://arxiv.org/search/cs?searchtype=author&query=Pirsiavash%2C+H) (2), [Thomas Brox](https://arxiv.org/search/cs?searchtype=author&query=Brox%2C+T) (1) ((1) University of Freiburg, (2) University of Maryland Baltimore County)

> Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at [this https URL](https://github.com/gingsi/coot-videotext)

| Comments:    | 27 pages, 5 figures, 19 tables. To be published in the 34th conference on Neural Information Processing Systems (NeurIPS 2020). The first two authors contributed equally to this work |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| ACM classes: | I.2.7; I.2.10                                                |
| Cite as:     | **[arXiv:2011.00597](https://arxiv.org/abs/2011.00597) [cs.CV]** |
|              | (or **[arXiv:2011.00597v1](https://arxiv.org/abs/2011.00597v1) [cs.CV]** for this version) |





<h2 id="2020-11-03-2">2. The 2020s Political Economy of Machine Translation</h2>

Title: [The 2020s Political Economy of Machine Translation](https://arxiv.org/abs/2011.01007)

Authors: [Steven Weber](https://arxiv.org/search/cs?searchtype=author&query=Weber%2C+S)

> This paper explores the hypothesis that the diversity of human languages, right now a barrier to interoperability in communication and trade, will become significantly less of a barrier as machine translation technologies are deployed over the next several years.But this new boundary-breaking technology does not reduce all boundaries equally, and it creates new challenges for the distribution of ideas and thus for innovation and economic growth.

| Comments: | 42 pages, 0 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2011.01007](https://arxiv.org/abs/2011.01007) [cs.CY]** |
|           | (or **[arXiv:2011.01007v1](https://arxiv.org/abs/2011.01007v1) [cs.CY]** for this version) |





<h2 id="2020-11-03-3">3. Streaming Simultaneous Speech Translation with Augmented Memory Transformer</h2>

Title: [Streaming Simultaneous Speech Translation with Augmented Memory Transformer](https://arxiv.org/abs/2011.00033)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Yongqiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> Transformer-based models have achieved state-of-the-art performance on speech translation tasks. However, the model architecture is not efficient enough for streaming scenarios since self-attention is computed over an entire input sequence and the computational cost grows quadratically with the length of the input sequence. Nevertheless, most of the previous work on simultaneous speech translation, the task of generating translations from partial audio input, ignores the time spent in generating the translation when analyzing the latency. With this assumption, a system may have good latency quality trade-offs but be inapplicable in real-time scenarios. In this paper, we focus on the task of streaming simultaneous speech translation, where the systems are not only capable of translating with partial input but are also able to handle very long or continuous input. We propose an end-to-end transformer-based sequence-to-sequence model, equipped with an augmented memory transformer encoder, which has shown great success on the streaming automatic speech recognition task with hybrid or transducer-based models. We conduct an empirical evaluation of the proposed model on segment, context and memory sizes and we compare our approach to a transformer with a unidirectional mask.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.00033](https://arxiv.org/abs/2011.00033) [cs.CL]** |
|           | (or **[arXiv:2011.00033v1](https://arxiv.org/abs/2011.00033v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-4">4. Joint Masked CPC and CTC Training for ASR</h2>

Title: [Joint Masked CPC and CTC Training for ASR](https://arxiv.org/abs/2011.00093)

Authors: [Chaitanya Talnikar](https://arxiv.org/search/cs?searchtype=author&query=Talnikar%2C+C), [Tatiana Likhomanenko](https://arxiv.org/search/cs?searchtype=author&query=Likhomanenko%2C+T), [Ronan Collobert](https://arxiv.org/search/cs?searchtype=author&query=Collobert%2C+R), [Gabriel Synnaeve](https://arxiv.org/search/cs?searchtype=author&query=Synnaeve%2C+G)

> Self-supervised learning (SSL) has shown promise in learning representations of audio that are useful for automatic speech recognition (ASR). But, training SSL models like wav2vec~2.0 requires a two-stage pipeline. In this paper we demonstrate a single-stage training of ASR models that can utilize both unlabeled and labeled data. During training, we alternately minimize two losses: an unsupervised masked Contrastive Predictive Coding (CPC) loss and the supervised audio-to-text alignment loss Connectionist Temporal Classification (CTC). We show that this joint training method directly optimizes performance for the downstream ASR task using unsupervised data while achieving similar word error rates to wav2vec~2.0 on the Librispeech 100-hour dataset. Finally, we postulate that solving the contrastive task is a regularization for the supervised CTC loss.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.00093](https://arxiv.org/abs/2011.00093) [cs.CL]** |
|           | (or **[arXiv:2011.00093v1](https://arxiv.org/abs/2011.00093v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-5">5. Evaluating Bias In Dutch Word Embeddings</h2>

Title: [Evaluating Bias In Dutch Word Embeddings](https://arxiv.org/abs/2011.00244)

Authors: [Rodrigo Alejandro Chávez Mulsa](https://arxiv.org/search/cs?searchtype=author&query=Mulsa%2C+R+A+C), [Gerasimos Spanakis](https://arxiv.org/search/cs?searchtype=author&query=Spanakis%2C+G)

> Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications. This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch. We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks. The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings. We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language. Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2% decrease in performance in contextualized embeddings. Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.

| Comments: | Accepted at GeBNLP 2020, data at [this https URL](https://github.com/Noixas/Official-Evaluating-Bias-In-Dutch) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.00244](https://arxiv.org/abs/2011.00244) [cs.CL]** |
|           | (or **[arXiv:2011.00244v1](https://arxiv.org/abs/2011.00244v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-6">6. Targeted Poisoning Attacks on Black-Box Neural Machine Translation
</h2>

Title: [Targeted Poisoning Attacks on Black-Box Neural Machine Translation](https://arxiv.org/abs/2011.00675)

Authors: [Chang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Jun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Benjamin I. P. Rubinstein](https://arxiv.org/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T)

> As modern neural machine translation (NMT) systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been shown to be vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less useful when the target system is black-box and unknown to the adversary (e.g., secured commercial systems). In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data. We demonstrate that this attack can be realised practically via targeted corruption of web documents crawled to form the system's training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios, which are the one-off training and pre-train & fine-tune paradigms. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data (tens of millions), the attacks are still successful (over 50% success rate) under surprisingly low poisoning rates (e.g., 0.006%). Lastly, we discuss potential defences to counter such attacks.

| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.00675](https://arxiv.org/abs/2011.00675) [cs.CL]** |
|           | (or **[arXiv:2011.00675v1](https://arxiv.org/abs/2011.00675v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-7">7. Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation</h2>

Title: [Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation](https://arxiv.org/abs/2011.00678)

Authors: [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Neural machine translation (NMT) models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters (neurons). The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings.

| Comments: | Coling2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2011.00678](https://arxiv.org/abs/2011.00678) [cs.CL]** |
|           | (or **[arXiv:2011.00678v1](https://arxiv.org/abs/2011.00678v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-8">8. Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</h2>

Title: [Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation](https://arxiv.org/abs/2011.00747)

Authors: [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at [this https URL](https://github.com/formiel/speech-translation).

| Comments:          | Accepted at COLING 2020 (Oral)                               |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | The 28th International Conference on Computational Linguistics (COLING 2020) |
| Cite as:           | **[arXiv:2011.00747](https://arxiv.org/abs/2011.00747) [cs.CL]** |
|                    | (or **[arXiv:2011.00747v1](https://arxiv.org/abs/2011.00747v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-9">9. Context-Aware Cross-Attention for Non-Autoregressive Translation</h2>

Title: [Context-Aware Cross-Attention for Non-Autoregressive Translation](https://arxiv.org/abs/2011.00770)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Non-autoregressive translation (NAT) significantly accelerates the inference process by predicting the entire target sequence. However, due to the lack of target dependency modelling in the decoder, the conditional generation process heavily depends on the cross-attention. In this paper, we reveal a localness perception problem in NAT cross-attention, for which it is difficult to adequately capture source context. To alleviate this problem, we propose to enhance signals of neighbour source tokens into conventional cross-attention. Experimental results on several representative datasets show that our approach can consistently improve translation quality over strong NAT baselines. Extensive analyses demonstrate that the enhanced cross-attention achieves better exploitation of source contexts by leveraging both local and global information.

| Comments: | To appear in COLING 2020                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.00770](https://arxiv.org/abs/2011.00770) [cs.CL]** |
|           | (or **[arXiv:2011.00770v1](https://arxiv.org/abs/2011.00770v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-10">10. Emergent Communication Pretraining for Few-Shot Machine Translation</h2>

Title: [Emergent Communication Pretraining for Few-Shot Machine Translation](https://arxiv.org/abs/2011.00890)

Authors: [Yaoyiran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Edoardo M. Ponti](https://arxiv.org/search/cs?searchtype=author&query=Ponti%2C+E+M), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A)

> While state-of-the-art models that rely upon massively multilingual pretrained encoders achieve sample efficiency in downstream applications, they still require abundant amounts of unlabelled text. Nevertheless, most of the world's languages lack such resources. Hence, we investigate a more radical form of unsupervised knowledge transfer in the absence of linguistic data. In particular, for the first time we pretrain neural networks via emergent communication from referential games. Our key assumption is that grounding communication on images---as a crude approximation of real-world environments---inductively biases the model towards learning natural languages. On the one hand, we show that this substantially benefits machine translation in few-shot settings. On the other hand, this also provides an extrinsic evaluation protocol to probe the properties of emergent languages ex vitro. Intuitively, the closer they are to natural languages, the higher the gains from pretraining on them should be. For instance, in this work we measure the influence of communication success and maximum sequence length on downstream performances. Finally, we introduce a customised adapter layer and annealing strategies for the regulariser of maximum-a-posteriori inference during fine-tuning. These turn out to be crucial to facilitate knowledge transfer and prevent catastrophic forgetting. Compared to a recurrent baseline, our method yields gains of 59.0%∼147.6% in BLEU score with only 500 NMT training instances and 65.1%∼196.7% with 1,000 NMT training instances across four language pairs. These proof-of-concept results reveal the potential of emergent communication pretraining for both natural language processing tasks in resource-poor settings and extrinsic evaluation of artificial languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.00890](https://arxiv.org/abs/2011.00890) [cs.CL]** |
|           | (or **[arXiv:2011.00890v1](https://arxiv.org/abs/2011.00890v1) [cs.CL]** for this version) |





<h2 id="2020-11-03-11">11. How Far Does BERT Look At:Distance-based Clustering and Analysis of BERTś Attention</h2>

Title: [How Far Does BERT Look At:Distance-based Clustering and Analysis of BERTś Attention](https://arxiv.org/abs/2011.00943)

Authors: [Yue Guan](https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+Y), [Jingwen Leng](https://arxiv.org/search/cs?searchtype=author&query=Leng%2C+J), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Quan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Minyi Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M)

> Recent research on the multi-head attention mechanism, especially that in pre-trained modelssuch as BERT, has shown us heuristics and clues in analyzing various aspects of the [this http URL](http://mechanism.as/) most of the research focus on probing tasks or hidden states, previous works have found someprimitive patterns of attention head behavior by heuristic analytical methods, but a more system-atic analysis specific on the attention patterns still remains primitive. In this work, we clearlycluster the attention heatmaps into significantly different patterns through unsupervised cluster-ing on top of a set of proposed features, which corroborates with previous observations. Wefurther study their corresponding functions through analytical study. In addition, our proposedfeatures can be used to explain and calibrate different attention heads in Transformer models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2011.00943](https://arxiv.org/abs/2011.00943) [cs.CL]** |
|           | (or **[arXiv:2011.00943v1](https://arxiv.org/abs/2011.00943v1) [cs.CL]** for this version) |



<h2 id="2020-11-03-12">12. Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders</h2>

Title: [Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders](https://arxiv.org/abs/2011.01097)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Carlos Segura](https://arxiv.org/search/cs?searchtype=author&query=Segura%2C+C)

> Current end-to-end approaches to Spoken Language Translation (SLT) rely on limited training resources, especially for multilingual settings. On the other hand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on higher quality and more massive data sets. Our proposed method extends a MultiNMT architecture based on language-specific encoders-decoders to the task of Multilingual SLT (MultiSLT) Our experiments on four different languages show that coupling the speech encoder to the MultiNMT architecture produces similar quality translations compared to a bilingual baseline (±0.2 BLEU) while effectively allowing for zero-shot MultiSLT. Additionally, we propose using Adapter networks for SLT that produce consistent improvements of +1 BLEU points in all tested languages.

| Comments:    | Submitted to ICASSP 2021                                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2011.01097](https://arxiv.org/abs/2011.01097) [cs.CL]** |
|              | (or **[arXiv:2011.01097v1](https://arxiv.org/abs/2011.01097v1) [cs.CL]** for this version) |



<h2 id="2020-11-03-13">13. The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks</h2>

Title: [The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks](https://arxiv.org/abs/2011.01196)

Authors: [Brihi Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+B), [Neil Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+N), [Francesco Barbieri](https://arxiv.org/search/cs?searchtype=author&query=Barbieri%2C+F), [Leonardo Neves](https://arxiv.org/search/cs?searchtype=author&query=Neves%2C+L)

> Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as question answering, sentiment analysis, and textual similarity in recent years. Extensive work shows how accurately such models can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such models' performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives: matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into models that use contextual embeddings, achieving relative improvements of up to 36% on granular tasks.

| Comments: | Accepted at COLING 2020. Code available at [this https URL](https://github.com/brihijoshi/granular-similarity-COLING-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2011.01196](https://arxiv.org/abs/2011.01196) [cs.CL]** |
|           | (or **[arXiv:2011.01196v1](https://arxiv.org/abs/2011.01196v1) [cs.CL]** for this version) |







# 2020-11-02

[Return to Index](#Index)



<h2 id="2020-11-02-1">1. VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation</h2>

Title: [VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation](https://arxiv.org/abs/2010.16046)

Authors: [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Jiahao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Yijia Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Luo Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+L)

> Recent studies about learning multilingual representations have achieved significant performance gains across a wide range of downstream cross-lingual tasks. They train either an encoder-only Transformer mainly for understanding tasks, or an encoder-decoder Transformer specifically for generation tasks, ignoring the correlation between the two tasks and frameworks. In contrast, this paper presents a variable encoder-decoder (VECO) pre-training approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub-modules for understanding and generation tasks during inference. Such a workflow not only ensures to train the most streamlined parameters necessary for two kinds of tasks, but also enables them to boost each other via sharing common sub-modules. As a result, VECO delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval. For generation tasks, VECO also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1∼2 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.16046](https://arxiv.org/abs/2010.16046) [cs.CL] |
|           | (or [arXiv:2010.16046v1](https://arxiv.org/abs/2010.16046v1) [cs.CL] for this version) |





<h2 id="2020-11-02-2">2. Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents</h2>

Title: [Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents](https://arxiv.org/abs/2010.16363)

Authors: [Gregory Yauney](https://arxiv.org/search/cs?searchtype=author&query=Yauney%2C+G), [Jack Hessel](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+J), [David Mimno](https://arxiv.org/search/cs?searchtype=author&query=Mimno%2C+D)

> Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as "kitchen" and "bedroom", and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating "granite" with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.

| Subjects:          | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Published in EMNLP 2020                                      |
| Cite as:           | [arXiv:2010.16363](https://arxiv.org/abs/2010.16363) [cs.CL] |
|                    | (or [arXiv:2010.16363v1](https://arxiv.org/abs/2010.16363v1) [cs.CL] for this version) |