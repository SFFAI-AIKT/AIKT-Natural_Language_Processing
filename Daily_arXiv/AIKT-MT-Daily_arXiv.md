# Daily arXiv: Machine Translation - June, 2020

# Index


- [2020-06-04](#2020-06-04)

  - [1. The Typology of Polysemy: A Multilingual Distributional Framework](#2020-06-01-1)
  - [2. Norm-Based Curriculum Learning for Neural Machine Translation](#2020-06-01-2)
  - [3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](#2020-06-01-3)
  - [4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer](#2020-06-01-4)
- [2020-06-03](#2020-06-03)

  - [1. WikiBERT models: deep transfer learning for many languages](#2020-06-03-1)
  - [2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](#2020-06-03-2)
- [2020-06-02](#2020-06-02)
- [1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](#2020-06-02-1)
  - [2. Dynamic Masking for Improved Stability in Spoken Language Translation](#2020-06-02-2)
  - [3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](#2020-06-02-3)
  - [4. Neural Unsupervised Domain Adaptation in NLP---A Survey](#2020-06-02-4)
  - [5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](#2020-06-02-5)
  - [6. Attention Word Embedding](#2020-06-02-6)
  - [7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](#2020-06-02-7)
  - [8. Cascaded Text Generation with Markov Transformers](#2020-06-02-8)
- [2020-06-01](#2020-06-01)

  - [1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP](#2020-06-01-1)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-06-04

[Return to Index](#Index)



<h2 id="2020-06-04-1">1. The Typology of Polysemy: A Multilingual Distributional Framework</h2>

Title: [The Typology of Polysemy: A Multilingual Distributional Framework](https://arxiv.org/abs/2006.01966)

Authors: [Ella Rabinovich](https://arxiv.org/search/cs?searchtype=author&query=Rabinovich%2C+E), [Yang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Suzanne Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+S)

> Lexical semantic typology has identified important cross-linguistic generalizations about the variation and commonalities in polysemy patterns---how languages package up meanings into words. Recent computational research has enabled investigation of lexical semantics at a much larger scale, but little work has explored lexical typology across semantic domains, nor the factors that influence cross-linguistic similarities. We present a novel computational framework that quantifies semantic affinity, the cross-linguistic similarity of lexical semantics for a concept. Our approach defines a common multilingual semantic space that enables a direct comparison of the lexical expression of concepts across languages. We validate our framework against empirical findings on lexical semantic typology at both the concept and domain levels. Our results reveal an intricate interaction between semantic domains and extra-linguistic factors, beyond language phylogeny, that co-shape the typology of polysemy across languages.

| Comments: | CogSci 2020 (Annual Meeting of the Cognitive Science Society) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01966](https://arxiv.org/abs/2006.01966) [cs.CL]** |
|           | (or **[arXiv:2006.01966v1](https://arxiv.org/abs/2006.01966v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-2">2. Norm-Based Curriculum Learning for Neural Machine Translation</h2>

Title: [Norm-Based Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2006.02014)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Houtim Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+H), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).

| Comments: | Accepted to ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.02014](https://arxiv.org/abs/2006.02014) [cs.CL]** |
|           | (or **[arXiv:2006.02014v1](https://arxiv.org/abs/2006.02014v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-3">3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation</h2>

Title: [Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](https://arxiv.org/abs/2006.02163)

Authors: [Xuan-Phi Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Wu Kui](https://arxiv.org/search/cs?searchtype=author&query=Kui%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T)

> Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply these principles differently. This work introduces another component to this framework: Multi-Agent Cross-translated Diversification (MACD). The method trains multiple UMT agents and then translates monolingual data back and forth using non-duplicative agents to acquire synthetic parallel data for supervised MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation tasks. Through extensive experimental analyses, we show that MACD is effective because it embraces data diversity while other similar variants do not.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.02163](https://arxiv.org/abs/2006.02163) [cs.CL]** |
|           | (or **[arXiv:2006.02163v1](https://arxiv.org/abs/2006.02163v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-4">4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer</h2>

Title: [Improved acoustic word embeddings for zero-resource languages using multilingual transfer](https://arxiv.org/abs/2006.02295)

Authors: [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Yevgen Matusevych](https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

> Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. Such embeddings can form the basis for speech search, indexing and discovery systems when conventional speech recognition is not possible. In zero-resource settings where unlabelled speech is the only available resource, we need a method that gives robust embeddings on an arbitrary language. Here we explore multilingual transfer: we train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. We consider three multilingual recurrent neural network (RNN) models: a classifier trained on the joint vocabularies of all training languages; a Siamese RNN trained to discriminate between same and different words from multiple languages; and a correspondence autoencoder (CAE) RNN trained to reconstruct word pairs. In a word discrimination task on six target languages, all of these models outperform state-of-the-art unsupervised models trained on the zero-resource languages themselves, giving relative improvements of more than 30% in average precision. When using only a few training languages, the multilingual CAE performs better, but with more training languages the other multilingual models perform similarly. Using more training languages is generally beneficial, but improvements are marginal on some languages. We present probing experiments which show that the CAE encodes more phonetic, word duration, language identity and speaker information than the other multilingual models.

| Comments: | 11 pages, 7 figures, 8 tables. arXiv admin note: text overlap with [arXiv:2002.02109](https://arxiv.org/abs/2002.02109) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02295](https://arxiv.org/abs/2006.02295) [cs.CL]** |
|           | (or **[arXiv:2006.02295v1](https://arxiv.org/abs/2006.02295v1) [cs.CL]** for this version) |







# 2020-06-03

[Return to Index](#Index)



<h2 id="2020-06-03-1">1. WikiBERT models: deep transfer learning for many languages</h2>

Title: [WikiBERT models: deep transfer learning for many languages](https://arxiv.org/abs/2006.01538)

Authors: [Sampo Pyysalo](https://arxiv.org/search/cs?searchtype=author&query=Pyysalo%2C+S), [Jenna Kanerva](https://arxiv.org/search/cs?searchtype=author&query=Kanerva%2C+J), [Antti Virtanen](https://arxiv.org/search/cs?searchtype=author&query=Virtanen%2C+A), [Filip Ginter](https://arxiv.org/search/cs?searchtype=author&query=Ginter%2C+F)

> Deep neural language models such as BERT have enabled substantial recent advances in many natural language processing tasks. Due to the effort and computational cost involved in their pre-training, language-specific models are typically introduced only for a small number of high-resource languages such as English. While multilingual models covering large numbers of languages are available, recent work suggests monolingual training can produce better models, and our understanding of the tradeoffs between mono- and multilingual training is incomplete. In this paper, we introduce a simple, fully automated pipeline for creating language-specific BERT models from Wikipedia data and introduce 42 new such models, most for languages up to now lacking dedicated deep neural language models. We assess the merits of these models using the state-of-the-art UDify parser on Universal Dependencies data, contrasting performance with results using the multilingual BERT model. We find that UDify using WikiBERT models outperforms the parser using mBERT on average, with the language-specific models showing substantially improved performance for some languages, yet limited improvement or a decrease in performance for others. We also present preliminary results as first steps toward an understanding of the conditions under which language-specific models are most beneficial. All of the methods and models introduced in this work are available under open licenses from [this https URL](https://github.com/turkunlp/wikibert).

| Comments: | 7 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.01538](https://arxiv.org/abs/2006.01538) [cs.CL]** |
|           | (or **[arXiv:2006.01538v1](https://arxiv.org/abs/2006.01538v1) [cs.CL]** for this version) |





<h2 id="2020-06-03-2">2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders</h2>

Title: [Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](https://arxiv.org/abs/2006.01594)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

> We propose a modular architecture of language-specific encoder-decoders that constitutes a multilingual machine translation system that can be incrementally extended to new languages without the need for retraining the existing system when adding new languages. Differently from previous works, we simultaneously train N languages in all translation directions by alternately freezing encoder or decoder modules, which indirectly forces the system to train in a common intermediate representation for all languages. Experimental results from multilingual machine translation show that we can successfully train this modular architecture improving on the initial languages while falling slightly behind when adding new languages or doing zero-shot translation. Additional comparison of the quality of sentence representation in the task of natural language inference shows that the alternately freezing training is also beneficial in this direction.

| Comments:    | arXiv admin note: text overlap with [arXiv:2004.06575](https://arxiv.org/abs/2004.06575) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2006.01594](https://arxiv.org/abs/2006.01594) [cs.CL]** |
|              | (or **[arXiv:2006.01594v1](https://arxiv.org/abs/2006.01594v1) [cs.CL]** for this version) |





# 2020-06-02

[Return to Index](#Index)



<h2 id="2020-06-02-1">1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models</h2>

Title: [A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](https://arxiv.org/abs/2006.00031)

Authors: [Nikolay Arefyev](https://arxiv.org/search/cs?searchtype=author&query=Arefyev%2C+N), [Boris Sheludko](https://arxiv.org/search/cs?searchtype=author&query=Sheludko%2C+B), [Alexander Podolskiy](https://arxiv.org/search/cs?searchtype=author&query=Podolskiy%2C+A), [Alexander Panchenko](https://arxiv.org/search/cs?searchtype=author&query=Panchenko%2C+A)

> Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00031](https://arxiv.org/abs/2006.00031) [cs.CL]** |
|           | (or **[arXiv:2006.00031v1](https://arxiv.org/abs/2006.00031v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-2">2. Dynamic Masking for Improved Stability in Spoken Language Translation</h2>

Title: [Dynamic Masking for Improved Stability in Spoken Language Translation](https://arxiv.org/abs/2006.00249)

Authors: [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> For spoken language translation (SLT) in live scenarios such as conferences, lectures and meetings, it is desirable to show the translation to the user as quickly as possible, avoiding an annoying lag between speaker and translated captions. In other words, we would like low-latency, online SLT. If we assume a pipeline of automatic speech recognition (ASR) and machine translation (MT) then a viable approach to online SLT is to pair an online ASR system, with a a retranslation strategy, where the MT system re-translates every update received from ASR. However this can result in annoying "flicker" as the MT system updates its translation. A possible solution is to add a fixed delay, or "mask" to the the output of the MT system, but a fixed global mask introduces undesirable latency to the output. We show how this mask can be set dynamically, improving the latency-flicker trade-off without sacrificing translation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00249](https://arxiv.org/abs/2006.00249) [cs.CL]** |
|           | (or **[arXiv:2006.00249v1](https://arxiv.org/abs/2006.00249v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-3">3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation</h2>

Title: [Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](https://arxiv.org/abs/2006.00262)

Authors: [Sosuke Nishikawa](https://arxiv.org/search/cs?searchtype=author&query=Nishikawa%2C+S), [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Unsupervised bilingual word embedding (BWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method assumes that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we propose using a pseudo-parallel corpus generated by an unsupervised machine translation model to facilitate structural similarity of the two embedding spaces and improve the quality of BWEs in the mapping method. We show that our approach substantially outperforms baselines and other alternative approaches given the same amount of data, and, through detailed analysis, we argue that data augmentation with the pseudo data from unsupervised machine translation is especially effective for BWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data reflects some nature of the original language that helps learning similar embedding spaces between the source and target languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00262](https://arxiv.org/abs/2006.00262) [cs.CL]** |
|           | (or **[arXiv:2006.00262v1](https://arxiv.org/abs/2006.00262v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-4">4. Neural Unsupervised Domain Adaptation in NLP---A Survey</h2>

Title: [Neural Unsupervised Domain Adaptation in NLP---A Survey](https://arxiv.org/abs/2006.00632)

Authors: [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00632](https://arxiv.org/abs/2006.00632) [cs.CL]** |
|           | (or **[arXiv:2006.00632v1](https://arxiv.org/abs/2006.00632v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-5">5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English</h2>

Title: [Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](https://arxiv.org/abs/2006.00814)

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Michael Ustaszewski](https://arxiv.org/search/cs?searchtype=author&query=Ustaszewski%2C+M), [Emmanuelle Esperança-Rodier](https://arxiv.org/search/cs?searchtype=author&query=Esperança-Rodier%2C+E), [Francis Brunet Manquat](https://arxiv.org/search/cs?searchtype=author&query=Manquat%2C+F+B), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> We conduct in this work an evaluation study comparing offline and online neural machine translation architectures. Two sequence-to-sequence models: convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based Transformer (Vaswani et al. 2017) are considered. We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English-German and German-English language pairs, the latter being particularly sensitive to latency constraints. The evaluation results allow us to identify the strengths and shortcomings of each model when we shift to the online setup.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00814](https://arxiv.org/abs/2006.00814) [cs.CL]** |
|           | (or **[arXiv:2006.00814v1](https://arxiv.org/abs/2006.00814v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-6">6. Attention Word Embedding</h2>

Title: [Attention Word Embedding](https://arxiv.org/abs/2006.00988)

Authors: [Shashank Sonkar](https://arxiv.org/search/cs?searchtype=author&query=Sonkar%2C+S), [Andrew E. Waters](https://arxiv.org/search/cs?searchtype=author&query=Waters%2C+A+E), [Richard G. Baraniuk](https://arxiv.org/search/cs?searchtype=author&query=Baraniuk%2C+R+G)

> Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00988](https://arxiv.org/abs/2006.00988) [cs.CL]** |
|           | (or **[arXiv:2006.00988v1](https://arxiv.org/abs/2006.00988v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-7">7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</h2>

Title: [Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](https://arxiv.org/abs/2006.01080)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.

| Comments: | Accepted at IWSLT 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01080](https://arxiv.org/abs/2006.01080) [cs.CL]** |
|           | (or **[arXiv:2006.01080v1](https://arxiv.org/abs/2006.01080v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-8">8. Cascaded Text Generation with Markov Transformers</h2>

Title: [Cascaded Text Generation with Markov Transformers](https://arxiv.org/abs/2006.01112)

Authors: [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.01112](https://arxiv.org/abs/2006.01112) [cs.CL]** |
|           | (or **[arXiv:2006.01112v1](https://arxiv.org/abs/2006.01112v1) [cs.CL]** for this version) |







# 2020-06-01

[Return to Index](#Index)



<h2 id="2020-06-01-1">1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP</h2>

Title: [Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP]()

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy use of fine-tuning BERT-like models in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of NLP tasks in a uniform toolkit, from text classification to sequence labeling and dependency parsing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.14672](https://arxiv.org/abs/2005.14672) [cs.CL]** |
|           | (or **[arXiv:2005.14672v1](https://arxiv.org/abs/2005.14672v1) [cs.CL]** for this version) |