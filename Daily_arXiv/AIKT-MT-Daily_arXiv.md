# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-06](#2019-08-06)
  - [1. Invariance-based Adversarial Attack on Neural Machine Translation Systems](#2019-08-06-1)
  - [2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](#2019-08-06-2)
  - [3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](#2019-08-06-3)
  - [4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](#2019-08-06-4)
  - [5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](#2019-08-06-5)
  - [6. Predicting Actions to Help Predict Translations](#2019-08-06-6)
  - [7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](#2019-08-06-7)

- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-06

[Return to Index](#Index)

<h2 id="2019-08-06-1">1. Invariance-based Adversarial Attack on Neural Machine Translation Systems</h2> 

Title: [Invariance-based Adversarial Attack on Neural Machine Translation Systems](https://arxiv.org/abs/1908.01165)

Authors: [Akshay Chaturvedi](https://arxiv.org/search/cs?searchtype=author&query=Chaturvedi%2C+A), [Abijith KP](https://arxiv.org/search/cs?searchtype=author&query=KP%2C+A), [Utpal Garain](https://arxiv.org/search/cs?searchtype=author&query=Garain%2C+U)

*(Submitted on 3 Aug 2019)*

> Recently, NLP models have been shown to be susceptible to adversarial attacks. In this paper, we explore adversarial attacks on neural machine translation (NMT) systems. Given a sentence in the source language, the goal of the proposed attack is to change multiple words while ensuring that the predicted translation remains unchanged. In order to choose the word from the source vocabulary, we propose a soft-attention based technique. The experiments are conducted on two language pairs: English-German (en-de) and English-French (en-fr) and two state-of-the-art NMT systems: BLSTM-based encoder-decoder with attention and Transformer. The proposed soft-attention based technique outperforms existing methods like HotFlip by a significant margin for all the conducted experiments The results demonstrate that state-of-the-art NMT systems are unable to capture the semantics of the source language.

| Comments: | Under review in IEEE/ACM TASLP                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.01165 [cs.LG]**                                 |
|           | (or **arXiv:1908.01165v1 [cs.LG]** for this version)         |



<h2 id="2019-08-06-2">2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content</h2> 

Title: [Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](https://arxiv.org/abs/1908.01587)

Authors: [Muhammad Zubair Asghar](https://arxiv.org/search/cs?searchtype=author&query=Asghar%2C+M+Z), [Fazli Subhan](https://arxiv.org/search/cs?searchtype=author&query=Subhan%2C+F), [Muhammad Imran](https://arxiv.org/search/cs?searchtype=author&query=Imran%2C+M), [Fazal Masud Kundi](https://arxiv.org/search/cs?searchtype=author&query=Kundi%2C+F+M), [Shahboddin Shamshirband](https://arxiv.org/search/cs?searchtype=author&query=Shamshirband%2C+S), [Amir Mosavi](https://arxiv.org/search/cs?searchtype=author&query=Mosavi%2C+A), [Peter Csiba](https://arxiv.org/search/cs?searchtype=author&query=Csiba%2C+P), [Annamaria R. Varkonyi-Koczy](https://arxiv.org/search/cs?searchtype=author&query=Varkonyi-Koczy%2C+A+R)

*(Submitted on 5 Aug 2019)*

> Emotion detection from the text is an important and challenging problem in text analytics. The opinion-mining experts are focusing on the development of emotion detection applications as they have received considerable attention of online community including users and business organization for collecting and interpreting public emotions. However, most of the existing works on emotion detection used less efficient machine learning classifiers with limited datasets, resulting in performance degradation. To overcome this issue, this work aims at the evaluation of the performance of different machine learning classifiers on a benchmark emotion dataset. The experimental results show the performance of different machine learning classifiers in terms of different evaluation metrics like precision, recall ad f-measure. Finally, a classifier with the best performance is recommended for the emotion classification.

| Comments:    | 30 pages, 13 tables, 1 figure                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| MSC classes: | 68T01                                                        |
| DOI:         | [10.20944/preprints201908.0019.v1](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.20944%2Fpreprints201908.0019.v1&v=7cef63df) |
| Cite as:     | **arXiv:1908.01587 [cs.IR]**                                 |
|              | (or **arXiv:1908.01587v1 [cs.IR]** for this version)         |





<h2 id="2019-08-06-3">3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation</h2> 

Title: [The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](https://arxiv.org/abs/1908.01192)

Authors: [Magdalena Biesialska](https://arxiv.org/search/cs?searchtype=author&query=Biesialska%2C+M), [Lluis Guardia](https://arxiv.org/search/cs?searchtype=author&query=Guardia%2C+L), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

*(Submitted on 3 Aug 2019)*

> Although the problem of similar language translation has been an area of research interest for many years, yet it is still far from being solved. In this paper, we study the performance of two popular approaches: statistical and neural. We conclude that both methods yield similar results; however, the performance varies depending on the language pair. While the statistical approach outperforms the neural one by a difference of 6 BLEU points for the Spanish-Portuguese language pair, the proposed neural model surpasses the statistical one by a difference of 2 BLEU points for Czech-Polish. In the former case, the language similarity (based on perplexity) is much higher than in the latter case. Additionally, we report negative results for the system combination with back-translation. Our TALP-UPC system submission won 1st place for Czech-to-Polish and 2nd place for Spanish-to-Portuguese in the official evaluation of the 1st WMT Similar Language Translation task.

| Comments: | WMT 2019 Shared Task paper                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.01192 [cs.CL]**                         |
|           | (or **arXiv:1908.01192v1 [cs.CL]** for this version) |





<h2 id="2019-08-06-4">4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English</h2> 

Title: [JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](https://arxiv.org/abs/1908.01349)

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Avishek Garain](https://arxiv.org/search/cs?searchtype=author&query=Garain%2C+A), [Adityar Rayala](https://arxiv.org/search/cs?searchtype=author&query=Rayala%2C+A), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 1 Aug 2019)*

> In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task. The system was created to translate news text from Lithuanian to English. To accomplish the given task, our system used a Word Embedding based Neural Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1908.00323](https://arxiv.org/abs/1908.00323) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.01349 [cs.CL]**                                 |
|           | (or **arXiv:1908.01349v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-5">5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian</h2> 

Title: [Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](https://arxiv.org/abs/1908.01519)

Authors: [Momchil Hardalov](https://arxiv.org/search/cs?searchtype=author&query=Hardalov%2C+M), [Ivan Koychev](https://arxiv.org/search/cs?searchtype=author&query=Koychev%2C+I), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

*(Submitted on 5 Aug 2019)*

> Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23%, which is well above the baseline of 24.89%.

| Comments: | Accepted at RANLP 2019 (13 pages, 2 figures, 6 tables)       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **arXiv:1908.01519 [cs.CL]**                                 |
|           | (or **arXiv:1908.01519v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-6">6. Predicting Actions to Help Predict Translations</h2> 

Title: [Predicting Actions to Help Predict Translations](https://arxiv.org/abs/1908.01665)

Authors: [Zixiu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Josiah Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 5 Aug 2019)*

> We address the task of text translation on the How2 dataset using a state of the art transformer-based multimodal approach. The question we ask ourselves is whether visual features can support the translation process, in particular, given that this is a dataset extracted from videos, we focus on the translation of actions, which we believe are poorly captured in current static image-text datasets currently used for multimodal translation. For that purpose, we extract different types of action features from the videos and carefully investigate how helpful this visual information is by testing whether it can increase translation quality when used in conjunction with (i) the original text and (ii) the original text where action-related words (or all verbs) are masked out. The latter is a simulation that helps us assess the utility of the image in cases where the text does not provide enough context about the action, or in the presence of noise in the input text.

| Comments: | Accepted to workshop "The How2 Challenge: New Tasks for Vision & Language" of International Conference on Machine Learning 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1908.01665 [cs.CL]**                                 |
|           | (or **arXiv:1908.01665v1 [cs.CL]** for this version)         |





<h2 id="2019-08-06-7">7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing</h2> 

Title: [Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](https://arxiv.org/abs/1908.01699)

Authors: [David Awad](https://arxiv.org/search/cs?searchtype=author&query=Awad%2C+D)

*(Submitted on 5 Aug 2019)*

> Thoth is a tool designed to combine many different types of speed reading technology. The largest insight is using natural language parsing for more optimal rapid serial visual presentation and more effective reading information.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **arXiv:1908.01699 [cs.CL]**                                 |
|           | (or **arXiv:1908.01699v1 [cs.CL]** for this version)         |





# 2019-08-02

[Return to Index](#Index)

<h2 id="2019-08-02-1">1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data</h2> 
Title: [Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](https://arxiv.org/abs/1908.00449)

Authors: [Jacob Harer](https://arxiv.org/search/cs?searchtype=author&query=Harer%2C+J), [Chris Reale](https://arxiv.org/search/cs?searchtype=author&query=Reale%2C+C), [Peter Chin](https://arxiv.org/search/cs?searchtype=author&query=Chin%2C+P)

*(Submitted on 1 Aug 2019)*

> Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of 25% F0.5 over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a 10% improvement in recall on the CoNLL 2014 benchmark and the highest to date F0.5 score on the AESW benchmark of 50.43.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.00449 [cs.LG]**                                 |
|           | (or **arXiv:1908.00449v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-2">2. Learning Joint Acoustic-Phonetic Word Embeddings</h2> 
Title: [Learning Joint Acoustic-Phonetic Word Embeddings](https://arxiv.org/abs/1908.00493)

Authors: [Mohamed El-Geish](https://arxiv.org/search/cs?searchtype=author&query=El-Geish%2C+M)

*(Submitted on 1 Aug 2019)*

> Most speech recognition tasks pertain to mapping words across two modalities: acoustic and orthographic. In this work, we suggest learning encoders that map variable-length, acoustic or phonetic, sequences that represent words into fixed-dimensional vectors in a shared latent space; such that the distance between two word vectors represents how closely the two words sound. Instead of directly learning the distances between word vectors, we employ weak supervision and model a binary classification task to predict whether two inputs, one of each modality, represent the same word given a distance threshold. We explore various deep-learning models, bimodal contrastive losses, and techniques for mining hard negative examples such as the semi-supervised technique of self-labeling. Our best model achieves an F1 score of 0.95 for the binary classification task.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.00493 [cs.LG]**                                 |
|           | (or **arXiv:1908.00493v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-3">3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English</h2> 
Title: [JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](https://arxiv.org/abs/1908.00323)

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 1 Aug 2019)*

> In the current work, we present a description of the system submitted to WMT 2018 News Translation Shared task. The system was created to translate news text from Finnish to English. The system used a Character Based Neural Machine Translation model to accomplish the given task. The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same. Our system garnered a BLEU score of 12.9.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.00323 [cs.CL]**                         |
|           | (or **arXiv:1908.00323v1 [cs.CL]** for this version) |