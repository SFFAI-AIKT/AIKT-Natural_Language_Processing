# Daily arXiv: Machine Translation - Nov., 2019

# Index

- [2019-11-13](#2019-11-13)
  - [1. How to Evaluate Word Representations of Informal Domain](#2019-11-13-1)
  - [2. Character-based NMT with Transformer](#2019-11-13-2)
- [2019-11-12](#2019-11-12)
  - [1. Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation](#2019-11-12-1)
  - [2. Learning to Copy for Automatic Post-Editing](#2019-11-12-2)
  - [3. A Reinforced Generation of Adversarial Samples for Neural Machine Translation](#2019-11-12-3)
  - [4. Code-Mixed to Monolingual Translation Framework](#2019-11-12-4)
  - [5. Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models](#2019-11-12-5)
  - [6. Translationese as a Language in "Multilingual" NMT](#2019-11-12-6)
  - [7. Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser](#2019-11-12-7)
  - [8. Semantic Noise Matters for Neural Natural Language Generation](#2019-11-12-8)
  - [9. Language Model-Driven Unsupervised Neural Machine Translation](#2019-11-12-9)
  - [10. BP-Transformer: Modelling Long-Range Context via Binary Partitioning](#2019-11-12-10)
  - [11. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables](#2019-11-12-11)
  - [12. Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning](#2019-11-12-12)
  - [13. Diversity by Phonetics and its Application in Neural Machine Translation](#2019-11-12-13)
- [2019-11-11](#2019-11-11)
  - [1. Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing](#2019-11-11-1)
  - [2. Low-Resource Machine Translation using Interlinear Glosses](#2019-11-11-2)
  - [3. Understanding Knowledge Distillation in Non-autoregressive Machine Translation](#2019-11-11-3)
  - [4. SubCharacter Chinese-English Neural Machine Translation with Wubi encoding](#2019-11-11-4)
  - [5. Improving Grammatical Error Correction with Machine Translation Pairs](#2019-11-11-5)
  - [6. The LIG system for the English-Czech Text Translation Task of IWSLT 2019](#2019-11-11-6)
  - [7. Should All Cross-Lingual Embeddings Speak English?](#2019-11-11-7)
  - [8. Interactive Refinement of Cross-Lingual Word Embeddings](#2019-11-11-8)
  - [9. Domain Robustness in Neural Machine Translation](#2019-11-11-9)
  - [10. Pretrained Language Models for Document-Level Neural Machine Translation](#2019-11-11-10)
  - [11. How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?](#2019-11-11-11)
  - [12. Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates](#2019-11-11-12)
  - [13. Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization](#2019-11-11-13)
  - [14. Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation](#2019-11-11-4)
- [2019-11-07](#2019-11-07)
  - [1. Fast Transformer Decoding: One Write-Head is All You Need](#2019-11-07-1)
  - [2. Unsupervised Cross-lingual Representation Learning at Scale](#2019-11-07-2)
  - [3. Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information](#2019-11-07-3)
- [2019-11-06](#2019-11-06)
  - [1. Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)](#2019-11-06-1)
  - [2. Emerging Cross-lingual Structure in Pretrained Language Models](#2019-11-06-2)
  - [3. On Compositionality in Neural Machine Translation](#2019-11-06-3)
  - [4. Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation](#2019-11-06-4)
  - [5. Adversarial Language Games for Advanced Natural Language Intelligence](#2019-11-06-5)
  - [6. Data Diversification: An Elegant Strategy For Neural Machine Translation](#2019-11-06-6)
- [2019-11-05](#2019-11-05)
  - [1. Attributed Sequence Embedding](#2019-11-05-1)
  - [2. Machine Translation Evaluation using Bi-directional Entailment](#2019-11-05-2)
  - [3. Controlling Text Complexity in Neural Machine Translation](#2019-11-05-3)
  - [4. Machine Translation in Pronunciation Space](#2019-11-05-4)
  - [5. Analysing Coreference in Transformer Outputs](#2019-11-05-5)
  - [6. Ordering Matters: Word Ordering Aware Unsupervised NMT](#2019-11-05-6)
- [2019-11-04](#2019-11-04)
  - [1. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping](#2019-11-04-1)
  - [2. Sequence Modeling with Unconstrained Generation Order](#2019-11-04-2)
  - [3. On the Linguistic Representational Power of Neural Machine Translation Models](#2019-11-04-3)
- [2019-11-01](#2019-11-01)
  - [1. Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation](#2019-11-01-1)
  - [2. Document-level Neural Machine Translation with Inter-Sentence Attention](#2019-11-01-2)
  - [3. Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019](#2019-11-01-3)
  - [4. Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness](#2019-11-01-4)
  - [5. Adversarial NLI: A New Benchmark for Natural Language Understanding](#2019-11-01-5)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-11-13

[Return to Index](#Index)



<h2 id="2019-11-13-1">1. How to Evaluate Word Representations of Informal Domain</h2>

Title: [How to Evaluate Word Representations of Informal Domain]( https://arxiv.org/abs/1911.04669 )

Authors: [Yekun Chai](https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+Y), [Naomi Saphra](https://arxiv.org/search/cs?searchtype=author&query=Saphra%2C+N), [Adam Lopez](https://arxiv.org/search/cs?searchtype=author&query=Lopez%2C+A)

*(Submitted on 12 Nov 2019)*

> Diverse word representations have surged in most state-of-the-art natural language processing (NLP) applications. Nevertheless, how to efficiently evaluate such word embeddings in the informal domain such as Twitter or forums, remains an ongoing challenge due to the lack of sufficient evaluation dataset. We derived a large list of variant spelling pairs from UrbanDictionary with the automatic approaches of weakly-supervised pattern-based bootstrapping and self-training linear-chain conditional random field (CRF). With these extracted relation pairs we promote the odds of eliding the text normalization procedure of traditional NLP pipelines and directly adopting representations of non-standard words in the informal domain. Our code is available.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04669](https://arxiv.org/abs/1911.04669) [cs.CL] |
|           | (or [arXiv:1911.04669v1](https://arxiv.org/abs/1911.04669v1) [cs.CL] for this version) |





<h2 id="2019-11-13-2">2. Character-based NMT with Transformer</h2>

Title: [Character-based NMT with Transformer]( https://arxiv.org/abs/1911.04997 )

Authors: [Rohit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+R), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Marc Dymetman](https://arxiv.org/search/cs?searchtype=author&query=Dymetman%2C+M), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M)

*(Submitted on 12 Nov 2019)*

> Character-based translation has several appealing advantages, but its performance is in general worse than a carefully tuned BPE baseline. In this paper we study the impact of character-based input and output with the Transformer architecture. In particular, our experiments on EN-DE show that character-based Transformer models are more robust than their BPE counterpart, both when translating noisy text, and when translating text from a different domain. To obtain comparable BLEU scores in clean, in-domain data and close the gap with BPE-based models we use known techniques to train deeper Transformer models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04997](https://arxiv.org/abs/1911.04997) [cs.CL] |
|           | (or [arXiv:1911.04997v1](https://arxiv.org/abs/1911.04997v1) [cs.CL] for this version) |









# 2019-11-12

[Return to Index](#Index)



<h2 id="2019-11-12-1">1. Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation</h2>
Title: [Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation]( https://arxiv.org/abs/1911.03531 )

Authors: [Ali Fadel](https://arxiv.org/search/cs?searchtype=author&query=Fadel%2C+A), [Ibraheem Tuffaha](https://arxiv.org/search/cs?searchtype=author&query=Tuffaha%2C+I), [Bara' Al-Jawarneh](https://arxiv.org/search/cs?searchtype=author&query=Al-Jawarneh%2C+B), [Mahmoud Al-Ayyoub](https://arxiv.org/search/cs?searchtype=author&query=Al-Ayyoub%2C+M)

*(Submitted on 8 Nov 2019)*

> In this work, we present several deep learning models for the automatic diacritization of Arabic text. Our models are built using two main approaches, viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with several enhancements such as 100-hot encoding, embeddings, Conditional Random Field (CRF) and Block-Normalized Gradient (BNG). The models are tested on the only freely available benchmark dataset and the results show that our models are either better or on par with other models, which require language-dependent post-processing steps, unlike ours. Moreover, we show that diacritics in Arabic can be used to enhance the models of NLP tasks such as Machine Translation (MT) by proposing the Translation over Diacritization (ToD) approach.

| Comments: | 18 pages, 17 figures, 14 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| DOI:      | [10.18653/v1/D19-5229](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FD19-5229&v=0855f1dd) |
| Cite as:  | [arXiv:1911.03531](https://arxiv.org/abs/1911.03531) [cs.CL] |
|           | (or [arXiv:1911.03531v1](https://arxiv.org/abs/1911.03531v1) [cs.CL] for this version) |





<h2 id="2019-11-12-2">2. Learning to Copy for Automatic Post-Editing</h2>
Title: [Learning to Copy for Automatic Post-Editing]( https://arxiv.org/abs/1911.03627 )

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Huanbo Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+H), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

*(Submitted on 9 Nov 2019)*

> Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03627](https://arxiv.org/abs/1911.03627) [cs.CL] |
|           | (or [arXiv:1911.03627v1](https://arxiv.org/abs/1911.03627v1) [cs.CL] for this version) |





<h2 id="2019-11-12-3">3. A Reinforced Generation of Adversarial Samples for Neural Machine Translation</h2>
Title: [A Reinforced Generation of Adversarial Samples for Neural Machine Translation]( https://arxiv.org/abs/1911.03677 )

Authors: [Wei Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+W), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Xinyu Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

*(Submitted on 9 Nov 2019)*

> Neural machine translation systems tend to fail on less de-cent inputs despite its great efficacy, which may greatly harm the credibility of these systems. Fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial samples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g.BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial samples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, showing its capability of pitfall exposure.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03677](https://arxiv.org/abs/1911.03677) [cs.CL] |
|           | (or [arXiv:1911.03677v1](https://arxiv.org/abs/1911.03677v1) [cs.CL] for this version) |





<h2 id="2019-11-12-4">4. Code-Mixed to Monolingual Translation Framework</h2>
Title: [Code-Mixed to Monolingual Translation Framework]( https://arxiv.org/abs/1911.03772 )

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Soumil Mandal](https://arxiv.org/search/cs?searchtype=author&query=Mandal%2C+S), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 9 Nov 2019)*

> The use of multilingualism in the new generation is widespread in the form of code-mixed data on social media, and therefore a robust translation system is required for catering to the monolingual users, as well as for easier comprehension by language processing models. In this work, we present a translation framework that uses a translation-transliteration strategy for translating code-mixed data into their equivalent monolingual instances. For converting the output to a more fluent form, it is reordered using a target language model. The most important advantage of the proposed framework is that it does not require a code-mixed to monolingual parallel corpus at any point. On testing the framework, it achieved BLEU and TER scores of 16.47 and 55.45, respectively. Since the proposed framework deals with various sub-modules, we dive deeper into the importance of each of them, analyze the errors and finally, discuss some improvement strategies.

| Comments: | 6 pages, 3 figures, 2 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03772](https://arxiv.org/abs/1911.03772) [cs.CL] |
|           | (or [arXiv:1911.03772v1](https://arxiv.org/abs/1911.03772v1) [cs.CL] for this version) |





<h2 id="2019-11-12-5">5. Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models</h2>
Title: [Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models]( https://arxiv.org/abs/1911.03782 )

Authors: [Siddharth Dalmia](https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S), [Abdelrahman Mohamed](https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+A), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 9 Nov 2019)*

> Inspired by modular software design principles of independence, interchangeability, and clarity of interface, we introduce a method for enforcing encoder-decoder modularity in seq2seq models without sacrificing the overall model quality or its full differentiability. We discretize the encoder output units into a predefined interpretable vocabulary space using the Connectionist Temporal Classification (CTC) loss. Our modular systems achieve near SOTA performance on the 300h Switchboard benchmark, with WER of 8.3% and 17.6% on the SWB and CH subsets, using seq2seq models with encoder and decoder modules which are independent and interchangeable.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03782](https://arxiv.org/abs/1911.03782) [cs.CL] |
|           | (or [arXiv:1911.03782v1](https://arxiv.org/abs/1911.03782v1) [cs.CL] for this version) |





<h2 id="2019-11-12-6">6. Translationese as a Language in "Multilingual" NMT</h2>
Title: [Translationese as a Language in "Multilingual" NMT]( https://arxiv.org/abs/1911.03823 )

Authors: [Parker Riley](https://arxiv.org/search/cs?searchtype=author&query=Riley%2C+P), [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

*(Submitted on 10 Nov 2019)*

> Machine translation has an undesirable propensity to produce "translationese" artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train sentence-level classifiers to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these models using metrics to measure the degree of translationese in the output, and present an analysis of the capriciousness of heuristically-based train-data tagging.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03823](https://arxiv.org/abs/1911.03823) [cs.CL] |
|           | (or [arXiv:1911.03823v1](https://arxiv.org/abs/1911.03823v1) [cs.CL] for this version) |





<h2 id="2019-11-12-7">7. Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser</h2>
Title: [Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser]( https://arxiv.org/abs/1911.03875 )

Authors: [Khalil Mrini](https://arxiv.org/search/cs?searchtype=author&query=Mrini%2C+K), [Franck Dernoncourt](https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F), [Trung Bui](https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T), [Walter Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+W), [Ndapa Nakashole](https://arxiv.org/search/cs?searchtype=author&query=Nakashole%2C+N)

*(Submitted on 10 Nov 2019)*

> Attention mechanisms have improved the performance of NLP tasks while providing for appearance of model interpretability. Self-attention is currently widely used in NLP models, however it is difficult to interpret due to the numerous attention distributions. We hypothesize that model representations can benefit from label-specific information, while facilitating interpretation of predictions. We introduce the Label Attention Layer: a new form of self-attention where attention heads represent labels. We validate our hypothesis by running experiments in constituency and dependency parsing and show our new model obtains new state-of-the-art results for both tasks on the English Penn Treebank. Our neural parser obtains 96.34 F1 score for constituency parsing, and 97.33 UAS and 96.29 LAS for dependency parsing. Additionally, our model requires fewer layers, therefore, fewer parameters compared to existing work.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03875](https://arxiv.org/abs/1911.03875) [cs.CL] |
|           | (or [arXiv:1911.03875v1](https://arxiv.org/abs/1911.03875v1) [cs.CL] for this version) |





<h2 id="2019-11-12-8">8. Semantic Noise Matters for Neural Natural Language Generation</h2>
Title: [Semantic Noise Matters for Neural Natural Language Generation]( https://arxiv.org/abs/1911.03905 )

Authors: [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [David M. Howcroft](https://arxiv.org/search/cs?searchtype=author&query=Howcroft%2C+D+M), [Verena Rieser](https://arxiv.org/search/cs?searchtype=author&query=Rieser%2C+V)

*(Submitted on 10 Nov 2019)*

> Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.

| Comments:    | In Proceedings of INLG 2019, Tokyo, Japan                    |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:1911.03905](https://arxiv.org/abs/1911.03905) [cs.CL] |
|              | (or [arXiv:1911.03905v1](https://arxiv.org/abs/1911.03905v1) [cs.CL] for this version) |





<h2 id="2019-11-12-9">9. Language Model-Driven Unsupervised Neural Machine Translation</h2>
Title: [Language Model-Driven Unsupervised Neural Machine Translation]( https://arxiv.org/abs/1911.03937 )

Authors: [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Youyuan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Ruoran Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+R), [Xiaodong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhenshuang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z)

*(Submitted on 10 Nov 2019)*

> Unsupervised neural machine translation(NMT) is associated with noise and errors in synthetic data when executing vanilla back-translations. Here, we explicitly exploits language model(LM) to drive construction of an unsupervised NMT system. This features two steps. First, we initialize NMT models using synthetic data generated via temporary statistical machine translation(SMT). Second, unlike vanilla back-translation, we formulate a weight function, that scores synthetic data at each step of subsequent iterative training; this allows unsupervised training to an improved outcome. We present the detailed mathematical construction of our method. Experimental WMT2014 English-French, and WMT2016 English-German and English-Russian translation tasks revealed that our method outperforms the best prior systems by more than 3 BLEU points.

| Comments: | 11 pages, 3 figures, 7 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03937](https://arxiv.org/abs/1911.03937) [cs.CL] |
|           | (or [arXiv:1911.03937v1](https://arxiv.org/abs/1911.03937v1) [cs.CL] for this version) |





<h2 id="2019-11-12-10">10. BP-Transformer: Modelling Long-Range Context via Binary Partitioning</h2>
Title: [BP-Transformer: Modelling Long-Range Context via Binary Partitioning]( https://arxiv.org/abs/1911.04070 )

Authors: [Zihao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Qipeng Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Q), [Quan Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Q), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Zheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z)

*(Submitted on 11 Nov 2019)*

> The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields O(k⋅nlog(n/k)) connections where k is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04070](https://arxiv.org/abs/1911.04070) [cs.CL] |
|           | (or [arXiv:1911.04070v1](https://arxiv.org/abs/1911.04070v1) [cs.CL] for this version) |





<h2 id="2019-11-12-11">11. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables</h2>
Title: [Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables]( https://arxiv.org/abs/1911.04081 )

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jamin Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+J), [Yan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Andrea Madotto](https://arxiv.org/search/cs?searchtype=author&query=Madotto%2C+A), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

*(Submitted on 11 Nov 2019)*

> Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.

| Comments: | Accepted in EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.04081](https://arxiv.org/abs/1911.04081) [cs.CL] |
|           | (or [arXiv:1911.04081v1](https://arxiv.org/abs/1911.04081v1) [cs.CL] for this version) |





<h2 id="2019-11-12-12">12. Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning</h2>
Title: [Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning]()

Authors: [Sathish Indurthi](https://arxiv.org/search/cs?searchtype=author&query=Indurthi%2C+S), [Houjeung Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+H), [Nikhil Kumar Lakumarapu](https://arxiv.org/search/cs?searchtype=author&query=Lakumarapu%2C+N+K), [Beomseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+B), [Insoo Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+I), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Chanwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C)

*(Submitted on 11 Nov 2019)*

> End-to-end Speech Translation (ST) models have several advantages such as lower latency, smaller model size, and less error compounding over conventional pipelines that combine Automatic Speech Recognition (ASR) and text Machine Translation (MT) models. However, collecting large amounts of parallel data for ST task is more difficult compared to the ASR and MT tasks. Previous studies have proposed the use of transfer learning approaches to overcome the above difficulty. These approaches benefit from weakly supervised training data, such as ASR speech-to-transcript or MT text-to-text translation pairs. However, the parameters in these models are updated independently of each task, which may lead to sub-optimal solutions. In this work, we adopt a meta-learning algorithm to train a modality agnostic multi-task model that transfers knowledge from source tasks=ASR+MT to target task=ST where ST task severely lacks data. In the meta-learning phase, the parameters of the model are exposed to vast amounts of speech transcripts (e.g., English ASR) and text translations (e.g., English-German MT). During this phase, parameters are updated in such a way to understand speech, text representations, the relation between them, as well as act as a good initialization point for the target ST task. We evaluate the proposed meta-learning approach for ST tasks on English-German (En-De) and English-French (En-Fr) language pairs from the Multilingual Speech Translation Corpus (MuST-C). Our method outperforms the previous transfer learning approaches and sets new state-of-the-art results for En-De and En-Fr ST tasks by obtaining 9.18, and 11.76 BLEU point improvements, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04283](https://arxiv.org/abs/1911.04283) [cs.CL] |
|           | (or [arXiv:1911.04283v1](https://arxiv.org/abs/1911.04283v1) [cs.CL] for this version) |





<h2 id="2019-11-12-13">13. Diversity by Phonetics and its Application in Neural Machine Translation</h2>
Title: [Diversity by Phonetics and its Application in Neural Machine Translation]( https://arxiv.org/abs/1911.04292 )

Authors: [Abdul Rafae Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A+R), [Jia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J)

*(Submitted on 11 Nov 2019)*

> We introduce a powerful approach for Neural Machine Translation (NMT), whereby, during training and testing, together with the input we provide its phonetic encoding and the variants of such an encoding. This way we obtain very significant improvements up to 4 BLEU points over the state-of-the-art large-scale system. The phonetic encoding is the first part of our contribution, with a second being a theory that aims to understand the reason for this improvement. Our hypothesis states that the phonetic encoding helps NMT because it encodes a procedure to emphasize the difference between semantically diverse sentences. We conduct an empirical geometric validation of our hypothesis in support of which we obtain overwhelming evidence. Subsequently, as our third contribution and based on our theory, we develop artificial mechanisms that leverage during learning the hypothesized (and verified) effect phonetics. We achieve significant and consistent improvements overall language pairs and datasets: French-English, German-English, and Chinese-English in medium task IWSLT'17 and French-English in large task WMT'18 Bio, with up to 4 BLEU points over the state-of-the-art. Moreover, our approaches are more robust than baselines when evaluated on unknown out-of-domain test sets with up to a 5 BLEU point increase.

| Comments: | In [this http URL](http://openreview.net/) (28 May 2019)     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.04292](https://arxiv.org/abs/1911.04292) [cs.CL] |
|           | (or [arXiv:1911.04292v1](https://arxiv.org/abs/1911.04292v1) [cs.CL] for this version) |









# 2019-11-11

[Return to Index](#Index)



<h2 id="2019-11-11-1">1. Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing</h2>
Title: [Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing]( https://arxiv.org/abs/1911.02692 )

Authors:[Haoming Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H), [Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [Chong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Tuo Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 7 Nov 2019)*

> Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing, and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02692](https://arxiv.org/abs/1911.02692) [cs.CL] |
|           | (or [arXiv:1911.02692v1](https://arxiv.org/abs/1911.02692v1) [cs.CL] for this version) |





<h2 id="2019-11-11-2">2. Low-Resource Machine Translation using Interlinear Glosses</h2>
Title: [Low-Resource Machine Translation using Interlinear Glosses]( https://arxiv.org/abs/1911.02709 )

Authors:[Zhong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Lori Levin](https://arxiv.org/search/cs?searchtype=author&query=Levin%2C+L), [David R. Mortensen](https://arxiv.org/search/cs?searchtype=author&query=Mortensen%2C+D+R), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

*(Submitted on 7 Nov 2019)*

> Neural Machine Translation (NMT) does not handle low-resource translation well because NMT is data-hungry and low-resource languages, by their nature, have limited parallel data. Many low-resource languages are morphologically rich, which complicates matters further by increasing data sparsity. However, a good linguist is capable of building a morphological analyzer in far fewer hours than it would take to collect and translate the amount of parallel data needed for conventional NMT. We combine the benefits of both NMT and linguistic information in our work. We use morphological analyzer to automatically generate interlinear glosses with dictionary or parallel data, and translate the source text to interlinear gloss as an interlingua representation, and finally translate into the target text using NMT trained on the ODIN dataset that includes a large collection of interlinear glosses and their corresponding target translations. Our result for translating from the interlinear gloss to the target text using the entire ODIN dataset achieves a BLEU score of 35.07. And our qualitative results show positive findings in a low-resource scenario of Turkish-English translation using 865 lines of training data. Our translation system yield better results than training NMT directly from the source language to the target language in a constrained-data setting, and is helpful to produce translation with sufficiently good content and fluency when data is scarce.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02709](https://arxiv.org/abs/1911.02709) [cs.CL] |
|           | (or [arXiv:1911.02709v1](https://arxiv.org/abs/1911.02709v1) [cs.CL] for this version) |





<h2 id="2019-11-11-3">3. Understanding Knowledge Distillation in Non-autoregressive Machine Translation</h2>
Title: [Understanding Knowledge Distillation in Non-autoregressive Machine Translation]( https://arxiv.org/abs/1911.02727 )

Authors:[Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 7 Nov 2019)*

> Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02727](https://arxiv.org/abs/1911.02727) [cs.CL] |
|           | (or [arXiv:1911.02727v1](https://arxiv.org/abs/1911.02727v1) [cs.CL] for this version) |





<h2 id="2019-11-11-4">4. SubCharacter Chinese-English Neural Machine Translation with Wubi encoding</h2>
Title: [SubCharacter Chinese-English Neural Machine Translation with Wubi encoding]( https://arxiv.org/abs/1911.02737 )

Authors:[Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Feifei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+F), [Xiaodong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhenshuang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z)

*(Submitted on 7 Nov 2019)*

> Neural machine translation (NMT) is one of the best methods for understanding the differences in semantic rules between two languages. Especially for Indo-European languages, subword-level models have achieved impressive results. However, when the translation task involves Chinese, semantic granularity remains at the word and character level, so there is still need more fine-grained translation model of Chinese. In this paper, we introduce a simple and effective method for Chinese translation at the sub-character level. Our approach uses the Wubi method to translate Chinese into English; byte-pair encoding (BPE) is then applied. Our method for Chinese-English translation eliminates the need for a complicated word segmentation algorithm during preprocessing. Furthermore, our method allows for sub-character-level neural translation based on recurrent neural network (RNN) architecture, without preprocessing. The empirical results show that for Chinese-English translation tasks, our sub-character-level model has a comparable BLEU score to the subword model, despite having a much smaller vocabulary. Additionally, the small vocabulary is highly advantageous for NMT model compression.

| Comments: | 10 pages, 3 figures, 7 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02737](https://arxiv.org/abs/1911.02737) [cs.CL] |
|           | (or [arXiv:1911.02737v1](https://arxiv.org/abs/1911.02737v1) [cs.CL] for this version) |





<h2 id="2019-11-11-5">5. Improving Grammatical Error Correction with Machine Translation Pairs</h2>
Title: [Improving Grammatical Error Correction with Machine Translation Pairs]( https://arxiv.org/abs/1911.02825 )

Authors:[Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Chang Mu](https://arxiv.org/search/cs?searchtype=author&query=Mu%2C+C), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 7 Nov 2019)*

> We propose a novel data synthesis method to generate diverse error-corrected sentence pairs for improving grammatical error correction, which is based on a pair of machine translation models of different qualities (i.e., poor and good). The poor translation model resembles the ESL (English as a second language) learner and tends to generate translations of low quality in terms of fluency and grammatical correctness, while the good translation model generally generates fluent and grammatically correct translations. We build the poor and good translation model with phrase-based statistical machine translation model with decreased language model weight and neural machine translation model respectively. By taking the pair of their translations of the same sentences in a bridge language as error-corrected sentence pairs, we can construct unlimited pseudo parallel data. Our approach is capable of generating diverse fluency-improving patterns without being limited by the pre-defined rule set and the seed error-corrected data. Experimental results demonstrate the effectiveness of our approach and show that it can be combined with other synthetic data sources to yield further improvements.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02825](https://arxiv.org/abs/1911.02825) [cs.CL] |
|           | (or [arXiv:1911.02825v1](https://arxiv.org/abs/1911.02825v1) [cs.CL] for this version) |





<h2 id="2019-11-11-6">6. The LIG system for the English-Czech Text Translation Task of IWSLT 2019</h2>
Title: [The LIG system for the English-Czech Text Translation Task of IWSLT 2019]( https://arxiv.org/abs/1911.02898 )

Authors:[Loïc Vial](https://arxiv.org/search/cs?searchtype=author&query=Vial%2C+L), [Benjamin Lecouteux](https://arxiv.org/search/cs?searchtype=author&query=Lecouteux%2C+B), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 7 Nov 2019)*

> In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations: 1) without any pre-trained language model (constrained), 2) using a language model trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a language model trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own language model (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text: one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our system by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own language model trained only on the allowed parallel data seems to improve the machine translation performances only when the system is trained on the smallest dataset.

| Comments: | IWSLT 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02898](https://arxiv.org/abs/1911.02898) [cs.CL] |
|           | (or [arXiv:1911.02898v1](https://arxiv.org/abs/1911.02898v1) [cs.CL] for this version) |





<h2 id="2019-11-11-7">7. Should All Cross-Lingual Embeddings Speak English?</h2>
Title: [Should All Cross-Lingual Embeddings Speak English?]( https://arxiv.org/abs/1911.03058 )

Authors:[Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 8 Nov 2019)*

> Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction performance. Second, we both expand the current evaluation dictionary collection to include all language pairs using triangulation, and also create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embeddings baselines, based on more than just Anglocentric experiments.

| Comments: | pre-print                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03058](https://arxiv.org/abs/1911.03058) [cs.CL] |
|           | (or [arXiv:1911.03058v1](https://arxiv.org/abs/1911.03058v1) [cs.CL] for this version) |





<h2 id="2019-11-11-8">8. Interactive Refinement of Cross-Lingual Word Embeddings</h2>
Title: [Interactive Refinement of Cross-Lingual Word Embeddings]( https://arxiv.org/abs/1911.03070 )

Authors:[Michelle Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+M), [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B), [Leah Findlater](https://arxiv.org/search/cs?searchtype=author&query=Findlater%2C+L), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

*(Submitted on 8 Nov 2019)*

> Cross-lingual word embeddings transfer knowledge between languages: models trained for a high-resource language can be used in a low-resource language. These embeddings are usually trained on general-purpose corpora but used for a domain-specific task. We introduce CLIME, an interactive system that allows a user to quickly adapt cross-lingual word embeddings for a given classification problem. First, words in the vocabulary are ranked by their salience to the downstream task. Then, salient keywords are displayed on an interface. Users mark the similarity between each keyword and its nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We experiment clime on a cross-lingual text classification benchmark for four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME also improves test accuracy faster than an active learning baseline, and a simple combination of CLIME with active learning has the highest test accuracy.

| Comments: | First two authors contribute equally                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.03070](https://arxiv.org/abs/1911.03070) [cs.CL] |
|           | (or [arXiv:1911.03070v1](https://arxiv.org/abs/1911.03070v1) [cs.CL] for this version) |





<h2 id="2019-11-11-9">9. Domain Robustness in Neural Machine Translation</h2>
Title: [Domain Robustness in Neural Machine Translation]( https://arxiv.org/abs/1911.03109 )

Authors:[Mathias Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+M), [Annette Rios](https://arxiv.org/search/cs?searchtype=author&query=Rios%2C+A), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 8 Nov 2019)*

> Translating text that diverges from the training domain is a key challenge for neural machine translation (NMT). Domain robustness - the generalization of models to unseen test domains - is low compared to statistical machine translation. In this paper, we investigate the performance of NMT on out-of-domain test sets, and ways to improve it.
> We observe that hallucination (translations that are fluent but unrelated to the source) is common in out-of-domain settings, and we empirically compare methods that improve adequacy (reconstruction), out-of-domain translation (subword regularization), or robustness against adversarial examples (defensive distillation), as well as noisy channel models.
> In experiments on German to English OPUS data, and German to Romansh, a low-resource scenario, we find that several methods improve domain robustness, reconstruction standing out as a method that not only improves automatic scores, but also shows improvements in a manual assessments of adequacy, albeit at some loss in fluency. However, out-of-domain performance is still relatively low and domain robustness remains an open problem.

| Comments: | V1                                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03109](https://arxiv.org/abs/1911.03109) [cs.CL] |
|           | (or [arXiv:1911.03109v1](https://arxiv.org/abs/1911.03109v1) [cs.CL] for this version) |





<h2 id="2019-11-11-10">10. Pretrained Language Models for Document-Level Neural Machine Translation</h2>
Title: [Pretrained Language Models for Document-Level Neural Machine Translation]( https://arxiv.org/abs/1911.03110 )

Authors:[Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 8 Nov 2019)*

> Previous work on document-level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In this paper, we investigate on using large contexts with three main contributions: (1) Different from previous work which pertrained models on large-scale sentence-level parallel corpora, we use pretrained language models, specifically BERT, which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts, which lead to comparable results on systems using small and large contexts; (3) We introduce a multi-task training for regularization to avoid models overfitting our training corpora, which further improves our systems together with a deeper encoder. Experiments are conducted on the widely used IWSLT data sets with three language pairs, i.e., Chinese--English, French--English and Spanish--English. Results show that our systems are significantly better than three previously reported document-level systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03110](https://arxiv.org/abs/1911.03110) [cs.CL] |
|           | (or [arXiv:1911.03110v1](https://arxiv.org/abs/1911.03110v1) [cs.CL] for this version) |





<h2 id="2019-11-11-11">11. How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?</h2>
Title: [How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?]( https://arxiv.org/abs/1911.03154 )

Authors:[Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 8 Nov 2019)*

> Despite the success of neural machine translation (NMT), simultaneous neural machine translation (SNMT), the task of translating in real time before a full sentence has been observed, remains challenging due to the syntactic structure difference and simultaneity requirements. In this paper, we propose a general framework to improve simultaneous translation with a pretrained consecutive neural machine translation (CNMT) model. Our framework contains two parts: prefix translation that utilizes a pretrained CNMT model to better translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in simultaneous translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03154](https://arxiv.org/abs/1911.03154) [cs.CL] |
|           | (or [arXiv:1911.03154v1](https://arxiv.org/abs/1911.03154v1) [cs.CL] for this version) |





<h2 id="2019-11-11-12">12. Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates</h2>
Title: [Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates]( https://arxiv.org/abs/1911.03167 )

Authors:[Javier Iranzo-Sánchez](https://arxiv.org/search/cs?searchtype=author&query=Iranzo-Sánchez%2C+J), [Joan Albert Silvestre-Cerdà](https://arxiv.org/search/cs?searchtype=author&query=Silvestre-Cerdà%2C+J+A), [Javier Jorge](https://arxiv.org/search/cs?searchtype=author&query=Jorge%2C+J), [Nahuel Roselló](https://arxiv.org/search/cs?searchtype=author&query=Roselló%2C+N), [Adrià Giménez](https://arxiv.org/search/cs?searchtype=author&query=Giménez%2C+A), [Albert Sanchis](https://arxiv.org/search/cs?searchtype=author&query=Sanchis%2C+A), [Jorge Civera](https://arxiv.org/search/cs?searchtype=author&query=Civera%2C+J), [Alfons Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+A)

*(Submitted on 8 Nov 2019)*

> Current research into spoken language translation (SLT) is often hampered by the lack of specific data resources for this task, as currently available SLT datasets are restricted to a limited set of language pairs. In this paper we present Europarl-ST, a novel multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. This paper describes the corpus creation process and presents a series of automatic speech recognition, machine translation and spoken language translation experiments that highlight the potential of this new resource. The corpus is released under a Creative Commons license and is freely accessible and downloadable.

| Comments: | Submitted to ICASSP2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.03167](https://arxiv.org/abs/1911.03167) [cs.CL] |
|           | (or [arXiv:1911.03167v1](https://arxiv.org/abs/1911.03167v1) [cs.CL] for this version) |





<h2 id="2019-11-11-13">13. Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization</h2>
Title: [Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization]( https://arxiv.org/abs/1911.03179 )

Authors:[Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Qiuhui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Jingyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

*(Submitted on 8 Nov 2019)*

> The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. While several previous works show that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly a Transformer model with more than 12 encoder/decoder layers fails to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation which changes the computation order of residual connection and layer normalization can effectively ease the optimization of deep Transformers. In addition, we deeply compare the subtle difference in computation order, and propose a parameter initialization method which simply puts Lipschitz restriction on the initialization of Transformers but can effectively ensure their convergence. We empirically show that with proper parameter initialization, deep Transformers with the original computation order can converge, which is quite in contrast to all previous works, and obtain significant improvements with up to 24 layers. Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders.

| Comments: | A similar work (Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention) is accepted by EMNLP 2019, but there are differences between in analysis and approaches |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.03179](https://arxiv.org/abs/1911.03179) [cs.CL] |
|           | (or [arXiv:1911.03179v1](https://arxiv.org/abs/1911.03179v1) [cs.CL] for this version) |





<h2 id="2019-11-11-14">14. Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation</h2>
Title: [Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation]( https://arxiv.org/abs/1911.03362 )

Authors:[Nikolay Bogoychev](https://arxiv.org/search/cs?searchtype=author&query=Bogoychev%2C+N), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 6 Nov 2019)*

> The quality of neural machine translation can be improved by leveraging additional monolingual resources to create synthetic training data. Source-side monolingual data can be (forward-)translated into the target language for self-training; target-side monolingual data can be back-translated. It has been widely reported that back-translation delivers superior results, but could this be due to artefacts in the test sets? We perform a case study using French-English news translation task and separate test sets based on their original languages. We show that forward translation delivers superior gains in terms of BLEU on sentences that were originally in the source language, complementing previous studies which show large improvements with back-translation on sentences that were originally in the target language. To better understand when and why forward and back-translation are effective, we study the role of domains, translationese, and noise. While translationese effects are well known to influence MT evaluation, we also find evidence that news data from different languages shows subtle domain differences, which is another explanation for varying performance on different portions of the test set. We perform additional low-resource experiments which demonstrate that forward translation is more sensitive to the quality of the initial translation system than back-translation, and tends to perform worse in low-resource settings.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03362](https://arxiv.org/abs/1911.03362) [cs.CL] |
|           | (or [arXiv:1911.03362v1](https://arxiv.org/abs/1911.03362v1) [cs.CL] for this version) |







# 2019-11-07

[Return to Index](#Index)



<h2 id="2019-11-07-1">1. Fast Transformer Decoding: One Write-Head is All You Need</h2>
Title: [Fast Transformer Decoding: One Write-Head is All You Need]( https://arxiv.org/abs/1911.02150 )

Authors:[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)

*(Submitted on 6 Nov 2019)*

> Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.

| Subjects: | **Neural and Evolutionary Computing (cs.NE)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02150](https://arxiv.org/abs/1911.02150) [cs.NE] |
|           | (or [arXiv:1911.02150v1](https://arxiv.org/abs/1911.02150v1) [cs.NE] for this version) |



<h2 id="2019-11-07-2">2. Unsupervised Cross-lingual Representation Learning at Scale</h2>
Title: [Unsupervised Cross-lingual Representation Learning at Scale]( https://arxiv.org/abs/1911.02116 )

Authors:[Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Kartikay Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+K), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Veselin Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V)

*(Submitted on 5 Nov 2019)*

> This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.

| Comments: | 12 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02116](https://arxiv.org/abs/1911.02116) [cs.CL] |
|           | (or [arXiv:1911.02116v1](https://arxiv.org/abs/1911.02116v1) [cs.CL] for this version) |





<h2 id="2019-11-07-3">3. Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information</h2>
Title: [Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information]( https://arxiv.org/abs/1911.02215 )

Authors:[Qiu Ran](https://arxiv.org/search/cs?searchtype=author&query=Ran%2C+Q), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

*(Submitted on 6 Nov 2019)*

> Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the enormous decoding space. To address this problem, we propose a novel NAT framework named ReorderNAT which explicitly models the reordering information in the decoding procedure. We further introduce deterministic and non-deterministic decoding strategies that utilize reordering information to narrow the decoding search space in our proposed ReorderNAT. Experimental results on various widely-used datasets show that our proposed model achieves better performance compared to existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a significant speedup.

| Comments: | 12 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02215](https://arxiv.org/abs/1911.02215) [cs.CL] |
|           | (or [arXiv:1911.02215v1](https://arxiv.org/abs/1911.02215v1) [cs.CL] for this version) |





# 2019-11-06

[Return to Index](#Index)



<h2 id="2019-11-06-1">1. Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)</h2>
Title: [Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)]( https://arxiv.org/abs/1911.01933 )

Authors: [Amelia Drew](https://arxiv.org/search/cs?searchtype=author&query=Drew%2C+A), [Alexander Heinecke](https://arxiv.org/search/cs?searchtype=author&query=Heinecke%2C+A)

*(Submitted on 5 Nov 2019)*

> We implement a Tensor Train layer in the TensorFlow Neural Machine Translation (NMT) model using the t3f library. We perform training runs on the IWSLT English-Vietnamese '15 and WMT German-English '16 datasets with learning rates ∈{0.0004,0.0008,0.0012}, maximum ranks ∈{2,4,8,16} and a range of core dimensions. We compare against a target BLEU test score of 24.0, obtained by our benchmark run. For the IWSLT English-Vietnamese training, we obtain BLEU test/dev scores of 24.0/21.9 and 24.2/21.9 using core dimensions (2,2,256)×(2,2,512) with learning rate 0.0012 and rank distributions (1,4,4,1) and (1,4,16,1) respectively. These runs use 113\% and 397\% of the flops of the benchmark run respectively. We find that, of the parameters surveyed, a higher learning rate and more `rectangular' core dimensions generally produce higher BLEU scores. For the WMT German-English dataset, we obtain BLEU scores of 24.0/23.8 using core dimensions (4,4,128)×(4,4,256) with learning rate 0.0012 and rank distribution (1,2,2,1). We discuss the potential for future optimization and application of Tensor Train decomposition to other NMT models.

| Comments: | 10 pages, 2 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.01933](https://arxiv.org/abs/1911.01933) [cs.LG] |
|           | (or [arXiv:1911.01933v1](https://arxiv.org/abs/1911.01933v1) [cs.LG] for this version) |





<h2 id="2019-11-06-2">2. Emerging Cross-lingual Structure in Pretrained Language Models</h2>
Title: [Emerging Cross-lingual Structure in Pretrained Language Models]( https://arxiv.org/abs/1911.01464 )

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Veselin Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V)

*(Submitted on 4 Nov 2019)*

> We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from independently trained models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries seem to be automatically discovered and aligned during the joint training process.

| Comments: | 10 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.01464](https://arxiv.org/abs/1911.01464) [cs.CL] |
|           | (or [arXiv:1911.01464v1](https://arxiv.org/abs/1911.01464v1) [cs.CL] for this version) |





<h2 id="2019-11-06-3">3. On Compositionality in Neural Machine Translation</h2>
Title: [On Compositionality in Neural Machine Translation]( https://arxiv.org/abs/1911.01497 )

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Vaibhav Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Jaimie Callan](https://arxiv.org/search/cs?searchtype=author&query=Callan%2C+J)

*(Submitted on 4 Nov 2019)*

> We investigate two specific manifestations of compositionality in Neural Machine Translation (NMT) : (1) Productivity - the ability of the model to extend its predictions beyond the observed length in training data and (2) Systematicity - the ability of the model to systematically recombine known parts and rules. We evaluate a standard Sequence to Sequence model on tests designed to assess these two properties in NMT. We quantitatively demonstrate that inadequate temporal processing, in the form of poor encoder representations is a bottleneck for both Productivity and Systematicity. We propose a simple pre-training mechanism which alleviates model performance on the two properties and leads to a significant improvement in BLEU scores.

| Comments: | Accepted at Context and Compositionality Workshop, NeurIPS 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.01497](https://arxiv.org/abs/1911.01497) [cs.CL] |
|           | (or [arXiv:1911.01497v1](https://arxiv.org/abs/1911.01497v1) [cs.CL] for this version) |





<h2 id="2019-11-06-4">4. Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation</h2>
Title: [Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation]( https://arxiv.org/abs/1911.01597 )

Authors: [Yong Shan](https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+Y), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Wen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W)

*(Submitted on 5 Nov 2019)*

> Generally, Neural Machine Translation models generate target words in a left-to-right (L2R) manner and fail to exploit any future (right) semantics information, which usually produces an unbalanced translation. Recent works attempt to utilize the right-to-left (R2L) decoder in bidirectional decoding to alleviate this problem. In this paper, we propose a novel \textbf{D}ynamic \textbf{I}nteraction \textbf{M}odule (\textbf{DIM}) to dynamically exploit target semantics from R2L translation for enhancing the L2R translation quality. Different from other bidirectional decoding approaches, DIM firstly extracts helpful target information through addressing and reading operations, then updates target semantics for tracking the interactive history. Additionally, we further introduce an \textbf{agreement regularization} term into the training objective to narrow the gap between L2R and R2L translations. Experimental results on NIST Chinese⇒English and WMT'16 English⇒Romanian translation tasks show that our system achieves significant improvements over baseline systems, which also reaches comparable results compared to the state-of-the-art Transformer model with much fewer parameters of it.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.01597](https://arxiv.org/abs/1911.01597) [cs.CL] |
|           | (or [arXiv:1911.01597v1](https://arxiv.org/abs/1911.01597v1) [cs.CL] for this version) |





<h2 id="2019-11-06-5">5. Adversarial Language Games for Advanced Natural Language Intelligence</h2>
Title: [Adversarial Language Games for Advanced Natural Language Intelligence]( https://arxiv.org/abs/1911.01622 )

Authors: [Yuan Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Haoxi Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+H), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Chaojun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Guoyang Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+G), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

*(Submitted on 5 Nov 2019)*

> While adversarial games have been well studied in various board games and electronic sports games, etc., such adversarial games remain a nearly blank field in natural language processing. As natural language is inherently an interactive game, we propose a challenging pragmatics game called Adversarial Taboo, in which an attacker and a defender compete with each other through sequential natural language interactions. The attacker is tasked with inducing the defender to speak a target word invisible to the defender, while the defender is tasked with detecting the target word before being induced by the attacker. In Adversarial Taboo, a successful attacker must hide its intention and subtly induce the defender, while a competitive defender must be cautious with its utterances and infer the intention of the attacker. To instantiate the game, we create a game environment and a competition platform. Sufficient pilot experiments and empirical studies on several baseline attack and defense strategies show promising and interesting results. Based on the analysis on the game and experiments, we discuss multiple promising directions for future research.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.01622](https://arxiv.org/abs/1911.01622) [cs.CL] |
|           | (or [arXiv:1911.01622v1](https://arxiv.org/abs/1911.01622v1) [cs.CL] for this version) |





<h2 id="2019-11-06-6">6. Data Diversification: An Elegant Strategy For Neural Machine Translation</h2>
Title: [Data Diversification: An Elegant Strategy For Neural Machine Translation]( https://arxiv.org/abs/1911.01986 )

Authors: [Xuan-Phi Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Wu Kui](https://arxiv.org/search/cs?searchtype=author&query=Kui%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T)

*(Submitted on 5 Nov 2019)*

> A common approach to improve neural machine translation is to invent new architectures. However, the research process of designing and refining such new models is often exhausting. Another approach is to resort to huge extra monolingual data to conduct semi-supervised training, like back-translation. But extra monolingual data is not always available, especially for low resource languages. In this paper, we propose to diversify the available training data by using multiple forward and backward peer models to augment the original training dataset. Our method does not require extra data like back-translation, nor additional computations and parameters like using pretrained models. Our data diversification method achieves state-of-the-art BLEU score of 30.7 in the WMT'14 English-German task. It also consistently and substantially improves translation quality in 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.01986](https://arxiv.org/abs/1911.01986) [cs.CL] |
|           | (or [arXiv:1911.01986v1](https://arxiv.org/abs/1911.01986v1) [cs.CL] for this version) |



# 2019-11-05

[Return to Index](#Index)



<h2 id="2019-11-05-1">1. Attributed Sequence Embedding</h2>
Title: [Attributed Sequence Embedding]( https://arxiv.org/abs/1911.00949 )

Authors: [Zhongfang Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z), [Xiangnan Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X), [Elke Rundensteiner](https://arxiv.org/search/cs?searchtype=author&query=Rundensteiner%2C+E), [Jihane Zouaoui](https://arxiv.org/search/cs?searchtype=author&query=Zouaoui%2C+J), [Aditya Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+A)

*(Submitted on 3 Nov 2019)*

> Mining tasks over sequential data, such as clickstreams and gene sequences, require a careful design of embeddings usable by learning algorithms. Recent research in feature learning has been extended to sequential data, where each instance consists of a sequence of heterogeneous items with a variable length. However, many real-world applications often involve attributed sequences, where each instance is composed of both a sequence of categorical items and a set of attributes. In this paper, we study this new problem of attributed sequence embedding, where the goal is to learn the representations of attributed sequences in an unsupervised fashion. This problem is core to many important data mining tasks ranging from user behavior analysis to the clustering of gene sequences. This problem is challenging due to the dependencies between sequences and their associated attributes. We propose a deep multimodal learning framework, called NAS, to produce embeddings of attributed sequences. The embeddings are task independent and can be used on various mining tasks of attributed sequences. We demonstrate the effectiveness of our embeddings of attributed sequences in various unsupervised learning tasks on real-world datasets.

| Comments: | Accepted by IEEE Big Data 2019                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.00949](https://arxiv.org/abs/1911.00949) [cs.LG] |
|           | (or [arXiv:1911.00949v1](https://arxiv.org/abs/1911.00949v1) [cs.LG] for this version) |





<h2 id="2019-11-05-2">2. Machine Translation Evaluation using Bi-directional Entailment</h2>
Title: [Machine Translation Evaluation using Bi-directional Entailment]( https://arxiv.org/abs/1911.00681 )

Authors: [Rakesh Khobragade](https://arxiv.org/search/cs?searchtype=author&query=Khobragade%2C+R), [Heaven Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+H), [Anand Namdev](https://arxiv.org/search/cs?searchtype=author&query=Namdev%2C+A), [Anish Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+A), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

*(Submitted on 2 Nov 2019)*

> In this paper, we propose a new metric for Machine Translation (MT) evaluation, based on bi-directional entailment. We show that machine generated translation can be evaluated by determining paraphrasing with a reference translation provided by a human translator. We hypothesize, and show through experiments, that paraphrasing can be detected by evaluating entailment relationship in the forward and backward direction. Unlike conventional metrics, like BLEU or METEOR, our approach uses deep learning to determine the semantic similarity between candidate and reference translation for generating scores rather than relying upon simple n-gram overlap. We use BERT's pre-trained implementation of transformer networks, fine-tuned on MNLI corpus, for natural language inferencing. We apply our evaluation metric on WMT'14 and WMT'17 dataset to evaluate systems participating in the translation task and find that our metric has a better correlation with the human annotated score compared to the other traditional metrics at system level.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.00681](https://arxiv.org/abs/1911.00681) [cs.CL] |
|           | (or [arXiv:1911.00681v1](https://arxiv.org/abs/1911.00681v1) [cs.CL] for this version) |





<h2 id="2019-11-05-3">3. Controlling Text Complexity in Neural Machine Translation</h2>
Title: [Controlling Text Complexity in Neural Machine Translation]( https://arxiv.org/abs/1911.00835 )

Authors: [Sweta Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+S), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

*(Submitted on 3 Nov 2019)*

> This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence-to-sequence models that translate Spanish into English targeted at an easier reading grade level than the original Spanish. We show that these multi-task models outperform pipeline approaches that translate and simplify text independently.

| Comments: | Accepted to EMNLP-IJCNLP 2019                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00835](https://arxiv.org/abs/1911.00835) [cs.CL] |
|           | (or [arXiv:1911.00835v1](https://arxiv.org/abs/1911.00835v1) [cs.CL] for this version) |





<h2 id="2019-11-05-4">4. Machine Translation in Pronunciation Space</h2>
Title: [Machine Translation in Pronunciation Space]( https://arxiv.org/abs/1911.00932 )

Authors: [Hairong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

*(Submitted on 3 Nov 2019)*

> The research in machine translation community focus on translation in text space. However, humans are in fact also good at direct translation in pronunciation space. Some existing translation systems, such as simultaneous machine translation, are inherently more natural and thus potentially more robust by directly translating in pronunciation space. In this paper, we conduct large scale experiments on a self-built dataset with about 20M En-Zh pairs of text sentences and corresponding pronunciation sentences. We proposed three new categories of translations: 1) translating a pronunciation sentence in source language into a pronunciation sentence in target language (P2P-Tran), 2) translating a text sentence in source language into a pronunciation sentence in target language (T2P-Tran), and 3) translating a pronunciation sentence in source language into a text sentence in target language (P2T-Tran), and compare them with traditional text translation (T2T-Tran). Our experiments clearly show that all 4 categories of translations have comparable performances, with small and sometimes ignorable differences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.00932](https://arxiv.org/abs/1911.00932) [cs.CL] |
|           | (or [arXiv:1911.00932v1](https://arxiv.org/abs/1911.00932v1) [cs.CL] for this version) |





<h2 id="2019-11-05-5">5. Analysing Coreference in Transformer Outputs</h2>
Title: [Analysing Coreference in Transformer Outputs]( https://arxiv.org/abs/1911.01188 )

Authors: [Ekaterina Lapshinova-Koltunski](https://arxiv.org/search/cs?searchtype=author&query=Lapshinova-Koltunski%2C+E), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 4 Nov 2019)*

> We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare system performance on two different genres: news and TED talks. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and human translations. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.

| Comments:          | 12 pages, 1 figure                                           |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019) |
| Cite as:           | [arXiv:1911.01188](https://arxiv.org/abs/1911.01188) [cs.CL] |
|                    | (or [arXiv:1911.01188v1](https://arxiv.org/abs/1911.01188v1) [cs.CL] for this version) |





<h2 id="2019-11-05-6">6. Ordering Matters: Word Ordering Aware Unsupervised NMT</h2>
Title: [Ordering Matters: Word Ordering Aware Unsupervised NMT]( https://arxiv.org/abs/1911.01212 )

Authors: [Tamali Banerjee](https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+T), [Rudra Murthy V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+R+M), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

*(Submitted on 30 Oct 2019)*

> Denoising-based Unsupervised Neural Machine Translation (U-NMT) models typically employ denoising strategy at the encoder module to prevent the model from memorizing the input source sentence. Specifically, given an input sentence of length n, the model applies n/2 random swaps between consecutive words and trains the denoising-based U-NMT model. Though effective, applying denoising strategy on every sentence in the training data leads to uncertainty in the model thereby, limiting the benefits from the denoising-based U-NMT model. In this paper, we propose a simple fine-tuning strategy where we fine-tune the trained denoising-based U-NMT system without the denoising strategy. The input sentences are presented as is i.e., without any shuffling noise added. We observe significant improvements in translation performance on many language pairs from our fine-tuning strategy. Our analysis reveals that our proposed models lead to increase in higher n-gram BLEU score compared to the denoising U-NMT models.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.01212](https://arxiv.org/abs/1911.01212) [cs.CL] |
|           | (or [arXiv:1911.01212v1](https://arxiv.org/abs/1911.01212v1) [cs.CL] for this version) |





# 2019-11-04

[Return to Index](#Index)



<h2 id="2019-11-04-1">1. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</h2>
Title: [Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping]( https://arxiv.org/abs/1911.00069 )

Authors: [Jian Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Radu Florian](https://arxiv.org/search/cs?searchtype=author&query=Florian%2C+R)

*(Submitted on 31 Oct 2019)*

> Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.

| Comments: | 11 pages, Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.00069](https://arxiv.org/abs/1911.00069) [cs.CL] |
|           | (or [arXiv:1911.00069v1](https://arxiv.org/abs/1911.00069v1) [cs.CL] for this version) |





<h2 id="2019-11-04-2">2. Sequence Modeling with Unconstrained Generation Order</h2>
Title: [Sequence Modeling with Unconstrained Generation Order]( https://arxiv.org/abs/1911.00176 )

Authors: [Dmitrii Emelianenko](https://arxiv.org/search/cs?searchtype=author&query=Emelianenko%2C+D), [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Pavel Serdyukov](https://arxiv.org/search/cs?searchtype=author&query=Serdyukov%2C+P)

*(Submitted on 1 Nov 2019)*

> The dominant approach to sequence generation is to produce a sequence in some predefined order, e.g. left to right. In contrast, we propose a more general model that can generate the output sequence by inserting tokens in any arbitrary order. Our model learns decoding order as a result of its training procedure. Our experiments show that this model is superior to fixed order models on a number of sequence generation tasks, such as Machine Translation, Image-to-LaTeX and Image Captioning.

| Comments: | Camera-ready version for NeurIPS2019                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00176](https://arxiv.org/abs/1911.00176) [cs.CL] |
|           | (or [arXiv:1911.00176v1](https://arxiv.org/abs/1911.00176v1) [cs.CL] for this version) |





<h2 id="2019-11-04-3">3. On the Linguistic Representational Power of Neural Machine Translation Models</h2>
Title: [On the Linguistic Representational Power of Neural Machine Translation Models]( https://arxiv.org/abs/1911.00317 )

Authors: [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J)

*(Submitted on 1 Nov 2019)*

> Despite the recent success of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.

| Comments: | Accepted to appear in the Journal of Computational Linguistics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00317](https://arxiv.org/abs/1911.00317) [cs.CL] |
|           | (or [arXiv:1911.00317v1](https://arxiv.org/abs/1911.00317v1) [cs.CL] for this version) |





# 2019-11-01

[Return to Index](#Index)



<h2 id="2019-11-01-1">1. Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</h2>
Title: [Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation]( https://arxiv.org/abs/1910.14075 )

Authors: [Sébastien Jean](https://arxiv.org/search/cs?searchtype=author&query=Jean%2C+S), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 30 Oct 2019)*

> Most neural machine translation systems still translate sentences in isolation. To make further progress, a promising line of research additionally considers the surrounding context in order to provide the model potentially missing source-side information, as well as to maintain a coherent output. One difficulty in training such larger-context (i.e. document-level) machine translation systems is that context may be missing from many parallel examples. To circumvent this issue, two-stage approaches, in which sentence-level translations are post-edited in context, have recently been proposed. In this paper, we instead consider the viability of filling in the missing context. In particular, we consider three distinct approaches to generate the missing context: using random contexts, applying a copy heuristic or generating it with a language model. In particular, the copy heuristic significantly helps with lexical coherence, while using completely random contexts hurts performance on many long-distance linguistic phenomena. We also validate the usefulness of tagged back-translation. In addition to improving BLEU scores as expected, using back-translated data helps larger-context machine translation systems to better capture long-range phenomena.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14075](https://arxiv.org/abs/1910.14075) [cs.CL] |
|           | (or [arXiv:1910.14075v1](https://arxiv.org/abs/1910.14075v1) [cs.CL] for this version) |





<h2 id="2019-11-01-2">2. Document-level Neural Machine Translation with Inter-Sentence Attention</h2>
Title: [Document-level Neural Machine Translation with Inter-Sentence Attention]( https://arxiv.org/abs/1910.14528 )

Authors: [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Bao-liang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+B)

*(Submitted on 31 Oct 2019)*

> Standard neural machine translation (NMT) is on the assumption of document-level context independent. Most existing document-level NMT methods only focus on briefly introducing document-level information but fail to concern about selecting the most related part inside document context. The capacity of memory network for detecting the most relevant part of the current sentence from the memory provides a natural solution for the requirement of modeling document-level context by document-level NMT. In this work, we propose a Transformer NMT system with associated memory network (AMN) to both capture the document-level context and select the most salient part related to the concerned translation from the memory. Experiments on several tasks show that the proposed method significantly improves the NMT performance over strong Transformer baselines and other related studies.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14528](https://arxiv.org/abs/1910.14528) [cs.CL] |
|           | (or [arXiv:1910.14528v1](https://arxiv.org/abs/1910.14528v1) [cs.CL] for this version) |





<h2 id="2019-11-01-3">3. Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019</h2>
Title: [Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019]( https://arxiv.org/abs/1910.14539 )

Authors: [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Ioan Calapodescu](https://arxiv.org/search/cs?searchtype=author&query=Calapodescu%2C+I), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 31 Oct 2019)*

> Recently, neural models led to significant improvements in both machine translation (MT) and natural language generation tasks (NLG). However, generation of long descriptive summaries conditioned on structured data remains an open challenge. Likewise, MT that goes beyond sentence-level context is still an open issue (e.g., document-level MT or MT with metadata). To address these challenges, we propose to leverage data from both tasks and do transfer learning between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of parallel data. Then, we adapt these models to pure NLG and MT+NLG tasks by fine-tuning with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the "Document Generation and Translation" task at WNGT 2019, and ranked first in all tracks.

| Comments: | WNGT 2019 - System Description Paper                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1910.14539](https://arxiv.org/abs/1910.14539) [cs.CL] |
|           | (or [arXiv:1910.14539v1](https://arxiv.org/abs/1910.14539v1) [cs.CL] for this version) |





<h2 id="2019-11-01-4">4. Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness</h2>
Title: [Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness]( https://arxiv.org/abs/1910.14589 )

Authors: [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Ioan Calapodescu](https://arxiv.org/search/cs?searchtype=author&query=Calapodescu%2C+I), [Marc Dymetman](https://arxiv.org/search/cs?searchtype=author&query=Dymetman%2C+M), [Claude Roux](https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+C), [Jean-Luc Meunier](https://arxiv.org/search/cs?searchtype=author&query=Meunier%2C+J), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

*(Submitted on 31 Oct 2019)*

> We share a French-English parallel corpus of Foursquare restaurant reviews ([this https URL](https://europe.naverlabs.com/research/natural-language-processing/machine-translation-of-restaurant-reviews)), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.

| Comments: | WNGT 2019 Paper                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1910.14589](https://arxiv.org/abs/1910.14589) [cs.CL] |
|           | (or [arXiv:1910.14589v1](https://arxiv.org/abs/1910.14589v1) [cs.CL] for this version) |







<h2 id="2019-11-01-5">5. Adversarial NLI: A New Benchmark for Natural Language Understanding</h2>
Title: [Adversarial NLI: A New Benchmark for Natural Language Understanding]( https://arxiv.org/abs/1910.14599 )

Authors: [Yixin Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A), [Emily Dinan](https://arxiv.org/search/cs?searchtype=author&query=Dinan%2C+E), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Jason Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+J), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

*(Submitted on 31 Oct 2019)*

> We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14599](https://arxiv.org/abs/1910.14599) [cs.CL] |
|           | (or [arXiv:1910.14599v1](https://arxiv.org/abs/1910.14599v1) [cs.CL] for this version) |