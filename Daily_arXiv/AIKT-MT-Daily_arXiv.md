# Daily arXiv: Machine Translation - October, 2020

# Index


- [2020-10-21](#2020-10-21)

  - [1. Word Shape Matters: Robust Machine Translation with Visual Embedding](#2020-10-21-1)
  - [2. Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization](#2020-10-21-2)
  - [3. Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training](#2020-10-21-3)
  - [4. Complete Multilingual Neural Machine Translation](#2020-10-21-4)
  - [5. Human-Paraphrased References Improve Neural Machine Translation](#2020-10-21-5)
  - [6. CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](#2020-10-21-6)
  - [7. Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages](#2020-10-21-7)
  - [8. Optimal Subarchitecture Extraction For BERT](#2020-10-21-8)
- [2020-10-20](#2020-10-20)

  - [1. Emerging Trends of Multimodal Research in Vision and Language](#2020-10-20-1)
  - [2. A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences](#2020-10-20-2)
  - [3. Incorporate Semantic Structures into Machine Translation Evaluation via UCCA](#2020-10-20-3)
  - [4. Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach](#2020-10-20-4)
  - [5. Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation](#2020-10-20-5)
  - [6. Revisiting Modularized Multilingual NMT to Meet Industrial Demands](#2020-10-20-6)
  - [7. Diving Deep into Context-Aware Neural Machine Translation](#2020-10-20-7)
  - [8. Cold-start Active Learning through Self-supervised Language Modeling](#2020-10-20-8)
  - [9. Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines](#2020-10-20-9)
- [2020-10-19](#2020-10-19)

  - [1. DiDi's Machine Translation System for WMT2020](#2020-10-19-1)
  - [2. Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation](#2020-10-19-2)
  - [3. It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT](#2020-10-19-3)
  - [4. Multi-Adversarial Learning for Cross-Lingual Word Embeddings](#2020-10-19-4)
  - [5. Adaptive Feature Selection for End-to-End Speech Translation](#2020-10-19-5)
  - [6. Mischief: A Simple Black-Box Attack Against Transformer Architectures](#2020-10-19-6)
  - [7. Explicit Alignment Objectives for Multilingual Bidirectional Encoders](#2020-10-19-7)
- [2020-10-16](#2020-10-16)
- [1. Decoding Methods for Neural Narrative Generation](#2020-10-16-1)
  - [2. Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses](#2020-10-16-2)
  - [3. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses](#2020-10-16-3)
  - [4. Does Chinese BERT Encode Word Structure?](#2020-10-16-4)
  - [5. Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings](#2020-10-16-5)
  - [6. Tokenization Repair in the Presence of Spelling Errors](#2020-10-16-6)
- [2020-10-15](#2020-10-15)

  - [1. The EOS Decision and Length Extrapolation](#2020-10-15-1)
  - [2. Dissecting the components and factors of Neural Text Generation](#2020-10-15-2)
  - [3. Random Network Distillation as a Diversity Metric for Both Image and Text Generation](#2020-10-15-3)
  - [4. MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings](#2020-10-15-4)
  - [5. Memformer: The Memory-Augmented Transformer](#2020-10-15-5)
  - [6. DA-Transformer: Distance-aware Transformer](#2020-10-15-6)
  - [7. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](#2020-10-15-7)
- [2020-10-14](#2020-10-14)

  - [1. Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation](#2020-10-14-1)
  - [2. Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model](#2020-10-14-2)
  - [3. Towards Machine Translation for the Kurdish Language](#2020-10-14-3)
  - [4. Incorporating BERT into Parallel Sequence Decoding with Adapters](#2020-10-14-4)
  - [5. Mitigating Gender Bias in Machine Translation with Target Gender Annotations](#2020-10-14-5)
  - [6. CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations](#2020-10-14-6)
  - [7. The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT](#2020-10-14-7)
  - [8. Fine-grained linguistic evaluation for state-of-the-art Machine Translation](#2020-10-14-8)
  - [9. Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language](#2020-10-14-9)
  - [10. Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!](#2020-10-14-10)
- [2020-10-13](#2020-10-13)

  - [1. Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation](#2020-10-13-1)
  - [2. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?](#2020-10-13-2)
  - [3. Load What You Need: Smaller Versions of Multilingual BERT](#2020-10-13-3)
  - [4. Controllable Paraphrasing and Translation with a Syntactic Exemplar](#2020-10-13-4)
  - [5. Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models](#2020-10-13-5)
  - [6. Do Language Embeddings Capture Scales?](#2020-10-13-6)
  - [7. Gradient-based Analysis of NLP Models is Manipulable](#2020-10-13-7)
  - [8. It's not a Non-Issue: Negation as a Source of Error in Machine Translation](#2020-10-13-8)
  - [9. Structural Knowledge Distillation](#2020-10-13-9)
  - [10. SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task](#2020-10-13-10)
  - [11. fairseq S2T: Fast Speech-to-Text Modeling with fairseq](#2020-10-13-11)
  - [12. Machine Translation of Mathematical Text](#2020-10-13-12)
  - [13. Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It](#2020-10-13-13)
  - [14. Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task](#2020-10-13-14)
  - [15. ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization](#2020-10-13-15)
  - [16. What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](#2020-10-13-16)
  - [17. On Long-Tailed Phenomena in Neural Machine Translation](#2020-10-13-17)
  - [18. Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns](#2020-10-13-18)
- [2020-10-12](#2020-10-12)

  - [1. Query-Key Normalization for Transformers](#2020-10-12-1)
  - [2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](#2020-10-12-2)
  - [3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](#2020-10-12-3)
  - [4. Token-level Adaptive Training for Neural Machine Translation](#2020-10-12-4)
  - [5. A Survey of Knowledge-Enhanced Text Generation](#2020-10-12-5)
  - [6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](#2020-10-12-6)
  - [7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](#2020-10-12-7)
  - [8. Self-Paced Learning for Neural Machine Translation](#2020-10-12-8)
  - [9. Recursive Top-Down Production for Sentence Generation with Latent Trees](#2020-10-12-9)
- [2020-10-09](#2020-10-09)

  - [1. Shallow-to-Deep Training for Neural Machine Translation](#2020-10-09-1)
  - [2. Improving Attention Mechanism with Query-Value Interaction](#2020-10-09-2)
  - [3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](#2020-10-09-3)
  - [4. What Can We Do to Improve Peer Review in NLP?](#2020-10-09-4)
  - [5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks](#2020-10-09-5)
  - [6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](#2020-10-09-6)
  - [7. Cross-Thought for Sentence Encoder Pre-training](#2020-10-09-7)
  - [8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation](#2020-10-09-8)
  - [9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](#2020-10-09-9)
- [2020-10-08](#2020-10-08)

  - [1. Plug and Play Autoencoders for Conditional Text Generation](#2020-10-08-1)
  - [2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](#2020-10-08-2)
  - [3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](#2020-10-08-3)
  - [4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](#2020-10-08-4)
  - [5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](#2020-10-08-5)
  - [6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](#2020-10-08-6)
  - [7. WER we are and WER we think we are](#2020-10-08-7)
  - [8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](#2020-10-08-8)
  - [9. TeaForN: Teacher-Forcing with N-grams](#2020-10-08-9)
  - [10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](#2020-10-08-10)
- [2020-10-07](#2020-10-07)

  - [1. Multi-task Learning for Multilingual Neural Machine Translation](#2020-10-07-1)
  - [2. Do Explicit Alignments Robustly Improve Multilingual Encoders?](#2020-10-07-2)
  - [3. The Multilingual Amazon Reviews Corpus](#2020-10-07-3)
  - [4. On the Sparsity of Neural Machine Translation Models](#2020-10-07-4)
  - [5. On the Sub-Layer Functionalities of Transformer Decoder](#2020-10-07-5)
  - [6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](#2020-10-07-6)
  - [7. Analyzing Individual Neurons in Pre-trained Language Models](#2020-10-07-7)
  - [8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](#2020-10-07-8)
  - [9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](#2020-10-07-9)
  - [10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](#2020-10-07-10)
  - [11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](#2020-10-07-11)
  - [12. Inference Strategies for Machine Translation with Conditional Masking](#2020-10-07-12)
  - [13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](#2020-10-07-13)
  - [14. Guiding Attention for Self-Supervised Learning with Transformers](#2020-10-07-14)
  - [15. Adversarial Grammatical Error Correction](#2020-10-07-15)
  - [16. Efficient Inference For Neural Machine Translation](#2020-10-07-16)
  - [17. Iterative Domain-Repaired Back-Translation](#2020-10-07-17)
- [2020-10-06](#2020-10-06)

  - [1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](#2020-10-06-1)
  - [2. Transformer-Based Neural Text Generation with Syntactic Guidance](#2020-10-06-2)
  - [3. Second-Order NLP Adversarial Examples](#2020-10-06-3)
  - [4. GenAug: Data Augmentation for Finetuning Text Generators](#2020-10-06-4)
  - [5. Lifelong Language Knowledge Distillation](#2020-10-06-5)
  - [6. A Streaming Approach For Efficient Batched Beam Search](#2020-10-06-6)
  - [7. Self-training Improves Pre-training for Natural Language Understanding](#2020-10-06-7)
  - [8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](#2020-10-06-8)
- [2020-10-05](#2020-10-05)

  - [1. Nearest Neighbor Machine Translation](#2020-10-05-1)
  - [2. A Survey of the State of Explainable AI for Natural Language Processing](#2020-10-05-2)
  - [3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](#2020-10-05-3)
  - [4. Which *BERT? A Survey Organizing Contextualized Encoders](#2020-10-05-4)


- [2020-10-02](#2020-10-2)

  - [1. WeChat Neural Machine Translation Systems for WMT20](#2020-10-2-1)
- [2020-10-01](#2020-10-01)

  - [1. Rethinking Attention with Performers](#2020-10-01-1)
  - [2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](#2020-10-01-2)
  - [3. Can Automatic Post-Editing Improve NMT?](#2020-10-01-3)
  - [4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](#2020-10-01-4)
  - [5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation](#2020-10-01-5)
- [2020-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-09.md)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-10-21

[Return to Index](#Index)



<h2 id="2020-10-21-1">1. Word Shape Matters: Robust Machine Translation with Visual Embedding</h2>

Title: [Word Shape Matters: Robust Machine Translation with Visual Embedding](https://arxiv.org/abs/2010.09997)

Authors: [Haohan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Peiyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+E+P)

> Neural machine translation has achieved remarkable empirical performance over standard benchmark datasets, yet recent evidence suggests that the models can still fail easily dealing with substandard inputs such as misspelled words, To overcome this issue, we introduce a new encoding heuristic of the input symbols for character-level NLP models: it encodes the shape of each character through the images depicting the letters when printed. We name this new strategy visual embedding and it is expected to improve the robustness of NLP models because humans also process the corpus visually through printed letters, instead of machinery one-hot vectors. Empirically, our method improves models' robustness against substandard inputs, even in the test scenario where the models are tested with the noises that are beyond what is available during the training phase.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.09997](https://arxiv.org/abs/2010.09997) [cs.CL]** |
|           | (or **[arXiv:2010.09997v1](https://arxiv.org/abs/2010.09997v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-2">2. Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization</h2>

Title: [Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization](https://arxiv.org/abs/2010.10041)

Authors: [Chi-Liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Tsung-Yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> A token embedding in multilingual BERT (m-BERT) contains both language and semantic information. We find that representation of a language can be obtained by simply averaging the embeddings of the tokens of the language. With the language representation, we can control the output languages of multilingual BERT by manipulating the token embeddings and achieve unsupervised token translation. We further propose a computationally cheap but effective approach to improve the cross-lingual ability of m-BERT based on the observation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.10041](https://arxiv.org/abs/2010.10041) [cs.CL]** |
|           | (or **[arXiv:2010.10041v1](https://arxiv.org/abs/2010.10041v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-3">3. Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training</h2>

Title: [Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training](https://arxiv.org/abs/2010.10048)

Authors: [Zheng Renjie](https://arxiv.org/search/cs?searchtype=author&query=Renjie%2C+Z), [Ma Mingbo](https://arxiv.org/search/cs?searchtype=author&query=Mingbo%2C+M), [Zheng Baigong](https://arxiv.org/search/cs?searchtype=author&query=Baigong%2C+Z), [Liu Kaibo](https://arxiv.org/search/cs?searchtype=author&query=Kaibo%2C+L), [Yuan Jiahong](https://arxiv.org/search/cs?searchtype=author&query=Jiahong%2C+Y), [Church Kenneth](https://arxiv.org/search/cs?searchtype=author&query=Kenneth%2C+C), [Huang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+H)

> Simultaneous speech-to-speech translation is widely useful but extremely challenging, since it needs to generate target-language speech concurrently with the source-language speech, with only a few seconds delay. In addition, it needs to continuously translate a stream of sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches accumulate latencies progressively when the speaker talks faster, and introduce unnatural pauses when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation (SAT) which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech (as measured by the naturalness metric MOS) with substantially lower latency than the baseline, in both Zh <-> En directions.

| Comments:          | 10 pages, accepted by Findings of EMNLP 2020                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | Findings of EMNLP 2020                                       |
| Cite as:           | **[arXiv:2010.10048](https://arxiv.org/abs/2010.10048) [cs.CL]** |
|                    | (or **[arXiv:2010.10048v1](https://arxiv.org/abs/2010.10048v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-4">4. Complete Multilingual Neural Machine Translation</h2>

Title: [Complete Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.10239)

Authors: [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as the source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111*112=12,432 language pairs that provides competitive translation quality for all language pairs.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10239](https://arxiv.org/abs/2010.10239) [cs.CL]** |
|           | (or **[arXiv:2010.10239v1](https://arxiv.org/abs/2010.10239v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-5">5. Human-Paraphrased References Improve Neural Machine Translation</h2>

Title: [Human-Paraphrased References Improve Neural Machine Translation](https://arxiv.org/abs/2010.10245)

Authors: [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by Freitag et al. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10245](https://arxiv.org/abs/2010.10245) [cs.CL]** |
|           | (or **[arXiv:2010.10245v1](https://arxiv.org/abs/2010.10245v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-6">6. CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</h2>

Title: [CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](https://arxiv.org/abs/2010.10392)

Authors: [Hicham El Boukkouri](https://arxiv.org/search/cs?searchtype=author&query=Boukkouri%2C+H+E), [Olivier Ferret](https://arxiv.org/search/cs?searchtype=author&query=Ferret%2C+O), [Thomas Lavergne](https://arxiv.org/search/cs?searchtype=author&query=Lavergne%2C+T), [Hiroshi Noji](https://arxiv.org/search/cs?searchtype=author&query=Noji%2C+H), [Pierre Zweigenbaum](https://arxiv.org/search/cs?searchtype=author&query=Zweigenbaum%2C+P), [Junichi Tsujii](https://arxiv.org/search/cs?searchtype=author&query=Tsujii%2C+J)

> Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system even if it is not intrinsically linked to the notion of Transformer. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level and open-vocabulary representations.

| Comments: | 13 pages, 8 figures and 3 tables. Accepted at COLING 2020    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.10392](https://arxiv.org/abs/2010.10392) [cs.CL]** |
|           | (or **[arXiv:2010.10392v1](https://arxiv.org/abs/2010.10392v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-7">7. Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages</h2>

Title: [Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages](https://arxiv.org/abs/2010.10472)

Authors: [Yiyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> Spelling normalization for low resource languages is a challenging task because the patterns are hard to predict and large corpora are usually required to collect enough examples. This work shows a comparison of a neural model and character language models with varying amounts on target language data. Our usage scenario is interactive correction with nearly zero amounts of training examples, improving models as more data is collected, for example within a chat app. Such models are designed to be incrementally improved as feedback is given from users. In this work, we design a knowledge-base and prediction model embedded system for spelling correction in low-resource languages. Experimental results on multiple languages show that the model could become effective with a small amount of data. We perform experiments on both natural and synthetic data, as well as on data from two endangered languages (Ainu and Griko). Last, we built a prototype system that was used for a small case study on Hinglish, which further demonstrated the suitability of our approach in real world scenarios.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.10472](https://arxiv.org/abs/2010.10472) [cs.CL]** |
|           | (or **[arXiv:2010.10472v1](https://arxiv.org/abs/2010.10472v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-8">8. Optimal Subarchitecture Extraction For BERT</h2>

Title: [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499)

Authors: [Adrian de Wynter](https://arxiv.org/search/cs?searchtype=author&query=de+Wynter%2C+A), [Daniel J. Perry](https://arxiv.org/search/cs?searchtype=author&query=Perry%2C+D+J)

> We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by applying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as "Bort", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the original BERT-large architecture, and 16% of the net size. Bort is also able to be pretrained in 288 GPU hours, which is 1.2% of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large (Liu et al., 2019), and about 33% of that of the world-record, in GPU hours, required to train BERT-large on the same hardware. It is also 7.9x faster on a CPU, as well as being better performing than other compressed variants of the architecture, and some of the non-compressed variants: it obtains performance improvements of between 0.3% and 31%, absolute, with respect to BERT-large, on multiple public natural language understanding (NLU) benchmarks.

| Comments: | Preprint. Under review                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10499](https://arxiv.org/abs/2010.10499) [cs.CL]** |
|           | (or **[arXiv:2010.10499v1](https://arxiv.org/abs/2010.10499v1) [cs.CL]** for this version) |







# 2020-10-20

[Return to Index](#Index)



<h2 id="2020-10-20-1">1. Emerging Trends of Multimodal Research in Vision and Language</h2>

Title: [Emerging Trends of Multimodal Research in Vision and Language](https://arxiv.org/abs/2010.09522)

Authors: [Shagun Uppal](https://arxiv.org/search/cs?searchtype=author&query=Uppal%2C+S), [Sarthak Bhagat](https://arxiv.org/search/cs?searchtype=author&query=Bhagat%2C+S), [Devamanyu Hazarika](https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+D), [Navonil Majumdar](https://arxiv.org/search/cs?searchtype=author&query=Majumdar%2C+N), [Soujanya Poria](https://arxiv.org/search/cs?searchtype=author&query=Poria%2C+S), [Roger Zimmermann](https://arxiv.org/search/cs?searchtype=author&query=Zimmermann%2C+R), [Amir Zadeh](https://arxiv.org/search/cs?searchtype=author&query=Zadeh%2C+A)

> Deep Learning and its applications have cascaded impactful research and development with a diverse range of modalities present in the real-world data. More recently, this has enhanced research interests in the intersection of the Vision and Language arena with its numerous applications and fast-paced growth. In this paper, we present a detailed overview of the latest trends in research pertaining to visual and language modalities. We look at its applications in their task formulations and how to solve various problems related to semantic perception and content generation. We also address task-specific trends, along with their evaluation strategies and upcoming challenges. Moreover, we shed some light on multi-disciplinary patterns and insights that have emerged in the recent past, directing this field towards more modular and transparent intelligent systems. This survey identifies key trends gravitating recent literature in VisLang research and attempts to unearth directions that the field is heading towards.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.09522](https://arxiv.org/abs/2010.09522) [cs.CV]** |
|           | (or **[arXiv:2010.09522v1](https://arxiv.org/abs/2010.09522v1) [cs.CV]** for this version) |





<h2 id="2020-10-20-2">2. A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences</h2>

Title: [A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences](https://arxiv.org/abs/2010.08725)

Authors: [Andrew Merritt](https://arxiv.org/search/cs?searchtype=author&query=Merritt%2C+A), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Yuki Arase](https://arxiv.org/search/cs?searchtype=author&query=Arase%2C+Y)

> Multimodal neural machine translation (NMT) has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08725](https://arxiv.org/abs/2010.08725) [cs.CL]** |
|           | (or **[arXiv:2010.08725v1](https://arxiv.org/abs/2010.08725v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-3">3. Incorporate Semantic Structures into Machine Translation Evaluation via UCCA</h2>

Title: [Incorporate Semantic Structures into Machine Translation Evaluation via UCCA](https://arxiv.org/abs/2010.08728)

Authors: [Jin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yinuo Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Junfeng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J)

> Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.

| Comments: | WMT2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.08728](https://arxiv.org/abs/2010.08728) [cs.CL]** |
|           | (or **[arXiv:2010.08728v1](https://arxiv.org/abs/2010.08728v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-4">4. Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach</h2>

Title: [Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach](https://arxiv.org/abs/2010.08961)

Authors: [Zewei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Discourse context has been proven useful when translating documents. It is quite a challenge to incorporate long document context in the prevailing neural machine translation models such as Transformer. In this paper, we propose multi-resolutional (MR) Doc2Doc, a method to train a neural sequence-to-sequence model for document-level translation. Our trained model can simultaneously translate sentence by sentence as well as a document as a whole. We evaluate our method and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that MR Doc2Doc outperforms sentence-level models and previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08961](https://arxiv.org/abs/2010.08961) [cs.CL]** |
|           | (or **[arXiv:2010.08961v1](https://arxiv.org/abs/2010.08961v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-5">5. Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation</h2>

Title: [Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation](https://arxiv.org/abs/2010.09046)

Authors: [Yunwon Tae](https://arxiv.org/search/cs?searchtype=author&query=Tae%2C+Y), [Cheonbok Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C), [Taehee Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T), [Soyoung Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Mohammad Azam Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+M+A), [Eunjeong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+E), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J)

> Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09046](https://arxiv.org/abs/2010.09046) [cs.CL]** |
|           | (or **[arXiv:2010.09046v1](https://arxiv.org/abs/2010.09046v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-6">6. Revisiting Modularized Multilingual NMT to Meet Industrial Demands</h2>

Title: [Revisiting Modularized Multilingual NMT to Meet Industrial Demands](https://arxiv.org/abs/2010.09402)

Authors: [Sungwon Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+S), [Bokyung Son](https://arxiv.org/search/cs?searchtype=author&query=Son%2C+B), [Kichang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Jaekyoung Bae](https://arxiv.org/search/cs?searchtype=author&query=Bae%2C+J)

> The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.

| Comments: | The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09402](https://arxiv.org/abs/2010.09402) [cs.CL]** |
|           | (or **[arXiv:2010.09402v1](https://arxiv.org/abs/2010.09402v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-7">7. Diving Deep into Context-Aware Neural Machine Translation</h2>

Title: [Diving Deep into Context-Aware Neural Machine Translation](https://arxiv.org/abs/2010.09482)

Authors: [Jingjing Huo](https://arxiv.org/search/cs?searchtype=author&query=Huo%2C+J), [Christian Herold](https://arxiv.org/search/cs?searchtype=author&query=Herold%2C+C), [Yingbo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Leonard Dahlmann](https://arxiv.org/search/cs?searchtype=author&query=Dahlmann%2C+L), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.

| Comments: | Accepted at 5th Conference on Machine Translation (WMT20)    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.09482](https://arxiv.org/abs/2010.09482) [cs.CL]** |
|           | (or **[arXiv:2010.09482v1](https://arxiv.org/abs/2010.09482v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-8">8. Cold-start Active Learning through Self-supervised Language Modeling</h2>

Title: [Cold-start Active Learning through Self-supervised Language Modeling](https://arxiv.org/abs/2010.09535)

Authors: [Michelle Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+M), [Hsuan-Tien Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

> Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.

| Comments: | Published in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09535](https://arxiv.org/abs/2010.09535) [cs.CL]** |
|           | (or **[arXiv:2010.09535v1](https://arxiv.org/abs/2010.09535v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-9">9. Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines</h2>

Title: [Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines](https://arxiv.org/abs/2010.09693)

Authors: [David Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+D), [Zhengping Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Chris Kedzie](https://arxiv.org/search/cs?searchtype=author&query=Kedzie%2C+C), [Elsbeth Turcan](https://arxiv.org/search/cs?searchtype=author&query=Turcan%2C+E), [Peter Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P), [Kathleen McKeown](https://arxiv.org/search/cs?searchtype=author&query=McKeown%2C+K)

> In this work, we focus on improving ASR output segmentation in the context of low-resource language speech-to-text translation. ASR output segmentation is crucial, as ASR systems segment the input audio using purely acoustic information and are not guaranteed to output sentence-like segments. Since most MT systems expect sentences as input, feeding in longer unsegmented passages can lead to sub-optimal performance. We explore the feasibility of using datasets of subtitles from TV shows and movies to train better ASR segmentation models. We further incorporate part-of-speech (POS) tag and dependency label information (derived from the unsegmented ASR outputs) into our segmentation model. We show that this noisy syntactic information can improve model accuracy. We evaluate our models intrinsically on segmentation quality and extrinsically on downstream MT performance, as well as downstream tasks including cross-lingual information retrieval (CLIR) tasks and human relevance assessments. Our model shows improved performance on downstream tasks for Lithuanian and Bulgarian.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | CLSST@LREC 2020 68-73                                        |
| Cite as:           | **[arXiv:2010.09693](https://arxiv.org/abs/2010.09693) [cs.CL]** |
|                    | (or **[arXiv:2010.09693v1](https://arxiv.org/abs/2010.09693v1) [cs.CL]** for this version) |







# 2020-10-19

[Return to Index](#Index)



<h2 id="2020-10-19-1">1. DiDi's Machine Translation System for WMT2020</h2>

Title: [DiDi's Machine Translation System for WMT2020](https://arxiv.org/abs/2010.08185)

Authors: [Tanfang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T), [Weiwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Wenyang Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+W), [Xing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X), [Xiangang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Jieping Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K)

> This paper describes DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model, and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese->English.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.08185](https://arxiv.org/abs/2010.08185) [cs.CL]** |
|           | (or **[arXiv:2010.08185v1](https://arxiv.org/abs/2010.08185v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-2">2. Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation</h2>

Title: [Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation](https://arxiv.org/abs/2010.08265)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method -- LayerDrop.

| Comments: | Accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.08265](https://arxiv.org/abs/2010.08265) [cs.CL]** |
|           | (or **[arXiv:2010.08265v1](https://arxiv.org/abs/2010.08265v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-3">3. It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT</h2>

Title: [It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT](https://arxiv.org/abs/2010.08275)

Authors: [Hila Gonen](https://arxiv.org/search/cs?searchtype=author&query=Gonen%2C+H), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations.

| Comments: | BlackboxNLP 2020                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.08275](https://arxiv.org/abs/2010.08275) [cs.CL]** |
|           | (or **[arXiv:2010.08275v1](https://arxiv.org/abs/2010.08275v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-4">4. Multi-Adversarial Learning for Cross-Lingual Word Embeddings</h2>

Title: [Multi-Adversarial Learning for Cross-Lingual Word Embeddings](https://arxiv.org/abs/2010.08432)

Authors: [Haozhou Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J), [Paola Merlo](https://arxiv.org/search/cs?searchtype=author&query=Merlo%2C+P)

> Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings -- maps of matching words across languages -- without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction show that this method improves performance over previous single-mapping methods, especially for distant languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08432](https://arxiv.org/abs/2010.08432) [cs.CL]** |
|           | (or **[arXiv:2010.08432v1](https://arxiv.org/abs/2010.08432v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-5">5. Adaptive Feature Selection for End-to-End Speech Translation</h2>

Title: [Adaptive Feature Selection for End-to-End Speech Translation](https://arxiv.org/abs/2010.08518)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to SR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84% temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)

| Comments: | EMNLP2020 Findings; source code is at [this https URL](https://github.com/bzhangGo/zero) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.08518](https://arxiv.org/abs/2010.08518) [cs.CL]** |
|           | (or **[arXiv:2010.08518v1](https://arxiv.org/abs/2010.08518v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-6">6. Mischief: A Simple Black-Box Attack Against Transformer Architectures</h2>

Title: [Mischief: A Simple Black-Box Attack Against Transformer Architectures](https://arxiv.org/abs/2010.08542)

Authors: [Adrian de Wynter](https://arxiv.org/search/cs?searchtype=author&query=de+Wynter%2C+A)

> We introduce Mischief, a simple and lightweight method to produce a class of human-readable, realistic adversarial examples for language models. We perform exhaustive experimentations of our algorithm on four transformer-based architectures, across a variety of downstream tasks, as well as under varying concentrations of said examples. Our findings show that the presence of Mischief-generated adversarial samples in the test set significantly degrades (by up to 20%) the performance of these models with respect to their reported baselines. Nonetheless, we also demonstrate that, by including similar examples in the training set, it is possible to restore the baseline scores on the adversarial test set. Moreover, for certain tasks, the models trained with Mischief set show a modest increase on performance with respect to their original, non-adversarial baseline.

| Comments: | Technical report                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.08542](https://arxiv.org/abs/2010.08542) [cs.CL]** |
|           | (or **[arXiv:2010.08542v1](https://arxiv.org/abs/2010.08542v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-7">7. Explicit Alignment Objectives for Multilingual Bidirectional Encoders</h2>

Title: [Explicit Alignment Objectives for Multilingual Bidirectional Encoders](https://arxiv.org/abs/2010.07972)

Authors: [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have proven to be impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLMR-large model which has 4.6x the parameters of AMBER.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.07972](https://arxiv.org/abs/2010.07972) [cs.CL]** |
|           | (or **[arXiv:2010.07972v1](https://arxiv.org/abs/2010.07972v1) [cs.CL]** for this version) |





# 2020-10-16

[Return to Index](#Index)



<h2 id="2020-10-16-1">1. Decoding Methods for Neural Narrative Generation</h2>

Title: [Decoding Methods for Neural Narrative Generation](https://arxiv.org/abs/2010.07375)

Authors:[Alexandra DeLucia](https://arxiv.org/search/cs?searchtype=author&query=DeLucia%2C+A), [Aaron Mueller](https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+A), [Xiang Lisa Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X+L), [Joo Sedoc](https://arxiv.org/search/cs?searchtype=author&query=Sedoc%2C+J)

> Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters---specifically, maximum mutual information---analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best within 0.7p0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.

| Comments: | 20 pages. Submitted to INLG 2020                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07375](https://arxiv.org/abs/2010.07375) [cs.CL]** |
|           | (or **[arXiv:2010.07375v1](https://arxiv.org/abs/2010.07375v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-2">2. Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses</h2>

Title: [Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses](https://arxiv.org/abs/2010.07574)

Authors:[Simon Flachs](https://arxiv.org/search/cs?searchtype=author&query=Flachs%2C+S), [Ophlie Lacroix](https://arxiv.org/search/cs?searchtype=author&query=Lacroix%2C+O), [Helen Yannakoudakis](https://arxiv.org/search/cs?searchtype=author&query=Yannakoudakis%2C+H), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M), [Anders Sgaard](https://arxiv.org/search/cs?searchtype=author&query=Sgaard%2C+A)

> Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.

| Comments: | Accepted at EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07574](https://arxiv.org/abs/2010.07574) [cs.CL]** |
|           | (or **[arXiv:2010.07574v1](https://arxiv.org/abs/2010.07574v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-3">3. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses</h2>

Title: [Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses](https://arxiv.org/abs/2010.07638)

Authors:[Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Youlin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y)

> Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En. Code available at <[this https URL](https://github.com/ntunlp/pronoun-finetuning)>.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07638](https://arxiv.org/abs/2010.07638) [cs.CL]** |
|           | (or **[arXiv:2010.07638v1](https://arxiv.org/abs/2010.07638v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-4">4. Does Chinese BERT Encode Word Structure?</h2>

Title: [Does Chinese BERT Encode Word Structure?](https://arxiv.org/abs/2010.07711)

Authors:[Yile Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Leyang Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the features captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in BERT. However, little work has investigated word features for character-based languages such as Chinese. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT; (2) word-level features are mostly in the middle representation layers; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.

| Comments: | Accepted by COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07711](https://arxiv.org/abs/2010.07711) [cs.CL]** |
|           | (or **[arXiv:2010.07711v1](https://arxiv.org/abs/2010.07711v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-5">5. Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings</h2>

Title: [Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings](https://arxiv.org/abs/2010.07761)

Authors:[Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We describe an unsupervised method to create pseudo-parallel corpora for machine translation (MT) from unaligned text. We use multilingual BERT to create source and target sentence embeddings for nearest-neighbor search and adapt the model via self-training. We validate our technique by extracting parallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a 24.5 point increase (absolute) in F1 scores over previous unsupervised methods. We then improve an XLM-based unsupervised neural MT system pre-trained on Wikipedia by supplementing it with pseudo-parallel text mined from the same corpus, boosting unsupervised translation performance by up to 3.5 BLEU on the WMT'14 French-English and WMT'16 German-English tasks and outperforming the previous state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese corpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU improvement on the low-resource MT task. We demonstrate that unsupervised bitext mining is an effective way of augmenting MT datasets and complements existing techniques like initializing with pre-trained contextual embeddings.

| Comments: | To appear in the Transactions of the Association for Computational Linguistics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07761](https://arxiv.org/abs/2010.07761) [cs.CL]** |
|           | (or **[arXiv:2010.07761v1](https://arxiv.org/abs/2010.07761v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-6">6. Tokenization Repair in the Presence of Spelling Errors
</h2>

Title: [Tokenization Repair in the Presence of Spelling Errors](https://arxiv.org/abs/2010.07878)

Authors:[Hannah Bast](https://arxiv.org/search/cs?searchtype=author&query=Bast%2C+H), [Matthias Hertel](https://arxiv.org/search/cs?searchtype=author&query=Hertel%2C+M), [Mostafa M. Mohamed](https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+M+M)

> We consider the following tokenization repair problem: Given a natural language text with any combination of missing or spurious spaces, correct these. Spelling errors can be present, but it's not part of the problem to correct them. For example, given: "Tispa per isabout token izaionrep air", compute "Tis paper is about tokenizaion repair". It is tempting to think of this problem as a special case of spelling correction or to treat the two problems together. We make a case that tokenization repair and spelling correction should and can be treated as separate problems. We investigate a variety of neural models as well as a number of strong baselines. We identify three main ingredients to high-quality tokenization repair: deep language models with a bidirectional component, training the models on text with spelling errors, and making use of the space information already present. Our best methods can repair all tokenization errors on 97.5% of the correctly spelled test sentences and on 96.0% of the misspelled test sentences. With all spaces removed from the given text (the scenario from previous work), the accuracy falls to 94.5% and 90.1%, respectively. We conduct a detailed error analysis.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.07878](https://arxiv.org/abs/2010.07878) [cs.CL]** |
|           | (or **[arXiv:2010.07878v1](https://arxiv.org/abs/2010.07878v1) [cs.CL]** for this version) |









# 2020-10-15

[Return to Index](#Index)



<h2 id="2020-10-15-1">1. The EOS Decision and Length Extrapolation</h2>

Title: [The EOS Decision and Length Extrapolation](https://arxiv.org/abs/2010.07174)

Authors: [Benjamin Newman](https://arxiv.org/search/cs?searchtype=author&query=Newman%2C+B), [John Hewitt](https://arxiv.org/search/cs?searchtype=author&query=Hewitt%2C+J), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.

| Comments: | 16 page, 7 Figures, 9 Tables, Blackbox NLP Workshop at EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07174](https://arxiv.org/abs/2010.07174) [cs.CL]** |
|           | (or **[arXiv:2010.07174v1](https://arxiv.org/abs/2010.07174v1) [cs.CL]** for this version) |





<h2 id="2020-10-15-2">2. Dissecting the components and factors of Neural Text Generation</h2>

Title: [Dissecting the components and factors of Neural Text Generation](https://arxiv.org/abs/2010.07279)

Authors: [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07279](https://arxiv.org/abs/2010.07279) [cs.CL]** |
|           | (or **[arXiv:2010.07279v1](https://arxiv.org/abs/2010.07279v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-3">3. Random Network Distillation as a Diversity Metric for Both Image and Text Generation</h2>

Title: [Random Network Distillation as a Diversity Metric for Both Image and Text Generation](https://arxiv.org/abs/2010.06715)

Authors: [Liam Fowl](https://arxiv.org/search/cs?searchtype=author&query=Fowl%2C+L), [Micah Goldblum](https://arxiv.org/search/cs?searchtype=author&query=Goldblum%2C+M), [Arjun Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Amr Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+A), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T)

> Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06715](https://arxiv.org/abs/2010.06715) [cs.LG]** |
|           | (or **[arXiv:2010.06715v1](https://arxiv.org/abs/2010.06715v1) [cs.LG]** for this version) |







<h2 id="2020-10-15-4">4. MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings</h2>

Title: [MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings](https://arxiv.org/abs/2010.07152)

Authors: [Kai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Yu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Qian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Q), [Quan Z. Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+Q+Z)

> Link prediction based on knowledge graph embedding (KGE) aims to predict new triples to complete knowledge graphs (KGs) automatically. However, recent KGE models tend to improve performance by excessively increasing vector dimensions, which would cause enormous training costs and save storage in practical applications. To address this problem, we first theoretically analyze the capacity of low-dimensional space for KG embeddings based on the principle of minimum entropy. Then, we propose a novel knowledge distillation framework for knowledge graph embedding, utilizing multiple low-dimensional KGE models as teachers. Under a novel iterative distillation strategy, the MulDE model produces soft labels according to training epochs and student performance adaptively. The experimental results show that MulDE can effectively improve the performance and training speed of low-dimensional KGE models. The distilled 32-dimensional models are very competitive compared to some of state-or-the-art (SotA) high-dimensional methods on several commonly-used datasets.

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07152](https://arxiv.org/abs/2010.07152) [cs.AI]** |
|           | (or **[arXiv:2010.07152v1](https://arxiv.org/abs/2010.07152v1) [cs.AI]** for this version) |







<h2 id="2020-10-15-5">5. Memformer: The Memory-Augmented Transformer</h2>

Title: [Memformer: The Memory-Augmented Transformer](https://arxiv.org/abs/2010.06891)

Authors: [Qingyang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Zhenzhong Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Z), [Jing Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z)

> Transformer models have obtained remarkable accomplishments in various NLP tasks. However, these models have efficiency issues on long sequences, as the complexity of their self-attention module scales quadratically with the sequence length. To remedy the limitation, we present Memformer, a novel language model that utilizes a single unified memory to encode and retrieve past information. It includes a new optimization scheme, Memory Replay Back-Propagation, which promotes long-range back-propagation through time with a significantly reduced memory requirement. Memformer achieves (n) time complexity and (1) space complexity in processing long sequences, meaning that the model can handle an infinite length sequence during inference. Our model is also compatible with other self-supervised tasks to further improve the performance on language modeling. Experimental results show that Memformer outperforms the previous long-range sequence models on WikiText-103, including Transformer-XL and compressive Transformer.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06891](https://arxiv.org/abs/2010.06891) [cs.CL]** |
|           | (or **[arXiv:2010.06891v1](https://arxiv.org/abs/2010.06891v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-6">6. DA-Transformer: Distance-aware Transformer</h2>

Title: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06925](https://arxiv.org/abs/2010.06925) [cs.CL]** |
|           | (or **[arXiv:2010.06925v1](https://arxiv.org/abs/2010.06925v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-7">7. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search</h2>

Title: [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://arxiv.org/abs/2010.07003)

Authors: [Gyuwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> Although transformers have achieved impressive accuracies in various tasks in natural language processing, they often come with a prohibitive computational cost, that prevents their use in scenarios with limited computational resources for inference. This need for computational efficiency in inference has been addressed by for instance PoWER-BERT (Goyal et al., 2020) which gradually decreases the length of a sequence as it is passed through layers. These approaches however often assume that the target computational complexity is known in advance at the time of training. This implies that a separate model must be trained for each inference scenario with its distinct computational budget. In this paper, we extend PoWER-BERT to address this issue of inefficiency and redundancy. The proposed extension enables us to train a large-scale transformer, called Length-Adaptive Transformer, once and uses it for various inference scenarios without re-training it. To do so, we train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. We then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification such as span-based question-answering, by introducing the idea of Drop-and-Restore. With Drop-and-Restore, word-vectors are dropped temporarily in intermediate layers and restored at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including SQuAD 1.1, MNLI-m, and SST-2. Code is available at [this https URL](https://github.com/clovaai/length-adaptive-transformer).

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07003](https://arxiv.org/abs/2010.07003) [cs.CL]** |
|           | (or **[arXiv:2010.07003v1](https://arxiv.org/abs/2010.07003v1) [cs.CL]** for this version) |















# 2020-10-14

[Return to Index](#Index)



<h2 id="2020-10-14-1">1. Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation</h2>

Title: [Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation](https://arxiv.org/abs/2010.05997)

Authors: [Xing Jie Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+X+J), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

> Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for "attaching" dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.

| Comments: | Accepted for publication in Proceedings of WMT 2020          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.05997](https://arxiv.org/abs/2010.05997) [cs.CL]** |
|           | (or **[arXiv:2010.05997v1](https://arxiv.org/abs/2010.05997v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-2">2. Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model</h2>

Title: [Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model](https://arxiv.org/abs/2010.06040)

Authors: [Mingzhi Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+M), [Dinghan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Lin Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+L)

> Masked Language Model (MLM) framework has been widely adopted for self-supervised language pre-training. In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. Thus, we theoretically quantify the gradient variance via correlating the gradient covariance with the Hamming distance between two different masks (given a certain text sequence). To reduce the variance due to the sampling of masks, we propose a fully-explored masking strategy, where a text sequence is divided into a certain number of non-overlapping segments. Thereafter, the tokens within one segment are masked for training. We prove, from a theoretical perspective, that the gradients derived from this new masking schema have a smaller variance and can lead to more efficient self-supervised training. We conduct extensive experiments on both continual pre-training and general pre-training from scratch. Empirical results confirm that this new masking strategy can consistently outperform standard random masking. Detailed efficiency analysis and ablation studies further validate the advantages of our fully-explored masking strategy under the MLM framework.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06040](https://arxiv.org/abs/2010.06040) [cs.CL]** |
|           | (or **[arXiv:2010.06040v1](https://arxiv.org/abs/2010.06040v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-3">3. Towards Machine Translation for the Kurdish Language</h2>

Title: [Towards Machine Translation for the Kurdish Language](https://arxiv.org/abs/2010.06041)

Authors: [Sina Ahmadi](https://arxiv.org/search/cs?searchtype=author&query=Ahmadi%2C+S), [Mariam Masoud](https://arxiv.org/search/cs?searchtype=author&query=Masoud%2C+M)

> Machine translation is the task of translating texts from one language to another using computers. It has been one of the major tasks in natural language processing and computational linguistics and has been motivating to facilitate human communication. Kurdish, an Indo-European language, has received little attention in this realm due to the language being less-resourced. Therefore, in this paper, we are addressing the main issues in creating a machine translation system for the Kurdish language, with a focus on the Sorani dialect. We describe the available scarce parallel data suitable for training a neural machine translation model for Sorani Kurdish-English translation. We also discuss some of the major challenges in Kurdish language translation and demonstrate how fundamental text processing tasks, such as tokenization, can improve translation performance.

| Comments: | 12 pages - under review in the ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06041](https://arxiv.org/abs/2010.06041) [cs.CL]** |
|           | (or **[arXiv:2010.06041v1](https://arxiv.org/abs/2010.06041v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-4">4. Incorporating BERT into Parallel Sequence Decoding with Adapters</h2>

Title: [Incorporating BERT into Parallel Sequence Decoding with Adapters](https://arxiv.org/abs/2010.06138)

Authors: [Junliang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Linli Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Enhong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+E)

> While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49/33.57 BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60/43.56 BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models.

| Comments: | NeurIPS 2020                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06138](https://arxiv.org/abs/2010.06138) [cs.CL]** |
|           | (or **[arXiv:2010.06138v1](https://arxiv.org/abs/2010.06138v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-5">5. Mitigating Gender Bias in Machine Translation with Target Gender Annotations</h2>

Title: [Mitigating Gender Bias in Machine Translation with Target Gender Annotations](https://arxiv.org/abs/2010.06203)

Authors: [Toms Bergmanis](https://arxiv.org/search/cs?searchtype=author&query=Bergmanis%2C+T), [Artrs Stafanovis](https://arxiv.org/search/cs?searchtype=author&query=Stafanovis%2C+A), [Mrcis Pinnis](https://arxiv.org/search/cs?searchtype=author&query=Pinnis%2C+M)

> When translating "The secretary asked for details." to a language with grammatical gender, it might be necessary to determine the gender of the subject "secretary". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.

| Comments: | EMNLP 2020 Fifth Conference on Machine Translation (WMT20)   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06203](https://arxiv.org/abs/2010.06203) [cs.CL]** |
|           | (or **[arXiv:2010.06203v1](https://arxiv.org/abs/2010.06203v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-6">6. CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations</h2>

Title: [CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations](https://arxiv.org/abs/2010.06351)

Authors: [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Pengcheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+P), [Shicheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

> Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6% absolute gain on GLUE benchmarks and 0.8% absolute increment on NLVR.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06351](https://arxiv.org/abs/2010.06351) [cs.CL]** |
|           | (or **[arXiv:2010.06351v1](https://arxiv.org/abs/2010.06351v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-7">7. The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT</h2>

Title: [The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT](https://arxiv.org/abs/2010.06354)

Authors: [Jrg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

> This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.

| Comments: | to be appear at the 5th Conference on Machine Translation (WMT20) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06354](https://arxiv.org/abs/2010.06354) [cs.CL]** |
|           | (or **[arXiv:2010.06354v1](https://arxiv.org/abs/2010.06354v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-8">8. Fine-grained linguistic evaluation for state-of-the-art Machine Translation</h2>

Title: [Fine-grained linguistic evaluation for state-of-the-art Machine Translation](https://arxiv.org/abs/2010.06359)

Authors: [Eleftherios Avramidis](https://arxiv.org/search/cs?searchtype=author&query=Avramidis%2C+E), [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Ursula Strohriegel](https://arxiv.org/search/cs?searchtype=author&query=Strohriegel%2C+U), [Aljoscha Burchardt](https://arxiv.org/search/cs?searchtype=author&query=Burchardt%2C+A), [Sebastian Mller](https://arxiv.org/search/cs?searchtype=author&query=Mller%2C+S)

> This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.

| Comments: | 11 pages, 1 figure, Fifth Conference of Machine Translation, WMT20 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06359](https://arxiv.org/abs/2010.06359) [cs.CL]** |
|           | (or **[arXiv:2010.06359v1](https://arxiv.org/abs/2010.06359v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-9">9. Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language</h2>

Title: [Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language](https://arxiv.org/abs/2010.06447)

Authors: [Dan John Velasco](https://arxiv.org/search/cs?searchtype=author&query=Velasco%2C+D+J)

> Low-resource languages such as Filipino suffer from data scarcity which makes it challenging to develop NLP applications for Filipino language. The use of Transfer Learning (TL) techniques alleviates this problem in low-resource setting. In recent years, transformer-based models are proven to be effective in low-resource tasks but faces challenges in accessibility due to its high compute and memory requirements. There's a need for a cheaper but effective alternative. This paper has three contributions. First, release a pre-trained AWD LSTM language model for Filipino language. Second, benchmark AWD LSTM in the Hate Speech classification task and show that it performs on par with transformer-based models. Third, analyze the degradation rate of AWD-LSTM to smaller data using degradation test and compare it with transformer-based models.
> \-----
> Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer Learning (TL) techniques ay malaking tulong para sa mga pagkakataong gipit tayo sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay magastos sa resources. Kaya nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una, maglabas ng pre-trained AWD LSTM language model sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag benchmark ng AWD LSTM sa Hate Speech classification task at ipakita na kayang nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang degradation rate ng AWD-LSTM sa mas maliit na data gamit ang degradation test at ikumpara ito sa mga transformer-based models.

| Comments:    | 5 pages, 3 tables, 1 figure. in Filipino language            |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.06447](https://arxiv.org/abs/2010.06447) [cs.CL]** |
|              | (or **[arXiv:2010.06447v1](https://arxiv.org/abs/2010.06447v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-10">10. Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!</h2>

Title: [Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!](https://arxiv.org/abs/2010.06572)

Authors: [Jack Hessel](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+J), [Lillian Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+L)

> Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.

| Subjects:          | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Published in EMNLP 2020                                      |
| Cite as:           | **[arXiv:2010.06572](https://arxiv.org/abs/2010.06572) [cs.CL]** |
|                    | (or **[arXiv:2010.06572v1](https://arxiv.org/abs/2010.06572v1) [cs.CL]** for this version) |









# 2020-10-13

[Return to Index](#Index)



<h2 id="2020-10-13-1">1. Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation</h2>

Title: [Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation](https://arxiv.org/abs/2010.05445)

Authors: [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Wray Buntine](https://arxiv.org/search/cs?searchtype=author&query=Buntine%2C+W), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

> Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation (NMT) models in bilingually low-resource scenarios. A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest. However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal. In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model. As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvement compared to strong baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05445](https://arxiv.org/abs/2010.05445) [cs.CL]** |
|           | (or **[arXiv:2010.05445v1](https://arxiv.org/abs/2010.05445v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-2">2. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</h2>

Title: [The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?](https://arxiv.org/abs/2010.05607)

Authors: [Jasmijn Bastings](https://arxiv.org/search/cs?searchtype=author&query=Bastings%2C+J), [Katja Filippova](https://arxiv.org/search/cs?searchtype=author&query=Filippova%2C+K)

> There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.

| Comments:          | Accepted at BlackboxNLP 2020                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP |
| Cite as:           | **[arXiv:2010.05607](https://arxiv.org/abs/2010.05607) [cs.CL]** |
|                    | (or **[arXiv:2010.05607v1](https://arxiv.org/abs/2010.05607v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-3">3. Load What You Need: Smaller Versions of Multilingual BERT</h2>

Title: [Load What You Need: Smaller Versions of Multilingual BERT](https://arxiv.org/abs/2010.05609)

Authors: [Amine Abdaoui](https://arxiv.org/search/cs?searchtype=author&query=Abdaoui%2C+A), [Camille Pradel](https://arxiv.org/search/cs?searchtype=author&query=Pradel%2C+C), [Grgoire Sigel](https://arxiv.org/search/cs?searchtype=author&query=Sigel%2C+G)

> Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to generate smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | SustaiNLP / EMNLP 2020                                       |
| Cite as:           | **[arXiv:2010.05609](https://arxiv.org/abs/2010.05609) [cs.CL]** |
|                    | (or **[arXiv:2010.05609v1](https://arxiv.org/abs/2010.05609v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-4">4. Controllable Paraphrasing and Translation with a Syntactic Exemplar</h2>

Title: [Controllable Paraphrasing and Translation with a Syntactic Exemplar](https://arxiv.org/abs/2010.05856)

Authors: [Mingda Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> Most prior work on exemplar-based syntactically controlled paraphrase generation relies on automatically-constructed large-scale paraphrase datasets. We sidestep this prerequisite by adapting models from prior work to be able to learn solely from bilingual text (bitext). Despite only using bitext for training, and in near zero-shot conditions, our single proposed model can perform four tasks: controlled paraphrase generation in both languages and controlled machine translation in both language directions. To evaluate these tasks quantitatively, we create three novel evaluation datasets. Our experimental results show that our models achieve competitive results on controlled paraphrase generation and strong performance on controlled machine translation. Analysis shows that our models learn to disentangle semantics and syntax in their latent representations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05856](https://arxiv.org/abs/2010.05856) [cs.CL]** |
|           | (or **[arXiv:2010.05856v1](https://arxiv.org/abs/2010.05856v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-5">5. Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models</h2>

Title: [Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models](https://arxiv.org/abs/2010.05874)

Authors: [Zirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y)

> Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05874](https://arxiv.org/abs/2010.05874) [cs.CL]** |
|           | (or **[arXiv:2010.05874v1](https://arxiv.org/abs/2010.05874v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-6">6. Do Language Embeddings Capture Scales?</h2>

Title: [Do Language Embeddings Capture Scales?](https://arxiv.org/abs/2010.05345)

Authors: [Xikun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Deepak Ramachandran](https://arxiv.org/search/cs?searchtype=author&query=Ramachandran%2C+D), [Ian Tenney](https://arxiv.org/search/cs?searchtype=author&query=Tenney%2C+I), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results.

| Comments:    | Accepted at EMNLP Findings 2020 and EMNLP BlackboxNLP workshop 2020 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.05345](https://arxiv.org/abs/2010.05345) [cs.CL]** |
|              | (or **[arXiv:2010.05345v1](https://arxiv.org/abs/2010.05345v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-7">7. Gradient-based Analysis of NLP Models is Manipulable</h2>

Title: [Gradient-based Analysis of NLP Models is Manipulable](https://arxiv.org/abs/2010.05419)

Authors: [Junlin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Jens Tuyls](https://arxiv.org/search/cs?searchtype=author&query=Tuyls%2C+J), [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Sameer Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

> Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, their faithfulness. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade that overwhelms the gradients without affecting the predictions. This Facade can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (text classification, NLI, and QA), we show that our method can manipulate numerous gradient-based analysis techniques: saliency maps, input reduction, and adversarial perturbations all identify unimportant or targeted tokens as being highly important. The code and a tutorial of this paper is available at [this http URL](http://ucinlp.github.io/facade).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05419](https://arxiv.org/abs/2010.05419) [cs.CL]** |
|           | (or **[arXiv:2010.05419v1](https://arxiv.org/abs/2010.05419v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-8">8. It's not a Non-Issue: Negation as a Source of Error in Machine Translation</h2>

Title: [It's not a Non-Issue: Negation as a Source of Error in Machine Translation](https://arxiv.org/abs/2010.05432)

Authors: [Md Mosharaf Hossain](https://arxiv.org/search/cs?searchtype=author&query=Hossain%2C+M+M), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Eduardo Blanco](https://arxiv.org/search/cs?searchtype=author&query=Blanco%2C+E), [Alexis Palmer](https://arxiv.org/search/cs?searchtype=author&query=Palmer%2C+A)

> As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for modern MT systems using 17 translation directions as test bed. Through thorough analysis, we find that indeed the presence of negation can significantly impact downstream quality, in some cases resulting in quality reductions of more than 60%. We also provide a linguistically motivated analysis that directly explains the majority of our findings. We release our annotations and code to replicate our analysis here: [this https URL](https://github.com/mosharafhossain/negation-mt).

| Comments: | Accepted at the Findings of EMNLP2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.05432](https://arxiv.org/abs/2010.05432) [cs.CL]** |
|           | (or **[arXiv:2010.05432v1](https://arxiv.org/abs/2010.05432v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-9">9. Structural Knowledge Distillation</h2>

Title: [Structural Knowledge Distillation](https://arxiv.org/abs/2010.05010)

Authors: [Xinyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yong Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Zhaohui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Z), [Zixia Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Z), [Nguyen Bach](https://arxiv.org/search/cs?searchtype=author&query=Bach%2C+N), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Zhongqiang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Kewei Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+K)

> Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a smaller one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student's output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces smaller substructures than the teacher factorization; 3) the teacher factorization produces smaller substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible.

| Comments: | Under review as a conference paper of ICLR 2021. 15 pages    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.05010](https://arxiv.org/abs/2010.05010) [cs.CL]** |
|           | (or **[arXiv:2010.05010v1](https://arxiv.org/abs/2010.05010v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-10">10. SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task</h2>

Title: [SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task](https://arxiv.org/abs/2010.05122)

Authors: [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

> In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.

| Comments: | WMT20                                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05122](https://arxiv.org/abs/2010.05122) [cs.CL]** |
|           | (or **[arXiv:2010.05122v1](https://arxiv.org/abs/2010.05122v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-11">11. fairseq S2T: Fast Speech-to-Text Modeling with fairseq</h2>

Title: [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Dmytro Okhonko](https://arxiv.org/search/cs?searchtype=author&query=Okhonko%2C+D), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T) modeling tasks such as end-to-end speech recognition and speech-to-text translation. It follows fairseq's careful design for scalability and extensibility. We provide end-to-end workflows from data pre-processing, model training to offline (online) inference. We implement state-of-the-art RNN-based as well as Transformer-based models and open-source detailed training recipes. Fairseq's machine translation models and language models can be seamlessly integrated into S2T workflows for multi-task learning or transfer learning. Fairseq S2T documentation and examples are available at [this https URL](https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text).

| Comments: | Accepted to AACL 2020 Demo                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.05171](https://arxiv.org/abs/2010.05171) [cs.CL]** |
|           | (or **[arXiv:2010.05171v1](https://arxiv.org/abs/2010.05171v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-12">12. Machine Translation of Mathematical Text</h2>

Title: [Machine Translation of Mathematical Text](https://arxiv.org/abs/2010.05229)

Authors: [Aditya Ohri](https://arxiv.org/search/cs?searchtype=author&query=Ohri%2C+A), [Tanya Schmah](https://arxiv.org/search/cs?searchtype=author&query=Schmah%2C+T)

> We have implemented a machine translation system, the PolyMath Translator, for LaTeX documents containing mathematical text. The current implementation translates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a held-out test corpus of mathematical sentences. It produces LaTeX documents that can be compiled to PDF without further editing. The system first converts the body of an input LaTeX document into English sentences containing math tokens, using the pandoc universal document converter to parse LaTeX input. We have trained a Transformer-based translator model, using OpenNMT, on a combined corpus containing a small proportion of domain-specific sentences. Our full system uses both this Transformer model and Google Translate, the latter being used as a backup to better handle linguistic features that do not appear in our training dataset. If the Transformer model does not have confidence in its translation, as determined by a high perplexity score, then we use Google Translate with a custom glossary. This backup was used 26% of the time on our test corpus of mathematical sentences. The PolyMath Translator is available as a web service at [this http URL](http://www.polymathtrans.ai/).

| Comments:    | 14 pages, 2 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2010.05229](https://arxiv.org/abs/2010.05229) [cs.CL]** |
|              | (or **[arXiv:2010.05229v1](https://arxiv.org/abs/2010.05229v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-13">13. Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It</h2>

Title: [Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It](https://arxiv.org/abs/2010.05332)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Rosie Sallis](https://arxiv.org/search/cs?searchtype=author&query=Sallis%2C+R), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> Neural Machine Translation (NMT) has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.
> In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, assessing on English-to-Spanish and English-to-German translation.
> We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest an effective alternative in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention in the inflected target language.

| Comments: | Workshop on Gender Bias in NLP, 2020                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05332](https://arxiv.org/abs/2010.05332) [cs.CL]** |
|           | (or **[arXiv:2010.05332v1](https://arxiv.org/abs/2010.05332v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-14">14. Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task</h2>

Title: [Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task](https://arxiv.org/abs/2010.05333)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence.
> The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.

| Comments: | WMT20 biomedical task                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05333](https://arxiv.org/abs/2010.05333) [cs.CL]** |
|           | (or **[arXiv:2010.05333v1](https://arxiv.org/abs/2010.05333v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-15">15. ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization</h2>

Title: [ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization](https://arxiv.org/abs/2010.04791)

Authors: [Shiyue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Benjamin Frey](https://arxiv.org/search/cs?searchtype=author&query=Frey%2C+B), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively, and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization. Our data, code, and demo will be publicly available at [this https URL](https://github.com/ZhangShiyue/ChrEn)

| Comments: | EMNLP 2020 (19 pages)                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.04791](https://arxiv.org/abs/2010.04791) [cs.CL]** |
|           | (or **[arXiv:2010.04791v1](https://arxiv.org/abs/2010.04791v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-16">16. What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding</h2>

Title: [What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/abs/2010.04903)

Authors: [Yu-An Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.04903](https://arxiv.org/abs/2010.04903) [cs.CL]** |
|           | (or **[arXiv:2010.04903v1](https://arxiv.org/abs/2010.04903v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-17">17. On Long-Tailed Phenomena in Neural Machine Translation</h2>

Title: [On Long-Tailed Phenomena in Neural Machine Translation](https://arxiv.org/abs/2010.04924)

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Siddharth Dalmia](https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S), [Vivek Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+V), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F)

> State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04924](https://arxiv.org/abs/2010.04924) [cs.CL]** |
|           | (or **[arXiv:2010.04924v1](https://arxiv.org/abs/2010.04924v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-18">18. Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns</h2>

Title: [Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns](https://arxiv.org/abs/2010.04989)

Authors: [Lei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Koichi Takeda](https://arxiv.org/search/cs?searchtype=author&query=Takeda%2C+K)

> This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the \textit{mismatching issue} when directly adopting BERTScore to QE. Specifically, there exist lots of mismatching errors between the source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross-lingual patterns, \textit{e.g.} word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.

| Comments: | To appear in WMT2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04989](https://arxiv.org/abs/2010.04989) [cs.CL]** |
|           | (or **[arXiv:2010.04989v1](https://arxiv.org/abs/2010.04989v1) [cs.CL]** for this version) |







# 2020-10-12

[Return to Index](#Index)



<h2 id="2020-10-12-1">1. Query-Key Normalization for Transformers</h2>

Title: [Query-Key Normalization for Transformers](https://arxiv.org/abs/2010.04245)

Authors: [Alex Henry](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+A), [Prudhvi Raj Dachapally](https://arxiv.org/search/cs?searchtype=author&query=Dachapally%2C+P+R), [Shubham Pawar](https://arxiv.org/search/cs?searchtype=author&query=Pawar%2C+S), [Yuxuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer's normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply 2 normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT'15.

| Comments: | 8 pages, 2 figures, accepted at Findings of EMNLP 2020       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04245](https://arxiv.org/abs/2010.04245) [cs.CL]** |
|           | (or **[arXiv:2010.04245v1](https://arxiv.org/abs/2010.04245v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-2">2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task</h2>

Title: [Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](https://arxiv.org/abs/2010.04297)

Authors: [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Amy Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+A), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Qijun Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Q), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Dipanjan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

> The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. Our submission is based on BLEURT, a previously published metric based on transfer learning. We extend the metric beyond English and evaluate it on 12 languages for which training examples are available, as well as four "zero-shot" languages, for which we have no fine-tuning data. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that BLEURT achieves competitive results on the WMT Metrics 2019 Shared Task, indicating its promise for the 2020 edition.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04297](https://arxiv.org/abs/2010.04297) [cs.CL]** |
|           | (or **[arXiv:2010.04297v1](https://arxiv.org/abs/2010.04297v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-3">3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning</h2>

Title: [Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](https://arxiv.org/abs/2010.04314)

Authors: [Xiaomian Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+X), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.

| Comments: | Accepted to EMNLP 2020 long paper                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04314](https://arxiv.org/abs/2010.04314) [cs.CL]** |
|           | (or **[arXiv:2010.04314v1](https://arxiv.org/abs/2010.04314v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-4">4. Token-level Adaptive Training for Neural Machine Translation</h2>

Title: [Token-level Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2010.04380)

Authors: [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.

| Comments: | 12 pages; Accepted by EMNLP 2020                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04380](https://arxiv.org/abs/2010.04380) [cs.CL]** |
|           | (or **[arXiv:2010.04380v1](https://arxiv.org/abs/2010.04380v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-5">5. A Survey of Knowledge-Enhanced Text Generation</h2>

Title: [A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/abs/2010.04389)

Authors: [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Zaitang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhiting Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Z), [Qingyun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M)

> The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.

| Comments: | 44 pages; Preprint; A paper and code collection is available at [this https URL](https://github.com/wyu97/KENLG-Reading) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04389](https://arxiv.org/abs/2010.04389) [cs.CL]** |
|           | (or **[arXiv:2010.04389v1](https://arxiv.org/abs/2010.04389v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-6">6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation</h2>

Title: [Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](https://arxiv.org/abs/2010.04411)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.

| Comments: | Accepted to EMNLP 2020, 12 pages, 2 figures, 9 tables        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04411](https://arxiv.org/abs/2010.04411) [cs.CL]** |
|           | (or **[arXiv:2010.04411v1](https://arxiv.org/abs/2010.04411v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-7">7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels</h2>

Title: [Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](https://arxiv.org/abs/2010.04438)

Authors: [Harris Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+H), [Jamie Kiros](https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W)

> A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.

| Comments: | 10 pages (+3 appendix), 11 figures, 5 tables. Accepted to Findings of EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2010.04438](https://arxiv.org/abs/2010.04438) [cs.CL]** |
|           | (or **[arXiv:2010.04438v1](https://arxiv.org/abs/2010.04438v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-8">8. Self-Paced Learning for Neural Machine Translation</h2>

Title: [Self-Paced Learning for Neural Machine Translation](https://arxiv.org/abs/2010.04505)

Authors: [Yu Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Yikai Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B)

> Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.

| Comments: | Accepted by EMNLP2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04505](https://arxiv.org/abs/2010.04505) [cs.CL]** |
|           | (or **[arXiv:2010.04505v1](https://arxiv.org/abs/2010.04505v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-9">9. Recursive Top-Down Production for Sentence Generation with Latent Trees</h2>

Title: [Recursive Top-Down Production for Sentence Generation with Latent Trees](https://arxiv.org/abs/2010.04704)

Authors: [Shawn Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Timothy J. O'Donnell](https://arxiv.org/search/cs?searchtype=author&query=O'Donnell%2C+T+J), [Alessandro Sordoni](https://arxiv.org/search/cs?searchtype=author&query=Sordoni%2C+A), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville%2C+A)

> We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017), where it outperforms previous models on the LENGTH split, and English question formation (McCoy et al., 2020), where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset (Elliott et al., 2016), and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04704](https://arxiv.org/abs/2010.04704) [cs.CL]** |
|           | (or **[arXiv:2010.04704v1](https://arxiv.org/abs/2010.04704v1) [cs.CL]** for this version) |







# 2020-10-09

[Return to Index](#Index)



<h2 id="2020-10-09-1">1. Shallow-to-Deep Training for Neural Machine Translation</h2>

Title: [Shallow-to-Deep Training for Neural Machine Translation](https://arxiv.org/abs/2010.03737)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Huizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is 1.4  faster than training from scratch, and achieves a BLEU score of 30.33 and 43.29 on two tasks. The code is publicly available at [this https URL](https://github.com/libeineu/SDT-Training/).

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03737](https://arxiv.org/abs/2010.03737) [cs.CL]** |
|           | (or **[arXiv:2010.03737v1](https://arxiv.org/abs/2010.03737v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-2">2. Improving Attention Mechanism with Query-Value Interaction</h2>

Title: [Improving Attention Mechanism with Query-Value Interaction](https://arxiv.org/abs/2010.03766)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Attention mechanism has played critical roles in various state-of-the-art NLP models such as Transformer and BERT. It can be formulated as a ternary function that maps the input queries, keys and values into an output by using a summation of values weighted by the attention weights derived from the interactions between queries and keys. Similar with query-key interactions, there is also inherent relatedness between queries and values, and incorporating query-value interactions has the potential to enhance the output by learning customized values according to the characteristics of queries. However, the query-value interactions are ignored by existing attention methods, which may be not optimal. In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions. We propose a query-value interaction function which can learn query-aware attention values, and combine them with the original values and attention weights to form the final output. Extensive experiments on four datasets for different tasks show that our approach can consistently improve the performance of many attention-based models by incorporating query-value interactions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03766](https://arxiv.org/abs/2010.03766) [cs.CL]** |
|           | (or **[arXiv:2010.03766v1](https://arxiv.org/abs/2010.03766v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-3">3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning</h2>

Title: [ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768)

Authors: [Mohit Shridhar](https://arxiv.org/search/cs?searchtype=author&query=Shridhar%2C+M), [Xingdi Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+X), [Marc-Alexandre Ct](https://arxiv.org/search/cs?searchtype=author&query=Ct%2C+M), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Adam Trischler](https://arxiv.org/search/cs?searchtype=author&query=Trischler%2C+A), [Matthew Hausknecht](https://arxiv.org/search/cs?searchtype=author&query=Hausknecht%2C+M)

> Given a simple request (e.g., Put a washed apple in the kitchen fridge), humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Ct et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, visual scene understanding, and so forth).

| Comments: | Data, code, and videos are available at [this http URL](http://alfworld.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO) |
| Cite as:  | **[arXiv:2010.03768](https://arxiv.org/abs/2010.03768) [cs.CL]** |
|           | (or **[arXiv:2010.03768v1](https://arxiv.org/abs/2010.03768v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-4">4. What Can We Do to Improve Peer Review in NLP?</h2>

Title: [What Can We Do to Improve Peer Review in NLP?](https://arxiv.org/abs/2010.03863)

Authors: [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

> Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.

| Comments: | To appear at Findings of EMNLP                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.03863](https://arxiv.org/abs/2010.03863) [cs.CL]** |
|           | (or **[arXiv:2010.03863v1](https://arxiv.org/abs/2010.03863v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-5">5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks</h2>

Title: [Dense Relational Image Captioning via Multi-task Triple-Stream Networks](https://arxiv.org/abs/2010.03855)

Authors: [Dong-Jin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Tae-Hyun oh](https://arxiv.org/search/cs?searchtype=author&query=oh%2C+T), [Jinsoo Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J), [In So Kweon](https://arxiv.org/search/cs?searchtype=author&query=Kweon%2C+I+S)

> We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions of each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS, i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to not only learn to generate captions but also predict the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. We additionally extend analysis to an ablation study, applications on holistic image captioning, scene graph generation, and retrieval tasks

| Comments: | Journal extension of our CVPR 2019 paper [arXiv:1903.05942](https://arxiv.org/abs/1903.05942) . Source code : [this https URL](https://github.com/Dong-JinKim/DenseRelationalCaptioning) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2010.03855](https://arxiv.org/abs/2010.03855) [cs.CV]** |
|           | (or **[arXiv:2010.03855v1](https://arxiv.org/abs/2010.03855v1) [cs.CV]** for this version) |





<h2 id="2020-10-09-6">6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations</h2>

Title: [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://arxiv.org/abs/2010.03644)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [Pradyumna Narayana](https://arxiv.org/search/cs?searchtype=author&query=Narayana%2C+P), [Kazoo Sone](https://arxiv.org/search/cs?searchtype=author&query=Sone%2C+K), [Sugato Basu](https://arxiv.org/search/cs?searchtype=author&query=Basu%2C+S), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2010.03644](https://arxiv.org/abs/2010.03644) [cs.CL]** |
|           | (or **[arXiv:2010.03644v1](https://arxiv.org/abs/2010.03644v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-7">7. Cross-Thought for Sentence Encoder Pre-training</h2>

Title: [Cross-Thought for Sentence Encoder Pre-training](https://arxiv.org/abs/2010.03652)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03652](https://arxiv.org/abs/2010.03652) [cs.CL]** |
|           | (or **[arXiv:2010.03652v1](https://arxiv.org/abs/2010.03652v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-8">8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation</h2>

Title: [Leveraging Discourse Rewards for Document-Level Neural Machine Translation](https://arxiv.org/abs/2010.03732)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Nazanin Esmaili](https://arxiv.org/search/cs?searchtype=author&query=Esmaili%2C+N), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion (LC) and coherence (COH), by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F_BERT.

| Comments: | Accepted at COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03732](https://arxiv.org/abs/2010.03732) [cs.CL]** |
|           | (or **[arXiv:2010.03732v1](https://arxiv.org/abs/2010.03732v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-9">9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</h2>

Title: [Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](https://arxiv.org/abs/2010.03790)

Authors: [Keerthiram Murugesan](https://arxiv.org/search/cs?searchtype=author&query=Murugesan%2C+K), [Mattia Atzeni](https://arxiv.org/search/cs?searchtype=author&query=Atzeni%2C+M), [Pavan Kapanipathi](https://arxiv.org/search/cs?searchtype=author&query=Kapanipathi%2C+P), [Pushkar Shukla](https://arxiv.org/search/cs?searchtype=author&query=Shukla%2C+P), [Sadhana Kumaravel](https://arxiv.org/search/cs?searchtype=author&query=Kumaravel%2C+S), [Gerald Tesauro](https://arxiv.org/search/cs?searchtype=author&query=Tesauro%2C+G), [Kartik Talamadupula](https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K), [Mrinmaya Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+M), [Murray Campbell](https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M)

> Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03790](https://arxiv.org/abs/2010.03790) [cs.AI]** |
|           | (or **[arXiv:2010.03790v1](https://arxiv.org/abs/2010.03790v1) [cs.AI]** for this version) |





# 2020-10-08

[Return to Index](#Index)



<h2 id="2020-10-08-1">1. Plug and Play Autoencoders for Conditional Text Generation</h2>

Title: [Plug and Play Autoencoders for Conditional Text Generation](https://arxiv.org/abs/2010.02983)

Authors: [Florian Mai](https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+F) (1 and 2), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N) (3), [Ivan Montero](https://arxiv.org/search/cs?searchtype=author&query=Montero%2C+I) (3), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A) (3 and 4), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J) (1) ((1) Idiap Research Institute, (2) EPFL, (3) University of Washington, (4) Allen Institute for Artificial Intelligence)

> Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.

| Comments: | To be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02983](https://arxiv.org/abs/2010.02983) [cs.CL]** |
|           | (or **[arXiv:2010.02983v1](https://arxiv.org/abs/2010.02983v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-2">2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</h2>

Title: [Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](https://arxiv.org/abs/2010.03142)

Authors: [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple low-resource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pre-training corpus. Code, data, and pre-trained models are available at [this https URL](https://github.com/linzehui/mRASP).

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03142](https://arxiv.org/abs/2010.03142) [cs.CL]** |
|           | (or **[arXiv:2010.03142v1](https://arxiv.org/abs/2010.03142v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-3">3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction</h2>

Title: [A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](https://arxiv.org/abs/2010.03155)

Authors: [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found that there is a non-negligible amount of "noise" where errors were inappropriately edited or left uncorrected. To address this, we designed a self-refinement method where the key idea is to denoise these datasets by leveraging the prediction consistency of existing models, and outperformed strong denoising baseline methods. We further applied task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. We then analyzed the effect of the proposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance.

| Comments: | accepted by EMNLP 2020 (Findings)                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03155](https://arxiv.org/abs/2010.03155) [cs.CL]** |
|           | (or **[arXiv:2010.03155v1](https://arxiv.org/abs/2010.03155v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-4">4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages</h2>

Title: [Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](https://arxiv.org/abs/2010.03179)

Authors: [Michael A. Hedderich](https://arxiv.org/search/cs?searchtype=author&query=Hedderich%2C+M+A), [David Adelani](https://arxiv.org/search/cs?searchtype=author&query=Adelani%2C+D), [Dawei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+D), [Jesujoba Alabi](https://arxiv.org/search/cs?searchtype=author&query=Alabi%2C+J), [Udia Markus](https://arxiv.org/search/cs?searchtype=author&query=Markus%2C+U), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and Yorb on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.

| Comments: | Accepted at EMNLP'20                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03179](https://arxiv.org/abs/2010.03179) [cs.CL]** |
|           | (or **[arXiv:2010.03179v1](https://arxiv.org/abs/2010.03179v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-5">5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction</h2>

Title: [Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](https://arxiv.org/abs/2010.03260)

Authors: [Mengyun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Xingxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03260](https://arxiv.org/abs/2010.03260) [cs.CL]** |
|           | (or **[arXiv:2010.03260v1](https://arxiv.org/abs/2010.03260v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-6">6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation</h2>

Title: [Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](https://arxiv.org/abs/2010.03412)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.

| Comments: | Accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03412](https://arxiv.org/abs/2010.03412) [cs.CL]** |
|           | (or **[arXiv:2010.03412v1](https://arxiv.org/abs/2010.03412v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-7">7. WER we are and WER we think we are</h2>

Title: [WER we are and WER we think we are](https://arxiv.org/abs/2010.03432)

Authors: [Piotr Szymaski](https://arxiv.org/search/cs?searchtype=author&query=Szymaski%2C+P), [Piotr elasko](https://arxiv.org/search/cs?searchtype=author&query=elasko%2C+P), [Mikolaj Morzy](https://arxiv.org/search/cs?searchtype=author&query=Morzy%2C+M), [Adrian Szymczak](https://arxiv.org/search/cs?searchtype=author&query=Szymczak%2C+A), [Marzena ya-Hoppe](https://arxiv.org/search/cs?searchtype=author&query=ya-Hoppe%2C+M), [Joanna Banaszczak](https://arxiv.org/search/cs?searchtype=author&query=Banaszczak%2C+J), [Lukasz Augustyniak](https://arxiv.org/search/cs?searchtype=author&query=Augustyniak%2C+L), [Jan Mizgajski](https://arxiv.org/search/cs?searchtype=author&query=Mizgajski%2C+J), [Yishay Carmiel](https://arxiv.org/search/cs?searchtype=author&query=Carmiel%2C+Y)

> Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.

| Comments: | Accepted to EMNLP Findings                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.03432](https://arxiv.org/abs/2010.03432) [cs.CL]** |
|           | (or **[arXiv:2010.03432v1](https://arxiv.org/abs/2010.03432v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-8">8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation</h2>

Title: [Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](https://arxiv.org/abs/2010.03486)

Authors: [Valentin Barriere](https://arxiv.org/search/cs?searchtype=author&query=Barriere%2C+V), [Alexandra Balahur](https://arxiv.org/search/cs?searchtype=author&query=Balahur%2C+A)

> Tweets are specific text data when compared to general text. Although sentiment analysis over tweets has become very popular in the last decade for English, it is still difficult to find huge annotated corpora for non-English languages. The recent rise of the transformer models in Natural Language Processing allows to achieve unparalleled performances in many tasks, but these models need a consequent quantity of text to adapt to the tweet domain. We propose the use of a multilingual transformer model, that we pre-train over English tweets and apply data-augmentation using automatic translation to adapt the model to non-English languages. Our experiments in French, Spanish, German and Italian suggest that the proposed technique is an efficient way to improve the results of the transformers over small corpora of tweets in a non-English language.

| Comments: | Accepted to COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03486](https://arxiv.org/abs/2010.03486) [cs.CL]** |
|           | (or **[arXiv:2010.03486v1](https://arxiv.org/abs/2010.03486v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-9">9. TeaForN: Teacher-Forcing with N-grams</h2>

Title: [TeaForN: Teacher-Forcing with N-grams](https://arxiv.org/abs/2010.03494)

Authors: [Sebastian Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.

| Comments: | to be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03494](https://arxiv.org/abs/2010.03494) [cs.CL]** |
|           | (or **[arXiv:2010.03494v1](https://arxiv.org/abs/2010.03494v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-10">10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models</h2>

Title: [Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](https://arxiv.org/abs/2010.03542)

Authors: [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y)

> This paper describes Galileo's performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.

| Comments: | 8 pages, 2 figures, 6 tables. Accepted at Proceedings of 14th International Workshop on Semantic Evaluation (SemEval-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03542](https://arxiv.org/abs/2010.03542) [cs.CL]** |
|           | (or **[arXiv:2010.03542v1](https://arxiv.org/abs/2010.03542v1) [cs.CL]** for this version) |



# 2020-10-07

[Return to Index](#Index)



<h2 id="2020-10-07-1">1. Multi-task Learning for Multilingual Neural Machine Translation</h2>

Title: [Multi-task Learning for Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.02523)

Authors: [Yiren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [ChengXiang Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+C), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H)

> While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02523](https://arxiv.org/abs/2010.02523) [cs.CL]** |
|           | (or **[arXiv:2010.02523v1](https://arxiv.org/abs/2010.02523v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-2">2. Do Explicit Alignments Robustly Improve Multilingual Encoders?</h2>

Title: [Do Explicit Alignments Robustly Improve Multilingual Encoders?](https://arxiv.org/abs/2010.02537)

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M)

> Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02537](https://arxiv.org/abs/2010.02537) [cs.CL]** |
|           | (or **[arXiv:2010.02537v1](https://arxiv.org/abs/2010.02537v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-3">3. The Multilingual Amazon Reviews Corpus</h2>

Title: [The Multilingual Amazon Reviews Corpus](https://arxiv.org/abs/2010.02573)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Gyrgy Szarvas](https://arxiv.org/search/cs?searchtype=author&query=Szarvas%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.

| Comments: | To appear in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02573](https://arxiv.org/abs/2010.02573) [cs.CL]** |
|           | (or **[arXiv:2010.02573v1](https://arxiv.org/abs/2010.02573v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-4">4. On the Sparsity of Neural Machine Translation Models</h2>

Title: [On the Sparsity of Neural Machine Translation Models](https://arxiv.org/abs/2010.02646)

Authors: [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02646](https://arxiv.org/abs/2010.02646) [cs.CL]** |
|           | (or **[arXiv:2010.02646v1](https://arxiv.org/abs/2010.02646v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-5">5. On the Sub-Layer Functionalities of Transformer Decoder</h2>

Title: [On the Sub-Layer Functionalities of Transformer Decoder](https://arxiv.org/abs/2010.02648)

Authors: [Yilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Prasad Tadepalli](https://arxiv.org/search/cs?searchtype=author&query=Tadepalli%2C+P), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages -- developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance -- a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.

| Comments: | Findings of the 2020 Conference on Empirical Methods in Natural Language Processing (Long) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02648](https://arxiv.org/abs/2010.02648) [cs.CL]** |
|           | (or **[arXiv:2010.02648v1](https://arxiv.org/abs/2010.02648v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-6">6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</h2>

Title: [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://arxiv.org/abs/2010.02684)

Authors: [Alvin Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+A), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A)

> This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a 'backdoor poisoning' attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier's predictions can be steered to the poison target class with success rates of >80% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.

| Comments: | Accepted in EMNLP-Findings 2020, Camera Ready Version        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2010.02684](https://arxiv.org/abs/2010.02684) [cs.CL]** |
|           | (or **[arXiv:2010.02684v1](https://arxiv.org/abs/2010.02684v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-7">7. Analyzing Individual Neurons in Pre-trained Language Models</h2>

Title: [Analyzing Individual Neurons in Pre-trained Language Models](https://arxiv.org/abs/2010.02695)

Authors: [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

> While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.

| Comments: | Accepted in EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02695](https://arxiv.org/abs/2010.02695) [cs.CL]** |
|           | (or **[arXiv:2010.02695v1](https://arxiv.org/abs/2010.02695v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-8">8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</h2>

Title: [Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](https://arxiv.org/abs/2010.02705)

Authors: [Minki Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+M), [Moonsu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Sung Ju Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J)

> We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.

| Comments: | 19 pages, 9 figures, EMNLP 2020                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02705](https://arxiv.org/abs/2010.02705) [cs.CL]** |
|           | (or **[arXiv:2010.02705v1](https://arxiv.org/abs/2010.02705v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-9">9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs</h2>

Title: [Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](https://arxiv.org/abs/2010.02847)

Authors: [Haiyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Alison Sneyd](https://arxiv.org/search/cs?searchtype=author&query=Sneyd%2C+A), [Mark Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+M)

> It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy "man is to computer-programmer as woman is to homemaker" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.

| Comments: | Accepted at AACL-IJCNLP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02847](https://arxiv.org/abs/2010.02847) [cs.CL]** |
|           | (or **[arXiv:2010.02847v1](https://arxiv.org/abs/2010.02847v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-10">10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</h2>

Title: [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://arxiv.org/abs/2010.02301)

Authors: [Xinyu Hua](https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X), [Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L)

> Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often "rambling" without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.

| Comments: | Accepted at EMNLP 2020 as a long paper                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02301](https://arxiv.org/abs/2010.02301) [cs.CL]** |
|           | (or **[arXiv:2010.02301v1](https://arxiv.org/abs/2010.02301v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-11">11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation</h2>

Title: [We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](https://arxiv.org/abs/2010.02339)

Authors: [Ashiqur R. KhudaBukhsh](https://arxiv.org/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R), [Rupak Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+R), [Mark S. Kamlet](https://arxiv.org/search/cs?searchtype=author&query=Kamlet%2C+M+S), [Tom M. Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell%2C+T+M)

> Polarization among US political parties, media and elites is a widely studied topic. Prominent lines of prior research across multiple disciplines have observed and analyzed growing polarization in social media. In this paper, we present a new methodology that offers a fresh perspective on interpreting polarization through the lens of machine translation. With a novel proposition that two sub-communities are speaking in two different \emph{languages}, we demonstrate that modern machine translation methods can provide a simple yet powerful and interpretable framework to understand the differences between two (or more) large-scale social media discussion data sets at the granularity of words. Via a substantial corpus of 86.6 million comments by 6.5 million users on over 200,000 news videos hosted by YouTube channels of four prominent US news networks, we demonstrate that simple word-level and phrase-level translation pairs can reveal deep insights into the current political divide -- what is \emph{black lives matter} to one can be \emph{all lives matter} to the other.

| Subjects: | **Computation and Language (cs.CL)**; Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02339](https://arxiv.org/abs/2010.02339) [cs.CL]** |
|           | (or **[arXiv:2010.02339v1](https://arxiv.org/abs/2010.02339v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-12">12. Inference Strategies for Machine Translation with Conditional Masking</h2>

Title: [Inference Strategies for Machine Translation with Conditional Masking](https://arxiv.org/abs/2010.02352)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard "mask-predict" algorithm, and provide analyses of its behavior on machine translation tasks.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02352](https://arxiv.org/abs/2010.02352) [cs.CL]** |
|           | (or **[arXiv:2010.02352v1](https://arxiv.org/abs/2010.02352v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-13">13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</h2>

Title: [Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](https://arxiv.org/abs/2010.02394)

Authors: [Lichao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L), [Congying Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+C), [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Tingting Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+T), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S), [Lifang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+L)

> Mixup is the latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT. To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named "mixup-transformer", for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUE benchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.

| Comments: | Accepted by COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02394](https://arxiv.org/abs/2010.02394) [cs.CL]** |
|           | (or **[arXiv:2010.02394v1](https://arxiv.org/abs/2010.02394v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-14">14. Guiding Attention for Self-Supervised Learning with Transformers</h2>

Title: [Guiding Attention for Self-Supervised Learning with Transformers](https://arxiv.org/abs/2010.02399)

Authors: [Ameet Deshpande](https://arxiv.org/search/cs?searchtype=author&query=Deshpande%2C+A), [Karthik Narasimhan](https://arxiv.org/search/cs?searchtype=author&query=Narasimhan%2C+K)

> In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.

| Comments: | Accepted to Findings of EMNLP, 2020                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02399](https://arxiv.org/abs/2010.02399) [cs.CL]** |
|           | (or **[arXiv:2010.02399v1](https://arxiv.org/abs/2010.02399v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-15">15. Adversarial Grammatical Error Correction</h2>

Title: [Adversarial Grammatical Error Correction](https://arxiv.org/abs/2010.02407)

Authors: [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V), [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D)

> Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs) have been successful in generating realistic texts across many different tasks by learning to directly minimize the difference between human-generated and synthetic text. In this work, we present an adversarial learning approach to GEC, using the generator-discriminator framework. The generator is a Transformer model, trained to produce grammatically correct sentences given grammatically incorrect ones. The discriminator is a sentence-pair classification model, trained to judge a given pair of grammatically incorrect-correct sentences on the quality of grammatical correction. We pre-train both the discriminator and the generator on parallel texts and then fine-tune them further using a policy gradient method that assigns high rewards to sentences which could be true corrections of the grammatically incorrect text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that Adversarial-GEC can achieve competitive GEC quality compared to NMT-based baselines.

| Comments: | 13 Pages, EMNLP 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02407](https://arxiv.org/abs/2010.02407) [cs.CL]** |
|           | (or **[arXiv:2010.02407v1](https://arxiv.org/abs/2010.02407v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-16">16. Efficient Inference For Neural Machine Translation</h2>

Title: [Efficient Inference For Neural Machine Translation](https://arxiv.org/abs/2010.02416)

Authors: [Yi-Te Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+Y), [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Yi-Hsiu Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Y), [Ilya Chatsviorkin](https://arxiv.org/search/cs?searchtype=author&query=Chatsviorkin%2C+I)

> Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02416](https://arxiv.org/abs/2010.02416) [cs.CL]** |
|           | (or **[arXiv:2010.02416v1](https://arxiv.org/abs/2010.02416v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-17">17. Iterative Domain-Repaired Back-Translation</h2>

Title: [Iterative Domain-Repaired Back-Translation](https://arxiv.org/abs/2010.02473)

Authors: [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02473](https://arxiv.org/abs/2010.02473) [cs.CL]** |
|           | (or **[arXiv:2010.02473v1](https://arxiv.org/abs/2010.02473v1) [cs.CL]** for this version) |







# 2020-10-06

[Return to Index](#Index)



<h2 id="2020-10-06-1">1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples</h2>

Title: [A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](https://arxiv.org/abs/2010.01345)

Authors: [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.

| Comments: | COLING 2020 Long Paper                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01345](https://arxiv.org/abs/2010.01345) [cs.CL]** |
|           | (or **[arXiv:2010.01345v1](https://arxiv.org/abs/2010.01345v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-2">2. Transformer-Based Neural Text Generation with Syntactic Guidance</h2>

Title: [Transformer-Based Neural Text Generation with Syntactic Guidance](https://arxiv.org/abs/2010.01737)

Authors: [Yinghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y) (Georgia Institute of Technology), [Rui Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+R) (Georgia Institute of Technology), [Isaac Rehg](https://arxiv.org/search/cs?searchtype=author&query=Rehg%2C+I) (Georgia Institute of Technology), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C) (Georgia Institute of Technology)

> We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.

| Comments: | 11 pages, 4 figures and 5 tables                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01737](https://arxiv.org/abs/2010.01737) [cs.CL]** |
|           | (or **[arXiv:2010.01737v1](https://arxiv.org/abs/2010.01737v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-3">3. Second-Order NLP Adversarial Examples</h2>

Title: [Second-Order NLP Adversarial Examples](https://arxiv.org/abs/2010.01770)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X)

> Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at [this https URL](https://github.com/jxmorris12/second-order-adversarial-examples).

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01770](https://arxiv.org/abs/2010.01770) [cs.CL]** |
|           | (or **[arXiv:2010.01770v2](https://arxiv.org/abs/2010.01770v2) [cs.CL]** for this version) |





<h2 id="2020-10-06-4">4. GenAug: Data Augmentation for Finetuning Text Generators</h2>

Title: [GenAug: Data Augmentation for Finetuning Text Generators](https://arxiv.org/abs/2010.01794)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Dongyeop Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+D), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.

| Comments: | EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available at [this https URL](https://github.com/styfeng/GenAug) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.01794](https://arxiv.org/abs/2010.01794) [cs.CL]** |
|           | (or **[arXiv:2010.01794v1](https://arxiv.org/abs/2010.01794v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-5">5. Lifelong Language Knowledge Distillation</h2>

Title: [Lifelong Language Knowledge Distillation](https://arxiv.org/abs/2010.02123)

Authors: [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Shang-Yu Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+S), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02123](https://arxiv.org/abs/2010.02123) [cs.CL]** |
|           | (or **[arXiv:2010.02123v1](https://arxiv.org/abs/2010.02123v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-6">6. A Streaming Approach For Efficient Batched Beam Search</h2>

Title: [A Streaming Approach For Efficient Batched Beam Search](https://arxiv.org/abs/2010.02164)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Violet Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+V), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically ``refills" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF) |
| Cite as:  | **[arXiv:2010.02164](https://arxiv.org/abs/2010.02164) [cs.CL]** |
|           | (or **[arXiv:2010.02164v1](https://arxiv.org/abs/2010.02164v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-7">7. Self-training Improves Pre-training for Natural Language Understanding</h2>

Title: [Self-training Improves Pre-training for Natural Language Understanding](https://arxiv.org/abs/2010.02194)

Authors: [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Beliz Gunel](https://arxiv.org/search/cs?searchtype=author&query=Gunel%2C+B), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02194](https://arxiv.org/abs/2010.02194) [cs.CL]** |
|           | (or **[arXiv:2010.02194v1](https://arxiv.org/abs/2010.02194v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-8">8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation</h2>

Title: [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.01667)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> To improve the performance of Neural Machine Translation~(NMT) for low-resource languages~(LRL), one effective strategy is to leverage parallel data from a related high-resource language~(HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate \emph{into} the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01667](https://arxiv.org/abs/2010.01667) [cs.CL]** |
|           | (or **[arXiv:2010.01667v1](https://arxiv.org/abs/2010.01667v1) [cs.CL]** for this version) |





# 2020-10-05

[Return to Index](#Index)



<h2 id="2020-10-05-1">1. Nearest Neighbor Machine Translation</h2>

Title: [Nearest Neighbor Machine Translation](https://arxiv.org/abs/2010.00710)

Authors: [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We introduce k-nearest-neighbor machine translation (kNN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.00710](https://arxiv.org/abs/2010.00710) [cs.CL]** |
|           | (or **[arXiv:2010.00710v1](https://arxiv.org/abs/2010.00710v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-2">2. A Survey of the State of Explainable AI for Natural Language Processing</h2>

Title: [A Survey of the State of Explainable AI for Natural Language Processing](https://arxiv.org/abs/2010.00711)

Authors: [Marina Danilevsky](https://arxiv.org/search/cs?searchtype=author&query=Danilevsky%2C+M), [Kun Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+K), [Ranit Aharonov](https://arxiv.org/search/cs?searchtype=author&query=Aharonov%2C+R), [Yannis Katsis](https://arxiv.org/search/cs?searchtype=author&query=Katsis%2C+Y), [Ban Kawas](https://arxiv.org/search/cs?searchtype=author&query=Kawas%2C+B), [Prithviraj Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+P)

> Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.

| Comments:    | To appear in AACL-IJCNLP 2020                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.00711](https://arxiv.org/abs/2010.00711) [cs.CL]** |
|              | (or **[arXiv:2010.00711v1](https://arxiv.org/abs/2010.00711v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-3">3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</h2>

Title: [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://arxiv.org/abs/2010.00784)

Authors: [Kristjan Arumae](https://arxiv.org/search/cs?searchtype=author&query=Arumae%2C+K), [Qing Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Parminder Bhatia](https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+P)

> Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.

| Comments: | arXiv admin note: text overlap with [arXiv:2004.03794](https://arxiv.org/abs/2004.03794) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.00784](https://arxiv.org/abs/2010.00784) [cs.CL]** |
|           | (or **[arXiv:2010.00784v1](https://arxiv.org/abs/2010.00784v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-4">4. Which *BERT? A Survey Organizing Contextualized Encoders</h2>

Title: [Which *BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/abs/2010.00854)

Authors: [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B)

> Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.00854](https://arxiv.org/abs/2010.00854) [cs.CL]** |
|           | (or **[arXiv:2010.00854v1](https://arxiv.org/abs/2010.00854v1) [cs.CL]** for this version) |



# 2020-10-02

[Return to Index](#Index)



<h2 id="2020-10-02-1">1. WeChat Neural Machine Translation Systems for WMT20</h2>

Title: [WeChat Neural Machine Translation Systems for WMT20](https://arxiv.org/abs/2010.00247)

Authors: [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Xianfeng Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Qinsong Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Q), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Sifan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H)

> We participate in the WMT 2020 shared news translation task on Chinese to English. Our system is based on the Transformer (Vaswani et al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese to English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.

| Comments: | Accepted at WMT 2020. Our Chinese to English system achieved the highest case-sensitive BLEU score among all submissions |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.00247](https://arxiv.org/abs/2010.00247) [cs.CL]** |
|           | (or **[arXiv:2010.00247v1](https://arxiv.org/abs/2010.00247v1) [cs.CL]** for this version) |



# 2020-10-01

[Return to Index](#Index)



<h2 id="2020-10-01-1">1. Rethinking Attention with Performers</h2>

Title: [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)

Authors: [Krzysztof Choromanski](https://arxiv.org/search/cs?searchtype=author&query=Choromanski%2C+K), [Valerii Likhosherstov](https://arxiv.org/search/cs?searchtype=author&query=Likhosherstov%2C+V), [David Dohan](https://arxiv.org/search/cs?searchtype=author&query=Dohan%2C+D), [Xingyou Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Andreea Gane](https://arxiv.org/search/cs?searchtype=author&query=Gane%2C+A), [Tamas Sarlos](https://arxiv.org/search/cs?searchtype=author&query=Sarlos%2C+T), [Peter Hawkins](https://arxiv.org/search/cs?searchtype=author&query=Hawkins%2C+P), [Jared Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+J), [Afroz Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+A), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [David Belanger](https://arxiv.org/search/cs?searchtype=author&query=Belanger%2C+D), [Lucy Colwell](https://arxiv.org/search/cs?searchtype=author&query=Colwell%2C+L), [Adrian Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+A)

> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.

| Comments: | 36 pages. This is an updated version of a previous submission which can be found at [arXiv:2006.03555](https://arxiv.org/abs/2006.03555). See [this https URL](https://github.com/google-research/google-research/tree/master/protein_lm) for protein language model code, and [this https URL](https://github.com/google-research/google-research/tree/master/performer) for Performer code |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.14794](https://arxiv.org/abs/2009.14794) [cs.LG]** |
|           | (or **[arXiv:2009.14794v1](https://arxiv.org/abs/2009.14794v1) [cs.LG]** for this version) |





<h2 id="2020-10-01-2">2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</h2>

Title: [Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](https://arxiv.org/abs/2009.14304)

Authors: [Saurabh Kulshreshtha](https://arxiv.org/search/cs?searchtype=author&query=Kulshreshtha%2C+S), [Jos Luis Redondo-Garca](https://arxiv.org/search/cs?searchtype=author&query=Redondo-Garca%2C+J+L), [Ching-Yun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C)

> Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work proposes several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs. dictionary-based supervision and rotational vs. fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.

| Comments: | Accepted as a long paper in Findings of EMNLP 2020           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14304](https://arxiv.org/abs/2009.14304) [cs.CL]** |
|           | (or **[arXiv:2009.14304v1](https://arxiv.org/abs/2009.14304v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-3">3. Can Automatic Post-Editing Improve NMT?</h2>

Title: [Can Automatic Post-Editing Improve NMT?](https://arxiv.org/abs/2009.14395)

Authors: [Shamil Chollampatt](https://arxiv.org/search/cs?searchtype=author&query=Chollampatt%2C+S), [Raymond Hendy Susanto](https://arxiv.org/search/cs?searchtype=author&query=Susanto%2C+R+H), [Liling Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L), [Ewa Szymanska](https://arxiv.org/search/cs?searchtype=author&query=Szymanska%2C+E)

> Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at [this https URL](https://github.com/shamilcm/pedra).

| Comments: | In EMNLP 2020                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14395](https://arxiv.org/abs/2009.14395) [cs.CL]** |
|           | (or **[arXiv:2009.14395v1](https://arxiv.org/abs/2009.14395v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-4">4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment</h2>

Title: [Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](https://arxiv.org/abs/2009.14510)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data.

| Comments: | EMNLP-2020 Long Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.14510](https://arxiv.org/abs/2009.14510) [cs.CL]** |
|           | (or **[arXiv:2009.14510v1](https://arxiv.org/abs/2009.14510v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-5">5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation</h2>

Title: [On Romanization for Model Transfer Between Scripts in Neural Machine Translation](https://arxiv.org/abs/2009.14824)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.

| Comments: | accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14824](https://arxiv.org/abs/2009.14824) [cs.CL]** |
|           | (or **[arXiv:2009.14824v1](https://arxiv.org/abs/2009.14824v1) [cs.CL]** for this version) |

