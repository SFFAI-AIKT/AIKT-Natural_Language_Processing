# Daily arXiv: Machine Translation - September, 2020

# Index


- [2020-09-16](#2020-09-16)

  - [1. Efficient Transformers: A Survey](#2020-09-16-1)
  - [2. Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models](#2020-09-16-2)
  - [3. Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](#2020-09-16-3)
  - [4. A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation](#2020-09-16-4)
  - [5. Autoregressive Knowledge Distillation through Imitation Learning](#2020-09-16-5)
- [2020-09-15](#2020-09-15)

  - [1. Unit Test Case Generation with Transformers](#2020-09-15-1)
  - [2. Improving Indonesian Text Classification Using Multilingual Language Model](#2020-09-15-2)
  - [3. Combining Word and Character Vector Representation on Neural Machine Translation](#2020-09-15-3)
  - [4. Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding](#2020-09-15-4)
  - [5. Contrastive Triple Extraction with Generative Transformer](#2020-09-15-5)
  - [6. Improving Language Generation with Sentence Coherence Objective](#2020-09-15-6)
  - [7. GeDi: Generative Discriminator Guided Sequence Generation](#2020-09-15-7)
- [2020-09-14](#2020-09-14)

  - [1. FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding](#2020-09-14-1)
  - [2. Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation](#2020-09-14-2)
- [2020-09-11](#2020-09-11)
- [1. Pay Attention when Required](#2020-09-11-1)
  - [2. Learning Universal Representations from Word to Sentence](#2020-09-11-2)
  - [3. Modern Methods for Text Generation](#2020-09-11-3)
  - [4. Investigating Gender Bias in BERT](#2020-09-11-4)
- [2020-09-10](#2020-09-10)

  - [1. Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages](#2020-09-10-1)
- [2020-09-08](#2020-09-08)

  - [1. Measuring Massive Multitask Language Understanding](#2020-09-08-1)
  - [2. Recent Trends in the Use of Deep Learning Models for Grammar Error Handling](#2020-09-08-2)
  - [3. Bio-inspired Structure Identification in Language Embeddings](#2020-09-08-3)
  - [4. Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity](#2020-09-08-4)
- [2020-09-07](#2020-09-07)
- [1. Data Readiness for Natural Language Processing](#2020-09-07-1)
  - [2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation](#2020-09-07-2)
  - [3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search](#2020-09-07-3)
  - [4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](#2020-09-07-4)
- [2020-09-01](#2020-09-01)

  - [1. Knowledge Efficient Deep Learning for Natural Language Processing](#2020-09-01-1)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-09-16

[Return to Index](#Index)



<h2 id="2020-09-16-1">1. Efficient Transformers: A Survey</h2>

Title: [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \emph{"X-former"} models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory \emph{efficiency}. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06732](https://arxiv.org/abs/2009.06732) [cs.LG]** |
|           | (or **[arXiv:2009.06732v1](https://arxiv.org/abs/2009.06732v1) [cs.LG]** for this version) |



<h2 id="2020-09-16-2">2. Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models</h2>

Title: [Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models](https://arxiv.org/abs/2009.07053)

Authors: [Joseph F DeRose](https://arxiv.org/search/cs?searchtype=author&query=DeRose%2C+J+F), [Jiayao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Matthew Berger](https://arxiv.org/search/cs?searchtype=author&query=Berger%2C+M)

> Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.

| Comments: | 11 pages, 12 figures, to be published in IEEE Transactions on Visualization and Computer Graphics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2009.07053](https://arxiv.org/abs/2009.07053) [cs.HC]** |
|           | (or **[arXiv:2009.07053v1](https://arxiv.org/abs/2009.07053v1) [cs.HC]** for this version) |



<h2 id="2020-09-16-3">3. Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation</h2>

Title: [Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2009.07177)

Authors: [Jason Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Raphael Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+R), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 En-De, WMT'16 Ro-En and IWSLT'16 De-En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 En-De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).

| Comments: | Accepted to EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.07177](https://arxiv.org/abs/2009.07177) [cs.CL]** |
|           | (or **[arXiv:2009.07177v1](https://arxiv.org/abs/2009.07177v1) [cs.CL]** for this version) |



<h2 id="2020-09-16-4">4. A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation</h2>

Title: [A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation](https://arxiv.org/abs/2009.07243)

Authors: [Moin Nadeem](https://arxiv.org/search/cs?searchtype=author&query=Nadeem%2C+M), [Tianxing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J)

> This work studies the widely adopted ancestral sampling algorithms for auto-regressive language models, which is not widely studied in the literature. We use the quality-diversity (Q-D) trade-off to investigate three popular sampling algorithms (top-k, nucleus and tempered sampling). We focus on the task of open-ended language generation. We first show that the existing sampling algorithms have similar performance. After carefully inspecting the transformations defined by different sampling algorithms, we identify three key properties that are shared among them: entropy reduction, order preservation, and slope preservation. To validate the importance of the identified properties, we design two sets of new sampling algorithms: one set in which each algorithm satisfies all three properties, and one set in which each algorithm violates at least one of the properties. We compare their performance with existing sampling algorithms, and find that violating the identified properties could lead to drastic performance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms. Our data and code are available at [this https URL](https://github.com/moinnadeem/characterizing-sampling-algorithms)

| Comments: | To appear at AACL 2020; 9 pages, 12 figures, 2 tables        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.07243](https://arxiv.org/abs/2009.07243) [cs.CL]** |
|           | (or **[arXiv:2009.07243v1](https://arxiv.org/abs/2009.07243v1) [cs.CL]** for this version) |



<h2 id="2020-09-16-5">5. Autoregressive Knowledge Distillation through Imitation Learning</h2>

Title: [Autoregressive Knowledge Distillation through Imitation Learning](https://arxiv.org/abs/2009.07253)

Authors: [Alexander Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+A), [Jeremy Wohlwend](https://arxiv.org/search/cs?searchtype=author&query=Wohlwend%2C+J), [Howard Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Tao Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+T)

> The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.07253](https://arxiv.org/abs/2009.07253) [cs.CL]** |
|           | (or **[arXiv:2009.07253v1](https://arxiv.org/abs/2009.07253v1) [cs.CL]** for this version) |







# 2020-09-15

[Return to Index](#Index)



<h2 id="2020-09-15-1">1. Unit Test Case Generation with Transformers</h2>

Title: [Unit Test Case Generation with Transformers](https://arxiv.org/abs/2009.05617)

Authors: [Michele Tufano](https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M), [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Shao Kun Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S+K), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N)

> Automated Unit Test Case generation has been the focus of extensive literature within the research community. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult to read or understand for developers. In this paper we propose AthenaTest, an approach that aims at generating unit test cases by learning from real-world, developer-written test cases. Our approach relies on a state-of-the-art sequence-to-sequence transformer model which is able to write useful test cases for a given method under test (i.e., focal method). We also introduce methods2test - the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 630k test cases mined from 70k open-source repositories hosted on GitHub. We use this dataset to train a transformer model to translate focal methods into the corresponding test cases. We evaluate the ability of our model in generating test cases using natural language processing as well as code-specific criteria. First, we assess the quality of the translation compared to the target test case, then we analyze properties of the test case such as syntactic correctness and number and variety of testing APIs (e.g., asserts). We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated test cases.

| Subjects: | **Software Engineering (cs.SE)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.05617](https://arxiv.org/abs/2009.05617) [cs.SE]** |
|           | (or **[arXiv:2009.05617v1](https://arxiv.org/abs/2009.05617v1) [cs.SE]** for this version) |





<h2 id="2020-09-15-2">2. Improving Indonesian Text Classification Using Multilingual Language Model</h2>

Title: [Improving Indonesian Text Classification Using Multilingual Language Model](https://arxiv.org/abs/2009.05713)

Authors: [Ilham Firdausi Putra](https://arxiv.org/search/cs?searchtype=author&query=Putra%2C+I+F) (1), [Ayu Purwarianti](https://arxiv.org/search/cs?searchtype=author&query=Purwarianti%2C+A) (1 and 2) ((1) Institut Teknologi Bandung, (2) U-CoE AI-VLB)

> Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | 2020 International Conference on Advanced Informatics: Concept, Theory and Application (ICAICTA) |
| Cite as:           | **[arXiv:2009.05713](https://arxiv.org/abs/2009.05713) [cs.CL]** |
|                    | (or **[arXiv:2009.05713v1](https://arxiv.org/abs/2009.05713v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-3">3. Combining Word and Character Vector Representation on Neural Machine Translation</h2>

Title: [Combining Word and Character Vector Representation on Neural Machine Translation](https://arxiv.org/abs/2009.05935)

Authors: [K. M. Shahih](https://arxiv.org/search/cs?searchtype=author&query=Shahih%2C+K+M), [Ayu Purwarianti](https://arxiv.org/search/cs?searchtype=author&query=Purwarianti%2C+A)

> This paper describes combinations of word vector representation and character vector representation in English-Indonesian neural machine translation (NMT). Six configurations of NMT models were built with different input vector representations: word-based, combination of word and character representation using bidirectional LSTM(bi-LSTM), combination of word and character representation using CNN, combination of word and character representation by combining bi-LSTM and CNN by three different vector operations: addition, pointwise multiplication, and averaging. The experiment results showed that NMT models with concatenation of word and character representation obtained BLEU score higher than baseline model, ranging from 9.14 points to 11.65 points, for all models that combining both word and character representation, except the model that combining word and character representation using both bi-LSTM and CNN by addition operation. The highest BLEU score achieved was 42.48 compared to the 30.83 of the baseline model.

| Comments: | 5 pages, 5 figures. Published in 2019 Fourth International Conference on Informatics and Computing (ICIC) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05935](https://arxiv.org/abs/2009.05935) [cs.CL]** |
|           | (or **[arXiv:2009.05935v1](https://arxiv.org/abs/2009.05935v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-4">4. Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding</h2>

Title: [Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding](https://arxiv.org/abs/2009.06097)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Luowei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Our proposed method allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.06097](https://arxiv.org/abs/2009.06097) [cs.CL]** |
|           | (or **[arXiv:2009.06097v1](https://arxiv.org/abs/2009.06097v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-5">5. Contrastive Triple Extraction with Generative Transformer</h2>

Title: [Contrastive Triple Extraction with Generative Transformer](https://arxiv.org/abs/2009.06207)

Authors: [Hongbin Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Shumin Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S), [Mosha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Chuanqi Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, We introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines. Our code and datasets will be released after publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06207](https://arxiv.org/abs/2009.06207) [cs.CL]** |
|           | (or **[arXiv:2009.06207v1](https://arxiv.org/abs/2009.06207v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-6">6. Improving Language Generation with Sentence Coherence Objective</h2>

Title: [Improving Language Generation with Sentence Coherence Objective](https://arxiv.org/abs/2009.06358)

Authors: [Ruixiao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+R), [Jie Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Mehrdad Yousefzadeh](https://arxiv.org/search/cs?searchtype=author&query=Yousefzadeh%2C+M)

> Conditional story generation and contextual text continuation have become increasingly popular topics in NLP community. Existing models are often prone to output paragraphs of texts that gradually diverge from the given prompt. Although the generated text may have a reasonable perplexity and diversity, it could easily be identified by human as gibberish. The goal of our project is to improve the coherence and consistency across sentences in a language-generation model. We aim to solve this issue by first training a sentence pair coherence classifier with GPT-2 pretrained model, and then co-train the GPT-2 language model with this new coherence objective using a method analogous to the REINFORCE algorithm. This fine-tuned language model is able to generate lengthy paragraph conditioned on a given topic without diverging too much. The simplicity of this model allows it to be applicable to a variety of underlying language model architecture since it only modifies the final layer of the pre-trained model.

| Comments: | 11 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.06358](https://arxiv.org/abs/2009.06358) [cs.CL]** |
|           | (or **[arXiv:2009.06358v1](https://arxiv.org/abs/2009.06358v1) [cs.CL]** for this version) |





<h2 id="2020-09-15-7">7. GeDi: Generative Discriminator Guided Sequence Generation</h2>

Title: [GeDi: Generative Discriminator Guided Sequence Generation](https://arxiv.org/abs/2009.06367)

Authors: [Ben Krause](https://arxiv.org/search/cs?searchtype=author&query=Krause%2C+B), [Akhilesh Deepak Gotmare](https://arxiv.org/search/cs?searchtype=author&query=Gotmare%2C+A+D), [Bryan McCann](https://arxiv.org/search/cs?searchtype=author&query=McCann%2C+B), [Nitish Shirish Keskar](https://arxiv.org/search/cs?searchtype=author&query=Keskar%2C+N+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Richard Socher](https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R), [Nazneen Fatema Rajani](https://arxiv.org/search/cs?searchtype=author&query=Rajani%2C+N+F)

> Class-conditional language models (CC-LMs) can be used to generate natural language with specific attributes, such as style or sentiment, by conditioning on an attribute label, or control code. However, we find that these models struggle to control generation when applied to out-of-domain prompts or unseen control codes. To overcome these limitations, we propose generative discriminator (GeDi) guided contrastive generation, which uses CC-LMs as generative discriminators (GeDis) to efficiently guide generation from a (potentially much larger) LM towards a desired attribute. In our human evaluation experiments, we show that GeDis trained for sentiment control on movie reviews are able to control the tone of book text. We also demonstrate that GeDis are able to detoxify generation and control topic while maintaining the same level of linguistic acceptability as direct generation from GPT-2 (1.5B parameters). Lastly, we show that a GeDi trained on only 4 topics can generalize to new control codes from word embeddings, allowing it to guide generation towards wide array of topics.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.06367](https://arxiv.org/abs/2009.06367) [cs.CL]** |
|           | (or **[arXiv:2009.06367v1](https://arxiv.org/abs/2009.06367v1) [cs.CL]** for this version) |





# 2020-09-14

[Return to Index](#Index)



<h2 id="2020-09-14-1">1. FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding</h2>

Title: [FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding](https://arxiv.org/abs/2009.05166)

Authors: [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that is essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs cross-lingual fusion to extract multilingual knowledge in the intermediate layers, and finally performs further language-specific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. For better model scalability, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art (77.0 on average) on the challenging multilingual multi-task benchmark, XTREME.

| Comments: | Top-1 Performance on XTREME leaderboard (https://sites.research.google/xtreme) on September 8, 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05166](https://arxiv.org/abs/2009.05166) [cs.CL]** |
|           | (or **[arXiv:2009.05166v1](https://arxiv.org/abs/2009.05166v1) [cs.CL]** for this version) |





<h2 id="2020-09-14-2">2. Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation</h2>

Title: [Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation](https://arxiv.org/abs/2009.05460)

Authors: [Toms Bergmanis](https://arxiv.org/search/cs?searchtype=author&query=Bergmanis%2C+T), [Artūrs Stafanovičs](https://arxiv.org/search/cs?searchtype=author&query=Stafanovičs%2C+A), [Mārcis Pinnis](https://arxiv.org/search/cs?searchtype=author&query=Pinnis%2C+M)

> Neural machine translation systems typically are trained on curated corpora and break when faced with non-standard orthography or punctuation. Resilience to spelling mistakes and typos, however, is crucial as machine translation systems are used to translate texts of informal origins, such as chat conversations, social media posts and web pages. We propose a simple generative noise model to generate adversarial examples of ten different types. We use these to augment machine translation systems' training data and show that, when tested on noisy data, systems trained using adversarial examples perform almost as well as when translating clean data, while baseline systems' performance drops by 2-3 BLEU points. To measure the robustness and noise invariance of machine translation systems' outputs, we use the average translation edit rate between the translation of the original sentence and its noised variants. Using this measure, we show that systems trained on adversarial examples on average yield 50% consistency improvements when compared to baselines trained on clean data.

| Comments: | Accepted in BALTIC HLT 2020                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.05460](https://arxiv.org/abs/2009.05460) [cs.CL]** |
|           | (or **[arXiv:2009.05460v1](https://arxiv.org/abs/2009.05460v1) [cs.CL]** for this version) |



# 2020-09-11

[Return to Index](#Index)



<h2 id="2020-09-11-1">1. Pay Attention when Required</h2>

Title: [Pay Attention when Required](https://arxiv.org/abs/2009.04534)

Authors: [Swetha Mandava](https://arxiv.org/search/cs?searchtype=author&query=Mandava%2C+S), [Szymon Migacz](https://arxiv.org/search/cs?searchtype=author&query=Migacz%2C+S), [Alex Fit Florea](https://arxiv.org/search/cs?searchtype=author&query=Florea%2C+A+F)

> Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.

| Comments: | 9 pages, 4 figures, 7 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2009.04534](https://arxiv.org/abs/2009.04534) [cs.LG]** |
|           | (or **[arXiv:2009.04534v1](https://arxiv.org/abs/2009.04534v1) [cs.LG]** for this version) |





<h2 id="2020-09-11-2">2. Learning Universal Representations from Word to Sentence</h2>

Title: [Learning Universal Representations from Word to Sentence](https://arxiv.org/abs/2009.04656)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific level of linguistic unit, which cause great inconvenience when being confronted with handling multiple layers of linguistic objects in a unified way. Thus this work introduces and explores the universal representation learning, i.e., embeddings of different levels of linguistic unit in a uniform vector space through a task-independent evaluation. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space. Then we empirically verify that well pre-trained Transformer models incorporated with appropriate training settings may effectively yield universal representation. Especially, our implementation of fine-tuning ALBERT on NLI and PPDB datasets achieves the highest accuracy on analogy tasks in different language levels. Further experiments on the insurance FAQ task show effectiveness of universal representation models in real-world applications.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04656](https://arxiv.org/abs/2009.04656) [cs.CL]** |
|           | (or **[arXiv:2009.04656v1](https://arxiv.org/abs/2009.04656v1) [cs.CL]** for this version) |



<h2 id="2020-09-11-3">3. Modern Methods for Text Generation</h2>

Title: [Modern Methods for Text Generation](https://arxiv.org/abs/2009.04968)

Authors: [Dimas Munoz Montesinos](https://arxiv.org/search/cs?searchtype=author&query=Montesinos%2C+D+M)

> Synthetic text generation is challenging and has limited success. Recently, a new architecture, called Transformers, allow machine learning models to understand better sequential data, such as translation or summarization. BERT and GPT-2, using Transformers in their cores, have shown a great performance in tasks such as text classification, translation and NLI tasks. In this article, we analyse both algorithms and compare their output quality in text generation tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04968](https://arxiv.org/abs/2009.04968) [cs.CL]** |
|           | (or **[arXiv:2009.04968v1](https://arxiv.org/abs/2009.04968v1) [cs.CL]** for this version) |





<h2 id="2020-09-11-4">4. Investigating Gender Bias in BERT</h2>

Title: [Investigating Gender Bias in BERT](https://arxiv.org/abs/2009.05021)

Authors: [Rishabh Bhardwaj](https://arxiv.org/search/cs?searchtype=author&query=Bhardwaj%2C+R), [Navonil Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+N), [Soujanya Poria](https://arxiv.org/search/cs?searchtype=author&query=Poria%2C+S)

> Contextual language models (CLMs) have pushed the NLP benchmarks to a new height. It has become a new norm to utilize CLM provided word embeddings in downstream tasks such as text classification. However, unless addressed, CLMs are prone to learn intrinsic gender-bias in the dataset. As a result, predictions of downstream NLP models can vary noticeably by varying gender words, such as replacing "he" to "she", or even gender-neutral words. In this paper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the gender-bias it induces in five downstream tasks related to emotion and sentiment intensity prediction. For each task, we train a simple regressor utilizing BERT's word embeddings. We then evaluate the gender-bias in regressors using an equity evaluation corpus. Ideally and from the specific design, the models should discard gender informative features from the input. However, the results show a significant dependence of the system's predictions on gender-particular words and phrases. We claim that such biases can be reduced by removing genderspecific features from word embedding. Hence, for each layer in BERT, we identify directions that primarily encode gender information. The space formed by such directions is referred to as the gender subspace in the semantic space of word embeddings. We propose an algorithm that finds fine-grained gender directions, i.e., one primary direction for each BERT layer. This obviates the need of realizing gender subspace in multiple dimensions and prevents other crucial information from being omitted. Experiments show that removing embedding components in such directions achieves great success in reducing BERT-induced bias in the downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.05021](https://arxiv.org/abs/2009.05021) [cs.CL]** |
|           | (or **[arXiv:2009.05021v1](https://arxiv.org/abs/2009.05021v1) [cs.CL]** for this version) |





# 2020-09-10

[Return to Index](#Index)



<h2 id="2020-09-10-1">1. Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages</h2>

Title: [Central Yup'ik and Machine Translation of Low-Resource Polysynthetic Languages](https://arxiv.org/abs/2009.04087)

Authors: [Christopher Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Laura Dominé](https://arxiv.org/search/cs?searchtype=author&query=Dominé%2C+L), [Kevin Chavez](https://arxiv.org/search/cs?searchtype=author&query=Chavez%2C+K), [Richard Socher](https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R)

> Machine translation tools do not yet exist for the Yup'ik language, a polysynthetic language spoken by around 8,000 people who live primarily in Southwest Alaska. We compiled a parallel text corpus for Yup'ik and English and developed a morphological parser for Yup'ik based on grammar rules. We trained a seq2seq neural machine translation model with attention to translate Yup'ik input into English. We then compared the influence of different tokenization methods, namely rule-based, unsupervised (byte pair encoding), and unsupervised morphological (Morfessor) parsing, on BLEU score accuracy for Yup'ik to English translation. We find that using tokenized input increases the translation accuracy compared to that of unparsed input. Although overall Morfessor did best with a vocabulary size of 30k, our first experiments show that BPE performed best with a reduced vocabulary size.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.04087](https://arxiv.org/abs/2009.04087) [cs.CL]** |
|           | (or **[arXiv:2009.04087v1](https://arxiv.org/abs/2009.04087v1) [cs.CL]** for this version) |







# 2020-09-08

[Return to Index](#Index)



<h2 id="2020-09-08-1">1. Measuring Massive Multitask Language Understanding</h2>

Title: [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)

Authors: [Dan Hendrycks](https://arxiv.org/search/cs?searchtype=author&query=Hendrycks%2C+D), [Collin Burns](https://arxiv.org/search/cs?searchtype=author&query=Burns%2C+C), [Steven Basart](https://arxiv.org/search/cs?searchtype=author&query=Basart%2C+S), [Andy Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+A), [Mantas Mazeika](https://arxiv.org/search/cs?searchtype=author&query=Mazeika%2C+M), [Dawn Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D), [Jacob Steinhardt](https://arxiv.org/search/cs?searchtype=author&query=Steinhardt%2C+J)

> We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach human-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.

| Comments: | The test and code is available at [this https URL](https://github.com/hendrycks/test) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.03300](https://arxiv.org/abs/2009.03300) [cs.CY]** |
|           | (or **[arXiv:2009.03300v1](https://arxiv.org/abs/2009.03300v1) [cs.CY]** for this version) |





<h2 id="2020-09-08-2">2. Recent Trends in the Use of Deep Learning Models for Grammar Error Handling</h2>

Title: [Recent Trends in the Use of Deep Learning Models for Grammar Error Handling](https://arxiv.org/abs/2009.02358)

Authors: [Mina Naghshnejad](https://arxiv.org/search/cs?searchtype=author&query=Naghshnejad%2C+M), [Tarun Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+T), [Vijayan N. Nair](https://arxiv.org/search/cs?searchtype=author&query=Nair%2C+V+N)

> Grammar error handling (GEH) is an important topic in natural language processing (NLP). GEH includes both grammar error detection and grammar error correction. Recent advances in computation systems have promoted the use of deep learning (DL) models for NLP problems such as GEH. In this survey we focus on two main DL approaches for GEH: neural machine translation models and editor models. We describe the three main stages of the pipeline for these models: data preparation, training, and inference. Additionally, we discuss different techniques to improve the performance of these models at each stage of the pipeline. We compare the performance of different models and conclude with proposed future directions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02358](https://arxiv.org/abs/2009.02358) [cs.CL]** |
|           | (or **[arXiv:2009.02358v1](https://arxiv.org/abs/2009.02358v1) [cs.CL]** for this version) |





<h2 id="2020-09-08-3">3. Bio-inspired Structure Identification in Language Embeddings</h2>

Title: [Bio-inspired Structure Identification in Language Embeddings](https://arxiv.org/abs/2009.02459)

Authors: [Hongwei](https://arxiv.org/search/cs?searchtype=author&query=Hongwei) (Henry)[Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou), [Oskar Elek](https://arxiv.org/search/cs?searchtype=author&query=Elek%2C+O), [Pranav Anand](https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+P), [Angus G. Forbes](https://arxiv.org/search/cs?searchtype=author&query=Forbes%2C+A+G)

> Word embeddings are a popular way to improve downstream per-formances in contemporary language modeling. However, the un-derlying geometric structure of the embedding space is not wellunderstood. We present a series of explorations using bio-inspiredmethodology to traverse and visualize word embeddings, demon-strating evidence of discernible structure. Moreover, our modelalso produces word similarity rankings that are plausible yet verydifferent from common similarity metrics, mainly cosine similarityand Euclidean distance. We show that our bio-inspired model canbe used to investigate how different word embedding techniquesresult in different semantic outputs, which can emphasize or obscureparticular interpretations in textual data.

| Comments: | 7 pages, 8 figures, 2 tables, Visualisation for the Digital Humanities 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2009.02459](https://arxiv.org/abs/2009.02459) [cs.CL]** |
|           | (or **[arXiv:2009.02459v1](https://arxiv.org/abs/2009.02459v1) [cs.CL]** for this version) |





<h2 id="2020-09-08-4">4. Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity</h2>

Title: [Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity](https://arxiv.org/abs/2009.03116)

Authors: [Tim Isbister](https://arxiv.org/search/cs?searchtype=author&query=Isbister%2C+T), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> This paper presents the first Swedish evaluation benchmark for textual semantic similarity. The benchmark is compiled by simply running the English STS-B dataset through the Google machine translation API. This paper discusses potential problems with using such a simple approach to compile a Swedish evaluation benchmark, including translation errors, vocabulary variation, and productive compounding. Despite some obvious problems with the resulting dataset, we use the benchmark to compare the majority of the currently existing Swedish text representations, demonstrating that native models outperform multilingual ones, and that simple bag of words performs remarkably well.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.03116](https://arxiv.org/abs/2009.03116) [cs.CL]** |
|           | (or **[arXiv:2009.03116v1](https://arxiv.org/abs/2009.03116v1) [cs.CL]** for this version) |





# 2020-09-07

[Return to Index](#Index)



<h2 id="2020-09-07-1">1. Data Readiness for Natural Language Processing</h2>

Title: [Data Readiness for Natural Language Processing](https://arxiv.org/abs/2009.02043)

Authors: [Fredrik Olsson](https://arxiv.org/search/cs?searchtype=author&query=Olsson%2C+F), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> This document concerns data readiness in the context of machine learning and Natural Language Processing. It describes how an organization may proceed to identify, make available, validate, and prepare data to facilitate automated analysis methods. The contents of the document is based on the practical challenges and frequently asked questions we have encountered in our work as an applied research institute with helping organizations and companies, both in the public and private sectors, to use data in their business processes.

| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02043](https://arxiv.org/abs/2009.02043) [cs.CY]** |
|           | (or **[arXiv:2009.02043v1](https://arxiv.org/abs/2009.02043v1) [cs.CY]** for this version) |





<h2 id="2020-09-07-2">2. Dynamic Context-guided Capsule Network for Multimodal Machine Translation</h2>

Title: [Dynamic Context-guided Capsule Network for Multimodal Machine Translation](https://arxiv.org/abs/2009.02016)

Authors: [Huan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Zhengyuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yubin Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on [this https URL](https://github.com/DeepLearnXMU/MM-DCCN).

| Subjects: | **Computation and Language (cs.CL)**; Multimedia (cs.MM)     |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3394171.3413715](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3394171.3413715&v=711e661a) |
| Cite as:  | **[arXiv:2009.02016](https://arxiv.org/abs/2009.02016) [cs.CL]** |
|           | (or **[arXiv:2009.02016v1](https://arxiv.org/abs/2009.02016v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-3">3. AutoTrans: Automating Transformer Design via Reinforced Architecture Search</h2>

Title: [AutoTrans: Automating Transformer Design via Reinforced Architecture Search](https://arxiv.org/abs/2009.02070)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xiaoling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Yuan Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+Y), [Guotong Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+G)

> Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc, so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02070](https://arxiv.org/abs/2009.02070) [cs.CL]** |
|           | (or **[arXiv:2009.02070v1](https://arxiv.org/abs/2009.02070v1) [cs.CL]** for this version) |





<h2 id="2020-09-07-4">4. Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings</h2>

Title: [Going Beyond T-SNE: Exposing \texttt{whatlies} in Text Embeddings](https://arxiv.org/abs/2009.02113)

Authors: [Vincent D. Warmerdam](https://arxiv.org/search/cs?searchtype=author&query=Warmerdam%2C+V+D), [Thomas Kober](https://arxiv.org/search/cs?searchtype=author&query=Kober%2C+T), [Rachael Tatman](https://arxiv.org/search/cs?searchtype=author&query=Tatman%2C+R)

> We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from [this https URL](https://rasahq.github.io/whatlies/).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2009.02113](https://arxiv.org/abs/2009.02113) [cs.CL]** |
|           | (or **[arXiv:2009.02113v1](https://arxiv.org/abs/2009.02113v1) [cs.CL]** for this version) |



# 2020-09-01

[Return to Index](#Index)



<h2 id="2020-09-01-1">1. Knowledge Efficient Deep Learning for Natural Language Processing</h2>

Title: [Knowledge Efficient Deep Learning for Natural Language Processing](https://arxiv.org/abs/2008.12878)

Authors: [Hai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Deep learning has become the workhorse for a wide range of natural language processing applications. But much of the success of deep learning relies on annotated examples. Annotation is time-consuming and expensive to produce at scale. Here we are interested in methods for reducing the required quantity of annotated data -- by making the learning methods more knowledge efficient so as to make them more applicable in low annotation (low resource) settings. There are various classical approaches to making the models more knowledge efficient such as multi-task learning, transfer learning, weakly supervised and unsupervised learning etc. This thesis focuses on adapting such classical methods to modern deep learning models and algorithms.
> This thesis describes four works aimed at making machine learning models more knowledge efficient. First, we propose a knowledge rich deep learning model (KRDL) as a unifying learning framework for incorporating prior knowledge into deep models. In particular, we apply KRDL built on Markov logic networks to denoise weak supervision. Second, we apply a KRDL model to assist the machine reading models to find the correct evidence sentences that can support their decision. Third, we investigate the knowledge transfer techniques in multilingual setting, where we proposed a method that can improve pre-trained multilingual BERT based on the bilingual dictionary. Fourth, we present an episodic memory network for language modelling, in which we encode the large external knowledge for the pre-trained GPT.

| Comments: | Ph.D thesis                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.12878](https://arxiv.org/abs/2008.12878) [cs.CL]** |
|           | (or **[arXiv:2008.12878v1](https://arxiv.org/abs/2008.12878v1) [cs.CL]** for this version) |

