# Daily arXiv: Machine Translation - May, 2020

# Index

- [2020-05-20](#2020-05-20)
  - [1. Are All Languages Created Equal in Multilingual BERT?](#2020-05-20-1)
  - [2. (Re)construing Meaning in NLP](#2020-05-20-2)
  - [3. Contextual Embeddings: When Are They Worth It?](#2020-05-20-3)
  - [4. A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks](#2020-05-20-4)
- [2020-05-19](#2020-05-19)
  - [1. Cross-Lingual Word Embeddings for Turkic Languages](#2020-05-19-1)
  - [2. Efficient Wait-k Models for Simultaneous Machine Translation](#2020-05-19-2)
  - [3. Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings](#2020-05-19-3)
  - [4. Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning](#2020-05-19-4)
  - [5. RPD: A Distance Function Between Word Embeddings](#2020-05-19-5)
  - [6. Encodings of Source Syntax: Similarities in NMT Representations Across Target Languages](#2020-05-19-6)
- [2020-05-14](#2020-05-14)
  - [1. A Comprehensive Survey of Grammar Error Correction](#2020-05-14-1)
  - [2. Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation](#2020-05-14-2)
  - [3. Neural Machine Translation for South Africa's Official Languages](#2020-05-14-3)
  - [4. schuBERT: Optimizing Elements of BERT](#2020-05-14-4)
  - [5. 4chan & 8chan embeddings](#2020-05-14-5)
- [2020-05-14](#2020-05-14)
  - [1. Parallel Corpus Filtering via Pre-trained Language Models](#2020-05-14-1)
- [2020-05-13](#2020-05-13)
  - [1. Schema-Guided Natural Language Generation](#2020-05-13-1)
  - [2. A Framework for Hierarchical Multilingual Machine Translation](#2020-05-13-2)
  - [3. DiscreTalk: Text-to-Speech as a Machine Translation Problem](#2020-05-13-3)
  - [4. Simultaneous paraphrasing and translation by fine-tuning Transformer models](#2020-05-13-4)
  - [5. On the Robustness of Language Encoders against Grammatical Errors](#2020-05-13-5)
  - [6. Reassessing Claims of Human Parity and Super-Human Performance in Machine Translation at WMT 2019](#2020-05-13-6)
  - [7. TextAttack: A Framework for Adversarial Attacks in Natural Language Processing](#2020-05-13-7)
- [2020-05-12](#2020-05-12)
  - [1. Finding Universal Grammatical Relations in Multilingual BERT](2020-05-12-1)
  - [2. Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation](2020-05-12-2)
  - [3. Multidirectional Associative Optimization of Function-Specific Word Representations](2020-05-12-3)
- [2020-05-11](#2020-05-11)
  - [1. Learning to Detect Unacceptable Machine Translations for Downstream Tasks](2020-05-11-1)
  - [2. Distilling Knowledge from Pre-trained Language Models via Text Smoothing](2020-05-11-2)
- [2020-05-08](#2020-05-08)
  - [1. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](#2020-05-08-1)
  - [2. JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation](#2020-05-08-2)
  - [3. Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](#2020-05-08-3)
  - [4. Practical Perspectives on Quality Estimation for Machine Translation](#2020-05-08-4)
  - [5. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](#2020-05-08-5)
- [2020-05-07](#2020-05-07)
  - [1. Exploring Controllable Text Generation Techniques](#2020-05-07-1)
  - [2. Understanding Scanned Receipts](#2020-05-07-2)
- [2020-05-06](#2020-05-06)
  - [1. IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization](#2020-05-06-1)
  - [2. Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration](#2020-05-06-2)
  - [3. It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](#2020-05-06-3)
- [2020-05-05](#2020-05-05)
  - [1. Quantifying Attention Flow in Transformers](#2020-05-05-1)
  - [2. Does Visual Self-Supervision Improve Learning of Speech Representations?](#2020-05-05-2)
  - [3. Evaluating Robustness to Input Perturbations for Neural Machine Translation](#2020-05-05-3)
  - [4. From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers](#2020-05-05-4)
  - [5. Opportunistic Decoding with Timely Correction for Simultaneous Translation](#2020-05-05-5)
  - [6. Synthesizer: Rethinking Self-Attention in Transformer Models](#2020-05-05-6)
  - [7. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](#2020-05-05-7)
  - [8. Improving Non-autoregressive Neural Machine Translation with Monolingual Data](#2020-05-05-8)
  - [9. On the Inference Calibration of Neural Machine Translation](#2020-05-05-9)
  - [10. Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](#2020-05-05-10)
  - [11. Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation](#2020-05-05-11)
  - [12. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](#2020-05-05-12)
  - [13. Using Context in Neural Machine Translation Training Objectives](#2020-05-05-13)
  - [14. Evaluating Explanation Methods for Neural Machine Translation](#2020-05-05-14)
- [2020-05-04](2020-05-04)
  - [1. MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer](#2020-05-04-1)
  - [2. Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation](#2020-05-04-2)
  - [3. Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](#2020-05-04-3)
  - [4. Identifying Necessary Elements for BERT's Multilinguality](#2020-05-04-4)
  - [5. Defense of Word-level Adversarial Attacks via Random Substitution Encoding](#2020-05-04-5)
  - [6. Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](#2020-05-04-6)
- [2020-05-01](#2020-05-01)
  - [1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation](2020-05-01-1)
  - [2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](2020-05-01-2)
  - [3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](2020-05-01-3)
  - [4. Capsule-Transformer for Neural Machine Translation](2020-05-01-4)
  - [5. End-to-End Neural Word Alignment Outperforms GIZA++](2020-05-01-5)
  - [6. Character-Level Translation with Self-attention](2020-05-01-6)
  - [7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](2020-05-01-7)
  - [8. Accurate Word Alignment Induction from Neural Machine Translation](2020-05-01-8)
  - [9. Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation](2020-05-01-9)
  - [10. Bridging linguistic typology and multilingual machine translation with multi-view language representations](2020-05-01-10)
  - [11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](2020-05-01-11)
  - [12. Language Model Prior for Low-Resource Neural Machine Translation](2020-05-01-12)
  - [13. A Call for More Rigor in Unsupervised Cross-lingual Learning](2020-05-01-13)
  - [14. Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages](2020-05-01-14)
  - [15. Investigating Transferability in Pretrained Language Models](2020-05-01-15)
  - [16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](2020-05-01-16)
  - [17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](2020-05-01-17)
  - [18. When does data augmentation help generalization in NLP?](2020-05-01-18)
  - [19. Imitation Attacks and Defenses for Black-box Machine Translation Systems](2020-05-01-19)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-05-20

[Return to Index](#Index)



<h2 id="2020-05-20-1">1. Are All Languages Created Equal in Multilingual BERT?</h2>

Title: [Are All Languages Created Equal in Multilingual BERT?](https://arxiv.org/abs/2005.09093)

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M)

> Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.

| Comments: | Repl4NLP Workshop 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.09093](https://arxiv.org/abs/2005.09093) [cs.CL]** |
|           | (or **[arXiv:2005.09093v1](https://arxiv.org/abs/2005.09093v1) [cs.CL]** for this version) |





<h2 id="2020-05-20-2">2. (Re)construing Meaning in NLP</h2>

Title: [(Re)construing Meaning in NLP](https://arxiv.org/abs/2005.09099)

Authors: [Sean Trott](https://arxiv.org/search/cs?searchtype=author&query=Trott%2C+S), [Tiago Timponi Torrent](https://arxiv.org/search/cs?searchtype=author&query=Torrent%2C+T+T), [Nancy Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+N), [Nathan Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+N)

> Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding--namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.

| Comments: | ACL 2020 camera-ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.09099](https://arxiv.org/abs/2005.09099) [cs.CL]** |
|           | (or **[arXiv:2005.09099v1](https://arxiv.org/abs/2005.09099v1) [cs.CL]** for this version) |





<h2 id="2020-05-20-3">3. Contextual Embeddings: When Are They Worth It?</h2>

Title: [Contextual Embeddings: When Are They Worth It?](https://arxiv.org/abs/2005.09117)

Authors: [Simran Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+S), [Avner May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+A), [Jian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Christopher Ré](https://arxiv.org/search/cs?searchtype=author&query=Ré%2C+C)

> We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.09117](https://arxiv.org/abs/2005.09117) [cs.CL]** |
|           | (or **[arXiv:2005.09117v1](https://arxiv.org/abs/2005.09117v1) [cs.CL]** for this version) |





<h2 id="2020-05-20-4">4. A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks</h2>

Title: [A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks](https://arxiv.org/abs/2005.09606)

Authors: [Angela S. Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+A+S), [Sudha Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+S), [Asli Celikyilmaz](https://arxiv.org/search/cs?searchtype=author&query=Celikyilmaz%2C+A), [Elnaz Nouri](https://arxiv.org/search/cs?searchtype=author&query=Nouri%2C+E), [Chris Brockett](https://arxiv.org/search/cs?searchtype=author&query=Brockett%2C+C), [Debadeepta Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+D), [Bill Dolan](https://arxiv.org/search/cs?searchtype=author&query=Dolan%2C+B)

> Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many partially-overlapping text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. To address these challenges, we first use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. We release the Microsoft Research Multimodal Aligned Recipe Corpus containing 150K pairwise alignments between recipes across 4,262 dishes with rich commonsense information.

| Comments:          | This paper has been accepted to be published at ACL 2020     |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association of Computational Linguistics 2020                |
| Cite as:           | **[arXiv:2005.09606](https://arxiv.org/abs/2005.09606) [cs.CL]** |
|                    | (or **[arXiv:2005.09606v1](https://arxiv.org/abs/2005.09606v1) [cs.CL]** for this version) |



# 2020-05-19

[Return to Index](#Index)



<h2 id="2020-05-19-1">1. Cross-Lingual Word Embeddings for Turkic Languages</h2>

Title: [Cross-Lingual Word Embeddings for Turkic Languages](https://arxiv.org/abs/2005.08340)

Authors: [Elmurod Kuriyozov](https://arxiv.org/search/cs?searchtype=author&query=Kuriyozov%2C+E), [Yerai Doval](https://arxiv.org/search/cs?searchtype=author&query=Doval%2C+Y), [Carlos Gómez-Rodríguez](https://arxiv.org/search/cs?searchtype=author&query=Gómez-Rodríguez%2C+C)

> There has been an increasing interest in learning cross-lingual word embeddings to transfer knowledge obtained from a resource-rich language, such as English, to lower-resource languages for which annotated data is scarce, such as Turkish, Russian, and many others. In this paper, we present the first viability study of established techniques to align monolingual embedding spaces for Turkish, Uzbek, Azeri, Kazakh and Kyrgyz, members of the Turkic family which is heavily affected by the low-resource constraint. Those techniques are known to require little explicit supervision, mainly in the form of bilingual dictionaries, hence being easily adaptable to different domains, including low-resource ones. We obtain new bilingual dictionaries and new word embeddings for these languages and show the steps for obtaining cross-lingual word embeddings using state-of-the-art techniques. Then, we evaluate the results using the bilingual dictionary induction task. Our experiments confirm that the obtained bilingual dictionaries outperform previously-available ones, and that word embeddings from a low-resource language can benefit from resource-rich closely-related languages when they are aligned together. Furthermore, evaluation on an extrinsic task (Sentiment analysis on Uzbek) proves that monolingual word embeddings can, although slightly, benefit from cross-lingual alignments.

| Comments:          | Final version, published in the proceedings of LREC 2020     |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| MSC classes:       | 68T50, 91F20                                                 |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Proceedings of The 12th Language Resources and Evaluation Conference, Marseille, France, 2020, pp. 4047-4055 |
| Cite as:           | **[arXiv:2005.08340](https://arxiv.org/abs/2005.08340) [cs.CL]** |
|                    | (or **[arXiv:2005.08340v1](https://arxiv.org/abs/2005.08340v1) [cs.CL]** for this version) |





<h2 id="2020-05-19-2">2. Efficient Wait-k Models for Simultaneous Machine Translation</h2>

Title: [Efficient Wait-k Models for Simultaneous Machine Translation](https://arxiv.org/abs/2005.08595)

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Jakob Verbeek](https://arxiv.org/search/cs?searchtype=author&query=Verbeek%2C+J)

> Simultaneous machine translation consists in starting output generation before the entire input sequence is available. Wait-k decoders offer a simple but efficient approach for this problem. They first read k source tokens, after which they alternate between producing a target token and reading another source token. We investigate the behavior of wait-k decoding in low resource settings for spoken corpora using IWSLT datasets. We improve training of these models using unidirectional encoders, and training across multiple values of k. Experiments with Transformer and 2D-convolutional architectures show that our wait-k models generalize well across a wide range of latency levels. We also show that the 2D-convolution architecture is competitive with Transformers for simultaneous translation of spoken language.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.08595](https://arxiv.org/abs/2005.08595) [cs.CL]** |
|           | (or **[arXiv:2005.08595v1](https://arxiv.org/abs/2005.08595v1) [cs.CL]** for this version) |





<h2 id="2020-05-19-3">3. Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings</h2>

Title: [Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings](https://arxiv.org/abs/2005.08864)

Authors: [Katherine McCurdy](https://arxiv.org/search/cs?searchtype=author&query=McCurdy%2C+K), [Oguz Serbetci](https://arxiv.org/search/cs?searchtype=author&query=Serbetci%2C+O)

> Recent research has demonstrated that vector space models of semantics can reflect undesirable biases in human culture. Our investigation of crosslinguistic word embeddings reveals that topical gender bias interacts with, and is surpassed in magnitude by, the effect of grammatical gender associations, and both may be attenuated by corpus lemmatization. This finding has implications for downstream applications such as machine translation.

| Comments: | Extended abstract presented at the WiNLP workshop, ACL 2017  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.08864](https://arxiv.org/abs/2005.08864) [cs.CL]** |
|           | (or **[arXiv:2005.08864v1](https://arxiv.org/abs/2005.08864v1) [cs.CL]** for this version) |





<h2 id="2020-05-19-4">4. Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning</h2>

Title: [Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning](https://arxiv.org/abs/2005.08081)

Authors: [Fenglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Guangxiang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+G), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

> In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is, the representations generated by the last encoder layer, which is supposed to be a global view of source sequences. Such implementation hinders the decoder from concrete, fine-grained, local source information. In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding, that is, different views of the source sequences are presented to different decoder layers. We investigate multiple, representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on neural machine translation task. Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.

| Comments: | Achieve state-of-the-art BLEU scores on WMT14 EN-DE, EN-FR, and IWSLT DE-EN datasets |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.08081](https://arxiv.org/abs/2005.08081) [cs.CL]** |
|           | (or **[arXiv:2005.08081v1](https://arxiv.org/abs/2005.08081v1) [cs.CL]** for this version) |





<h2 id="2020-05-19-5">5. RPD: A Distance Function Between Word Embeddings</h2>

Title: [RPD: A Distance Function Between Word Embeddings](https://arxiv.org/abs/2005.08113)

Authors: [Xuhui Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X), [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S)

> It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of embeddings deviate from each other. In this paper, we propose a novel metric called Relative pairwise inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This metric has a unified scale for comparing different sets of word embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding spaces.

| Comments: | ACL Student Research Workshop 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.08113](https://arxiv.org/abs/2005.08113) [cs.CL]** |
|           | (or **[arXiv:2005.08113v1](https://arxiv.org/abs/2005.08113v1) [cs.CL]** for this version) |





<h2 id="2020-05-19-6">6. Encodings of Source Syntax: Similarities in NMT Representations Across Target Languages</h2>

Title: [Encodings of Source Syntax: Similarities in NMT Representations Across Target Languages](https://arxiv.org/abs/2005.08177)

Authors: [Tyler A. Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+T+A), [Anna N. Rafferty](https://arxiv.org/search/cs?searchtype=author&query=Rafferty%2C+A+N)

> We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax. However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. Despite lower overall accuracy scores, the PCFG often performs well on sentences for which the RNN-based models perform poorly, suggesting that RNN architectures are constrained in the types of syntax they can learn.

| Comments: | To appear at the 5th Workshop on Representation Learning for NLP |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.08177](https://arxiv.org/abs/2005.08177) [cs.CL]** |
|           | (or **[arXiv:2005.08177v1](https://arxiv.org/abs/2005.08177v1) [cs.CL]** for this version) |





# 2020-05-15

[Return to Index](#Index)



<h2 id="2020-05-15-1">1. A Comprehensive Survey of Grammar Error Correction</h2>

Title: [A Comprehensive Survey of Grammar Error Correction](https://arxiv.org/abs/2005.06600)

Authors: [Yu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Yuelin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Jie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Zhuo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> Grammar error correction (GEC) is an important application aspect of natural language processing techniques. The past decade has witnessed significant progress achieved in GEC for the sake of increasing popularity of machine learning and deep learning, especially in late 2010s when near human-level GEC systems are available. However, there is no prior work focusing on the whole recapitulation of the progress. We present the first survey in GEC for a comprehensive retrospect of the literature in this area. We first give the introduction of five public datasets, data annotation schema, two important shared tasks and four standard evaluation metrics. More importantly, we discuss four kinds of basic approaches, including statistical machine translation based approach, neural machine translation based approach, classification based approach and language model based approach, six commonly applied performance boosting techniques for GEC systems and two data augmentation methods. Since GEC is typically viewed as a sister task of machine translation, many GEC systems are based on neural machine translation (NMT) approaches, where the neural sequence-to-sequence model is applied. Similarly, some performance boosting techniques are adapted from machine translation and are successfully combined with GEC systems for enhancement on the final performance. Furthermore, we conduct an analysis in level of basic approaches, performance boosting techniques and integrated GEC systems based on their experiment results respectively for more clear patterns and conclusions. Finally, we discuss five prospective directions for future GEC researches.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.06600](https://arxiv.org/abs/2005.06600) [cs.CL]** |
|           | (or **[arXiv:2005.06600v1](https://arxiv.org/abs/2005.06600v1) [cs.CL]** for this version) |





<h2 id="2020-05-15-2">2. Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation</h2>

Title: [Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation](https://arxiv.org/abs/2005.06606)

Authors: [Xuanli He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M)

> This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).

| Comments: | accepted to ACL2020                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.06606](https://arxiv.org/abs/2005.06606) [cs.CL]** |
|           | (or **[arXiv:2005.06606v1](https://arxiv.org/abs/2005.06606v1) [cs.CL]** for this version) |





<h2 id="2020-05-15-3">3. Neural Machine Translation for South Africa's Official Languages</h2>

Title: [Neural Machine Translation for South Africa's Official Languages](https://arxiv.org/abs/2005.06609)

Authors: [Laura Martinus](https://arxiv.org/search/cs?searchtype=author&query=Martinus%2C+L), [Jason Webster](https://arxiv.org/search/cs?searchtype=author&query=Webster%2C+J), [Joanne Moonsamy](https://arxiv.org/search/cs?searchtype=author&query=Moonsamy%2C+J), [Moses Shaba Jnr](https://arxiv.org/search/cs?searchtype=author&query=Jnr%2C+M+S), [Ridha Moosa](https://arxiv.org/search/cs?searchtype=author&query=Moosa%2C+R), [Robert Fairon](https://arxiv.org/search/cs?searchtype=author&query=Fairon%2C+R)

> Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there is has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.

| Comments: | workshop paper at AfricaNLP, ICLR 2020                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.06609](https://arxiv.org/abs/2005.06609) [cs.CL]** |
|           | (or **[arXiv:2005.06609v1](https://arxiv.org/abs/2005.06609v1) [cs.CL]** for this version) |





<h2 id="2020-05-15-4">4. schuBERT: Optimizing Elements of BERT</h2>

Title: [schuBERT: Optimizing Elements of BERT](https://arxiv.org/abs/2005.06628)

Authors: [Ashish Khetan](https://arxiv.org/search/cs?searchtype=author&query=Khetan%2C+A), [Zohar Karnin](https://arxiv.org/search/cs?searchtype=author&query=Karnin%2C+Z)

> Transformers \citep{vaswani2017attention} have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERT \citep{devlin2018bert} achieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives *[Math Processing Error]* higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.

| Comments: | 11 pages, 6 figures, Accepted for publication in ACL 2020 as a long paper |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.06628](https://arxiv.org/abs/2005.06628) [cs.CL]** |
|           | (or **[arXiv:2005.06628v1](https://arxiv.org/abs/2005.06628v1) [cs.CL]** for this version) |





<h2 id="2020-05-15-5">5. 4chan & 8chan embeddings</h2>

Title: [4chan & 8chan embeddings](https://arxiv.org/abs/2005.06946)

Authors: [Pierre Voué](https://arxiv.org/search/cs?searchtype=author&query=Voué%2C+P), [Tom De Smedt](https://arxiv.org/search/cs?searchtype=author&query=De+Smedt%2C+T), [Guy De Pauw](https://arxiv.org/search/cs?searchtype=author&query=De+Pauw%2C+G)

> We have collected over 30M messages from the publicly available /pol/ message boards on 4chan and 8chan, and compiled them into a model of toxic language use. The trained word embeddings (0.4GB) are released for free and may be useful for further study on toxic discourse or to boost hate speech detection systems: [this https URL](https://textgain.com/8chan).

| Comments:          | 4 pages                                                      |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Textgain Technical Reports 1 (2020)                          |
| Cite as:           | **[arXiv:2005.06946](https://arxiv.org/abs/2005.06946) [cs.CL]** |
|                    | (or **[arXiv:2005.06946v1](https://arxiv.org/abs/2005.06946v1) [cs.CL]** for this version) |



# 2020-05-14

[Return to Index](#Index)



<h2 id="2020-05-14-1">1. Parallel Corpus Filtering via Pre-trained Language Models</h2>

Title: [Parallel Corpus Filtering via Pre-trained Language Models](https://arxiv.org/abs/2005.06166)

Authors: [Boliang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ajay Nagesh](https://arxiv.org/search/cs?searchtype=author&query=Nagesh%2C+A), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K)

> Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new state-of-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.06166](https://arxiv.org/abs/2005.06166) [cs.CL]** |
|           | (or **[arXiv:2005.06166v1](https://arxiv.org/abs/2005.06166v1) [cs.CL]** for this version) |











# 2020-05-13

[Return to Index](#Index)



<h2 id="2020-05-13-1">1. Schema-Guided Natural Language Generation</h2>

Title: [Schema-Guided Natural Language Generation](https://arxiv.org/abs/2005.05480)

Authors: [Yuheng Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Shereen Oraby](https://arxiv.org/search/cs?searchtype=author&query=Oraby%2C+S), [Vittorio Perera](https://arxiv.org/search/cs?searchtype=author&query=Perera%2C+V), [Minmin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+M), [Anjali Narayan-Chen](https://arxiv.org/search/cs?searchtype=author&query=Narayan-Chen%2C+A), [Tagyoung Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+T), [Anu Venkatesh](https://arxiv.org/search/cs?searchtype=author&query=Venkatesh%2C+A), [Dilek Hakkani-Tur](https://arxiv.org/search/cs?searchtype=author&query=Hakkani-Tur%2C+D)

> Neural network based approaches to natural language generation (NLG) have gained popularity in recent years. The goal of the task is to generate a natural language string to realize an input meaning representation, hence large datasets of paired utterances and their meaning representations are used for training the network. However, dataset creation for language generation is an arduous task, and popular datasets designed for training these generators mostly consist of simple meaning representations composed of slot and value tokens to be realized. These simple meaning representations do not include any contextual information that may be helpful for training an NLG system to generalize, such as domain information and descriptions of slots and values. In this paper, we present the novel task of Schema-Guided Natural Language Generation, in which we repurpose an existing dataset for another task: dialog state tracking. Dialog state tracking data includes a large and rich schema spanning multiple different attributes, including information about the domain, user intent, and slot descriptions. We train different state-of-the-art models for neural natural language generation on this data and show that inclusion of the rich schema allows our models to produce higher quality outputs both in terms of semantics and diversity. We also conduct experiments comparing model performance on seen versus unseen domains. Finally, we present human evaluation results and analysis demonstrating high ratings for overall output quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.05480](https://arxiv.org/abs/2005.05480) [cs.CL]** |
|           | (or **[arXiv:2005.05480v1](https://arxiv.org/abs/2005.05480v1) [cs.CL]** for this version) |





<h2 id="2020-05-13-2">2. A Framework for Hierarchical Multilingual Machine Translation</h2>

Title: [A Framework for Hierarchical Multilingual Machine Translation](https://arxiv.org/abs/2005.05507)

Authors: [Ion Madrazo Azpiazu](https://arxiv.org/search/cs?searchtype=author&query=Azpiazu%2C+I+M), [Maria Soledad Pera](https://arxiv.org/search/cs?searchtype=author&query=Pera%2C+M+S)

> Multilingual machine translation has recently been in vogue given its potential for improving machine translation performance for low-resource languages via transfer learning. Empirical examinations demonstrating the success of existing multilingual machine translation strategies, however, are limited to experiments in specific language groups. In this paper, we present a hierarchical framework for building multilingual machine translation strategies that takes advantage of a typological language family tree for enabling transfer among similar languages while avoiding the negative effects that result from incorporating languages that are too different to each other. Exhaustive experimentation on a dataset with 41 languages demonstrates the validity of the proposed framework, especially when it comes to improving the performance of low-resource languages via the use of typologically related families for which richer sets of resources are available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.05507](https://arxiv.org/abs/2005.05507) [cs.CL]** |
|           | (or **[arXiv:2005.05507v1](https://arxiv.org/abs/2005.05507v1) [cs.CL]** for this version) |







<h2 id="2020-05-13-3">3. DiscreTalk: Text-to-Speech as a Machine Translation Problem</h2>

Title: [DiscreTalk: Text-to-Speech as a Machine Translation Problem](https://arxiv.org/abs/2005.05525)

Authors: [Tomoki Hayashi](https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.

| Comments: | Submitted to INTERSPEECH 2020. The demo is available on [this https URL](https://kan-bayashi.github.io/DiscreTalk/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2005.05525](https://arxiv.org/abs/2005.05525) [cs.CL]** |
|           | (or **[arXiv:2005.05525v1](https://arxiv.org/abs/2005.05525v1) [cs.CL]** for this version) |







<h2 id="2020-05-13-4">4. Simultaneous paraphrasing and translation by fine-tuning Transformer models</h2>

Title: [Simultaneous paraphrasing and translation by fine-tuning Transformer models](https://arxiv.org/abs/2005.05570)

Authors: [Rakesh Chada](https://arxiv.org/search/cs?searchtype=author&query=Chada%2C+R)

> This paper describes the third place submission to the shared task on simultaneous translation and paraphrasing for language education at the 4th workshop on Neural Generation and Translation (WNGT) for ACL 2020. The final system leverages pre-trained translation models and uses a Transformer architecture combined with an oversampling strategy to achieve a competitive performance. This system significantly outperforms the baseline on Hungarian (27% absolute improvement in Weighted Macro F1 score) and Portuguese (33% absolute improvement) languages.

| Comments: | Accepted to ACL 2020 4th workshop on Neural Generation and Translation |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.05570](https://arxiv.org/abs/2005.05570) [cs.CL]** |
|           | (or **[arXiv:2005.05570v1](https://arxiv.org/abs/2005.05570v1) [cs.CL]** for this version) |







<h2 id="2020-05-13-5">5. On the Robustness of Language Encoders against Grammatical Errors</h2>

Title: [On the Robustness of Language Encoders against Grammatical Errors](https://arxiv.org/abs/2005.05683)

Authors: [Fan Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+F), [Quanyu Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+Q), [Tao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+T), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)

> We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.05683](https://arxiv.org/abs/2005.05683) [cs.CL]** |
|           | (or **[arXiv:2005.05683v1](https://arxiv.org/abs/2005.05683v1) [cs.CL]** for this version) |







<h2 id="2020-05-13-6">6. Reassessing Claims of Human Parity and Super-Human Performance in Machine Translation at WMT 2019</h2>

Title: [Reassessing Claims of Human Parity and Super-Human Performance in Machine Translation at WMT 2019](https://arxiv.org/abs/2005.05738)

Authors: [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

> We reassess the claims of human parity and super-human performance made at the news shared task of WMT 2019 for three translation directions: English-to-German, English-to-Russian and German-to-English. First we identify three potential issues in the human evaluation of that shared task: (i) the limited amount of intersentential context available, (ii) the limited translation proficiency of the evaluators and (iii) the use of a reference translation. We then conduct a modified evaluation taking these issues into account. Our results indicate that all the claims of human parity and super-human performance made at WMT 2019 should be refuted, except the claim of human parity for English-to-German. Based on our findings, we put forward a set of recommendations and open questions for future assessments of human parity in machine translation.

| Comments: | Accepted at the 22nd Annual Conference of the European Association for Machine Translation (EAMT 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.05738](https://arxiv.org/abs/2005.05738) [cs.CL]** |
|           | (or **[arXiv:2005.05738v1](https://arxiv.org/abs/2005.05738v1) [cs.CL]** for this version) |







<h2 id="2020-05-13-7">7. TextAttack: A Framework for Adversarial Attacks in Natural Language Processing</h2>

Title: [TextAttack: A Framework for Adversarial Attacks in Natural Language Processing](https://arxiv.org/abs/2005.05909)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X), [Eli Lifland](https://arxiv.org/search/cs?searchtype=author&query=Lifland%2C+E), [Jin Yong Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+J+Y), [Yanjun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y)

> TextAttack is a library for running adversarial attacks against natural language processing (NLP) models. TextAttack builds attacks from four components: a search method, goal function, transformation, and a set of constraints. Researchers can use these components to easily assemble new attacks. Individual components can be isolated and compared for easier ablation studies. TextAttack currently supports attacks on models trained for text classification and entailment across a variety of datasets. Additionally, TextAttack's modular design makes it easily extensible to new NLP tasks, models, and attack strategies. TextAttack code and tutorials are available at [this https URL](https://github.com/QData/TextAttack)}{[this https URL](https://github.com/QData/TextAttack).

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.05909](https://arxiv.org/abs/2005.05909) [cs.CL]** |
|           | (or **[arXiv:2005.05909v1](https://arxiv.org/abs/2005.05909v1) [cs.CL]** for this version) |









# 2020-05-12

[Return to Index](#Index)



<h2 id="2020-05-12-1">1. Finding Universal Grammatical Relations in Multilingual BERT</h2>

Title: [Finding Universal Grammatical Relations in Multilingual BERT](https://arxiv.org/abs/2005.04511)

Authors: [Ethan A. Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+E+A), [John Hewitt](https://arxiv.org/search/cs?searchtype=author&query=Hewitt%2C+J), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.

| Comments:    | To appear in ACL 2020                                        |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2005.04511](https://arxiv.org/abs/2005.04511) [cs.CL]** |
|              | (or **[arXiv:2005.04511v1](https://arxiv.org/abs/2005.04511v1) [cs.CL]** for this version) |





<h2 id="2020-05-12-2">2. Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation</h2>

Title: [Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation](https://arxiv.org/abs/2005.04816)

Authors: [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Mia Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Sneha Kudugunta](https://arxiv.org/search/cs?searchtype=author&query=Kudugunta%2C+S), [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Yonghui Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y)

> Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ACL 2020                                                     |
| Cite as:           | **[arXiv:2005.04816](https://arxiv.org/abs/2005.04816) [cs.CL]** |
|                    | (or **[arXiv:2005.04816v1](https://arxiv.org/abs/2005.04816v1) [cs.CL]** for this version) |





<h2 id="2020-05-12-3">3. Multidirectional Associative Optimization of Function-Specific Word Representations</h2>

Title: [Multidirectional Associative Optimization of Function-Specific Word Representations](https://arxiv.org/abs/2005.05264)

Authors: [Daniela Gerz](https://arxiv.org/search/cs?searchtype=author&query=Gerz%2C+D), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M), [Roi Reichart](https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A)

> We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.

| Comments: | ACL 2020 (Long paper)                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.05264](https://arxiv.org/abs/2005.05264) [cs.CL]** |
|           | (or **[arXiv:2005.05264v1](https://arxiv.org/abs/2005.05264v1) [cs.CL]** for this version) |







# 2020-05-11

[Return to Index](#Index)



<h2 id="2020-05-11-1">1. Learning to Detect Unacceptable Machine Translations for Downstream Tasks</h2>

Title: [Learning to Detect Unacceptable Machine Translations for Downstream Tasks](https://arxiv.org/abs/2005.03925)

Authors: [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> The field of machine translation has progressed tremendously in recent years. Even though the translation quality has improved significantly, current systems are still unable to produce uniformly acceptable machine translations for the variety of possible use cases. In this work, we put machine translation in a cross-lingual pipeline and introduce downstream tasks to define task-specific acceptability of machine translations. This allows us to leverage parallel data to automatically generate acceptability annotations on a large scale, which in turn help to learn acceptability detectors for the downstream tasks. We conduct experiments to demonstrate the effectiveness of our framework for a range of downstream tasks and translation models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.03925](https://arxiv.org/abs/2005.03925) [cs.CL]** |
|           | (or **[arXiv:2005.03925v1](https://arxiv.org/abs/2005.03925v1) [cs.CL]** for this version) |



<h2 id="2020-05-11-2">2. Distilling Knowledge from Pre-trained Language Models via Text Smoothing</h2>

Title: [Distilling Knowledge from Pre-trained Language Models via Text Smoothing](https://arxiv.org/abs/2005.03848)

Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yibing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xiangyang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X), [Dianhai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> This paper studies compressing pre-trained language models, like BERT (Devlin et al.,2019), via teacher-student knowledge distillation. Previous works usually force the student model to strictly mimic the smoothed labels predicted by the teacher BERT. As an alternative, we propose a new method for BERT distillation, i.e., asking the teacher to generate smoothed word ids, rather than labels, for teaching the student model in knowledge distillation. We call this kind of methodTextSmoothing. Practically, we use the softmax prediction of the Masked Language Model(MLM) in BERT to generate word distributions for given texts and smooth those input texts using that predicted soft word ids. We assume that both the smoothed labels and the smoothed texts can implicitly augment the input corpus, while text smoothing is intuitively more efficient since it can generate more instances in one neural network forward step.Experimental results on GLUE and SQuAD demonstrate that our solution can achieve competitive results compared with existing BERT distillation methods.

| Comments: | 5 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2005.03848](https://arxiv.org/abs/2005.03848) [cs.CL]** |
|           | (or **[arXiv:2005.03848v1](https://arxiv.org/abs/2005.03848v1) [cs.CL]** for this version) |



# 2020-05-08

[Return to Index](#Index)



<h2 id="2020-05-08-1">1. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</h2>

Title: [Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](https://arxiv.org/abs/2005.03119)

Authors: [Po-Yao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Xiaojun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+X), [Alexander Hauptmann](https://arxiv.org/search/cs?searchtype=author&query=Hauptmann%2C+A)

> Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when the images are not available at the testing time.

| Comments: | Accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2005.03119](https://arxiv.org/abs/2005.03119) [cs.CL]** |
|           | (or **[arXiv:2005.03119v1](https://arxiv.org/abs/2005.03119v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-2">2. JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation</h2>

Title: [JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation](https://arxiv.org/abs/2005.03361)

Authors: [Zhuoyuan Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Fabien Cromieres](https://arxiv.org/search/cs?searchtype=author&query=Cromieres%2C+F), [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Haiyue Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese--English and News Commentary Japanese--Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks.

| Comments: | LREC 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03361](https://arxiv.org/abs/2005.03361) [cs.CL]** |
|           | (or **[arXiv:2005.03361v1](https://arxiv.org/abs/2005.03361v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-3">3. Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation</h2>

Title: [Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](https://arxiv.org/abs/2005.03393)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Tongran Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Changliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C)

> In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in documentlevel neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.

| Comments: | 5 pages, 2 figures, 5 tables, accpeted by ACL2020            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03393](https://arxiv.org/abs/2005.03393) [cs.CL]** |
|           | (or **[arXiv:2005.03393v1](https://arxiv.org/abs/2005.03393v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-4">4. Practical Perspectives on Quality Estimation for Machine Translation</h2>

Title: [Practical Perspectives on Quality Estimation for Machine Translation](https://arxiv.org/abs/2005.03519)

Authors: [Junpei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Yuezhang](https://arxiv.org/search/cs?searchtype=author&query=Yuezhang) (Music)Li

> Sentence level quality estimation (QE) for machine translation (MT) attempts to predict the translation edit rate (TER) cost of post-editing work required to correct MT output. We describe our view on sentence-level QE as dictated by several practical setups encountered in the industry. We find consumers of MT output---whether human or algorithmic ones---to be primarily interested in a binary quality metric: is the translated sentence adequate as-is or does it need post-editing? Motivated by this we propose a quality classification (QC) view on sentence-level QE whereby we focus on maximizing recall at precision above a given threshold. We demonstrate that, while classical QE regression models fare poorly on this task, they can be re-purposed by replacing the output regression layer with a binary classification one, achieving 50-60\% recall at 90\% precision. For a high-quality MT system producing 75-80\% correct translations, this promises a significant reduction in post-editing work indeed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.03519](https://arxiv.org/abs/2005.03519) [cs.CL]** |
|           | (or **[arXiv:2005.03519v1](https://arxiv.org/abs/2005.03519v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-5">5. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation</h2>

Title: [On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](https://arxiv.org/abs/2005.03642)

Authors: [Chaojun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03642](https://arxiv.org/abs/2005.03642) [cs.CL]** |
|           | (or **[arXiv:2005.03642v1](https://arxiv.org/abs/2005.03642v1) [cs.CL]** for this version) |





# 2020-05-07

[Return to Index](#Index)



<h2 id="2020-05-07-1">1. Exploring Controllable Text Generation Techniques</h2>

Title: [Exploring Controllable Text Generation Techniques](https://arxiv.org/abs/2005.01822)

Authors: [Shrimai Prabhumoye](https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R)

> Neural controllable text generation is an important area gaining attention due to its plethora of applications. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. We present an overview of the various techniques used to modulate each of these five modules to provide with control of attributes in the generation process. We also provide an analysis on the advantages and disadvantages of these techniques and open paths to develop new architectures based on the combination of the modules described in this paper.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.01822](https://arxiv.org/abs/2005.01822) [cs.CL]** |
|           | (or **[arXiv:2005.01822v1](https://arxiv.org/abs/2005.01822v1) [cs.CL]** for this version) |





<h2 id="2020-05-07-2">2. Understanding Scanned Receipts</h2>

Title: [Understanding Scanned Receipts](https://arxiv.org/abs/2005.01828)

Authors: [Eric Melz](https://arxiv.org/search/cs?searchtype=author&query=Melz%2C+E)

> Tasking machines with understanding receipts can have important applications such as enabling detailed analytics on purchases, enforcing expense policies, and inferring patterns of purchase behavior on large collections of receipts. In this paper, we focus on the task of Named Entity Linking (NEL) of scanned receipt line items; specifically, the task entails associating shorthand text from OCR'd receipts with a knowledge base (KB) of grocery products. For example, the scanned item "STO BABY SPINACH" should be linked to the catalog item labeled "Simple Truth Organic Baby Spinach". Experiments that employ a variety of Information Retrieval techniques in combination with statistical phrase detection shows promise for effective understanding of scanned receipt data.

| Comments:    | 8 pages, 3 figures, no conference submission                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2005.01828](https://arxiv.org/abs/2005.01828) [cs.CL]** |
|              | (or **[arXiv:2005.01828v1](https://arxiv.org/abs/2005.01828v1) [cs.CL]** for this version) |





# 2020-05-06

[Return to Index](#Index)



<h2 id="2020-05-06-1">1. IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization</h2>

Title: [IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization](https://arxiv.org/abs/2005.02178)

Authors: [Wenxuan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Bill Yuchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B+Y), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Fine-tuning pre-trained language models (PTLMs), such as BERT and its better variant RoBERTa, has been a common practice for advancing performance in natural language understanding (NLU) tasks. Recent advance in representation learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings can significantly improve performance on downstream tasks with faster convergence and better generalization. The isotropy of the pre-trained embeddings in PTLMs, however, is relatively under-explored. In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning. This simple yet effective fine-tuning method yields about 1.0 absolute increment on the average of seven benchmark NLU tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.02178](https://arxiv.org/abs/2005.02178) [cs.CL]** |
|           | (or **[arXiv:2005.02178v1](https://arxiv.org/abs/2005.02178v1) [cs.CL]** for this version) |



<h2 id="2020-05-06-2">2. Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration</h2>

Title: [Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration](https://arxiv.org/abs/2005.02325)

Authors: [Elhadji Mamadou Nguer](https://arxiv.org/search/cs?searchtype=author&query=Nguer%2C+E+M), [Diop Sokhna Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+D+S), [Yacoub Ahmed Fall](https://arxiv.org/search/cs?searchtype=author&query=Fall%2C+Y+A), [Mouhamadou Khoule](https://arxiv.org/search/cs?searchtype=author&query=Khoule%2C+M)

> The local languages in Senegal, like those of West African countries in general, are written based on two alphabets: supplemented Arabic alphabet (called Ajami) and Latin alphabet. Each writing has its own applications. Ajami writing is generally used by people educated in Koranic schools for communication, business, literature (religious texts, poetry, etc.), traditional religious medicine, etc. Writing with Latin characters is used for localization of ICT (Web, dictionaries, Windows and Google tools translated in Wolof, etc.), the translation of legal texts (commercial code and constitution translated in Wolof) and religious ones (Quran and Bible in Wolof), book edition, etc. To facilitate both populations general access to knowledge, it is useful to set up transliteration tools between these two scriptures. This work falls within the framework of the implementation of project for a collaborative online dictionary Wolof (Nguer E. M., Khoule M, Thiam M. N., Mbaye B. T., Thiare O., Cisse M. T., Mangeot M. 2014), which will involve people using Ajami writing. Our goal will consist, on the one hand in raising the issues related to the transliteration and the challenges that this will raise, and on the other one, presenting the perspectives.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | LTC 2015                                                     |
| Cite as:           | **[arXiv:2005.02325](https://arxiv.org/abs/2005.02325) [cs.CL]** |
|                    | (or **[arXiv:2005.02325v1](https://arxiv.org/abs/2005.02325v1) [cs.CL]** for this version) |



<h2 id="2020-05-06-3">3. It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information</h2>

Title: [It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](https://arxiv.org/abs/2005.02354)

Authors: [Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Sabrina J. Mielke](https://arxiv.org/search/cs?searchtype=author&query=Mielke%2C+S+J), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N)

> The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at [this https URL](https://github.com/e-bug/nmt-difficulty).

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.02354](https://arxiv.org/abs/2005.02354) [cs.CL]** |
|           | (or **[arXiv:2005.02354v1](https://arxiv.org/abs/2005.02354v1) [cs.CL]** for this version) |





# 2020-05-05

[Return to Index](#Index)



<h2 id="2020-05-05-1">1. Quantifying Attention Flow in Transformers</h2>

Title: [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)

Authors: [Samira Abnar](https://arxiv.org/search/cs?searchtype=author&query=Abnar%2C+S), [Willem Zuidema](https://arxiv.org/search/cs?searchtype=author&query=Zuidema%2C+W)

> In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00928](https://arxiv.org/abs/2005.00928) [cs.LG]** |
|           | (or **[arXiv:2005.00928v1](https://arxiv.org/abs/2005.00928v1) [cs.LG]** for this version) |





<h2 id="2020-05-05-2">2. Does Visual Self-Supervision Improve Learning of Speech Representations?</h2>

Title: [Does Visual Self-Supervision Improve Learning of Speech Representations?](https://arxiv.org/abs/2005.01400)

Authors: [Abhinav Shukla](https://arxiv.org/search/eess?searchtype=author&query=Shukla%2C+A), [Stavros Petridis](https://arxiv.org/search/eess?searchtype=author&query=Petridis%2C+S), [Maja Pantic](https://arxiv.org/search/eess?searchtype=author&query=Pantic%2C+M)

> Self-supervised learning has attracted plenty of recent research interest. However, most works are typically unimodal and there has been limited work that studies the interaction between audio and visual modalities for self-supervised learning. This work (1) investigates visual self-supervision via face reconstruction to guide the learning of audio representations; (2) proposes two audio-only self-supervision approaches for speech representation learning; (3) shows that a multi-task combination of the proposed visual and audio self-supervision is beneficial for learning richer features that are more robust in noisy conditions; (4) shows that self-supervised pretraining leads to a superior weight initialization, which is especially useful to prevent overfitting and lead to faster model convergence on smaller sized datasets. We evaluate our audio representations for emotion and speech recognition, achieving state of the art performance for both problems. Our results demonstrate the potential of visual self-supervision for audio feature learning and suggest that joint visual and audio self-supervision leads to more informative speech representations.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.01400](https://arxiv.org/abs/2005.01400) [eess.AS]** |
|           | (or **[arXiv:2005.01400v1](https://arxiv.org/abs/2005.01400v1) [eess.AS]** for this version) |





<h2 id="2020-05-05-3">3. Evaluating Robustness to Input Perturbations for Neural Machine Translation</h2>

Title: [Evaluating Robustness to Input Perturbations for Neural Machine Translation](https://arxiv.org/abs/2005.00580)

Authors: [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G), [Yaser Al-Onaizan](https://arxiv.org/search/cs?searchtype=author&query=Al-Onaizan%2C+Y)

> Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00580](https://arxiv.org/abs/2005.00580) [cs.CL]** |
|           | (or **[arXiv:2005.00580v1](https://arxiv.org/abs/2005.00580v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-4">4. From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers</h2>

Title: [From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers](https://arxiv.org/abs/2005.00633)

Authors: [Anne Lauscher](https://arxiv.org/search/cs?searchtype=author&query=Lauscher%2C+A), [Vinit Ravishankar](https://arxiv.org/search/cs?searchtype=author&query=Ravishankar%2C+V), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G)

> Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00633](https://arxiv.org/abs/2005.00633) [cs.CL]** |
|           | (or **[arXiv:2005.00633v1](https://arxiv.org/abs/2005.00633v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-5">5. Opportunistic Decoding with Timely Correction for Simultaneous Translation</h2>

Title: [Opportunistic Decoding with Timely Correction for Simultaneous Translation](https://arxiv.org/abs/2005.00675)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Kaibo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.

| Comments: | accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00675](https://arxiv.org/abs/2005.00675) [cs.CL]** |
|           | (or **[arXiv:2005.00675v1](https://arxiv.org/abs/2005.00675v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-6">6. Synthesizer: Rethinking Self-Attention in Transformer Models</h2>

Title: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D), [Da-Cheng Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+D), [Zhe Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Che Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+C)

> The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00743](https://arxiv.org/abs/2005.00743) [cs.CL]** |
|           | (or **[arXiv:2005.00743v1](https://arxiv.org/abs/2005.00743v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-7">7. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation</h2>

Title: [ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2005.00850)

Authors: [Lifu Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+L), [Richard Yuanzhe Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y), [Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.

| Comments: | ACL2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00850](https://arxiv.org/abs/2005.00850) [cs.CL]** |
|           | (or **[arXiv:2005.00850v1](https://arxiv.org/abs/2005.00850v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-8">8. Improving Non-autoregressive Neural Machine Translation with Monolingual Data</h2>

Title: [Improving Non-autoregressive Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2005.00932)

Authors: [Jiawei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P)

> Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with the goal of transferring the AR model's generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model's performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.

| Comments: | To appear in ACL 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00932](https://arxiv.org/abs/2005.00932) [cs.CL]** |
|           | (or **[arXiv:2005.00932v1](https://arxiv.org/abs/2005.00932v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-9">9. On the Inference Calibration of Neural Machine Translation</h2>

Title: [On the Inference Calibration of Neural Machine Translation]()

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.

| Comments: | Accepted by ACL2020                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00963](https://arxiv.org/abs/2005.00963) [cs.CL]** |
|           | (or **[arXiv:2005.00963v1](https://arxiv.org/abs/2005.00963v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-10">10. Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction</h2>

Title: [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](https://arxiv.org/abs/2005.00987)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: [this https URL](https://github.com/kanekomasahiro/bert-gec).

| Comments:          | Accepted as a short paper to the 58th Annual Conference of the Association for Computational Linguistics (ACL-2020) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association for Computational Linguistics (ACL-2020)         |
| Cite as:           | **[arXiv:2005.00987](https://arxiv.org/abs/2005.00987) [cs.CL]** |
|                    | (or **[arXiv:2005.00987v1](https://arxiv.org/abs/2005.00987v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-11">11. Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation</h2>

Title: [Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation](https://arxiv.org/abs/2005.01158)

Authors: [Kshitij Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+K), [Gerard de Melo](https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G)

> In this paper, we explore the artificial generation of typographical errors based on real-world statistics. We first draw on a small set of annotated data to compute spelling error statistics. These are then invoked to introduce errors into substantially larger corpora. The generation methodology allows us to generate particularly challenging errors that require context-aware error detection. We use it to create a set of English language error detection and correction datasets. Finally, we examine the effectiveness of machine learning models for detecting and correcting errors based on this data. The datasets are available at [this http URL](http://typo.nlproc.org/)

| Comments: | Accepted for publication at LREC 2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.01158](https://arxiv.org/abs/2005.01158) [cs.CL]** |
|           | (or **[arXiv:2005.01158v1](https://arxiv.org/abs/2005.01158v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-12">12. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</h2>

Title: [On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](https://arxiv.org/abs/2005.01196)

Authors: [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Maxime Peyrard](https://arxiv.org/search/cs?searchtype=author&query=Peyrard%2C+M), [Yang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R), [Steffen Eger](https://arxiv.org/search/cs?searchtype=author&query=Eger%2C+S)

> Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish "translationese", i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses the reference-based BLEU by 5.7 correlation points.

| Comments: | ACL2020 Camera Ready                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01196](https://arxiv.org/abs/2005.01196) [cs.CL]** |
|           | (or **[arXiv:2005.01196v1](https://arxiv.org/abs/2005.01196v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-13">13. Using Context in Neural Machine Translation Training Objectives</h2>

Title: [Using Context in Neural Machine Translation Training Objectives](https://arxiv.org/abs/2005.01483)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective.
> We first sample pseudo-documents from sentence samples. We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT). This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01483](https://arxiv.org/abs/2005.01483) [cs.CL]** |
|           | (or **[arXiv:2005.01483v1](https://arxiv.org/abs/2005.01483v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-14">14. Evaluating Explanation Methods for Neural Machine Translation</h2>

Title: [Evaluating Explanation Methods for Neural Machine Translation](https://arxiv.org/abs/2005.01672)

Authors: [Jierui Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.

| Comments: | Accepted to ACL 2020, 9 pages                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01672](https://arxiv.org/abs/2005.01672) [cs.CL]** |
|           | (or **[arXiv:2005.01672v1](https://arxiv.org/abs/2005.01672v1) [cs.CL]** for this version) |





# 2020-05-04

[Return to Index](#Index)



<h2 id="2020-05-04-1">1. MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</h2>

Title: [MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer](https://arxiv.org/abs/2005.00052)

Authors: [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S)

> The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and achieves competitive results on question answering.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00052](https://arxiv.org/abs/2005.00052) [cs.CL]** |
|           | (or **[arXiv:2005.00052v1](https://arxiv.org/abs/2005.00052v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-2">2. Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation</h2>

Title: [Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation](https://arxiv.org/abs/2005.00283)

Authors: [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [Rejwanul Haque](https://arxiv.org/search/cs?searchtype=author&query=Haque%2C+R), [Guodong Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+G), [Federico Gaspari](https://arxiv.org/search/cs?searchtype=author&query=Gaspari%2C+F), [Maja Popovic](https://arxiv.org/search/cs?searchtype=author&query=Popovic%2C+M), [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A)

> Every day, more people are becoming infected and dying from exposure to COVID-19. Some countries in Europe like Spain, France, the UK and Italy have suffered particularly badly from the virus. Others such as Germany appear to have coped extremely well. Both health professionals and the general public are keen to receive up-to-date information on the effects of the virus, as well as treatments that have proven to be effective. In cases where language is a barrier to access of pertinent information, machine translation (MT) may help people assimilate information published in different languages. Our MT systems trained on COVID-19 data are freely available for anyone to use to help translate information published in German, French, Italian, Spanish into English, as well as the reverse direction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00283](https://arxiv.org/abs/2005.00283) [cs.CL]** |
|           | (or **[arXiv:2005.00283v1](https://arxiv.org/abs/2005.00283v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-3">3. Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation</h2>

Title: [Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](https://arxiv.org/abs/2005.00308)

Authors: [Xabier Soto](https://arxiv.org/search/cs?searchtype=author&query=Soto%2C+X), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

> Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL (2020) |
| Cite as:           | **[arXiv:2005.00308](https://arxiv.org/abs/2005.00308) [cs.CL]** |
|                    | (or **[arXiv:2005.00308v1](https://arxiv.org/abs/2005.00308v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-4">4. Identifying Necessary Elements for BERT's Multilinguality</h2>

Title: [Identifying Necessary Elements for BERT's Multilinguality](https://arxiv.org/abs/2005.00396)

Authors: [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is suprising given that mBERT does not use any kind of crosslingual signal during training. While recent literature has studied this effect, the exact reason for mBERT's multilinguality is still unknown. We aim to identify architectural properties of BERT as well as linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models and synthetic as well as natural data. Overall, we identify six elements that are potentially necessary for BERT to be multilingual. Architectural factors that contribute to multilinguality are underparameterization, shared special tokens (e.g., "[CLS]"), shared position embeddings and replacing masked tokens with random tokens. Factors related to training data that are beneficial for multilinguality are similar word order and comparability of corpora.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00396](https://arxiv.org/abs/2005.00396) [cs.CL]** |
|           | (or **[arXiv:2005.00396v1](https://arxiv.org/abs/2005.00396v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-5">5. Defense of Word-level Adversarial Attacks via Random Substitution Encoding</h2>

Title: [Defense of Word-level Adversarial Attacks via Random Substitution Encoding](https://arxiv.org/abs/2005.00446)

Authors: [Zhaoyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hongtao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> The adversarial attacks against deep neural networks on computer version tasks has spawned many new technologies that help protect models avoiding false prediction. Recently, word-level adversarial attacks on deep models of Natural Language Processing (NLP) tasks have also demonstrated strong power, e.g., fooling a sentiment classification neural network to make wrong decision. Unfortunately, few previous literatures have discussed the defense of such word-level synonym substitution based attacks since they are hard to be perceived and detected. In this paper, we shed light on this problem and propose a novel defense framework called Random Substitution Encoding (RSE), which introduces a random substitution encoder into the training process of original neural networks. Extensive experiments on text classification tasks demonstrate the effectiveness of our framework on defense of word-level adversarial attacks, under various base and attack models.

| Comments: | 12 pages, 2 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00446](https://arxiv.org/abs/2005.00446) [cs.CL]** |
|           | (or **[arXiv:2005.00446v1](https://arxiv.org/abs/2005.00446v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-6">6. Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</h2>

Title: [Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](https://arxiv.org/abs/2005.00524)

Authors: [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Yoshinari Fujinuma](https://arxiv.org/search/cs?searchtype=author&query=Fujinuma%2C+Y), [Michael J. Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+M+J), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

> Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00524](https://arxiv.org/abs/2005.00524) [cs.CL]** |
|           | (or **[arXiv:2005.00524v1](https://arxiv.org/abs/2005.00524v1) [cs.CL]** for this version) |









# 2020-05-01

[Return to Index](#Index)



<h2 id="2020-05-01-1">1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation</h2>

Title: [Simulated Multiple Reference Training Improves Low-Resource Machine Translation](https://arxiv.org/abs/2004.14524)

Authors: [Huda Khayrallah](https://arxiv.org/search/cs?searchtype=author&query=Khayrallah%2C+H), [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Many valid translations exist for a given sentence, and yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce a novel MT training method that approximates the full space of possible translations by: sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser's distribution over possible tokens. With an English paraphraser, we demonstrate the effectiveness of our method in low-resource settings, with gains of 1.2 to 7 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14524](https://arxiv.org/abs/2004.14524) [cs.CL]** |
|           | (or **[arXiv:2004.14524v1](https://arxiv.org/abs/2004.14524v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-2">2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</h2>

Title: [Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://arxiv.org/abs/2004.14564)

Authors: [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> We propose the use of a sequence-to-sequence paraphraser for automatic machine translation evaluation. The paraphraser takes a human reference as input and then force-decodes and scores an MT system output. We propose training the aforementioned paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot "language pair" (e.g., Russian to Russian). We denote our paraphraser "unbiased" because the mode of our model's output probability is centered around a copy of the input sequence, which in our case represent the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT19 segment-level shared metrics task in all languages, excluding Gujarati where the model had no training data. We also explore using our model conditioned on the source instead of the reference, and find that it outperforms every quality estimation as a metric system from the WMT19 shared task on quality estimation by a statistically significant margin in every language pair.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14564](https://arxiv.org/abs/2004.14564) [cs.CL]** |
|           | (or **[arXiv:2004.14564v1](https://arxiv.org/abs/2004.14564v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-3">3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation</h2>

Title: [Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](https://arxiv.org/abs/2004.14607)

Authors: [Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Barbara Rychalska](https://arxiv.org/search/cs?searchtype=author&query=Rychalska%2C+B), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Dominika Basaj](https://arxiv.org/search/cs?searchtype=author&query=Basaj%2C+D)

> Despite increasing instances of machine translation (MT) systems including contextual information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark datasets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several baseline MT systems on the curated datasets. Surprisingly, we find that existing context-aware models do not improve discourse-related translations consistently across languages and phenomena.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14607](https://arxiv.org/abs/2004.14607) [cs.CL]** |
|           | (or **[arXiv:2004.14607v1](https://arxiv.org/abs/2004.14607v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-4">4. Capsule-Transformer for Neural Machine Translation</h2>

Title: [Capsule-Transformer for Neural Machine Translation](https://arxiv.org/abs/2004.14649)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Juncheng Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Transformer hugely benefits from its key design of the multi-head self-attention network (SAN), which extracts information from various perspectives through transforming the given input into different subspaces. However, its simple linear transformation aggregation strategy may still potentially fail to fully capture deeper contextualized information. In this paper, we thus propose the capsule-Transformer, which extends the linear transformation into a more general capsule routing algorithm by taking SAN as a special case of capsule network. So that the resulted capsule-Transformer is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words. Specifically, we see groups of attention weights in SAN as low layer capsules. By applying the iterative capsule routing algorithm they can be further aggregated into high layer capsules which contain deeper contextualized information. Experimental results on the widely-used machine translation datasets show our proposed capsule-Transformer outperforms strong Transformer baseline significantly.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14649](https://arxiv.org/abs/2004.14649) [cs.CL]** |
|           | (or **[arXiv:2004.14649v1](https://arxiv.org/abs/2004.14649v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-5">5. End-to-End Neural Word Alignment Outperforms GIZA++</h2>

Title: [End-to-End Neural Word Alignment Outperforms GIZA++](https://arxiv.org/abs/2004.14675)

Authors: [Thomas Zenkel](https://arxiv.org/search/cs?searchtype=author&query=Zenkel%2C+T), [Joern Wuebker](https://arxiv.org/search/cs?searchtype=author&query=Wuebker%2C+J), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J)

> Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14675](https://arxiv.org/abs/2004.14675) [cs.CL]** |
|           | (or **[arXiv:2004.14675v1](https://arxiv.org/abs/2004.14675v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-6">6. Character-Level Translation with Self-attention</h2>

Title: [Character-Level Translation with Self-attention](https://arxiv.org/abs/2004.14788)

Authors: [Yingqiang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Nikola I. Nikolov](https://arxiv.org/search/cs?searchtype=author&query=Nikolov%2C+N+I), [Yuhuang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Richard H.R. Hahnloser](https://arxiv.org/search/cs?searchtype=author&query=Hahnloser%2C+R+H)

> We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14788](https://arxiv.org/abs/2004.14788) [cs.CL]** |
|           | (or **[arXiv:2004.14788v1](https://arxiv.org/abs/2004.14788v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-7">7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation</h2>

Title: [Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](https://arxiv.org/abs/2004.14821)

Authors: [Shoetsu Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+S), [Jin Sakuma](https://arxiv.org/search/cs?searchtype=author&query=Sakuma%2C+J), [Naoki Yoshinaga](https://arxiv.org/search/cs?searchtype=author&query=Yoshinaga%2C+N), [Masashi Toyoda](https://arxiv.org/search/cs?searchtype=author&query=Toyoda%2C+M), [Masaru Kitsuregawa](https://arxiv.org/search/cs?searchtype=author&query=Kitsuregawa%2C+M)

> Neural machine translation (NMT) models do not work well in domains different from the training data. The standard approach to this problem is to build a small parallel data in the target domain and perform domain adaptation from a source domain where massive parallel data is available. However, domain adaptation between distant domains (e.g., subtitles and research papers) does not perform effectively because of mismatches in vocabulary; it will encounter many domain-specific unknown words (e.g., `angstrom') and words whose meanings shift across domains (e.g., `conductor'). In this study, aiming to solve these vocabulary mismatches in distant domain adaptation, we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pre-trained NMT model to the target domain. Prior to fine-tuning, our method replaces word embeddings in embedding layers of the NMT model, by projecting general word embeddings induced from monolingual data in the target domain onto the source-domain embedding space. Experimental results on distant domain adaptation for English-to-Japanese translation and German-to-English translation indicate that our vocabulary adaptation improves the performance of fine-tuning by 3.6 BLEU points.

| Comments: | 8pages + citations                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14821](https://arxiv.org/abs/2004.14821) [cs.CL]** |
|           | (or **[arXiv:2004.14821v1](https://arxiv.org/abs/2004.14821v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-8">8. Accurate Word Alignment Induction from Neural Machine Translation</h2>

Title: [Accurate Word Alignment Induction from Neural Machine Translation](https://arxiv.org/abs/2004.14837)

Authors: [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Guanhua Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Despite its original goal to jointly learn to align and translate, prior researches suggest that the state-of-the-art neural machine translation model Transformer captures poor word alignment through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignment, which could only be revealed if we choose the correct decoding step and layer to induce word alignment. We propose to induce alignment with the to-be-aligned target token as the decoder input and present two simple but effective interpretation methods for word alignment induction, either through the attention weights or the leave-one-out measures. In contrast to previous studies, we find that attention weights capture better word alignment than the leave-one-out measures under our setting. Using the proposed method with attention weights, we greatly improve over fast-align on word alignment induction. Finally, we present a multi-task learning framework to train the Transformer model and show that by incorporating GIZA++ alignments into our multi-task training, we can induce significantly better alignments than GIZA++.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14837](https://arxiv.org/abs/2004.14837) [cs.CL]** |
|           | (or **[arXiv:2004.14837v1](https://arxiv.org/abs/2004.14837v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-9">9. Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation</h2>

Title: [Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation](https://arxiv.org/abs/2004.14911)

Authors: [Asa Cooper Stickland](https://arxiv.org/search/cs?searchtype=author&query=Stickland%2C+A+C), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M)

> There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task. This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model trained only on English monolingual data, BART. 2) Fine-tuning a model trained on monolingual data from 25 languages, mBART. For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings. For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time. When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14911](https://arxiv.org/abs/2004.14911) [cs.CL]** |
|           | (or **[arXiv:2004.14911v1](https://arxiv.org/abs/2004.14911v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-10">10. Bridging linguistic typology and multilingual machine translation with multi-view language representations</h2>

Title: [Bridging linguistic typology and multilingual machine translation with multi-view language representations](https://arxiv.org/abs/2004.14923)

Authors: [Arturo Oncevay](https://arxiv.org/search/cs?searchtype=author&query=Oncevay%2C+A), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.

| Comments: | 15 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14923](https://arxiv.org/abs/2004.14923) [cs.CL]** |
|           | (or **[arXiv:2004.14923v1](https://arxiv.org/abs/2004.14923v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-11">11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</h2>

Title: [Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](https://arxiv.org/abs/2004.14927)

Authors: [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that document-level context can be used to capture domain generalities when in-domain parallel data is not available. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally present experiments showing the usefulness of large context when modeling multiple domains at once.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14927](https://arxiv.org/abs/2004.14927) [cs.CL]** |
|           | (or **[arXiv:2004.14927v1](https://arxiv.org/abs/2004.14927v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-12">12. Language Model Prior for Low-Resource Neural Machine Translation</h2>

Title: [Language Model Prior for Low-Resource Neural Machine Translation](https://arxiv.org/abs/2004.14928)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM "disagrees" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14928](https://arxiv.org/abs/2004.14928) [cs.CL]** |
|           | (or **[arXiv:2004.14928v1](https://arxiv.org/abs/2004.14928v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-13">13. A Call for More Rigor in Unsupervised Cross-lingual Learning</h2>

Title: [A Call for More Rigor in Unsupervised Cross-lingual Learning](https://arxiv.org/abs/2004.14958)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

> We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.14958](https://arxiv.org/abs/2004.14958) [cs.CL]** |
|           | (or **[arXiv:2004.14958v1](https://arxiv.org/abs/2004.14958v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-14">14. Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages</h2>

Title: [Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages](https://arxiv.org/abs/2004.14963)

Authors: [Emrah Budur](https://arxiv.org/search/cs?searchtype=author&query=Budur%2C+E), [Rıza Özçelik](https://arxiv.org/search/cs?searchtype=author&query=Özçelik%2C+R), [Tunga Güngör](https://arxiv.org/search/cs?searchtype=author&query=Güngör%2C+T), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts%2C+C)

> The large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress for other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response to this for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their quality. As examples of the new issues that these datasets help us address, we assess the value of Turkish-specific embeddings and the importance of morphological parsing for developing robust Turkish NLI models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14963](https://arxiv.org/abs/2004.14963) [cs.CL]** |
|           | (or **[arXiv:2004.14963v1](https://arxiv.org/abs/2004.14963v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-15">15. Investigating Transferability in Pretrained Language Models</h2>

Title: [Investigating Transferability in Pretrained Language Models](https://arxiv.org/abs/2004.14975)

Authors: [Alex Tamkin](https://arxiv.org/search/cs?searchtype=author&query=Tamkin%2C+A), [Trisha Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+T), [Davide Giovanardi](https://arxiv.org/search/cs?searchtype=author&query=Giovanardi%2C+D), [Noah Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+N)

> While probing is a common technique for identifying knowledge in the representations of pretrained models, it is unclear whether this technique can explain the downstream success of models like BERT which are trained end-to-end during finetuning. To address this question, we compare probing with a different measure of transferability: the decrease in finetuning performance of a partially-reinitialized model. This technique reveals that in BERT, layers with high probing accuracy on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. In addition, dataset size impacts layer transferability: the less finetuning data one has, the more important the middle and later layers of BERT become. Furthermore, BERT does not simply find a better initializer for individual layers; instead, interactions between layers matter and reordering BERT's layers prior to finetuning significantly harms evaluation metrics. These results provide a way of understanding the transferability of parameters in pretrained language models, revealing the fluidity and complexity of transfer learning in these models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14975](https://arxiv.org/abs/2004.14975) [cs.CL]** |
|           | (or **[arXiv:2004.14975v1](https://arxiv.org/abs/2004.14975v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-16">16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation</h2>

Title: [Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](https://arxiv.org/abs/2004.14989)

Authors: [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Lisa Yankovskaya](https://arxiv.org/search/cs?searchtype=author&query=Yankovskaya%2C+L), [Andre Tättar](https://arxiv.org/search/cs?searchtype=author&query=Tättar%2C+A), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Following previous work on automatic paraphrasing, we assess the feasibility of improving BLEU (Papineni et al., 2002) using state-of-the-art neural paraphrasing techniques to generate additional references. We explore the extent to which diverse paraphrases can adequately cover the space of valid translations and compare to an alternative approach of generating paraphrases constrained by MT outputs. We compare both approaches to human-produced references in terms of diversity and the improvement in BLEU's correlation with human judgements of MT quality. Our experiments on the WMT19 metrics tasks for all into-English language directions show that somewhat surprisingly, the addition of diverse paraphrases, even those produced by humans, leads to only small, inconsistent changes in BLEU's correlation with human judgments, suggesting that BLEU's ability to correctly exploit multiple references is limited

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14989](https://arxiv.org/abs/2004.14989) [cs.CL]** |
|           | (or **[arXiv:2004.14989v1](https://arxiv.org/abs/2004.14989v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-17">17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning</h2>

Title: [On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](https://arxiv.org/abs/2004.15001)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J), [Vikas Bhardwaj](https://arxiv.org/search/cs?searchtype=author&query=Bhardwaj%2C+V)

> Pre-trained multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on some source language (typically English) and evaluated on a different target language. However, published results for baseline mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot cross-lingual performance varies greatly within the same fine-tuning run and between different fine-tuning runs. We recommend providing oracle scores alongside the zero-shot results: still fine-tune using English, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding the variation from bad checkpoints.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15001](https://arxiv.org/abs/2004.15001) [cs.CL]** |
|           | (or **[arXiv:2004.15001v1](https://arxiv.org/abs/2004.15001v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-18">18. When does data augmentation help generalization in NLP?</h2>

Title: [When does data augmentation help generalization in NLP?](https://arxiv.org/abs/2004.15012)

Authors: [Rohan Jha](https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+R), [Charles Lovering](https://arxiv.org/search/cs?searchtype=author&query=Lovering%2C+C), [Ellie Pavlick](https://arxiv.org/search/cs?searchtype=author&query=Pavlick%2C+E)

> Neural models often exploit superficial ("weak") features to achieve good performance, rather than deriving the more general ("strong") features that we'd prefer a model to use. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. Recent work has proposed using data augmentation--that is, generating training examples on which these weak features fail--as a means of encouraging models to prefer the stronger features. We design a series of toy learning problems to investigate the conditions under which such data augmentation is helpful. We show that augmenting with training examples on which the weak feature fails ("counterexamples") does succeed in preventing the model from relying on the weak feature, but often does not succeed in encouraging the model to use the stronger feature in general. We also find in many cases that the number of counterexamples needed to reach a given error rate is independent of the amount of training data, and that this type of data augmentation becomes less effective as the target strong feature becomes harder to learn.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15012](https://arxiv.org/abs/2004.15012) [cs.CL]** |
|           | (or **[arXiv:2004.15012v1](https://arxiv.org/abs/2004.15012v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-19">19. Imitation Attacks and Defenses for Black-box Machine Translation Systems</h2>

Title: [Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://arxiv.org/abs/2004.15015)

Authors: [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Dawn Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15015](https://arxiv.org/abs/2004.15015) [cs.CL]** |
|           | (or **[arXiv:2004.15015v1](https://arxiv.org/abs/2004.15015v1) [cs.CL]** for this version) |