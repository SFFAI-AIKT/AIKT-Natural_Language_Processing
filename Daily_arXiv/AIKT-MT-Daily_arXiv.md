# Daily arXiv: Machine Translation - May, 2020

# Index

- [2020-05-08](#2020-05-08)
  - [1. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](#2020-05-08-1)
  - [2. JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation](#2020-05-08-2)
  - [3. Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](#2020-05-08-3)
  - [4. Practical Perspectives on Quality Estimation for Machine Translation](#2020-05-08-4)
  - [5. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](#2020-05-08-5)
- [2020-05-07](#2020-05-07)
  - [1. Exploring Controllable Text Generation Techniques](#2020-05-07-1)
  - [2. Understanding Scanned Receipts](#2020-05-07-2)
- [2020-05-06](#2020-05-06)
  - [1. IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization](#2020-05-06-1)
  - [2. Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration](#2020-05-06-2)
  - [3. It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](#2020-05-06-3)
- [2020-05-05](#2020-05-05)
  - [1. Quantifying Attention Flow in Transformers](#2020-05-05-1)
  - [2. Does Visual Self-Supervision Improve Learning of Speech Representations?](#2020-05-05-2)
  - [3. Evaluating Robustness to Input Perturbations for Neural Machine Translation](#2020-05-05-3)
  - [4. From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers](#2020-05-05-4)
  - [5. Opportunistic Decoding with Timely Correction for Simultaneous Translation](#2020-05-05-5)
  - [6. Synthesizer: Rethinking Self-Attention in Transformer Models](#2020-05-05-6)
  - [7. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](#2020-05-05-7)
  - [8. Improving Non-autoregressive Neural Machine Translation with Monolingual Data](#2020-05-05-8)
  - [9. On the Inference Calibration of Neural Machine Translation](#2020-05-05-9)
  - [10. Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](#2020-05-05-10)
  - [11. Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation](#2020-05-05-11)
  - [12. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](#2020-05-05-12)
  - [13. Using Context in Neural Machine Translation Training Objectives](#2020-05-05-13)
  - [14. Evaluating Explanation Methods for Neural Machine Translation](#2020-05-05-14)
- [2020-05-04](2020-05-04)
  - [1. MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer](#2020-05-04-1)
  - [2. Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation](#2020-05-04-2)
  - [3. Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](#2020-05-04-3)
  - [4. Identifying Necessary Elements for BERT's Multilinguality](#2020-05-04-4)
  - [5. Defense of Word-level Adversarial Attacks via Random Substitution Encoding](#2020-05-04-5)
  - [6. Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](#2020-05-04-6)
- [2020-05-01](#2020-05-01)
  - [1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation](2020-05-01-1)
  - [2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](2020-05-01-2)
  - [3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](2020-05-01-3)
  - [4. Capsule-Transformer for Neural Machine Translation](2020-05-01-4)
  - [5. End-to-End Neural Word Alignment Outperforms GIZA++](2020-05-01-5)
  - [6. Character-Level Translation with Self-attention](2020-05-01-6)
  - [7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](2020-05-01-7)
  - [8. Accurate Word Alignment Induction from Neural Machine Translation](2020-05-01-8)
  - [9. Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation](2020-05-01-9)
  - [10. Bridging linguistic typology and multilingual machine translation with multi-view language representations](2020-05-01-10)
  - [11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](2020-05-01-11)
  - [12. Language Model Prior for Low-Resource Neural Machine Translation](2020-05-01-12)
  - [13. A Call for More Rigor in Unsupervised Cross-lingual Learning](2020-05-01-13)
  - [14. Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages](2020-05-01-14)
  - [15. Investigating Transferability in Pretrained Language Models](2020-05-01-15)
  - [16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](2020-05-01-16)
  - [17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](2020-05-01-17)
  - [18. When does data augmentation help generalization in NLP?](2020-05-01-18)
  - [19. Imitation Attacks and Defenses for Black-box Machine Translation Systems](2020-05-01-19)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-05-08

[Return to Index](#Index)



<h2 id="2020-05-08-1">1. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</h2>

Title: [Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](https://arxiv.org/abs/2005.03119)

Authors: [Po-Yao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Xiaojun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+X), [Alexander Hauptmann](https://arxiv.org/search/cs?searchtype=author&query=Hauptmann%2C+A)

> Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when the images are not available at the testing time.

| Comments: | Accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2005.03119](https://arxiv.org/abs/2005.03119) [cs.CL]** |
|           | (or **[arXiv:2005.03119v1](https://arxiv.org/abs/2005.03119v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-2">2. JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation</h2>

Title: [JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation](https://arxiv.org/abs/2005.03361)

Authors: [Zhuoyuan Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Fabien Cromieres](https://arxiv.org/search/cs?searchtype=author&query=Cromieres%2C+F), [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Haiyue Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese--English and News Commentary Japanese--Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks.

| Comments: | LREC 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03361](https://arxiv.org/abs/2005.03361) [cs.CL]** |
|           | (or **[arXiv:2005.03361v1](https://arxiv.org/abs/2005.03361v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-3">3. Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation</h2>

Title: [Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](https://arxiv.org/abs/2005.03393)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Tongran Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Changliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C)

> In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in documentlevel neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.

| Comments: | 5 pages, 2 figures, 5 tables, accpeted by ACL2020            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03393](https://arxiv.org/abs/2005.03393) [cs.CL]** |
|           | (or **[arXiv:2005.03393v1](https://arxiv.org/abs/2005.03393v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-4">4. Practical Perspectives on Quality Estimation for Machine Translation</h2>

Title: [Practical Perspectives on Quality Estimation for Machine Translation](https://arxiv.org/abs/2005.03519)

Authors: [Junpei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Yuezhang](https://arxiv.org/search/cs?searchtype=author&query=Yuezhang) (Music)Li

> Sentence level quality estimation (QE) for machine translation (MT) attempts to predict the translation edit rate (TER) cost of post-editing work required to correct MT output. We describe our view on sentence-level QE as dictated by several practical setups encountered in the industry. We find consumers of MT output---whether human or algorithmic ones---to be primarily interested in a binary quality metric: is the translated sentence adequate as-is or does it need post-editing? Motivated by this we propose a quality classification (QC) view on sentence-level QE whereby we focus on maximizing recall at precision above a given threshold. We demonstrate that, while classical QE regression models fare poorly on this task, they can be re-purposed by replacing the output regression layer with a binary classification one, achieving 50-60\% recall at 90\% precision. For a high-quality MT system producing 75-80\% correct translations, this promises a significant reduction in post-editing work indeed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.03519](https://arxiv.org/abs/2005.03519) [cs.CL]** |
|           | (or **[arXiv:2005.03519v1](https://arxiv.org/abs/2005.03519v1) [cs.CL]** for this version) |





<h2 id="2020-05-08-5">5. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation</h2>

Title: [On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](https://arxiv.org/abs/2005.03642)

Authors: [Chaojun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.03642](https://arxiv.org/abs/2005.03642) [cs.CL]** |
|           | (or **[arXiv:2005.03642v1](https://arxiv.org/abs/2005.03642v1) [cs.CL]** for this version) |





# 2020-05-07

[Return to Index](#Index)



<h2 id="2020-05-07-1">1. Exploring Controllable Text Generation Techniques</h2>

Title: [Exploring Controllable Text Generation Techniques](https://arxiv.org/abs/2005.01822)

Authors: [Shrimai Prabhumoye](https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R)

> Neural controllable text generation is an important area gaining attention due to its plethora of applications. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. We present an overview of the various techniques used to modulate each of these five modules to provide with control of attributes in the generation process. We also provide an analysis on the advantages and disadvantages of these techniques and open paths to develop new architectures based on the combination of the modules described in this paper.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.01822](https://arxiv.org/abs/2005.01822) [cs.CL]** |
|           | (or **[arXiv:2005.01822v1](https://arxiv.org/abs/2005.01822v1) [cs.CL]** for this version) |





<h2 id="2020-05-07-2">2. Understanding Scanned Receipts</h2>

Title: [Understanding Scanned Receipts](https://arxiv.org/abs/2005.01828)

Authors: [Eric Melz](https://arxiv.org/search/cs?searchtype=author&query=Melz%2C+E)

> Tasking machines with understanding receipts can have important applications such as enabling detailed analytics on purchases, enforcing expense policies, and inferring patterns of purchase behavior on large collections of receipts. In this paper, we focus on the task of Named Entity Linking (NEL) of scanned receipt line items; specifically, the task entails associating shorthand text from OCR'd receipts with a knowledge base (KB) of grocery products. For example, the scanned item "STO BABY SPINACH" should be linked to the catalog item labeled "Simple Truth Organic Baby Spinach". Experiments that employ a variety of Information Retrieval techniques in combination with statistical phrase detection shows promise for effective understanding of scanned receipt data.

| Comments:    | 8 pages, 3 figures, no conference submission                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2005.01828](https://arxiv.org/abs/2005.01828) [cs.CL]** |
|              | (or **[arXiv:2005.01828v1](https://arxiv.org/abs/2005.01828v1) [cs.CL]** for this version) |





# 2020-05-06

[Return to Index](#Index)



<h2 id="2020-05-06-1">1. IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization</h2>

Title: [IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization](https://arxiv.org/abs/2005.02178)

Authors: [Wenxuan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Bill Yuchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B+Y), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Fine-tuning pre-trained language models (PTLMs), such as BERT and its better variant RoBERTa, has been a common practice for advancing performance in natural language understanding (NLU) tasks. Recent advance in representation learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings can significantly improve performance on downstream tasks with faster convergence and better generalization. The isotropy of the pre-trained embeddings in PTLMs, however, is relatively under-explored. In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning. This simple yet effective fine-tuning method yields about 1.0 absolute increment on the average of seven benchmark NLU tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.02178](https://arxiv.org/abs/2005.02178) [cs.CL]** |
|           | (or **[arXiv:2005.02178v1](https://arxiv.org/abs/2005.02178v1) [cs.CL]** for this version) |



<h2 id="2020-05-06-2">2. Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration</h2>

Title: [Digraph of Senegal s local languages: issues, challenges and prospects of their transliteration](https://arxiv.org/abs/2005.02325)

Authors: [Elhadji Mamadou Nguer](https://arxiv.org/search/cs?searchtype=author&query=Nguer%2C+E+M), [Diop Sokhna Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+D+S), [Yacoub Ahmed Fall](https://arxiv.org/search/cs?searchtype=author&query=Fall%2C+Y+A), [Mouhamadou Khoule](https://arxiv.org/search/cs?searchtype=author&query=Khoule%2C+M)

> The local languages in Senegal, like those of West African countries in general, are written based on two alphabets: supplemented Arabic alphabet (called Ajami) and Latin alphabet. Each writing has its own applications. Ajami writing is generally used by people educated in Koranic schools for communication, business, literature (religious texts, poetry, etc.), traditional religious medicine, etc. Writing with Latin characters is used for localization of ICT (Web, dictionaries, Windows and Google tools translated in Wolof, etc.), the translation of legal texts (commercial code and constitution translated in Wolof) and religious ones (Quran and Bible in Wolof), book edition, etc. To facilitate both populations general access to knowledge, it is useful to set up transliteration tools between these two scriptures. This work falls within the framework of the implementation of project for a collaborative online dictionary Wolof (Nguer E. M., Khoule M, Thiam M. N., Mbaye B. T., Thiare O., Cisse M. T., Mangeot M. 2014), which will involve people using Ajami writing. Our goal will consist, on the one hand in raising the issues related to the transliteration and the challenges that this will raise, and on the other one, presenting the perspectives.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | LTC 2015                                                     |
| Cite as:           | **[arXiv:2005.02325](https://arxiv.org/abs/2005.02325) [cs.CL]** |
|                    | (or **[arXiv:2005.02325v1](https://arxiv.org/abs/2005.02325v1) [cs.CL]** for this version) |



<h2 id="2020-05-06-3">3. It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information</h2>

Title: [It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](https://arxiv.org/abs/2005.02354)

Authors: [Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Sabrina J. Mielke](https://arxiv.org/search/cs?searchtype=author&query=Mielke%2C+S+J), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N)

> The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at [this https URL](https://github.com/e-bug/nmt-difficulty).

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.02354](https://arxiv.org/abs/2005.02354) [cs.CL]** |
|           | (or **[arXiv:2005.02354v1](https://arxiv.org/abs/2005.02354v1) [cs.CL]** for this version) |





# 2020-05-05

[Return to Index](#Index)



<h2 id="2020-05-05-1">1. Quantifying Attention Flow in Transformers</h2>

Title: [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)

Authors: [Samira Abnar](https://arxiv.org/search/cs?searchtype=author&query=Abnar%2C+S), [Willem Zuidema](https://arxiv.org/search/cs?searchtype=author&query=Zuidema%2C+W)

> In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00928](https://arxiv.org/abs/2005.00928) [cs.LG]** |
|           | (or **[arXiv:2005.00928v1](https://arxiv.org/abs/2005.00928v1) [cs.LG]** for this version) |





<h2 id="2020-05-05-2">2. Does Visual Self-Supervision Improve Learning of Speech Representations?</h2>

Title: [Does Visual Self-Supervision Improve Learning of Speech Representations?](https://arxiv.org/abs/2005.01400)

Authors: [Abhinav Shukla](https://arxiv.org/search/eess?searchtype=author&query=Shukla%2C+A), [Stavros Petridis](https://arxiv.org/search/eess?searchtype=author&query=Petridis%2C+S), [Maja Pantic](https://arxiv.org/search/eess?searchtype=author&query=Pantic%2C+M)

> Self-supervised learning has attracted plenty of recent research interest. However, most works are typically unimodal and there has been limited work that studies the interaction between audio and visual modalities for self-supervised learning. This work (1) investigates visual self-supervision via face reconstruction to guide the learning of audio representations; (2) proposes two audio-only self-supervision approaches for speech representation learning; (3) shows that a multi-task combination of the proposed visual and audio self-supervision is beneficial for learning richer features that are more robust in noisy conditions; (4) shows that self-supervised pretraining leads to a superior weight initialization, which is especially useful to prevent overfitting and lead to faster model convergence on smaller sized datasets. We evaluate our audio representations for emotion and speech recognition, achieving state of the art performance for both problems. Our results demonstrate the potential of visual self-supervision for audio feature learning and suggest that joint visual and audio self-supervision leads to more informative speech representations.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.01400](https://arxiv.org/abs/2005.01400) [eess.AS]** |
|           | (or **[arXiv:2005.01400v1](https://arxiv.org/abs/2005.01400v1) [eess.AS]** for this version) |





<h2 id="2020-05-05-3">3. Evaluating Robustness to Input Perturbations for Neural Machine Translation</h2>

Title: [Evaluating Robustness to Input Perturbations for Neural Machine Translation](https://arxiv.org/abs/2005.00580)

Authors: [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G), [Yaser Al-Onaizan](https://arxiv.org/search/cs?searchtype=author&query=Al-Onaizan%2C+Y)

> Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00580](https://arxiv.org/abs/2005.00580) [cs.CL]** |
|           | (or **[arXiv:2005.00580v1](https://arxiv.org/abs/2005.00580v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-4">4. From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers</h2>

Title: [From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers](https://arxiv.org/abs/2005.00633)

Authors: [Anne Lauscher](https://arxiv.org/search/cs?searchtype=author&query=Lauscher%2C+A), [Vinit Ravishankar](https://arxiv.org/search/cs?searchtype=author&query=Ravishankar%2C+V), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G)

> Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00633](https://arxiv.org/abs/2005.00633) [cs.CL]** |
|           | (or **[arXiv:2005.00633v1](https://arxiv.org/abs/2005.00633v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-5">5. Opportunistic Decoding with Timely Correction for Simultaneous Translation</h2>

Title: [Opportunistic Decoding with Timely Correction for Simultaneous Translation](https://arxiv.org/abs/2005.00675)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Kaibo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.

| Comments: | accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00675](https://arxiv.org/abs/2005.00675) [cs.CL]** |
|           | (or **[arXiv:2005.00675v1](https://arxiv.org/abs/2005.00675v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-6">6. Synthesizer: Rethinking Self-Attention in Transformer Models</h2>

Title: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D), [Da-Cheng Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+D), [Zhe Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Che Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+C)

> The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00743](https://arxiv.org/abs/2005.00743) [cs.CL]** |
|           | (or **[arXiv:2005.00743v1](https://arxiv.org/abs/2005.00743v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-7">7. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation</h2>

Title: [ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2005.00850)

Authors: [Lifu Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+L), [Richard Yuanzhe Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y), [Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.

| Comments: | ACL2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00850](https://arxiv.org/abs/2005.00850) [cs.CL]** |
|           | (or **[arXiv:2005.00850v1](https://arxiv.org/abs/2005.00850v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-8">8. Improving Non-autoregressive Neural Machine Translation with Monolingual Data</h2>

Title: [Improving Non-autoregressive Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2005.00932)

Authors: [Jiawei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P)

> Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with the goal of transferring the AR model's generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model's performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.

| Comments: | To appear in ACL 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00932](https://arxiv.org/abs/2005.00932) [cs.CL]** |
|           | (or **[arXiv:2005.00932v1](https://arxiv.org/abs/2005.00932v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-9">9. On the Inference Calibration of Neural Machine Translation</h2>

Title: [On the Inference Calibration of Neural Machine Translation]()

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.

| Comments: | Accepted by ACL2020                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.00963](https://arxiv.org/abs/2005.00963) [cs.CL]** |
|           | (or **[arXiv:2005.00963v1](https://arxiv.org/abs/2005.00963v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-10">10. Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction</h2>

Title: [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](https://arxiv.org/abs/2005.00987)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: [this https URL](https://github.com/kanekomasahiro/bert-gec).

| Comments:          | Accepted as a short paper to the 58th Annual Conference of the Association for Computational Linguistics (ACL-2020) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association for Computational Linguistics (ACL-2020)         |
| Cite as:           | **[arXiv:2005.00987](https://arxiv.org/abs/2005.00987) [cs.CL]** |
|                    | (or **[arXiv:2005.00987v1](https://arxiv.org/abs/2005.00987v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-11">11. Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation</h2>

Title: [Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation](https://arxiv.org/abs/2005.01158)

Authors: [Kshitij Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+K), [Gerard de Melo](https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G)

> In this paper, we explore the artificial generation of typographical errors based on real-world statistics. We first draw on a small set of annotated data to compute spelling error statistics. These are then invoked to introduce errors into substantially larger corpora. The generation methodology allows us to generate particularly challenging errors that require context-aware error detection. We use it to create a set of English language error detection and correction datasets. Finally, we examine the effectiveness of machine learning models for detecting and correcting errors based on this data. The datasets are available at [this http URL](http://typo.nlproc.org/)

| Comments: | Accepted for publication at LREC 2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.01158](https://arxiv.org/abs/2005.01158) [cs.CL]** |
|           | (or **[arXiv:2005.01158v1](https://arxiv.org/abs/2005.01158v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-12">12. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</h2>

Title: [On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](https://arxiv.org/abs/2005.01196)

Authors: [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Maxime Peyrard](https://arxiv.org/search/cs?searchtype=author&query=Peyrard%2C+M), [Yang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R), [Steffen Eger](https://arxiv.org/search/cs?searchtype=author&query=Eger%2C+S)

> Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish "translationese", i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses the reference-based BLEU by 5.7 correlation points.

| Comments: | ACL2020 Camera Ready                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01196](https://arxiv.org/abs/2005.01196) [cs.CL]** |
|           | (or **[arXiv:2005.01196v1](https://arxiv.org/abs/2005.01196v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-13">13. Using Context in Neural Machine Translation Training Objectives</h2>

Title: [Using Context in Neural Machine Translation Training Objectives](https://arxiv.org/abs/2005.01483)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective.
> We first sample pseudo-documents from sentence samples. We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT). This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01483](https://arxiv.org/abs/2005.01483) [cs.CL]** |
|           | (or **[arXiv:2005.01483v1](https://arxiv.org/abs/2005.01483v1) [cs.CL]** for this version) |





<h2 id="2020-05-05-14">14. Evaluating Explanation Methods for Neural Machine Translation</h2>

Title: [Evaluating Explanation Methods for Neural Machine Translation](https://arxiv.org/abs/2005.01672)

Authors: [Jierui Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.

| Comments: | Accepted to ACL 2020, 9 pages                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2005.01672](https://arxiv.org/abs/2005.01672) [cs.CL]** |
|           | (or **[arXiv:2005.01672v1](https://arxiv.org/abs/2005.01672v1) [cs.CL]** for this version) |





# 2020-05-04

[Return to Index](#Index)



<h2 id="2020-05-04-1">1. MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</h2>

Title: [MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer](https://arxiv.org/abs/2005.00052)

Authors: [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S)

> The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and achieves competitive results on question answering.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00052](https://arxiv.org/abs/2005.00052) [cs.CL]** |
|           | (or **[arXiv:2005.00052v1](https://arxiv.org/abs/2005.00052v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-2">2. Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation</h2>

Title: [Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation](https://arxiv.org/abs/2005.00283)

Authors: [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [Rejwanul Haque](https://arxiv.org/search/cs?searchtype=author&query=Haque%2C+R), [Guodong Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+G), [Federico Gaspari](https://arxiv.org/search/cs?searchtype=author&query=Gaspari%2C+F), [Maja Popovic](https://arxiv.org/search/cs?searchtype=author&query=Popovic%2C+M), [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A)

> Every day, more people are becoming infected and dying from exposure to COVID-19. Some countries in Europe like Spain, France, the UK and Italy have suffered particularly badly from the virus. Others such as Germany appear to have coped extremely well. Both health professionals and the general public are keen to receive up-to-date information on the effects of the virus, as well as treatments that have proven to be effective. In cases where language is a barrier to access of pertinent information, machine translation (MT) may help people assimilate information published in different languages. Our MT systems trained on COVID-19 data are freely available for anyone to use to help translate information published in German, French, Italian, Spanish into English, as well as the reverse direction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00283](https://arxiv.org/abs/2005.00283) [cs.CL]** |
|           | (or **[arXiv:2005.00283v1](https://arxiv.org/abs/2005.00283v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-3">3. Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation</h2>

Title: [Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](https://arxiv.org/abs/2005.00308)

Authors: [Xabier Soto](https://arxiv.org/search/cs?searchtype=author&query=Soto%2C+X), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

> Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL (2020) |
| Cite as:           | **[arXiv:2005.00308](https://arxiv.org/abs/2005.00308) [cs.CL]** |
|                    | (or **[arXiv:2005.00308v1](https://arxiv.org/abs/2005.00308v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-4">4. Identifying Necessary Elements for BERT's Multilinguality</h2>

Title: [Identifying Necessary Elements for BERT's Multilinguality](https://arxiv.org/abs/2005.00396)

Authors: [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is suprising given that mBERT does not use any kind of crosslingual signal during training. While recent literature has studied this effect, the exact reason for mBERT's multilinguality is still unknown. We aim to identify architectural properties of BERT as well as linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models and synthetic as well as natural data. Overall, we identify six elements that are potentially necessary for BERT to be multilingual. Architectural factors that contribute to multilinguality are underparameterization, shared special tokens (e.g., "[CLS]"), shared position embeddings and replacing masked tokens with random tokens. Factors related to training data that are beneficial for multilinguality are similar word order and comparability of corpora.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.00396](https://arxiv.org/abs/2005.00396) [cs.CL]** |
|           | (or **[arXiv:2005.00396v1](https://arxiv.org/abs/2005.00396v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-5">5. Defense of Word-level Adversarial Attacks via Random Substitution Encoding</h2>

Title: [Defense of Word-level Adversarial Attacks via Random Substitution Encoding](https://arxiv.org/abs/2005.00446)

Authors: [Zhaoyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hongtao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> The adversarial attacks against deep neural networks on computer version tasks has spawned many new technologies that help protect models avoiding false prediction. Recently, word-level adversarial attacks on deep models of Natural Language Processing (NLP) tasks have also demonstrated strong power, e.g., fooling a sentiment classification neural network to make wrong decision. Unfortunately, few previous literatures have discussed the defense of such word-level synonym substitution based attacks since they are hard to be perceived and detected. In this paper, we shed light on this problem and propose a novel defense framework called Random Substitution Encoding (RSE), which introduces a random substitution encoder into the training process of original neural networks. Extensive experiments on text classification tasks demonstrate the effectiveness of our framework on defense of word-level adversarial attacks, under various base and attack models.

| Comments: | 12 pages, 2 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00446](https://arxiv.org/abs/2005.00446) [cs.CL]** |
|           | (or **[arXiv:2005.00446v1](https://arxiv.org/abs/2005.00446v1) [cs.CL]** for this version) |





<h2 id="2020-05-04-6">6. Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</h2>

Title: [Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](https://arxiv.org/abs/2005.00524)

Authors: [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Yoshinari Fujinuma](https://arxiv.org/search/cs?searchtype=author&query=Fujinuma%2C+Y), [Michael J. Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+M+J), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

> Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2005.00524](https://arxiv.org/abs/2005.00524) [cs.CL]** |
|           | (or **[arXiv:2005.00524v1](https://arxiv.org/abs/2005.00524v1) [cs.CL]** for this version) |









# 2020-05-01

[Return to Index](#Index)



<h2 id="2020-05-01-1">1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation</h2>

Title: [Simulated Multiple Reference Training Improves Low-Resource Machine Translation](https://arxiv.org/abs/2004.14524)

Authors: [Huda Khayrallah](https://arxiv.org/search/cs?searchtype=author&query=Khayrallah%2C+H), [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Many valid translations exist for a given sentence, and yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce a novel MT training method that approximates the full space of possible translations by: sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser's distribution over possible tokens. With an English paraphraser, we demonstrate the effectiveness of our method in low-resource settings, with gains of 1.2 to 7 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14524](https://arxiv.org/abs/2004.14524) [cs.CL]** |
|           | (or **[arXiv:2004.14524v1](https://arxiv.org/abs/2004.14524v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-2">2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</h2>

Title: [Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://arxiv.org/abs/2004.14564)

Authors: [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> We propose the use of a sequence-to-sequence paraphraser for automatic machine translation evaluation. The paraphraser takes a human reference as input and then force-decodes and scores an MT system output. We propose training the aforementioned paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot "language pair" (e.g., Russian to Russian). We denote our paraphraser "unbiased" because the mode of our model's output probability is centered around a copy of the input sequence, which in our case represent the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT19 segment-level shared metrics task in all languages, excluding Gujarati where the model had no training data. We also explore using our model conditioned on the source instead of the reference, and find that it outperforms every quality estimation as a metric system from the WMT19 shared task on quality estimation by a statistically significant margin in every language pair.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14564](https://arxiv.org/abs/2004.14564) [cs.CL]** |
|           | (or **[arXiv:2004.14564v1](https://arxiv.org/abs/2004.14564v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-3">3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation</h2>

Title: [Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](https://arxiv.org/abs/2004.14607)

Authors: [Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Barbara Rychalska](https://arxiv.org/search/cs?searchtype=author&query=Rychalska%2C+B), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Dominika Basaj](https://arxiv.org/search/cs?searchtype=author&query=Basaj%2C+D)

> Despite increasing instances of machine translation (MT) systems including contextual information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark datasets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several baseline MT systems on the curated datasets. Surprisingly, we find that existing context-aware models do not improve discourse-related translations consistently across languages and phenomena.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14607](https://arxiv.org/abs/2004.14607) [cs.CL]** |
|           | (or **[arXiv:2004.14607v1](https://arxiv.org/abs/2004.14607v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-4">4. Capsule-Transformer for Neural Machine Translation</h2>

Title: [Capsule-Transformer for Neural Machine Translation](https://arxiv.org/abs/2004.14649)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Juncheng Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Transformer hugely benefits from its key design of the multi-head self-attention network (SAN), which extracts information from various perspectives through transforming the given input into different subspaces. However, its simple linear transformation aggregation strategy may still potentially fail to fully capture deeper contextualized information. In this paper, we thus propose the capsule-Transformer, which extends the linear transformation into a more general capsule routing algorithm by taking SAN as a special case of capsule network. So that the resulted capsule-Transformer is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words. Specifically, we see groups of attention weights in SAN as low layer capsules. By applying the iterative capsule routing algorithm they can be further aggregated into high layer capsules which contain deeper contextualized information. Experimental results on the widely-used machine translation datasets show our proposed capsule-Transformer outperforms strong Transformer baseline significantly.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14649](https://arxiv.org/abs/2004.14649) [cs.CL]** |
|           | (or **[arXiv:2004.14649v1](https://arxiv.org/abs/2004.14649v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-5">5. End-to-End Neural Word Alignment Outperforms GIZA++</h2>

Title: [End-to-End Neural Word Alignment Outperforms GIZA++](https://arxiv.org/abs/2004.14675)

Authors: [Thomas Zenkel](https://arxiv.org/search/cs?searchtype=author&query=Zenkel%2C+T), [Joern Wuebker](https://arxiv.org/search/cs?searchtype=author&query=Wuebker%2C+J), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J)

> Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14675](https://arxiv.org/abs/2004.14675) [cs.CL]** |
|           | (or **[arXiv:2004.14675v1](https://arxiv.org/abs/2004.14675v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-6">6. Character-Level Translation with Self-attention</h2>

Title: [Character-Level Translation with Self-attention](https://arxiv.org/abs/2004.14788)

Authors: [Yingqiang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Nikola I. Nikolov](https://arxiv.org/search/cs?searchtype=author&query=Nikolov%2C+N+I), [Yuhuang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Richard H.R. Hahnloser](https://arxiv.org/search/cs?searchtype=author&query=Hahnloser%2C+R+H)

> We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14788](https://arxiv.org/abs/2004.14788) [cs.CL]** |
|           | (or **[arXiv:2004.14788v1](https://arxiv.org/abs/2004.14788v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-7">7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation</h2>

Title: [Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](https://arxiv.org/abs/2004.14821)

Authors: [Shoetsu Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+S), [Jin Sakuma](https://arxiv.org/search/cs?searchtype=author&query=Sakuma%2C+J), [Naoki Yoshinaga](https://arxiv.org/search/cs?searchtype=author&query=Yoshinaga%2C+N), [Masashi Toyoda](https://arxiv.org/search/cs?searchtype=author&query=Toyoda%2C+M), [Masaru Kitsuregawa](https://arxiv.org/search/cs?searchtype=author&query=Kitsuregawa%2C+M)

> Neural machine translation (NMT) models do not work well in domains different from the training data. The standard approach to this problem is to build a small parallel data in the target domain and perform domain adaptation from a source domain where massive parallel data is available. However, domain adaptation between distant domains (e.g., subtitles and research papers) does not perform effectively because of mismatches in vocabulary; it will encounter many domain-specific unknown words (e.g., `angstrom') and words whose meanings shift across domains (e.g., `conductor'). In this study, aiming to solve these vocabulary mismatches in distant domain adaptation, we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pre-trained NMT model to the target domain. Prior to fine-tuning, our method replaces word embeddings in embedding layers of the NMT model, by projecting general word embeddings induced from monolingual data in the target domain onto the source-domain embedding space. Experimental results on distant domain adaptation for English-to-Japanese translation and German-to-English translation indicate that our vocabulary adaptation improves the performance of fine-tuning by 3.6 BLEU points.

| Comments: | 8pages + citations                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14821](https://arxiv.org/abs/2004.14821) [cs.CL]** |
|           | (or **[arXiv:2004.14821v1](https://arxiv.org/abs/2004.14821v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-8">8. Accurate Word Alignment Induction from Neural Machine Translation</h2>

Title: [Accurate Word Alignment Induction from Neural Machine Translation](https://arxiv.org/abs/2004.14837)

Authors: [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Guanhua Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Despite its original goal to jointly learn to align and translate, prior researches suggest that the state-of-the-art neural machine translation model Transformer captures poor word alignment through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignment, which could only be revealed if we choose the correct decoding step and layer to induce word alignment. We propose to induce alignment with the to-be-aligned target token as the decoder input and present two simple but effective interpretation methods for word alignment induction, either through the attention weights or the leave-one-out measures. In contrast to previous studies, we find that attention weights capture better word alignment than the leave-one-out measures under our setting. Using the proposed method with attention weights, we greatly improve over fast-align on word alignment induction. Finally, we present a multi-task learning framework to train the Transformer model and show that by incorporating GIZA++ alignments into our multi-task training, we can induce significantly better alignments than GIZA++.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14837](https://arxiv.org/abs/2004.14837) [cs.CL]** |
|           | (or **[arXiv:2004.14837v1](https://arxiv.org/abs/2004.14837v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-9">9. Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation</h2>

Title: [Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation](https://arxiv.org/abs/2004.14911)

Authors: [Asa Cooper Stickland](https://arxiv.org/search/cs?searchtype=author&query=Stickland%2C+A+C), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M)

> There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task. This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model trained only on English monolingual data, BART. 2) Fine-tuning a model trained on monolingual data from 25 languages, mBART. For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings. For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time. When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14911](https://arxiv.org/abs/2004.14911) [cs.CL]** |
|           | (or **[arXiv:2004.14911v1](https://arxiv.org/abs/2004.14911v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-10">10. Bridging linguistic typology and multilingual machine translation with multi-view language representations</h2>

Title: [Bridging linguistic typology and multilingual machine translation with multi-view language representations](https://arxiv.org/abs/2004.14923)

Authors: [Arturo Oncevay](https://arxiv.org/search/cs?searchtype=author&query=Oncevay%2C+A), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.

| Comments: | 15 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14923](https://arxiv.org/abs/2004.14923) [cs.CL]** |
|           | (or **[arXiv:2004.14923v1](https://arxiv.org/abs/2004.14923v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-11">11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</h2>

Title: [Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](https://arxiv.org/abs/2004.14927)

Authors: [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that document-level context can be used to capture domain generalities when in-domain parallel data is not available. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally present experiments showing the usefulness of large context when modeling multiple domains at once.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14927](https://arxiv.org/abs/2004.14927) [cs.CL]** |
|           | (or **[arXiv:2004.14927v1](https://arxiv.org/abs/2004.14927v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-12">12. Language Model Prior for Low-Resource Neural Machine Translation</h2>

Title: [Language Model Prior for Low-Resource Neural Machine Translation](https://arxiv.org/abs/2004.14928)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM "disagrees" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14928](https://arxiv.org/abs/2004.14928) [cs.CL]** |
|           | (or **[arXiv:2004.14928v1](https://arxiv.org/abs/2004.14928v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-13">13. A Call for More Rigor in Unsupervised Cross-lingual Learning</h2>

Title: [A Call for More Rigor in Unsupervised Cross-lingual Learning](https://arxiv.org/abs/2004.14958)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

> We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.14958](https://arxiv.org/abs/2004.14958) [cs.CL]** |
|           | (or **[arXiv:2004.14958v1](https://arxiv.org/abs/2004.14958v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-14">14. Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages</h2>

Title: [Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages](https://arxiv.org/abs/2004.14963)

Authors: [Emrah Budur](https://arxiv.org/search/cs?searchtype=author&query=Budur%2C+E), [Rıza Özçelik](https://arxiv.org/search/cs?searchtype=author&query=Özçelik%2C+R), [Tunga Güngör](https://arxiv.org/search/cs?searchtype=author&query=Güngör%2C+T), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts%2C+C)

> The large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress for other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response to this for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their quality. As examples of the new issues that these datasets help us address, we assess the value of Turkish-specific embeddings and the importance of morphological parsing for developing robust Turkish NLI models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14963](https://arxiv.org/abs/2004.14963) [cs.CL]** |
|           | (or **[arXiv:2004.14963v1](https://arxiv.org/abs/2004.14963v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-15">15. Investigating Transferability in Pretrained Language Models</h2>

Title: [Investigating Transferability in Pretrained Language Models](https://arxiv.org/abs/2004.14975)

Authors: [Alex Tamkin](https://arxiv.org/search/cs?searchtype=author&query=Tamkin%2C+A), [Trisha Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+T), [Davide Giovanardi](https://arxiv.org/search/cs?searchtype=author&query=Giovanardi%2C+D), [Noah Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+N)

> While probing is a common technique for identifying knowledge in the representations of pretrained models, it is unclear whether this technique can explain the downstream success of models like BERT which are trained end-to-end during finetuning. To address this question, we compare probing with a different measure of transferability: the decrease in finetuning performance of a partially-reinitialized model. This technique reveals that in BERT, layers with high probing accuracy on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. In addition, dataset size impacts layer transferability: the less finetuning data one has, the more important the middle and later layers of BERT become. Furthermore, BERT does not simply find a better initializer for individual layers; instead, interactions between layers matter and reordering BERT's layers prior to finetuning significantly harms evaluation metrics. These results provide a way of understanding the transferability of parameters in pretrained language models, revealing the fluidity and complexity of transfer learning in these models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14975](https://arxiv.org/abs/2004.14975) [cs.CL]** |
|           | (or **[arXiv:2004.14975v1](https://arxiv.org/abs/2004.14975v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-16">16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation</h2>

Title: [Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](https://arxiv.org/abs/2004.14989)

Authors: [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Lisa Yankovskaya](https://arxiv.org/search/cs?searchtype=author&query=Yankovskaya%2C+L), [Andre Tättar](https://arxiv.org/search/cs?searchtype=author&query=Tättar%2C+A), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Following previous work on automatic paraphrasing, we assess the feasibility of improving BLEU (Papineni et al., 2002) using state-of-the-art neural paraphrasing techniques to generate additional references. We explore the extent to which diverse paraphrases can adequately cover the space of valid translations and compare to an alternative approach of generating paraphrases constrained by MT outputs. We compare both approaches to human-produced references in terms of diversity and the improvement in BLEU's correlation with human judgements of MT quality. Our experiments on the WMT19 metrics tasks for all into-English language directions show that somewhat surprisingly, the addition of diverse paraphrases, even those produced by humans, leads to only small, inconsistent changes in BLEU's correlation with human judgments, suggesting that BLEU's ability to correctly exploit multiple references is limited

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14989](https://arxiv.org/abs/2004.14989) [cs.CL]** |
|           | (or **[arXiv:2004.14989v1](https://arxiv.org/abs/2004.14989v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-17">17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning</h2>

Title: [On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](https://arxiv.org/abs/2004.15001)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J), [Vikas Bhardwaj](https://arxiv.org/search/cs?searchtype=author&query=Bhardwaj%2C+V)

> Pre-trained multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on some source language (typically English) and evaluated on a different target language. However, published results for baseline mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot cross-lingual performance varies greatly within the same fine-tuning run and between different fine-tuning runs. We recommend providing oracle scores alongside the zero-shot results: still fine-tune using English, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding the variation from bad checkpoints.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15001](https://arxiv.org/abs/2004.15001) [cs.CL]** |
|           | (or **[arXiv:2004.15001v1](https://arxiv.org/abs/2004.15001v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-18">18. When does data augmentation help generalization in NLP?</h2>

Title: [When does data augmentation help generalization in NLP?](https://arxiv.org/abs/2004.15012)

Authors: [Rohan Jha](https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+R), [Charles Lovering](https://arxiv.org/search/cs?searchtype=author&query=Lovering%2C+C), [Ellie Pavlick](https://arxiv.org/search/cs?searchtype=author&query=Pavlick%2C+E)

> Neural models often exploit superficial ("weak") features to achieve good performance, rather than deriving the more general ("strong") features that we'd prefer a model to use. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. Recent work has proposed using data augmentation--that is, generating training examples on which these weak features fail--as a means of encouraging models to prefer the stronger features. We design a series of toy learning problems to investigate the conditions under which such data augmentation is helpful. We show that augmenting with training examples on which the weak feature fails ("counterexamples") does succeed in preventing the model from relying on the weak feature, but often does not succeed in encouraging the model to use the stronger feature in general. We also find in many cases that the number of counterexamples needed to reach a given error rate is independent of the amount of training data, and that this type of data augmentation becomes less effective as the target strong feature becomes harder to learn.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15012](https://arxiv.org/abs/2004.15012) [cs.CL]** |
|           | (or **[arXiv:2004.15012v1](https://arxiv.org/abs/2004.15012v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-19">19. Imitation Attacks and Defenses for Black-box Machine Translation Systems</h2>

Title: [Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://arxiv.org/abs/2004.15015)

Authors: [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Dawn Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15015](https://arxiv.org/abs/2004.15015) [cs.CL]** |
|           | (or **[arXiv:2004.15015v1](https://arxiv.org/abs/2004.15015v1) [cs.CL]** for this version) |