# Daily arXiv: Machine Translation - Nov., 2019

# Index

- [2019-11-28](#2019-11-28)
  - [1. Simultaneous Neural Machine Translation using Connectionist Temporal Classification](#2019-11-28-1)
  - [2. word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs](#2019-11-28-2)
  - [3. Jejueo Datasets for Machine Translation and Speech Synthesis](#2019-11-28-3)
  - [4. Findings of the 2016 WMT Shared Task on Cross-lingual Pronoun Prediction](#2019-11-28-4)
- [2019-11-27](#2019-11-27)
  - [1. Neural Machine Translation with Explicit Phrase Alignment](#2019-11-27-1)
- [2019-11-26](#2019-11-26)
  - [1. Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer](#2019-11-26-1)
  - [2. JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus](#2019-11-26-2)
  - [3. Non-autoregressive Transformer by Position Learning](#2019-11-26-3)
  - [4. Learning to Reuse Translations: Guiding Neural Machine Translation with Examples](#2019-11-26-4)
  - [5. Chinese Spelling Error Detection Using a Fusion Lattice LSTM](#2019-11-26-5)
  - [6. Outbound Translation User Interface Ptakopet: A Pilot Study](#2019-11-26-6)
  - [7. Towards robust word embeddings for noisy texts](#2019-11-26-7)
  - [8. Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue](#2019-11-26-8)
- [2019-11-25](#2019-11-25)
  - [1. Factorized Multimodal Transformer for Multimodal Sequential Learning](#2019-11-25-1)
  - [2. Neuron Interaction Based Representation Composition for Neural Machine Translation](#2019-11-25-2)
  - [3. Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks](#2019-11-25-3)
- [2019-11-22](#2019-11-22)
  - [1. Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation](#2019-11-22-1)
  - [2. Generating Diverse Translation by Manipulating Multi-Head Attention](#2019-11-22-2)
  - [3. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning](#2019-11-22-3)
- [2019-11-21](#2019-11-21)
  - [1. Controlling Neural Machine Translation Formality with Synthetic Supervision](2019-11-21-1)
  - [2. Natural Language Generation Challenges for Explainable AI](2019-11-21-2)
  - [3. A Comparative Study on End-to-end Speech to Text Translation](2019-11-21-3)
  - [4. On Using SpecAugment for End-to-End Speech Translation](2019-11-21-4)
- [2019-11-20](#2019-11-20)
  - [1. A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages](#2019-11-20-1)
- [2019-11-15](#2019-11-15)
  - [1. A Massive Collection of Cross-Lingual Web-Document Pairs](#2019-11-15-1)
  - [2. RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems](#2019-11-15-2)
  - [3. Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding](#2019-11-15-3)
  - [4. Unsupervised Pre-training for Natural Language Generation: A Literature Review](#2019-11-15-4)
  - [5. Microsoft Research Asia's Systems for WMT19](#2019-11-15-5)
- [2019-11-13](#2019-11-13)
  - [1. How to Evaluate Word Representations of Informal Domain](#2019-11-13-1)
  - [2. Character-based NMT with Transformer](#2019-11-13-2)
- [2019-11-12](#2019-11-12)
  - [1. Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation](#2019-11-12-1)
  - [2. Learning to Copy for Automatic Post-Editing](#2019-11-12-2)
  - [3. A Reinforced Generation of Adversarial Samples for Neural Machine Translation](#2019-11-12-3)
  - [4. Code-Mixed to Monolingual Translation Framework](#2019-11-12-4)
  - [5. Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models](#2019-11-12-5)
  - [6. Translationese as a Language in "Multilingual" NMT](#2019-11-12-6)
  - [7. Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser](#2019-11-12-7)
  - [8. Semantic Noise Matters for Neural Natural Language Generation](#2019-11-12-8)
  - [9. Language Model-Driven Unsupervised Neural Machine Translation](#2019-11-12-9)
  - [10. BP-Transformer: Modelling Long-Range Context via Binary Partitioning](#2019-11-12-10)
  - [11. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables](#2019-11-12-11)
  - [12. Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning](#2019-11-12-12)
  - [13. Diversity by Phonetics and its Application in Neural Machine Translation](#2019-11-12-13)
- [2019-11-11](#2019-11-11)
  - [1. Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing](#2019-11-11-1)
  - [2. Low-Resource Machine Translation using Interlinear Glosses](#2019-11-11-2)
  - [3. Understanding Knowledge Distillation in Non-autoregressive Machine Translation](#2019-11-11-3)
  - [4. SubCharacter Chinese-English Neural Machine Translation with Wubi encoding](#2019-11-11-4)
  - [5. Improving Grammatical Error Correction with Machine Translation Pairs](#2019-11-11-5)
  - [6. The LIG system for the English-Czech Text Translation Task of IWSLT 2019](#2019-11-11-6)
  - [7. Should All Cross-Lingual Embeddings Speak English?](#2019-11-11-7)
  - [8. Interactive Refinement of Cross-Lingual Word Embeddings](#2019-11-11-8)
  - [9. Domain Robustness in Neural Machine Translation](#2019-11-11-9)
  - [10. Pretrained Language Models for Document-Level Neural Machine Translation](#2019-11-11-10)
  - [11. How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?](#2019-11-11-11)
  - [12. Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates](#2019-11-11-12)
  - [13. Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization](#2019-11-11-13)
  - [14. Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation](#2019-11-11-4)
- [2019-11-07](#2019-11-07)
  - [1. Fast Transformer Decoding: One Write-Head is All You Need](#2019-11-07-1)
  - [2. Unsupervised Cross-lingual Representation Learning at Scale](#2019-11-07-2)
  - [3. Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information](#2019-11-07-3)
- [2019-11-06](#2019-11-06)
  - [1. Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)](#2019-11-06-1)
  - [2. Emerging Cross-lingual Structure in Pretrained Language Models](#2019-11-06-2)
  - [3. On Compositionality in Neural Machine Translation](#2019-11-06-3)
  - [4. Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation](#2019-11-06-4)
  - [5. Adversarial Language Games for Advanced Natural Language Intelligence](#2019-11-06-5)
  - [6. Data Diversification: An Elegant Strategy For Neural Machine Translation](#2019-11-06-6)
- [2019-11-05](#2019-11-05)
  - [1. Attributed Sequence Embedding](#2019-11-05-1)
  - [2. Machine Translation Evaluation using Bi-directional Entailment](#2019-11-05-2)
  - [3. Controlling Text Complexity in Neural Machine Translation](#2019-11-05-3)
  - [4. Machine Translation in Pronunciation Space](#2019-11-05-4)
  - [5. Analysing Coreference in Transformer Outputs](#2019-11-05-5)
  - [6. Ordering Matters: Word Ordering Aware Unsupervised NMT](#2019-11-05-6)
- [2019-11-04](#2019-11-04)
  - [1. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping](#2019-11-04-1)
  - [2. Sequence Modeling with Unconstrained Generation Order](#2019-11-04-2)
  - [3. On the Linguistic Representational Power of Neural Machine Translation Models](#2019-11-04-3)
- [2019-11-01](#2019-11-01)
  - [1. Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation](#2019-11-01-1)
  - [2. Document-level Neural Machine Translation with Inter-Sentence Attention](#2019-11-01-2)
  - [3. Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019](#2019-11-01-3)
  - [4. Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness](#2019-11-01-4)
  - [5. Adversarial NLI: A New Benchmark for Natural Language Understanding](#2019-11-01-5)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-11-28

[Return to Index](#Index)



<h2 id="2019-11-28-1">1. Simultaneous Neural Machine Translation using Connectionist Temporal Classification</h2>

Title: [Simultaneous Neural Machine Translation using Connectionist Temporal Classification]( https://arxiv.org/abs/1911.11933 )

Authors:[Katsuki Chousa](https://arxiv.org/search/cs?searchtype=author&query=Chousa%2C+K), [Katsuhito Sudoh](https://arxiv.org/search/cs?searchtype=author&query=Sudoh%2C+K), [Satoshi Nakamura](https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+S)

*(Submitted on 27 Nov 2019)*

> Simultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency. We have to determine when we start the translation for observed inputs so far, to achieve good practical performance. In this work, we propose a neural machine translation method to determine this timing in an adaptive manner. The proposed method introduces a special token '<wait>', which is generated when the translation model chooses to read the next input token instead of generating an output token. It also introduces an objective function to handle the ambiguity in wait timings that can be optimized using an algorithm called Connectionist Temporal Classification (CTC). The use of CTC enables the optimization to consider all possible output sequences including '<wait>' that are equivalent to the reference translations and to choose the best one adaptively. We apply the proposed method into simultaneous translation from English to Japanese and investigate its performance and remaining problems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.11933](https://arxiv.org/abs/1911.11933) [cs.CL] |
|           | (or [arXiv:1911.11933v1](https://arxiv.org/abs/1911.11933v1) [cs.CL] for this version) |





<h2 id="2019-11-28-2">2. word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs</h2>

Title: [word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs]( https://arxiv.org/abs/1911.12019 )

Authors:[Yo Joong Choe](https://arxiv.org/search/cs?searchtype=author&query=Choe%2C+Y+J), [Kyubyong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+K), [Dongwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D)

*(Submitted on 27 Nov 2019)*

> We present word2word, a publicly available dataset and an open-source Python package for cross-lingual word translations extracted from sentence-level parallel corpora. Our dataset provides top-k word translations in 3,564 (directed) language pairs across 62 languages in OpenSubtitles2018 (Lison et al., 2018). To obtain this dataset, we use a count-based bilingual lexicon extraction model based on the observation that not only source and target words but also source words themselves can be highly correlated. We illustrate that the resulting bilingual lexicons have high coverage and attain competitive translation quality for several language pairs. We wrap our dataset and model in an easy-to-use Python library, which supports downloading and retrieving top-k word translations in any of the supported language pairs as well as computing top-k word translations for custom parallel corpora.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.12019](https://arxiv.org/abs/1911.12019) [cs.CL] |
|           | (or [arXiv:1911.12019v1](https://arxiv.org/abs/1911.12019v1) [cs.CL] for this version) |





<h2 id="2019-11-28-3">3. Jejueo Datasets for Machine Translation and Speech Synthesis</h2>

Title: [Jejueo Datasets for Machine Translation and Speech Synthesis]( https://arxiv.org/abs/1911.12071 )

Authors:[Kyubyong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+K), [Yo Joong Choe](https://arxiv.org/search/cs?searchtype=author&query=Choe%2C+Y+J), [Jiyeon Ham](https://arxiv.org/search/cs?searchtype=author&query=Ham%2C+J)

*(Submitted on 27 Nov 2019)*

> Jejueo was classified as critically endangered by UNESCO in 2010. Although diverse efforts to revitalize it have been made, there have been few computational approaches. Motivated by this, we construct two new Jejueo datasets: Jejueo Interview Transcripts (JIT) and Jejueo Single Speaker Speech (JSS). The JIT dataset is a parallel corpus containing 170k+ Jejueo-Korean sentences, and the JSS dataset consists of 10k high-quality audio files recorded by a native Jejueo speaker and a transcript file. Subsequently, we build neural systems of machine translation and speech synthesis using them. All resources are publicly available via our GitHub repository. We hope that these datasets will attract interest of both language and machine learning communities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.12071](https://arxiv.org/abs/1911.12071) [cs.CL] |
|           | (or [arXiv:1911.12071v1](https://arxiv.org/abs/1911.12071v1) [cs.CL] for this version) |





<h2 id="2019-11-28-4">4. Findings of the 2016 WMT Shared Task on Cross-lingual Pronoun Prediction</h2>

Title: [Findings of the 2016 WMT Shared Task on Cross-lingual Pronoun Prediction]( https://arxiv.org/abs/1911.12091 )

Authors:[Liane Guillou](https://arxiv.org/search/cs?searchtype=author&query=Guillou%2C+L), [Christian Hardmeier](https://arxiv.org/search/cs?searchtype=author&query=Hardmeier%2C+C), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P), [Sara Stymne](https://arxiv.org/search/cs?searchtype=author&query=Stymne%2C+S), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J), [Yannick Versley](https://arxiv.org/search/cs?searchtype=author&query=Versley%2C+Y), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Bonnie Webber](https://arxiv.org/search/cs?searchtype=author&query=Webber%2C+B), [Andrei Popescu-Belis](https://arxiv.org/search/cs?searchtype=author&query=Popescu-Belis%2C+A)

*(Submitted on 27 Nov 2019)*

> We describe the design, the evaluation setup, and the results of the 2016 WMT shared task on cross-lingual pronoun prediction. This is a classification task in which participants are asked to provide predictions on what pronoun class label should replace a placeholder value in the target-language text, provided in lemmatised and PoS-tagged form. We provided four subtasks, for the English-French and English-German language pairs, in both directions. Eleven teams participated in the shared task; nine for the English-French subtask, five for French-English, nine for English-German, and six for German-English. Most of the submissions outperformed two strong language-model based baseline systems, with systems using deep recurrent neural networks outperforming those using other architectures for most language pairs.

| Comments:          | cross-lingual pronoun prediction, WMT, shared task, English, German, French |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR) |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | WMT-2016                                                     |
| Cite as:           | [arXiv:1911.12091](https://arxiv.org/abs/1911.12091) [cs.CL] |
|                    | (or [arXiv:1911.12091v1](https://arxiv.org/abs/1911.12091v1) [cs.CL] for this version) |







# 2019-11-27

[Return to Index](#Index)



<h2 id="2019-11-27-1">1. Neural Machine Translation with Explicit Phrase Alignment</h2>
Title: [Neural Machine Translation with Explicit Phrase Alignment]( https://arxiv.org/abs/1911.11520 )

Authors: [Jiacheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Huanbo Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+H), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [FeiFei Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+F), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

*(Submitted on 26 Nov 2019)*

> While neural machine translation (NMT) has achieved state-of-the-art translation performance, it is unable to capture the alignment between the input and output during the translation process. The lack of alignment in NMT models leads to three problems: it is hard to (1) interpret the translation process, (2) impose lexical constraints, and (3) impose structural constraints. To alleviate these problems, we propose to introduce explicit phrase alignment into the translation process of arbitrary NMT models. The key idea is to build a search space similar to that of phrase-based statistical machine translation for NMT where phrase alignment is readily available. We design a new decoding algorithm that can easily impose lexical and structural constraints. Experiments show that our approach makes the translation process of NMT more interpretable without sacrificing translation quality. In addition, our approach achieves significant improvements in lexically and structurally constrained translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.11520](https://arxiv.org/abs/1911.11520) [cs.CL] |
|           | (or [arXiv:1911.11520v1](https://arxiv.org/abs/1911.11520v1) [cs.CL] for this version) |







# 2019-11-26

[Return to Index](#Index)



<h2 id="2019-11-26-1">1. Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer</h2>
Title:[Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer]( https://arxiv.org/abs/1911.10666 )

Authors: [Henghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Feng Nan](https://arxiv.org/search/cs?searchtype=author&query=Nan%2C+F), [Zhiguo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Ramesh Nallapati](https://arxiv.org/search/cs?searchtype=author&query=Nallapati%2C+R), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B)

*(Submitted on 25 Nov 2019)*

> Conversation structure is useful for both understanding the nature of conversation dynamics and for providing features for many downstream applications such as summarization of conversations. In this work, we define the problem of conversation structure modeling as identifying the parent utterance(s) to which each utterance in the conversation responds to. Previous work usually took a pair of utterances to decide whether one utterance is the parent of the other. We believe the entire ancestral history is a very important information source to make accurate prediction. Therefore, we design a novel masking mechanism to guide the ancestor flow, and leverage the transformer model to aggregate all ancestors to predict parent utterances. Our experiments are performed on the Reddit dataset (Zhang, Culbertson, and Paritosh 2017) and the Ubuntu IRC dataset (Kummerfeld et al. 2019). In addition, we also report experiments on a new larger corpus from the Reddit platform and release this dataset. We show that the proposed model, that takes into account the ancestral history of the conversation, significantly outperforms several strong baselines including the BERT model on all datasets

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.10666](https://arxiv.org/abs/1911.10666) [cs.CL] |
|           | (or [arXiv:1911.10666v1](https://arxiv.org/abs/1911.10666v1) [cs.CL] for this version) |





<h2 id="2019-11-26-2">2. JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus</h2>
Title:[JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus]( https://arxiv.org/abs/1911.10668 )

Authors: [Makoto Morishita](https://arxiv.org/search/cs?searchtype=author&query=Morishita%2C+M), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Masaaki Nagata](https://arxiv.org/search/cs?searchtype=author&query=Nagata%2C+M)

*(Submitted on 25 Nov 2019)*

> Recent machine translation algorithms mainly rely on parallel corpora. However, since the availability of parallel corpora remains limited, only some resource-rich language pairs can benefit from them. In this paper, we constructed a parallel corpus for English-Japanese, where the amount of publicly available parallel corpora is still limited. We constructed a parallel corpus by broadly crawling the web and automatically aligning parallel sentences. Our collected corpus, called JParaCrawl, amassed over 8.7 million sentence pairs. We show how it includes broader domains, and the NMT model trained with it works as a good pre-trained model for fine-tuning specific domains. The pre-training and fine-tuning approaches surpassed or achieved comparable performance to the model training from the initial state and largely reduced the training cost. Additionally, we trained the model with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes.

| Comments: | [this http URL](http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.10668](https://arxiv.org/abs/1911.10668) [cs.CL] |
|           | (or [arXiv:1911.10668v1](https://arxiv.org/abs/1911.10668v1) [cs.CL] for this version) |





<h2 id="2019-11-26-3">3. Non-autoregressive Transformer by Position Learning</h2>
Title:[Non-autoregressive Transformer by Position Learning]( https://arxiv.org/abs/1911.10677 )

Authors: [Yu Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Y), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Lei LI](https://arxiv.org/search/cs?searchtype=author&query=LI%2C+L)

*(Submitted on 25 Nov 2019)*

> Non-autoregressive models are promising on various text generation tasks. Previous work hardly considers to explicitly model the positions of generated words. However, position modeling is an essential problem in non-autoregressive text generation. In this study, we propose PNAT, which incorporates positions as a latent variable into the text generative process. Experimental results show that PNAT achieves top results on machine translation and paraphrase generation tasks, outperforming several strong baselines.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.10677](https://arxiv.org/abs/1911.10677) [cs.CL] |
|           | (or [arXiv:1911.10677v1](https://arxiv.org/abs/1911.10677v1) [cs.CL] for this version) |





<h2 id="2019-11-26-4">4. Learning to Reuse Translations: Guiding Neural Machine Translation with Examples</h2>
Title:[Learning to Reuse Translations: Guiding Neural Machine Translation with Examples]( https://arxiv.org/abs/1911.10732 )

Authors: [Qian Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Q), [Shaohui Kuang](https://arxiv.org/search/cs?searchtype=author&query=Kuang%2C+S), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

*(Submitted on 25 Nov 2019)*

> In this paper, we study the problem of enabling neural machine translation (NMT) to reuse previous translations from similar examples in target prediction. Distinguishing reusable translations from noisy segments and learning to reuse them in NMT are non-trivial. To solve these challenges, we propose an Example-Guided NMT (EGNMT) framework with two models: (1) a noise-masked encoder model that masks out noisy words according to word alignments and encodes the noise-masked sentences with an additional example encoder and (2) an auxiliary decoder model that predicts reusable words via an auxiliary decoder sharing parameters with the primary decoder. We define and implement the two models with the state-of-the-art Transformer. Experiments show that the noise-masked encoder model allows NMT to learn useful information from examples with low fuzzy match scores (FMS) while the auxiliary decoder model is good for high-FMS examples. More experiments on Chinese-English, English-German and English-Spanish translation demonstrate that the combination of the two EGNMT models can achieve improvements of up to +9 BLEU points over the baseline system and +7 BLEU points over a two-encoder Transformer.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.10732](https://arxiv.org/abs/1911.10732) [cs.CL] |
|           | (or [arXiv:1911.10732v1](https://arxiv.org/abs/1911.10732v1) [cs.CL] for this version) |





<h2 id="2019-11-26-5">5. Chinese Spelling Error Detection Using a Fusion Lattice LSTM</h2>
Title:[Chinese Spelling Error Detection Using a Fusion Lattice LSTM]( https://arxiv.org/abs/1911.10750 )

Authors: [Hao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Bing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Jianyong Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+J), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

*(Submitted on 25 Nov 2019)*

> Spelling error detection serves as a crucial preprocessing in many natural language processing applications. Due to the characteristics of Chinese Language, Chinese spelling error detection is more challenging than error detection in English. Existing methods are mainly under a pipeline framework, which artificially divides error detection process into two steps. Thus, these methods bring error propagation and cannot always work well due to the complexity of the language environment. Besides existing methods only adopt character or word information, and ignore the positive effect of fusing character, word, pinyin1 information together. We propose an LF-LSTM-CRF model, which is an extension of the LSTMCRF with word lattices and character-pinyin-fusion inputs. Our model takes advantage of the end-to-end framework to detect errors as a whole process, and dynamically integrates character, word and pinyin information. Experiments on the SIGHAN data show that our LF-LSTM-CRF outperforms existing methods with similar external resources consistently, and confirm the feasibility of adopting the end-to-end framework and the availability of integrating of character, word and pinyin information.

| Comments: | 8 pages,5 figures                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.10750](https://arxiv.org/abs/1911.10750) [cs.CL] |
|           | (or [arXiv:1911.10750v1](https://arxiv.org/abs/1911.10750v1) [cs.CL] for this version) |





<h2 id="2019-11-26-6">6. Outbound Translation User Interface Ptakopet: A Pilot Study</h2>
Title:[Outbound Translation User Interface Ptakopet: A Pilot Study]( https://arxiv.org/abs/1911.10835 )

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

*(Submitted on 25 Nov 2019)*

> It is not uncommon for Internet users to have to produce text in a foreign language they have very little knowledge of and are unable to verify the translation quality. We call the task "outbound translation" and explore it by introducing an open-source modular system Ptakopět. Its main purpose is to inspect human interaction with MT systems enhanced with additional subsystems, such as backward translation and quality estimation. We follow up with an experiment on (Czech) human annotators tasked to produce questions in a language they do not speak (German), with the help of Ptakopět. We focus on three real-world use cases (communication with IT support, describing administrative issues and asking encyclopedic questions) from which we gain insight into different strategies users take when faced with outbound translation tasks. Round trip translation is known to be unreliable for evaluating MT systems but our experimental evaluation documents that it works very well for users, at least on MT systems of mid-range quality.

| Subjects:    | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | I.2.7, H.5.2                                                 |
| ACM classes: | I.2.7; H.5.2                                                 |
| Cite as:     | [arXiv:1911.10835](https://arxiv.org/abs/1911.10835) [cs.CL] |
|              | (or [arXiv:1911.10835v1](https://arxiv.org/abs/1911.10835v1) [cs.CL] for this version) |





<h2 id="2019-11-26-7">7. Towards robust word embeddings for noisy texts</h2>
Title:[Towards robust word embeddings for noisy texts]( https://arxiv.org/abs/1911.10876 )

Authors: [Yerai Doval](https://arxiv.org/search/cs?searchtype=author&query=Doval%2C+Y), [Jesús Vilares](https://arxiv.org/search/cs?searchtype=author&query=Vilares%2C+J), [Carlos Gómez-Rodríguez](https://arxiv.org/search/cs?searchtype=author&query=Gómez-Rodríguez%2C+C)

*(Submitted on 25 Nov 2019)*

> Research on word embeddings has mainly focused on improving their performance on standard corpora, disregarding the difficulties posed by noisy texts in the form of tweets and other types of non-standard writing from social media. In this work, we propose a simple extension to the skipgram model in which we introduce the concept of bridge-words, which are artificial words added to the model to strengthen the similarity between standard words and their noisy variants. Our new embeddings outperform the state of the art on noisy texts on a wide range of evaluation tasks, both intrinsic and extrinsic, while retaining a good performance on standard texts. To the best of our knowledge, this is the first explicit approach at dealing with this type of noisy texts at the word embedding level that goes beyond the support for out-of-vocabulary words.

| Comments: | 11 pages, 1 figure, 4 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.10876](https://arxiv.org/abs/1911.10876) [cs.CL] |
|           | (or [arXiv:1911.10876v1](https://arxiv.org/abs/1911.10876v1) [cs.CL] for this version) |





<h2 id="2019-11-26-8">8. Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue</h2>
Title:[Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue]( https://arxiv.org/abs/1911.11008 )

Authors: [Jeonghyeok Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

*(Submitted on 25 Nov 2019)*

> Korean-Chinese is a low resource language pair, but Korean and Chinese have a lot in common in terms of vocabulary. Sino-Korean words, which can be converted into corresponding Chinese characters, account for more than fifty of the entire Korean vocabulary. Motivated by this, we propose a simple linguistically motivated solution to improve the performance of the Korean-to-Chinese neural machine translation model by using their common vocabulary. We adopt Chinese characters as a translation pivot by converting Sino-Korean words in Korean sentences to Chinese characters and then train the machine translation model with the converted Korean sentences as source sentences. The experimental results on Korean-to-Chinese translation demonstrate that the models with the proposed method improve translation quality up to 1.5 BLEU points in comparison to the baseline models.

| Comments:          | 9 pages                                                      |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | 33rd Pacific Asia Conference on Language, Information and Computation (PACLIC 33), pages 558-566, Hakodate, Japan, September 13-15, 2019 |
| Cite as:           | [arXiv:1911.11008](https://arxiv.org/abs/1911.11008) [cs.CL] |
|                    | (or [arXiv:1911.11008v1](https://arxiv.org/abs/1911.11008v1) [cs.CL] for this version) |







# 2019-11-25

[Return to Index](#Index)



<h2 id="2019-11-25-1">1. Factorized Multimodal Transformer for Multimodal Sequential Learning</h2>
Title: [Factorized Multimodal Transformer for Multimodal Sequential Learning]( https://arxiv.org/abs/1911.09826 )

Authors: [Amir Zadeh](https://arxiv.org/search/cs?searchtype=author&query=Zadeh%2C+A), [Chengfeng Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+C), [Kelly Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+K), [Yiwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Paul Pu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P), [Soujanya Poria](https://arxiv.org/search/cs?searchtype=author&query=Poria%2C+S), [Louis-Philippe Morency](https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L)

*(Submitted on 22 Nov 2019)*

> The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably, modeling arbitrarily distributed spatio-temporal dynamics within and across modalities is the biggest challenge in this research area. In this paper, we present a new transformer model, called the Factorized Multimodal Transformer (FMT) for multimodal sequential learning. FMT inherently models the intramodal and intermodal (involving two or more modalities) dynamics within its multimodal input in a factorized manner. The proposed factorization allows for increasing the number of self-attentions to better model the multimodal phenomena at hand; without encountering difficulties during training (e.g. overfitting) even on relatively low-resource setups. All the attention mechanisms within FMT have a full time-domain receptive field which allows them to asynchronously capture long-range multimodal dynamics. In our experiments we focus on datasets that contain the three commonly studied modalities of language, vision and acoustic. We perform a wide range of experiments, spanning across 3 well-studied datasets and 21 distinct labels. FMT shows superior performance over previously proposed models, setting new state of the art in the studied datasets.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.09826](https://arxiv.org/abs/1911.09826) [cs.LG] |
|           | (or [arXiv:1911.09826v1](https://arxiv.org/abs/1911.09826v1) [cs.LG] for this version) |





<h2 id="2019-11-25-2">2. Neuron Interaction Based Representation Composition for Neural Machine Translation</h2>
Title: [Neuron Interaction Based Representation Composition for Neural Machine Translation]( https://arxiv.org/abs/1911.09877 )

Authors: [Jian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

*(Submitted on 22 Nov 2019)*

> Recent NLP studies reveal that substantial linguistic information can be attributed to single neurons, i.e., individual dimensions of the representation vectors. We hypothesize that modeling strong interactions among neurons helps to better capture complex information by composing the linguistic properties embedded in individual neurons. Starting from this intuition, we propose a novel approach to compose representations learned by different components in neural machine translation (e.g., multi-layer networks or multi-head attention), based on modeling strong interactions among neurons in the representation vectors. Specifically, we leverage bilinear pooling to model pairwise multiplicative interactions among individual neurons, and a low-rank approximation to make the model computationally feasible. We further propose extended bilinear pooling to incorporate first-order representations. Experiments on WMT14 English-German and English-French translation tasks show that our model consistently improves performances over the SOTA Transformer baseline. Further analyses demonstrate that our approach indeed captures more syntactic and semantic information as expected.

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.09877](https://arxiv.org/abs/1911.09877) [cs.CL] |
|           | (or [arXiv:1911.09877v1](https://arxiv.org/abs/1911.09877v1) [cs.CL] for this version) |





<h2 id="2019-11-25-3">3. Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks</h2>
Title: [Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks]( https://arxiv.org/abs/1911.09912 )

Authors: [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

*(Submitted on 22 Nov 2019)*

> The key challenge of multi-domain translation lies in simultaneously encoding both the general knowledge shared across domains and the particular knowledge distinctive to each domain in a unified model. Previous work shows that the standard neural machine translation (NMT) model, trained on mixed-domain data, generally captures the general knowledge, but misses the domain-specific knowledge. In response to this problem, we augment NMT model with additional domain transformation networks to transform the general representations to domain-specific representations, which are subsequently fed to the NMT decoder. To guarantee the knowledge transformation, we also propose two complementary supervision signals by leveraging the power of knowledge distillation and adversarial learning. Experimental results on several language pairs, covering both balanced and unbalanced multi-domain translation, demonstrate the effectiveness and universality of the proposed approach. Encouragingly, the proposed unified model achieves comparable results with the fine-tuning approach that requires multiple models to preserve the particular knowledge. Further analyses reveal that the domain transformation networks successfully capture the domain-specific knowledge as expected.

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.09912](https://arxiv.org/abs/1911.09912) [cs.CL] |
|           | (or [arXiv:1911.09912v1](https://arxiv.org/abs/1911.09912v1) [cs.CL] for this version) |







# 2019-11-22

[Return to Index](#Index)



<h2 id="2019-11-22-1">1. Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation</h2>
Title: [Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation]( https://arxiv.org/abs/1911.09320 )

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

*(Submitted on 21 Nov 2019)*

> Non-Autoregressive Neural Machine Translation (NAT) achieves significant decoding speedup through generating target words independently and simultaneously. However, in the context of non-autoregressive translation, the word-level cross-entropy loss cannot model the target-side sequential dependency properly, leading to its weak correlation with the translation quality. As a result, NAT tends to generate influent translations with over-translation and under-translation errors. In this paper, we propose to train NAT to minimize the Bag-of-Ngrams (BoN) difference between the model output and the reference sentence. The bag-of-ngrams training objective is differentiable and can be efficiently calculated, which encourages NAT to capture the target-side sequential dependency and correlates well with the translation quality. We validate our approach on three translation tasks and show that our approach largely outperforms the NAT baseline by about 5.0 BLEU scores on WMT14 En↔De and about 2.5 BLEU scores on WMT16 En↔Ro.

| Comments:    | AAAI 2020                                                    |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:1911.09320](https://arxiv.org/abs/1911.09320) [cs.CL] |
|              | (or [arXiv:1911.09320v1](https://arxiv.org/abs/1911.09320v1) [cs.CL] for this version) |





<h2 id="2019-11-22-2">2. Generating Diverse Translation by Manipulating Multi-Head Attention</h2>
Title: [Generating Diverse Translation by Manipulating Multi-Head Attention]( https://arxiv.org/abs/1911.09333 )

Authors: [Zewei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Xin-yu Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

*(Submitted on 21 Nov 2019)*

> Transformer model has been widely used on machine translation tasks and obtained state-of-the-art results. In this paper, we report an interesting phenomenon in its encoder-decoder multi-head attention: different attention heads of the final decoder layer align to different word translation candidates. We empirically verify this discovery and propose a method to generate diverse translations by manipulating heads. Furthermore, we make use of these diverse translations with the back-translation technique for better data augmentation. Experiment results show that our method generates diverse translations without severe drop in translation quality. Experiments also show that back-translation with these diverse translations could bring significant improvement on performance on translation tasks. An auxiliary experiment of conversation response generation task proves the effect of diversity as well.

| Comments: | Accepted by AAAI 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.09333](https://arxiv.org/abs/1911.09333) [cs.CL] |
|           | (or [arXiv:1911.09333v1](https://arxiv.org/abs/1911.09333v1) [cs.CL] for this version) |





<h2 id="2019-11-22-3">3. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning</h2>
Title: [MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning]( https://arxiv.org/abs/1911.09483 )

Authors: [Guangxiang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+G), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Zhiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Liangchen Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+L)

*(Submitted on 17 Nov 2019)*

> In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at [this https URL](https://github.com/lancopku/MUSE)

| Comments: | Achieve state-of-the-art BLEU scores on WMT14 En-De, En-Fr, and IWSLT De-En |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.09483](https://arxiv.org/abs/1911.09483) [cs.CL] |
|           | (or [arXiv:1911.09483v1](https://arxiv.org/abs/1911.09483v1) [cs.CL] for this version) |













# 2019-11-21

[Return to Index](#Index)



<h2 id="2019-11-21-1">1. Controlling Neural Machine Translation Formality with Synthetic Supervision</h2>
Title: [Controlling Neural Machine Translation Formality with Synthetic Supervision]( https://arxiv.org/abs/1911.08706 )

Authors: [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

*(Submitted on 20 Nov 2019)*

> This work aims to produce translations that convey source language content at a formality level that is appropriate for a particular audience. Framing this problem as a neural sequence-to-sequence task ideally requires training triplets consisting of a bilingual sentence pair labeled with target language formality. However, in practice, available training examples are limited to English sentence pairs of different styles, and bilingual parallel sentences of unknown formality. We introduce a novel training scheme for multi-task models that automatically generates synthetic training triplets by inferring the missing element on the fly, thus enabling end-to-end training. Comprehensive automatic and human assessments show that our best model outperforms existing models by producing translations that better match desired formality levels while preserving the source meaning.

| Comments: | Accepted at AAAI 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.08706](https://arxiv.org/abs/1911.08706) [cs.CL] |
|           | (or [arXiv:1911.08706v1](https://arxiv.org/abs/1911.08706v1) [cs.CL] for this version) |





<h2 id="2019-11-21-2">2. Natural Language Generation Challenges for Explainable AI</h2>
Title: [Natural Language Generation Challenges for Explainable AI]( https://arxiv.org/abs/1911.08794 )

Authors: [Ehud Reiter](https://arxiv.org/search/cs?searchtype=author&query=Reiter%2C+E)

*(Submitted on 20 Nov 2019)*

> Good quality explanations of artificial intelligence (XAI) reasoning must be written (and evaluated) for an explanatory purpose, targeted towards their readers, have a good narrative and causal structure, and highlight where uncertainty and data quality affect the AI output. I discuss these challenges from a Natural Language Generation (NLG) perspective, and highlight four specific NLG for XAI research challenges.

| Comments: | Presented at the NL4XAI workshop ([this https URL](https://sites.google.com/view/nl4xai2019/)) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.08794](https://arxiv.org/abs/1911.08794) [cs.CL] |
|           | (or [arXiv:1911.08794v1](https://arxiv.org/abs/1911.08794v1) [cs.CL] for this version) |





<h2 id="2019-11-21-3">3. A Comparative Study on End-to-end Speech to Text Translation</h2>
Title: [A Comparative Study on End-to-end Speech to Text Translation]( https://arxiv.org/abs/1911.08870 )

Authors: [Parnia Bahar](https://arxiv.org/search/cs?searchtype=author&query=Bahar%2C+P), [Tobias Bieschke](https://arxiv.org/search/cs?searchtype=author&query=Bieschke%2C+T), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 20 Nov 2019)*

> Recent advances in deep learning show that end-to-end speech to text translation model is a promising approach to direct the speech translation field. In this work, we provide an overview of different end-to-end architectures, as well as the usage of an auxiliary connectionist temporal classification (CTC) loss for better convergence. We also investigate on pre-training variants such as initializing different components of a model using pre-trained models, and their impact on the final performance, which gives boosts up to 4% in BLEU and 5% in TER. Our experiments are performed on 270h IWSLT TED-talks En->De, and 100h LibriSpeech Audiobooks En->Fr. We also show improvements over the current end-to-end state-of-the-art systems on both tasks.

| Comments: | 8 pages, IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Sentosa, Singapore, December 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.08870](https://arxiv.org/abs/1911.08870) [cs.CL] |
|           | (or [arXiv:1911.08870v1](https://arxiv.org/abs/1911.08870v1) [cs.CL] for this version) |





<h2 id="2019-11-21-4">4. On Using SpecAugment for End-to-End Speech Translation</h2>
Title: [On Using SpecAugment for End-to-End Speech Translation]( https://arxiv.org/abs/1911.08876 )

Authors: [Parnia Bahar](https://arxiv.org/search/cs?searchtype=author&query=Bahar%2C+P), [Albert Zeyer](https://arxiv.org/search/cs?searchtype=author&query=Zeyer%2C+A), [Ralf Schlüter](https://arxiv.org/search/cs?searchtype=author&query=Schlüter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 20 Nov 2019)*

> This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2\% \BLEU on LibriSpeech Audiobooks En->Fr and +1.2% on IWSLT TED-talks En->De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data.

| Comments: | 8 pages, International Workshop on Spoken Language Translation (IWSLT), Hong Kong, China, November 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.08876](https://arxiv.org/abs/1911.08876) [cs.CL] |
|           | (or [arXiv:1911.08876v1](https://arxiv.org/abs/1911.08876v1) [cs.CL] for this version) |





# 2019-11-20

[Return to Index](#Index)



<h2 id="2019-11-20-1">1. A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</h2>
Title: [A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages]( https://arxiv.org/abs/1911.08117 )

Authors: [Minh-Thang Luong](https://arxiv.org/search/cs?searchtype=author&query=Luong%2C+M), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P), [Min-Yen Kan](https://arxiv.org/search/cs?searchtype=author&query=Kan%2C+M)

*(Submitted on 19 Nov 2019)*

> We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | EMNLP-2010                                                   |
| Cite as:           | [arXiv:1911.08117](https://arxiv.org/abs/1911.08117) [cs.CL] |
|                    | (or [arXiv:1911.08117v1](https://arxiv.org/abs/1911.08117v1) [cs.CL] for this version) |





# 2019-11-15

[Return to Index](#Index)



<h2 id="2019-11-15-1">1. A Massive Collection of Cross-Lingual Web-Document Pairs</h2>
Title: [A Massive Collection of Cross-Lingual Web-Document Pairs]( https://arxiv.org/abs/1911.06154 )

Authors: [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

*(Submitted on 10 Nov 2019)*

> Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Small-scale efforts have been made to collect aligned document level data on a limited set of language-pairs such as English-German or on limited comparable collections such as Wikipedia. In this paper, we mine twelve snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of 54 million URL pairs from Common Crawl covering documents in 92 languages paired with English. We evaluate the quality of the dataset by measuring the quality of machine translations from models that have been trained on mined parallel sentence pairs from this aligned corpora and introduce a simple yet effective baseline for identifying these aligned documents. The objective of this dataset and paper is to foster new research in cross-lingual NLP across a variety of low, mid, and high-resource languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.06154](https://arxiv.org/abs/1911.06154) [cs.CL] |
|           | (or [arXiv:1911.06154v1](https://arxiv.org/abs/1911.06154v1) [cs.CL] for this version) |





<h2 id="2019-11-15-2">2. RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems</h2>
Title: [RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems]( https://arxiv.org/abs/1911.06155 )

Authors: [Jianmin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Yue Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Xueying Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Yu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Jiaguang Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J)

*(Submitted on 11 Nov 2019)*

> While huge efforts have been investigated in the adversarial testing of convolutional neural networks (CNN), the testing for recurrent neural networks (RNN) is still limited to the classification context and leave threats for vast sequential application domains. In this work, we propose a generic adversarial testing framework RNN-Test. First, based on the distinctive structure of RNNs, we define three novel coverage metrics to measure the testing completeness and guide the generation of adversarial inputs. Second, we propose the state inconsistency orientation to generate the perturbations by maximizing the inconsistency of the hidden states of RNN cells. Finally, we combine orientations with coverage guidance to produce minute perturbations. Given the RNN model and the sequential inputs, RNN-Test will modify one character or one word out of the whole inputs based on the perturbations obtained, so as to lead the RNN to produce wrong outputs. For evaluation, we apply RNN-Test on two models of common RNN structure - the PTB language model and the spell checker model. RNN-Test efficiently reduces the performance of the PTB language model by increasing its test perplexity by 58.11%, and finds numbers of incorrect behaviors of the spell checker model with the success rate of 73.44% on average. With our customization, RNN-Test using the redefined neuron coverage as guidance could achieve 35.71% higher perplexity than original strategy of DeepXplore.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.06155](https://arxiv.org/abs/1911.06155) [cs.CL] |
|           | (or [arXiv:1911.06155v1](https://arxiv.org/abs/1911.06155v1) [cs.CL] for this version) |





<h2 id="2019-11-15-3">3. Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding</h2>
Title: [Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding]( https://arxiv.org/abs/1911.06156 )

Authors: [Dhanasekar Sundararaman](https://arxiv.org/search/cs?searchtype=author&query=Sundararaman%2C+D), [Vivek Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+V), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Shijing Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+S), [Dinghan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Lawrence Carin](https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L)

*(Submitted on 10 Nov 2019)*

> Attention-based models have shown significant improvement over traditional algorithms in several NLP tasks. The Transformer, for instance, is an illustrative example that generates abstract representations of tokens inputted to an encoder based on their relationships to all tokens in a sequence. Recent studies have shown that although such models are capable of learning syntactic features purely by seeing examples, explicitly feeding this information to deep learning models can significantly enhance their performance. Leveraging syntactic information like part of speech (POS) may be particularly beneficial in limited training data settings for complex models such as the Transformer. We show that the syntax-infused Transformer with multiple features achieves an improvement of 0.7 BLEU when trained on the full WMT 14 English to German translation dataset and a maximum improvement of 1.99 BLEU points when trained on a fraction of the dataset. In addition, we find that the incorporation of syntax into BERT fine-tuning outperforms baseline on a number of downstream tasks from the GLUE benchmark.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.06156](https://arxiv.org/abs/1911.06156) [cs.CL] |
|           | (or [arXiv:1911.06156v1](https://arxiv.org/abs/1911.06156v1) [cs.CL] for this version) |





<h2 id="2019-11-15-4">4. Unsupervised Pre-training for Natural Language Generation: A Literature Review</h2>
Title: [Unsupervised Pre-training for Natural Language Generation: A Literature Review]( https://arxiv.org/abs/1911.06171 )

Authors: [Yuanxin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Zheng Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z)

*(Submitted on 13 Nov 2019)*

> Recently, unsupervised pre-training is gaining increasing popularity in the realm of computational linguistics, thanks to its surprising success in advancing natural language understanding (NLU) and the potential to effectively exploit large-scale unlabelled corpus. However, regardless of the success in NLU, the power of unsupervised pre-training is only partially excavated when it comes to natural language generation (NLG). The major obstacle stems from an idiosyncratic nature of NLG: Texts are usually generated based on certain context, which may vary with the target applications. As a result, it is intractable to design a universal architecture for pre-training as in NLU scenarios. Moreover, retaining the knowledge learned from pre-training when learning on the target task is also a non-trivial problem. This review summarizes the recent efforts to enhance NLG systems with unsupervised pre-training, with a special focus on the methods to catalyse the integration of pre-trained models into downstream tasks. They are classified into architecture-based methods and strategy-based methods, based on their way of handling the above obstacle. Discussions are also provided to give further insights into the relationship between these two lines of work, some informative empirical phenomenons, as well as some possible directions where future work can be devoted to.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.06171](https://arxiv.org/abs/1911.06171) [cs.CL] |
|           | (or [arXiv:1911.06171v1](https://arxiv.org/abs/1911.06171v1) [cs.CL] for this version) |





<h2 id="2019-11-15-5">5. Microsoft Research Asia's Systems for WMT19</h2>
Title: [Microsoft Research Asia's Systems for WMT19]( https://arxiv.org/abs/1911.06191 )

Authors: [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Fei Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+F), [Fei Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+F), [Weicong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Yang Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Y), [Linyuan Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+L), [Yichong Leng](https://arxiv.org/search/cs?searchtype=author&query=Leng%2C+Y), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Yiren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Jinhua Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 7 Nov 2019)*

> We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks. We won the first place for 8 of the 11 directions and the second place for the other three. Our basic systems are built on Transformer, back translation and knowledge distillation. We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA).

| Comments: | Accepted to "Fourth Conference on Machine Translation (WMT19)" |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.06191](https://arxiv.org/abs/1911.06191) [cs.CL] |
|           | (or [arXiv:1911.06191v1](https://arxiv.org/abs/1911.06191v1) [cs.CL] for this version) |



# 2019-11-13

[Return to Index](#Index)



<h2 id="2019-11-13-1">1. How to Evaluate Word Representations of Informal Domain</h2>
Title: [How to Evaluate Word Representations of Informal Domain]( https://arxiv.org/abs/1911.04669 )

Authors: [Yekun Chai](https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+Y), [Naomi Saphra](https://arxiv.org/search/cs?searchtype=author&query=Saphra%2C+N), [Adam Lopez](https://arxiv.org/search/cs?searchtype=author&query=Lopez%2C+A)

*(Submitted on 12 Nov 2019)*

> Diverse word representations have surged in most state-of-the-art natural language processing (NLP) applications. Nevertheless, how to efficiently evaluate such word embeddings in the informal domain such as Twitter or forums, remains an ongoing challenge due to the lack of sufficient evaluation dataset. We derived a large list of variant spelling pairs from UrbanDictionary with the automatic approaches of weakly-supervised pattern-based bootstrapping and self-training linear-chain conditional random field (CRF). With these extracted relation pairs we promote the odds of eliding the text normalization procedure of traditional NLP pipelines and directly adopting representations of non-standard words in the informal domain. Our code is available.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04669](https://arxiv.org/abs/1911.04669) [cs.CL] |
|           | (or [arXiv:1911.04669v1](https://arxiv.org/abs/1911.04669v1) [cs.CL] for this version) |





<h2 id="2019-11-13-2">2. Character-based NMT with Transformer</h2>
Title: [Character-based NMT with Transformer]( https://arxiv.org/abs/1911.04997 )

Authors: [Rohit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+R), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Marc Dymetman](https://arxiv.org/search/cs?searchtype=author&query=Dymetman%2C+M), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M)

*(Submitted on 12 Nov 2019)*

> Character-based translation has several appealing advantages, but its performance is in general worse than a carefully tuned BPE baseline. In this paper we study the impact of character-based input and output with the Transformer architecture. In particular, our experiments on EN-DE show that character-based Transformer models are more robust than their BPE counterpart, both when translating noisy text, and when translating text from a different domain. To obtain comparable BLEU scores in clean, in-domain data and close the gap with BPE-based models we use known techniques to train deeper Transformer models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04997](https://arxiv.org/abs/1911.04997) [cs.CL] |
|           | (or [arXiv:1911.04997v1](https://arxiv.org/abs/1911.04997v1) [cs.CL] for this version) |









# 2019-11-12

[Return to Index](#Index)



<h2 id="2019-11-12-1">1. Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation</h2>
Title: [Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation]( https://arxiv.org/abs/1911.03531 )

Authors: [Ali Fadel](https://arxiv.org/search/cs?searchtype=author&query=Fadel%2C+A), [Ibraheem Tuffaha](https://arxiv.org/search/cs?searchtype=author&query=Tuffaha%2C+I), [Bara' Al-Jawarneh](https://arxiv.org/search/cs?searchtype=author&query=Al-Jawarneh%2C+B), [Mahmoud Al-Ayyoub](https://arxiv.org/search/cs?searchtype=author&query=Al-Ayyoub%2C+M)

*(Submitted on 8 Nov 2019)*

> In this work, we present several deep learning models for the automatic diacritization of Arabic text. Our models are built using two main approaches, viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with several enhancements such as 100-hot encoding, embeddings, Conditional Random Field (CRF) and Block-Normalized Gradient (BNG). The models are tested on the only freely available benchmark dataset and the results show that our models are either better or on par with other models, which require language-dependent post-processing steps, unlike ours. Moreover, we show that diacritics in Arabic can be used to enhance the models of NLP tasks such as Machine Translation (MT) by proposing the Translation over Diacritization (ToD) approach.

| Comments: | 18 pages, 17 figures, 14 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| DOI:      | [10.18653/v1/D19-5229](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FD19-5229&v=0855f1dd) |
| Cite as:  | [arXiv:1911.03531](https://arxiv.org/abs/1911.03531) [cs.CL] |
|           | (or [arXiv:1911.03531v1](https://arxiv.org/abs/1911.03531v1) [cs.CL] for this version) |





<h2 id="2019-11-12-2">2. Learning to Copy for Automatic Post-Editing</h2>
Title: [Learning to Copy for Automatic Post-Editing]( https://arxiv.org/abs/1911.03627 )

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Huanbo Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+H), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

*(Submitted on 9 Nov 2019)*

> Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03627](https://arxiv.org/abs/1911.03627) [cs.CL] |
|           | (or [arXiv:1911.03627v1](https://arxiv.org/abs/1911.03627v1) [cs.CL] for this version) |





<h2 id="2019-11-12-3">3. A Reinforced Generation of Adversarial Samples for Neural Machine Translation</h2>
Title: [A Reinforced Generation of Adversarial Samples for Neural Machine Translation]( https://arxiv.org/abs/1911.03677 )

Authors: [Wei Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+W), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Xinyu Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

*(Submitted on 9 Nov 2019)*

> Neural machine translation systems tend to fail on less de-cent inputs despite its great efficacy, which may greatly harm the credibility of these systems. Fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial samples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g.BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial samples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, showing its capability of pitfall exposure.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03677](https://arxiv.org/abs/1911.03677) [cs.CL] |
|           | (or [arXiv:1911.03677v1](https://arxiv.org/abs/1911.03677v1) [cs.CL] for this version) |





<h2 id="2019-11-12-4">4. Code-Mixed to Monolingual Translation Framework</h2>
Title: [Code-Mixed to Monolingual Translation Framework]( https://arxiv.org/abs/1911.03772 )

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Soumil Mandal](https://arxiv.org/search/cs?searchtype=author&query=Mandal%2C+S), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 9 Nov 2019)*

> The use of multilingualism in the new generation is widespread in the form of code-mixed data on social media, and therefore a robust translation system is required for catering to the monolingual users, as well as for easier comprehension by language processing models. In this work, we present a translation framework that uses a translation-transliteration strategy for translating code-mixed data into their equivalent monolingual instances. For converting the output to a more fluent form, it is reordered using a target language model. The most important advantage of the proposed framework is that it does not require a code-mixed to monolingual parallel corpus at any point. On testing the framework, it achieved BLEU and TER scores of 16.47 and 55.45, respectively. Since the proposed framework deals with various sub-modules, we dive deeper into the importance of each of them, analyze the errors and finally, discuss some improvement strategies.

| Comments: | 6 pages, 3 figures, 2 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03772](https://arxiv.org/abs/1911.03772) [cs.CL] |
|           | (or [arXiv:1911.03772v1](https://arxiv.org/abs/1911.03772v1) [cs.CL] for this version) |





<h2 id="2019-11-12-5">5. Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models</h2>
Title: [Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models]( https://arxiv.org/abs/1911.03782 )

Authors: [Siddharth Dalmia](https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S), [Abdelrahman Mohamed](https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+A), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 9 Nov 2019)*

> Inspired by modular software design principles of independence, interchangeability, and clarity of interface, we introduce a method for enforcing encoder-decoder modularity in seq2seq models without sacrificing the overall model quality or its full differentiability. We discretize the encoder output units into a predefined interpretable vocabulary space using the Connectionist Temporal Classification (CTC) loss. Our modular systems achieve near SOTA performance on the 300h Switchboard benchmark, with WER of 8.3% and 17.6% on the SWB and CH subsets, using seq2seq models with encoder and decoder modules which are independent and interchangeable.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03782](https://arxiv.org/abs/1911.03782) [cs.CL] |
|           | (or [arXiv:1911.03782v1](https://arxiv.org/abs/1911.03782v1) [cs.CL] for this version) |





<h2 id="2019-11-12-6">6. Translationese as a Language in "Multilingual" NMT</h2>
Title: [Translationese as a Language in "Multilingual" NMT]( https://arxiv.org/abs/1911.03823 )

Authors: [Parker Riley](https://arxiv.org/search/cs?searchtype=author&query=Riley%2C+P), [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

*(Submitted on 10 Nov 2019)*

> Machine translation has an undesirable propensity to produce "translationese" artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train sentence-level classifiers to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these models using metrics to measure the degree of translationese in the output, and present an analysis of the capriciousness of heuristically-based train-data tagging.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03823](https://arxiv.org/abs/1911.03823) [cs.CL] |
|           | (or [arXiv:1911.03823v1](https://arxiv.org/abs/1911.03823v1) [cs.CL] for this version) |





<h2 id="2019-11-12-7">7. Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser</h2>
Title: [Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder Parser]( https://arxiv.org/abs/1911.03875 )

Authors: [Khalil Mrini](https://arxiv.org/search/cs?searchtype=author&query=Mrini%2C+K), [Franck Dernoncourt](https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F), [Trung Bui](https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T), [Walter Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+W), [Ndapa Nakashole](https://arxiv.org/search/cs?searchtype=author&query=Nakashole%2C+N)

*(Submitted on 10 Nov 2019)*

> Attention mechanisms have improved the performance of NLP tasks while providing for appearance of model interpretability. Self-attention is currently widely used in NLP models, however it is difficult to interpret due to the numerous attention distributions. We hypothesize that model representations can benefit from label-specific information, while facilitating interpretation of predictions. We introduce the Label Attention Layer: a new form of self-attention where attention heads represent labels. We validate our hypothesis by running experiments in constituency and dependency parsing and show our new model obtains new state-of-the-art results for both tasks on the English Penn Treebank. Our neural parser obtains 96.34 F1 score for constituency parsing, and 97.33 UAS and 96.29 LAS for dependency parsing. Additionally, our model requires fewer layers, therefore, fewer parameters compared to existing work.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03875](https://arxiv.org/abs/1911.03875) [cs.CL] |
|           | (or [arXiv:1911.03875v1](https://arxiv.org/abs/1911.03875v1) [cs.CL] for this version) |





<h2 id="2019-11-12-8">8. Semantic Noise Matters for Neural Natural Language Generation</h2>
Title: [Semantic Noise Matters for Neural Natural Language Generation]( https://arxiv.org/abs/1911.03905 )

Authors: [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [David M. Howcroft](https://arxiv.org/search/cs?searchtype=author&query=Howcroft%2C+D+M), [Verena Rieser](https://arxiv.org/search/cs?searchtype=author&query=Rieser%2C+V)

*(Submitted on 10 Nov 2019)*

> Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.

| Comments:    | In Proceedings of INLG 2019, Tokyo, Japan                    |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:1911.03905](https://arxiv.org/abs/1911.03905) [cs.CL] |
|              | (or [arXiv:1911.03905v1](https://arxiv.org/abs/1911.03905v1) [cs.CL] for this version) |





<h2 id="2019-11-12-9">9. Language Model-Driven Unsupervised Neural Machine Translation</h2>
Title: [Language Model-Driven Unsupervised Neural Machine Translation]( https://arxiv.org/abs/1911.03937 )

Authors: [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Youyuan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Ruoran Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+R), [Xiaodong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhenshuang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z)

*(Submitted on 10 Nov 2019)*

> Unsupervised neural machine translation(NMT) is associated with noise and errors in synthetic data when executing vanilla back-translations. Here, we explicitly exploits language model(LM) to drive construction of an unsupervised NMT system. This features two steps. First, we initialize NMT models using synthetic data generated via temporary statistical machine translation(SMT). Second, unlike vanilla back-translation, we formulate a weight function, that scores synthetic data at each step of subsequent iterative training; this allows unsupervised training to an improved outcome. We present the detailed mathematical construction of our method. Experimental WMT2014 English-French, and WMT2016 English-German and English-Russian translation tasks revealed that our method outperforms the best prior systems by more than 3 BLEU points.

| Comments: | 11 pages, 3 figures, 7 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03937](https://arxiv.org/abs/1911.03937) [cs.CL] |
|           | (or [arXiv:1911.03937v1](https://arxiv.org/abs/1911.03937v1) [cs.CL] for this version) |





<h2 id="2019-11-12-10">10. BP-Transformer: Modelling Long-Range Context via Binary Partitioning</h2>
Title: [BP-Transformer: Modelling Long-Range Context via Binary Partitioning]( https://arxiv.org/abs/1911.04070 )

Authors: [Zihao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Qipeng Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Q), [Quan Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Q), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Zheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z)

*(Submitted on 11 Nov 2019)*

> The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields O(k⋅nlog(n/k)) connections where k is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04070](https://arxiv.org/abs/1911.04070) [cs.CL] |
|           | (or [arXiv:1911.04070v1](https://arxiv.org/abs/1911.04070v1) [cs.CL] for this version) |





<h2 id="2019-11-12-11">11. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables</h2>
Title: [Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables]( https://arxiv.org/abs/1911.04081 )

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jamin Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+J), [Yan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Andrea Madotto](https://arxiv.org/search/cs?searchtype=author&query=Madotto%2C+A), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

*(Submitted on 11 Nov 2019)*

> Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.

| Comments: | Accepted in EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.04081](https://arxiv.org/abs/1911.04081) [cs.CL] |
|           | (or [arXiv:1911.04081v1](https://arxiv.org/abs/1911.04081v1) [cs.CL] for this version) |





<h2 id="2019-11-12-12">12. Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning</h2>
Title: [Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning]()

Authors: [Sathish Indurthi](https://arxiv.org/search/cs?searchtype=author&query=Indurthi%2C+S), [Houjeung Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+H), [Nikhil Kumar Lakumarapu](https://arxiv.org/search/cs?searchtype=author&query=Lakumarapu%2C+N+K), [Beomseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+B), [Insoo Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+I), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Chanwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C)

*(Submitted on 11 Nov 2019)*

> End-to-end Speech Translation (ST) models have several advantages such as lower latency, smaller model size, and less error compounding over conventional pipelines that combine Automatic Speech Recognition (ASR) and text Machine Translation (MT) models. However, collecting large amounts of parallel data for ST task is more difficult compared to the ASR and MT tasks. Previous studies have proposed the use of transfer learning approaches to overcome the above difficulty. These approaches benefit from weakly supervised training data, such as ASR speech-to-transcript or MT text-to-text translation pairs. However, the parameters in these models are updated independently of each task, which may lead to sub-optimal solutions. In this work, we adopt a meta-learning algorithm to train a modality agnostic multi-task model that transfers knowledge from source tasks=ASR+MT to target task=ST where ST task severely lacks data. In the meta-learning phase, the parameters of the model are exposed to vast amounts of speech transcripts (e.g., English ASR) and text translations (e.g., English-German MT). During this phase, parameters are updated in such a way to understand speech, text representations, the relation between them, as well as act as a good initialization point for the target ST task. We evaluate the proposed meta-learning approach for ST tasks on English-German (En-De) and English-French (En-Fr) language pairs from the Multilingual Speech Translation Corpus (MuST-C). Our method outperforms the previous transfer learning approaches and sets new state-of-the-art results for En-De and En-Fr ST tasks by obtaining 9.18, and 11.76 BLEU point improvements, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.04283](https://arxiv.org/abs/1911.04283) [cs.CL] |
|           | (or [arXiv:1911.04283v1](https://arxiv.org/abs/1911.04283v1) [cs.CL] for this version) |





<h2 id="2019-11-12-13">13. Diversity by Phonetics and its Application in Neural Machine Translation</h2>
Title: [Diversity by Phonetics and its Application in Neural Machine Translation]( https://arxiv.org/abs/1911.04292 )

Authors: [Abdul Rafae Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A+R), [Jia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J)

*(Submitted on 11 Nov 2019)*

> We introduce a powerful approach for Neural Machine Translation (NMT), whereby, during training and testing, together with the input we provide its phonetic encoding and the variants of such an encoding. This way we obtain very significant improvements up to 4 BLEU points over the state-of-the-art large-scale system. The phonetic encoding is the first part of our contribution, with a second being a theory that aims to understand the reason for this improvement. Our hypothesis states that the phonetic encoding helps NMT because it encodes a procedure to emphasize the difference between semantically diverse sentences. We conduct an empirical geometric validation of our hypothesis in support of which we obtain overwhelming evidence. Subsequently, as our third contribution and based on our theory, we develop artificial mechanisms that leverage during learning the hypothesized (and verified) effect phonetics. We achieve significant and consistent improvements overall language pairs and datasets: French-English, German-English, and Chinese-English in medium task IWSLT'17 and French-English in large task WMT'18 Bio, with up to 4 BLEU points over the state-of-the-art. Moreover, our approaches are more robust than baselines when evaluated on unknown out-of-domain test sets with up to a 5 BLEU point increase.

| Comments: | In [this http URL](http://openreview.net/) (28 May 2019)     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.04292](https://arxiv.org/abs/1911.04292) [cs.CL] |
|           | (or [arXiv:1911.04292v1](https://arxiv.org/abs/1911.04292v1) [cs.CL] for this version) |









# 2019-11-11

[Return to Index](#Index)



<h2 id="2019-11-11-1">1. Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing</h2>
Title: [Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing]( https://arxiv.org/abs/1911.02692 )

Authors:[Haoming Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H), [Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [Chong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Tuo Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 7 Nov 2019)*

> Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing, and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02692](https://arxiv.org/abs/1911.02692) [cs.CL] |
|           | (or [arXiv:1911.02692v1](https://arxiv.org/abs/1911.02692v1) [cs.CL] for this version) |





<h2 id="2019-11-11-2">2. Low-Resource Machine Translation using Interlinear Glosses</h2>
Title: [Low-Resource Machine Translation using Interlinear Glosses]( https://arxiv.org/abs/1911.02709 )

Authors:[Zhong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Lori Levin](https://arxiv.org/search/cs?searchtype=author&query=Levin%2C+L), [David R. Mortensen](https://arxiv.org/search/cs?searchtype=author&query=Mortensen%2C+D+R), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

*(Submitted on 7 Nov 2019)*

> Neural Machine Translation (NMT) does not handle low-resource translation well because NMT is data-hungry and low-resource languages, by their nature, have limited parallel data. Many low-resource languages are morphologically rich, which complicates matters further by increasing data sparsity. However, a good linguist is capable of building a morphological analyzer in far fewer hours than it would take to collect and translate the amount of parallel data needed for conventional NMT. We combine the benefits of both NMT and linguistic information in our work. We use morphological analyzer to automatically generate interlinear glosses with dictionary or parallel data, and translate the source text to interlinear gloss as an interlingua representation, and finally translate into the target text using NMT trained on the ODIN dataset that includes a large collection of interlinear glosses and their corresponding target translations. Our result for translating from the interlinear gloss to the target text using the entire ODIN dataset achieves a BLEU score of 35.07. And our qualitative results show positive findings in a low-resource scenario of Turkish-English translation using 865 lines of training data. Our translation system yield better results than training NMT directly from the source language to the target language in a constrained-data setting, and is helpful to produce translation with sufficiently good content and fluency when data is scarce.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02709](https://arxiv.org/abs/1911.02709) [cs.CL] |
|           | (or [arXiv:1911.02709v1](https://arxiv.org/abs/1911.02709v1) [cs.CL] for this version) |





<h2 id="2019-11-11-3">3. Understanding Knowledge Distillation in Non-autoregressive Machine Translation</h2>
Title: [Understanding Knowledge Distillation in Non-autoregressive Machine Translation]( https://arxiv.org/abs/1911.02727 )

Authors:[Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 7 Nov 2019)*

> Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02727](https://arxiv.org/abs/1911.02727) [cs.CL] |
|           | (or [arXiv:1911.02727v1](https://arxiv.org/abs/1911.02727v1) [cs.CL] for this version) |





<h2 id="2019-11-11-4">4. SubCharacter Chinese-English Neural Machine Translation with Wubi encoding</h2>
Title: [SubCharacter Chinese-English Neural Machine Translation with Wubi encoding]( https://arxiv.org/abs/1911.02737 )

Authors:[Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Feifei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+F), [Xiaodong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhenshuang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z)

*(Submitted on 7 Nov 2019)*

> Neural machine translation (NMT) is one of the best methods for understanding the differences in semantic rules between two languages. Especially for Indo-European languages, subword-level models have achieved impressive results. However, when the translation task involves Chinese, semantic granularity remains at the word and character level, so there is still need more fine-grained translation model of Chinese. In this paper, we introduce a simple and effective method for Chinese translation at the sub-character level. Our approach uses the Wubi method to translate Chinese into English; byte-pair encoding (BPE) is then applied. Our method for Chinese-English translation eliminates the need for a complicated word segmentation algorithm during preprocessing. Furthermore, our method allows for sub-character-level neural translation based on recurrent neural network (RNN) architecture, without preprocessing. The empirical results show that for Chinese-English translation tasks, our sub-character-level model has a comparable BLEU score to the subword model, despite having a much smaller vocabulary. Additionally, the small vocabulary is highly advantageous for NMT model compression.

| Comments: | 10 pages, 3 figures, 7 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02737](https://arxiv.org/abs/1911.02737) [cs.CL] |
|           | (or [arXiv:1911.02737v1](https://arxiv.org/abs/1911.02737v1) [cs.CL] for this version) |





<h2 id="2019-11-11-5">5. Improving Grammatical Error Correction with Machine Translation Pairs</h2>
Title: [Improving Grammatical Error Correction with Machine Translation Pairs]( https://arxiv.org/abs/1911.02825 )

Authors:[Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Chang Mu](https://arxiv.org/search/cs?searchtype=author&query=Mu%2C+C), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 7 Nov 2019)*

> We propose a novel data synthesis method to generate diverse error-corrected sentence pairs for improving grammatical error correction, which is based on a pair of machine translation models of different qualities (i.e., poor and good). The poor translation model resembles the ESL (English as a second language) learner and tends to generate translations of low quality in terms of fluency and grammatical correctness, while the good translation model generally generates fluent and grammatically correct translations. We build the poor and good translation model with phrase-based statistical machine translation model with decreased language model weight and neural machine translation model respectively. By taking the pair of their translations of the same sentences in a bridge language as error-corrected sentence pairs, we can construct unlimited pseudo parallel data. Our approach is capable of generating diverse fluency-improving patterns without being limited by the pre-defined rule set and the seed error-corrected data. Experimental results demonstrate the effectiveness of our approach and show that it can be combined with other synthetic data sources to yield further improvements.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02825](https://arxiv.org/abs/1911.02825) [cs.CL] |
|           | (or [arXiv:1911.02825v1](https://arxiv.org/abs/1911.02825v1) [cs.CL] for this version) |





<h2 id="2019-11-11-6">6. The LIG system for the English-Czech Text Translation Task of IWSLT 2019</h2>
Title: [The LIG system for the English-Czech Text Translation Task of IWSLT 2019]( https://arxiv.org/abs/1911.02898 )

Authors:[Loïc Vial](https://arxiv.org/search/cs?searchtype=author&query=Vial%2C+L), [Benjamin Lecouteux](https://arxiv.org/search/cs?searchtype=author&query=Lecouteux%2C+B), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 7 Nov 2019)*

> In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations: 1) without any pre-trained language model (constrained), 2) using a language model trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a language model trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own language model (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text: one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our system by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own language model trained only on the allowed parallel data seems to improve the machine translation performances only when the system is trained on the smallest dataset.

| Comments: | IWSLT 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02898](https://arxiv.org/abs/1911.02898) [cs.CL] |
|           | (or [arXiv:1911.02898v1](https://arxiv.org/abs/1911.02898v1) [cs.CL] for this version) |





<h2 id="2019-11-11-7">7. Should All Cross-Lingual Embeddings Speak English?</h2>
Title: [Should All Cross-Lingual Embeddings Speak English?]( https://arxiv.org/abs/1911.03058 )

Authors:[Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 8 Nov 2019)*

> Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction performance. Second, we both expand the current evaluation dictionary collection to include all language pairs using triangulation, and also create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embeddings baselines, based on more than just Anglocentric experiments.

| Comments: | pre-print                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03058](https://arxiv.org/abs/1911.03058) [cs.CL] |
|           | (or [arXiv:1911.03058v1](https://arxiv.org/abs/1911.03058v1) [cs.CL] for this version) |





<h2 id="2019-11-11-8">8. Interactive Refinement of Cross-Lingual Word Embeddings</h2>
Title: [Interactive Refinement of Cross-Lingual Word Embeddings]( https://arxiv.org/abs/1911.03070 )

Authors:[Michelle Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+M), [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B), [Leah Findlater](https://arxiv.org/search/cs?searchtype=author&query=Findlater%2C+L), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

*(Submitted on 8 Nov 2019)*

> Cross-lingual word embeddings transfer knowledge between languages: models trained for a high-resource language can be used in a low-resource language. These embeddings are usually trained on general-purpose corpora but used for a domain-specific task. We introduce CLIME, an interactive system that allows a user to quickly adapt cross-lingual word embeddings for a given classification problem. First, words in the vocabulary are ranked by their salience to the downstream task. Then, salient keywords are displayed on an interface. Users mark the similarity between each keyword and its nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We experiment clime on a cross-lingual text classification benchmark for four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME also improves test accuracy faster than an active learning baseline, and a simple combination of CLIME with active learning has the highest test accuracy.

| Comments: | First two authors contribute equally                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.03070](https://arxiv.org/abs/1911.03070) [cs.CL] |
|           | (or [arXiv:1911.03070v1](https://arxiv.org/abs/1911.03070v1) [cs.CL] for this version) |





<h2 id="2019-11-11-9">9. Domain Robustness in Neural Machine Translation</h2>
Title: [Domain Robustness in Neural Machine Translation]( https://arxiv.org/abs/1911.03109 )

Authors:[Mathias Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+M), [Annette Rios](https://arxiv.org/search/cs?searchtype=author&query=Rios%2C+A), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 8 Nov 2019)*

> Translating text that diverges from the training domain is a key challenge for neural machine translation (NMT). Domain robustness - the generalization of models to unseen test domains - is low compared to statistical machine translation. In this paper, we investigate the performance of NMT on out-of-domain test sets, and ways to improve it.
> We observe that hallucination (translations that are fluent but unrelated to the source) is common in out-of-domain settings, and we empirically compare methods that improve adequacy (reconstruction), out-of-domain translation (subword regularization), or robustness against adversarial examples (defensive distillation), as well as noisy channel models.
> In experiments on German to English OPUS data, and German to Romansh, a low-resource scenario, we find that several methods improve domain robustness, reconstruction standing out as a method that not only improves automatic scores, but also shows improvements in a manual assessments of adequacy, albeit at some loss in fluency. However, out-of-domain performance is still relatively low and domain robustness remains an open problem.

| Comments: | V1                                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.03109](https://arxiv.org/abs/1911.03109) [cs.CL] |
|           | (or [arXiv:1911.03109v1](https://arxiv.org/abs/1911.03109v1) [cs.CL] for this version) |





<h2 id="2019-11-11-10">10. Pretrained Language Models for Document-Level Neural Machine Translation</h2>
Title: [Pretrained Language Models for Document-Level Neural Machine Translation]( https://arxiv.org/abs/1911.03110 )

Authors:[Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 8 Nov 2019)*

> Previous work on document-level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In this paper, we investigate on using large contexts with three main contributions: (1) Different from previous work which pertrained models on large-scale sentence-level parallel corpora, we use pretrained language models, specifically BERT, which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts, which lead to comparable results on systems using small and large contexts; (3) We introduce a multi-task training for regularization to avoid models overfitting our training corpora, which further improves our systems together with a deeper encoder. Experiments are conducted on the widely used IWSLT data sets with three language pairs, i.e., Chinese--English, French--English and Spanish--English. Results show that our systems are significantly better than three previously reported document-level systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03110](https://arxiv.org/abs/1911.03110) [cs.CL] |
|           | (or [arXiv:1911.03110v1](https://arxiv.org/abs/1911.03110v1) [cs.CL] for this version) |





<h2 id="2019-11-11-11">11. How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?</h2>
Title: [How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?]( https://arxiv.org/abs/1911.03154 )

Authors:[Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 8 Nov 2019)*

> Despite the success of neural machine translation (NMT), simultaneous neural machine translation (SNMT), the task of translating in real time before a full sentence has been observed, remains challenging due to the syntactic structure difference and simultaneity requirements. In this paper, we propose a general framework to improve simultaneous translation with a pretrained consecutive neural machine translation (CNMT) model. Our framework contains two parts: prefix translation that utilizes a pretrained CNMT model to better translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in simultaneous translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03154](https://arxiv.org/abs/1911.03154) [cs.CL] |
|           | (or [arXiv:1911.03154v1](https://arxiv.org/abs/1911.03154v1) [cs.CL] for this version) |





<h2 id="2019-11-11-12">12. Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates</h2>
Title: [Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates]( https://arxiv.org/abs/1911.03167 )

Authors:[Javier Iranzo-Sánchez](https://arxiv.org/search/cs?searchtype=author&query=Iranzo-Sánchez%2C+J), [Joan Albert Silvestre-Cerdà](https://arxiv.org/search/cs?searchtype=author&query=Silvestre-Cerdà%2C+J+A), [Javier Jorge](https://arxiv.org/search/cs?searchtype=author&query=Jorge%2C+J), [Nahuel Roselló](https://arxiv.org/search/cs?searchtype=author&query=Roselló%2C+N), [Adrià Giménez](https://arxiv.org/search/cs?searchtype=author&query=Giménez%2C+A), [Albert Sanchis](https://arxiv.org/search/cs?searchtype=author&query=Sanchis%2C+A), [Jorge Civera](https://arxiv.org/search/cs?searchtype=author&query=Civera%2C+J), [Alfons Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+A)

*(Submitted on 8 Nov 2019)*

> Current research into spoken language translation (SLT) is often hampered by the lack of specific data resources for this task, as currently available SLT datasets are restricted to a limited set of language pairs. In this paper we present Europarl-ST, a novel multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. This paper describes the corpus creation process and presents a series of automatic speech recognition, machine translation and spoken language translation experiments that highlight the potential of this new resource. The corpus is released under a Creative Commons license and is freely accessible and downloadable.

| Comments: | Submitted to ICASSP2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.03167](https://arxiv.org/abs/1911.03167) [cs.CL] |
|           | (or [arXiv:1911.03167v1](https://arxiv.org/abs/1911.03167v1) [cs.CL] for this version) |





<h2 id="2019-11-11-13">13. Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization</h2>
Title: [Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization]( https://arxiv.org/abs/1911.03179 )

Authors:[Hongfei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Qiuhui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Jingyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

*(Submitted on 8 Nov 2019)*

> The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. While several previous works show that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly a Transformer model with more than 12 encoder/decoder layers fails to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation which changes the computation order of residual connection and layer normalization can effectively ease the optimization of deep Transformers. In addition, we deeply compare the subtle difference in computation order, and propose a parameter initialization method which simply puts Lipschitz restriction on the initialization of Transformers but can effectively ensure their convergence. We empirically show that with proper parameter initialization, deep Transformers with the original computation order can converge, which is quite in contrast to all previous works, and obtain significant improvements with up to 24 layers. Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders.

| Comments: | A similar work (Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention) is accepted by EMNLP 2019, but there are differences between in analysis and approaches |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:1911.03179](https://arxiv.org/abs/1911.03179) [cs.CL] |
|           | (or [arXiv:1911.03179v1](https://arxiv.org/abs/1911.03179v1) [cs.CL] for this version) |





<h2 id="2019-11-11-14">14. Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation</h2>
Title: [Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation]( https://arxiv.org/abs/1911.03362 )

Authors:[Nikolay Bogoychev](https://arxiv.org/search/cs?searchtype=author&query=Bogoychev%2C+N), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 6 Nov 2019)*

> The quality of neural machine translation can be improved by leveraging additional monolingual resources to create synthetic training data. Source-side monolingual data can be (forward-)translated into the target language for self-training; target-side monolingual data can be back-translated. It has been widely reported that back-translation delivers superior results, but could this be due to artefacts in the test sets? We perform a case study using French-English news translation task and separate test sets based on their original languages. We show that forward translation delivers superior gains in terms of BLEU on sentences that were originally in the source language, complementing previous studies which show large improvements with back-translation on sentences that were originally in the target language. To better understand when and why forward and back-translation are effective, we study the role of domains, translationese, and noise. While translationese effects are well known to influence MT evaluation, we also find evidence that news data from different languages shows subtle domain differences, which is another explanation for varying performance on different portions of the test set. We perform additional low-resource experiments which demonstrate that forward translation is more sensitive to the quality of the initial translation system than back-translation, and tends to perform worse in low-resource settings.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.03362](https://arxiv.org/abs/1911.03362) [cs.CL] |
|           | (or [arXiv:1911.03362v1](https://arxiv.org/abs/1911.03362v1) [cs.CL] for this version) |







# 2019-11-07

[Return to Index](#Index)



<h2 id="2019-11-07-1">1. Fast Transformer Decoding: One Write-Head is All You Need</h2>
Title: [Fast Transformer Decoding: One Write-Head is All You Need]( https://arxiv.org/abs/1911.02150 )

Authors:[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)

*(Submitted on 6 Nov 2019)*

> Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.

| Subjects: | **Neural and Evolutionary Computing (cs.NE)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.02150](https://arxiv.org/abs/1911.02150) [cs.NE] |
|           | (or [arXiv:1911.02150v1](https://arxiv.org/abs/1911.02150v1) [cs.NE] for this version) |



<h2 id="2019-11-07-2">2. Unsupervised Cross-lingual Representation Learning at Scale</h2>
Title: [Unsupervised Cross-lingual Representation Learning at Scale]( https://arxiv.org/abs/1911.02116 )

Authors:[Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Kartikay Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+K), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Veselin Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V)

*(Submitted on 5 Nov 2019)*

> This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.

| Comments: | 12 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02116](https://arxiv.org/abs/1911.02116) [cs.CL] |
|           | (or [arXiv:1911.02116v1](https://arxiv.org/abs/1911.02116v1) [cs.CL] for this version) |





<h2 id="2019-11-07-3">3. Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information</h2>
Title: [Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information]( https://arxiv.org/abs/1911.02215 )

Authors:[Qiu Ran](https://arxiv.org/search/cs?searchtype=author&query=Ran%2C+Q), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

*(Submitted on 6 Nov 2019)*

> Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the enormous decoding space. To address this problem, we propose a novel NAT framework named ReorderNAT which explicitly models the reordering information in the decoding procedure. We further introduce deterministic and non-deterministic decoding strategies that utilize reordering information to narrow the decoding search space in our proposed ReorderNAT. Experimental results on various widely-used datasets show that our proposed model achieves better performance compared to existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a significant speedup.

| Comments: | 12 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.02215](https://arxiv.org/abs/1911.02215) [cs.CL] |
|           | (or [arXiv:1911.02215v1](https://arxiv.org/abs/1911.02215v1) [cs.CL] for this version) |





# 2019-11-06

[Return to Index](#Index)



<h2 id="2019-11-06-1">1. Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)</h2>
Title: [Training Neural Machine Translation (NMT) Models using Tensor Train Decomposition on TensorFlow (T3F)]( https://arxiv.org/abs/1911.01933 )

Authors: [Amelia Drew](https://arxiv.org/search/cs?searchtype=author&query=Drew%2C+A), [Alexander Heinecke](https://arxiv.org/search/cs?searchtype=author&query=Heinecke%2C+A)

*(Submitted on 5 Nov 2019)*

> We implement a Tensor Train layer in the TensorFlow Neural Machine Translation (NMT) model using the t3f library. We perform training runs on the IWSLT English-Vietnamese '15 and WMT German-English '16 datasets with learning rates ∈{0.0004,0.0008,0.0012}, maximum ranks ∈{2,4,8,16} and a range of core dimensions. We compare against a target BLEU test score of 24.0, obtained by our benchmark run. For the IWSLT English-Vietnamese training, we obtain BLEU test/dev scores of 24.0/21.9 and 24.2/21.9 using core dimensions (2,2,256)×(2,2,512) with learning rate 0.0012 and rank distributions (1,4,4,1) and (1,4,16,1) respectively. These runs use 113\% and 397\% of the flops of the benchmark run respectively. We find that, of the parameters surveyed, a higher learning rate and more `rectangular' core dimensions generally produce higher BLEU scores. For the WMT German-English dataset, we obtain BLEU scores of 24.0/23.8 using core dimensions (4,4,128)×(4,4,256) with learning rate 0.0012 and rank distribution (1,2,2,1). We discuss the potential for future optimization and application of Tensor Train decomposition to other NMT models.

| Comments: | 10 pages, 2 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.01933](https://arxiv.org/abs/1911.01933) [cs.LG] |
|           | (or [arXiv:1911.01933v1](https://arxiv.org/abs/1911.01933v1) [cs.LG] for this version) |





<h2 id="2019-11-06-2">2. Emerging Cross-lingual Structure in Pretrained Language Models</h2>
Title: [Emerging Cross-lingual Structure in Pretrained Language Models]( https://arxiv.org/abs/1911.01464 )

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Veselin Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V)

*(Submitted on 4 Nov 2019)*

> We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from independently trained models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries seem to be automatically discovered and aligned during the joint training process.

| Comments: | 10 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.01464](https://arxiv.org/abs/1911.01464) [cs.CL] |
|           | (or [arXiv:1911.01464v1](https://arxiv.org/abs/1911.01464v1) [cs.CL] for this version) |





<h2 id="2019-11-06-3">3. On Compositionality in Neural Machine Translation</h2>
Title: [On Compositionality in Neural Machine Translation]( https://arxiv.org/abs/1911.01497 )

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Vaibhav Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Jaimie Callan](https://arxiv.org/search/cs?searchtype=author&query=Callan%2C+J)

*(Submitted on 4 Nov 2019)*

> We investigate two specific manifestations of compositionality in Neural Machine Translation (NMT) : (1) Productivity - the ability of the model to extend its predictions beyond the observed length in training data and (2) Systematicity - the ability of the model to systematically recombine known parts and rules. We evaluate a standard Sequence to Sequence model on tests designed to assess these two properties in NMT. We quantitatively demonstrate that inadequate temporal processing, in the form of poor encoder representations is a bottleneck for both Productivity and Systematicity. We propose a simple pre-training mechanism which alleviates model performance on the two properties and leads to a significant improvement in BLEU scores.

| Comments: | Accepted at Context and Compositionality Workshop, NeurIPS 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.01497](https://arxiv.org/abs/1911.01497) [cs.CL] |
|           | (or [arXiv:1911.01497v1](https://arxiv.org/abs/1911.01497v1) [cs.CL] for this version) |





<h2 id="2019-11-06-4">4. Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation</h2>
Title: [Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation]( https://arxiv.org/abs/1911.01597 )

Authors: [Yong Shan](https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+Y), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Wen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W)

*(Submitted on 5 Nov 2019)*

> Generally, Neural Machine Translation models generate target words in a left-to-right (L2R) manner and fail to exploit any future (right) semantics information, which usually produces an unbalanced translation. Recent works attempt to utilize the right-to-left (R2L) decoder in bidirectional decoding to alleviate this problem. In this paper, we propose a novel \textbf{D}ynamic \textbf{I}nteraction \textbf{M}odule (\textbf{DIM}) to dynamically exploit target semantics from R2L translation for enhancing the L2R translation quality. Different from other bidirectional decoding approaches, DIM firstly extracts helpful target information through addressing and reading operations, then updates target semantics for tracking the interactive history. Additionally, we further introduce an \textbf{agreement regularization} term into the training objective to narrow the gap between L2R and R2L translations. Experimental results on NIST Chinese⇒English and WMT'16 English⇒Romanian translation tasks show that our system achieves significant improvements over baseline systems, which also reaches comparable results compared to the state-of-the-art Transformer model with much fewer parameters of it.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.01597](https://arxiv.org/abs/1911.01597) [cs.CL] |
|           | (or [arXiv:1911.01597v1](https://arxiv.org/abs/1911.01597v1) [cs.CL] for this version) |





<h2 id="2019-11-06-5">5. Adversarial Language Games for Advanced Natural Language Intelligence</h2>
Title: [Adversarial Language Games for Advanced Natural Language Intelligence]( https://arxiv.org/abs/1911.01622 )

Authors: [Yuan Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Haoxi Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+H), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Chaojun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Guoyang Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+G), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

*(Submitted on 5 Nov 2019)*

> While adversarial games have been well studied in various board games and electronic sports games, etc., such adversarial games remain a nearly blank field in natural language processing. As natural language is inherently an interactive game, we propose a challenging pragmatics game called Adversarial Taboo, in which an attacker and a defender compete with each other through sequential natural language interactions. The attacker is tasked with inducing the defender to speak a target word invisible to the defender, while the defender is tasked with detecting the target word before being induced by the attacker. In Adversarial Taboo, a successful attacker must hide its intention and subtly induce the defender, while a competitive defender must be cautious with its utterances and infer the intention of the attacker. To instantiate the game, we create a game environment and a competition platform. Sufficient pilot experiments and empirical studies on several baseline attack and defense strategies show promising and interesting results. Based on the analysis on the game and experiments, we discuss multiple promising directions for future research.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.01622](https://arxiv.org/abs/1911.01622) [cs.CL] |
|           | (or [arXiv:1911.01622v1](https://arxiv.org/abs/1911.01622v1) [cs.CL] for this version) |





<h2 id="2019-11-06-6">6. Data Diversification: An Elegant Strategy For Neural Machine Translation</h2>
Title: [Data Diversification: An Elegant Strategy For Neural Machine Translation]( https://arxiv.org/abs/1911.01986 )

Authors: [Xuan-Phi Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Wu Kui](https://arxiv.org/search/cs?searchtype=author&query=Kui%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T)

*(Submitted on 5 Nov 2019)*

> A common approach to improve neural machine translation is to invent new architectures. However, the research process of designing and refining such new models is often exhausting. Another approach is to resort to huge extra monolingual data to conduct semi-supervised training, like back-translation. But extra monolingual data is not always available, especially for low resource languages. In this paper, we propose to diversify the available training data by using multiple forward and backward peer models to augment the original training dataset. Our method does not require extra data like back-translation, nor additional computations and parameters like using pretrained models. Our data diversification method achieves state-of-the-art BLEU score of 30.7 in the WMT'14 English-German task. It also consistently and substantially improves translation quality in 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.01986](https://arxiv.org/abs/1911.01986) [cs.CL] |
|           | (or [arXiv:1911.01986v1](https://arxiv.org/abs/1911.01986v1) [cs.CL] for this version) |



# 2019-11-05

[Return to Index](#Index)



<h2 id="2019-11-05-1">1. Attributed Sequence Embedding</h2>
Title: [Attributed Sequence Embedding]( https://arxiv.org/abs/1911.00949 )

Authors: [Zhongfang Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z), [Xiangnan Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X), [Elke Rundensteiner](https://arxiv.org/search/cs?searchtype=author&query=Rundensteiner%2C+E), [Jihane Zouaoui](https://arxiv.org/search/cs?searchtype=author&query=Zouaoui%2C+J), [Aditya Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+A)

*(Submitted on 3 Nov 2019)*

> Mining tasks over sequential data, such as clickstreams and gene sequences, require a careful design of embeddings usable by learning algorithms. Recent research in feature learning has been extended to sequential data, where each instance consists of a sequence of heterogeneous items with a variable length. However, many real-world applications often involve attributed sequences, where each instance is composed of both a sequence of categorical items and a set of attributes. In this paper, we study this new problem of attributed sequence embedding, where the goal is to learn the representations of attributed sequences in an unsupervised fashion. This problem is core to many important data mining tasks ranging from user behavior analysis to the clustering of gene sequences. This problem is challenging due to the dependencies between sequences and their associated attributes. We propose a deep multimodal learning framework, called NAS, to produce embeddings of attributed sequences. The embeddings are task independent and can be used on various mining tasks of attributed sequences. We demonstrate the effectiveness of our embeddings of attributed sequences in various unsupervised learning tasks on real-world datasets.

| Comments: | Accepted by IEEE Big Data 2019                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.00949](https://arxiv.org/abs/1911.00949) [cs.LG] |
|           | (or [arXiv:1911.00949v1](https://arxiv.org/abs/1911.00949v1) [cs.LG] for this version) |





<h2 id="2019-11-05-2">2. Machine Translation Evaluation using Bi-directional Entailment</h2>
Title: [Machine Translation Evaluation using Bi-directional Entailment]( https://arxiv.org/abs/1911.00681 )

Authors: [Rakesh Khobragade](https://arxiv.org/search/cs?searchtype=author&query=Khobragade%2C+R), [Heaven Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+H), [Anand Namdev](https://arxiv.org/search/cs?searchtype=author&query=Namdev%2C+A), [Anish Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+A), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

*(Submitted on 2 Nov 2019)*

> In this paper, we propose a new metric for Machine Translation (MT) evaluation, based on bi-directional entailment. We show that machine generated translation can be evaluated by determining paraphrasing with a reference translation provided by a human translator. We hypothesize, and show through experiments, that paraphrasing can be detected by evaluating entailment relationship in the forward and backward direction. Unlike conventional metrics, like BLEU or METEOR, our approach uses deep learning to determine the semantic similarity between candidate and reference translation for generating scores rather than relying upon simple n-gram overlap. We use BERT's pre-trained implementation of transformer networks, fine-tuned on MNLI corpus, for natural language inferencing. We apply our evaluation metric on WMT'14 and WMT'17 dataset to evaluate systems participating in the translation task and find that our metric has a better correlation with the human annotated score compared to the other traditional metrics at system level.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.00681](https://arxiv.org/abs/1911.00681) [cs.CL] |
|           | (or [arXiv:1911.00681v1](https://arxiv.org/abs/1911.00681v1) [cs.CL] for this version) |





<h2 id="2019-11-05-3">3. Controlling Text Complexity in Neural Machine Translation</h2>
Title: [Controlling Text Complexity in Neural Machine Translation]( https://arxiv.org/abs/1911.00835 )

Authors: [Sweta Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+S), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

*(Submitted on 3 Nov 2019)*

> This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence-to-sequence models that translate Spanish into English targeted at an easier reading grade level than the original Spanish. We show that these multi-task models outperform pipeline approaches that translate and simplify text independently.

| Comments: | Accepted to EMNLP-IJCNLP 2019                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00835](https://arxiv.org/abs/1911.00835) [cs.CL] |
|           | (or [arXiv:1911.00835v1](https://arxiv.org/abs/1911.00835v1) [cs.CL] for this version) |





<h2 id="2019-11-05-4">4. Machine Translation in Pronunciation Space</h2>
Title: [Machine Translation in Pronunciation Space]( https://arxiv.org/abs/1911.00932 )

Authors: [Hairong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

*(Submitted on 3 Nov 2019)*

> The research in machine translation community focus on translation in text space. However, humans are in fact also good at direct translation in pronunciation space. Some existing translation systems, such as simultaneous machine translation, are inherently more natural and thus potentially more robust by directly translating in pronunciation space. In this paper, we conduct large scale experiments on a self-built dataset with about 20M En-Zh pairs of text sentences and corresponding pronunciation sentences. We proposed three new categories of translations: 1) translating a pronunciation sentence in source language into a pronunciation sentence in target language (P2P-Tran), 2) translating a text sentence in source language into a pronunciation sentence in target language (T2P-Tran), and 3) translating a pronunciation sentence in source language into a text sentence in target language (P2T-Tran), and compare them with traditional text translation (T2T-Tran). Our experiments clearly show that all 4 categories of translations have comparable performances, with small and sometimes ignorable differences.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1911.00932](https://arxiv.org/abs/1911.00932) [cs.CL] |
|           | (or [arXiv:1911.00932v1](https://arxiv.org/abs/1911.00932v1) [cs.CL] for this version) |





<h2 id="2019-11-05-5">5. Analysing Coreference in Transformer Outputs</h2>
Title: [Analysing Coreference in Transformer Outputs]( https://arxiv.org/abs/1911.01188 )

Authors: [Ekaterina Lapshinova-Koltunski](https://arxiv.org/search/cs?searchtype=author&query=Lapshinova-Koltunski%2C+E), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 4 Nov 2019)*

> We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare system performance on two different genres: news and TED talks. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and human translations. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.

| Comments:          | 12 pages, 1 figure                                           |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019) |
| Cite as:           | [arXiv:1911.01188](https://arxiv.org/abs/1911.01188) [cs.CL] |
|                    | (or [arXiv:1911.01188v1](https://arxiv.org/abs/1911.01188v1) [cs.CL] for this version) |





<h2 id="2019-11-05-6">6. Ordering Matters: Word Ordering Aware Unsupervised NMT</h2>
Title: [Ordering Matters: Word Ordering Aware Unsupervised NMT]( https://arxiv.org/abs/1911.01212 )

Authors: [Tamali Banerjee](https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+T), [Rudra Murthy V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+R+M), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

*(Submitted on 30 Oct 2019)*

> Denoising-based Unsupervised Neural Machine Translation (U-NMT) models typically employ denoising strategy at the encoder module to prevent the model from memorizing the input source sentence. Specifically, given an input sentence of length n, the model applies n/2 random swaps between consecutive words and trains the denoising-based U-NMT model. Though effective, applying denoising strategy on every sentence in the training data leads to uncertainty in the model thereby, limiting the benefits from the denoising-based U-NMT model. In this paper, we propose a simple fine-tuning strategy where we fine-tune the trained denoising-based U-NMT system without the denoising strategy. The input sentences are presented as is i.e., without any shuffling noise added. We observe significant improvements in translation performance on many language pairs from our fine-tuning strategy. Our analysis reveals that our proposed models lead to increase in higher n-gram BLEU score compared to the denoising U-NMT models.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:1911.01212](https://arxiv.org/abs/1911.01212) [cs.CL] |
|           | (or [arXiv:1911.01212v1](https://arxiv.org/abs/1911.01212v1) [cs.CL] for this version) |





# 2019-11-04

[Return to Index](#Index)



<h2 id="2019-11-04-1">1. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</h2>
Title: [Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping]( https://arxiv.org/abs/1911.00069 )

Authors: [Jian Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Radu Florian](https://arxiv.org/search/cs?searchtype=author&query=Florian%2C+R)

*(Submitted on 31 Oct 2019)*

> Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.

| Comments: | 11 pages, Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1911.00069](https://arxiv.org/abs/1911.00069) [cs.CL] |
|           | (or [arXiv:1911.00069v1](https://arxiv.org/abs/1911.00069v1) [cs.CL] for this version) |





<h2 id="2019-11-04-2">2. Sequence Modeling with Unconstrained Generation Order</h2>
Title: [Sequence Modeling with Unconstrained Generation Order]( https://arxiv.org/abs/1911.00176 )

Authors: [Dmitrii Emelianenko](https://arxiv.org/search/cs?searchtype=author&query=Emelianenko%2C+D), [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Pavel Serdyukov](https://arxiv.org/search/cs?searchtype=author&query=Serdyukov%2C+P)

*(Submitted on 1 Nov 2019)*

> The dominant approach to sequence generation is to produce a sequence in some predefined order, e.g. left to right. In contrast, we propose a more general model that can generate the output sequence by inserting tokens in any arbitrary order. Our model learns decoding order as a result of its training procedure. Our experiments show that this model is superior to fixed order models on a number of sequence generation tasks, such as Machine Translation, Image-to-LaTeX and Image Captioning.

| Comments: | Camera-ready version for NeurIPS2019                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00176](https://arxiv.org/abs/1911.00176) [cs.CL] |
|           | (or [arXiv:1911.00176v1](https://arxiv.org/abs/1911.00176v1) [cs.CL] for this version) |





<h2 id="2019-11-04-3">3. On the Linguistic Representational Power of Neural Machine Translation Models</h2>
Title: [On the Linguistic Representational Power of Neural Machine Translation Models]( https://arxiv.org/abs/1911.00317 )

Authors: [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J)

*(Submitted on 1 Nov 2019)*

> Despite the recent success of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.

| Comments: | Accepted to appear in the Journal of Computational Linguistics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1911.00317](https://arxiv.org/abs/1911.00317) [cs.CL] |
|           | (or [arXiv:1911.00317v1](https://arxiv.org/abs/1911.00317v1) [cs.CL] for this version) |





# 2019-11-01

[Return to Index](#Index)



<h2 id="2019-11-01-1">1. Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</h2>
Title: [Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation]( https://arxiv.org/abs/1910.14075 )

Authors: [Sébastien Jean](https://arxiv.org/search/cs?searchtype=author&query=Jean%2C+S), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

*(Submitted on 30 Oct 2019)*

> Most neural machine translation systems still translate sentences in isolation. To make further progress, a promising line of research additionally considers the surrounding context in order to provide the model potentially missing source-side information, as well as to maintain a coherent output. One difficulty in training such larger-context (i.e. document-level) machine translation systems is that context may be missing from many parallel examples. To circumvent this issue, two-stage approaches, in which sentence-level translations are post-edited in context, have recently been proposed. In this paper, we instead consider the viability of filling in the missing context. In particular, we consider three distinct approaches to generate the missing context: using random contexts, applying a copy heuristic or generating it with a language model. In particular, the copy heuristic significantly helps with lexical coherence, while using completely random contexts hurts performance on many long-distance linguistic phenomena. We also validate the usefulness of tagged back-translation. In addition to improving BLEU scores as expected, using back-translated data helps larger-context machine translation systems to better capture long-range phenomena.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14075](https://arxiv.org/abs/1910.14075) [cs.CL] |
|           | (or [arXiv:1910.14075v1](https://arxiv.org/abs/1910.14075v1) [cs.CL] for this version) |





<h2 id="2019-11-01-2">2. Document-level Neural Machine Translation with Inter-Sentence Attention</h2>
Title: [Document-level Neural Machine Translation with Inter-Sentence Attention]( https://arxiv.org/abs/1910.14528 )

Authors: [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Bao-liang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+B)

*(Submitted on 31 Oct 2019)*

> Standard neural machine translation (NMT) is on the assumption of document-level context independent. Most existing document-level NMT methods only focus on briefly introducing document-level information but fail to concern about selecting the most related part inside document context. The capacity of memory network for detecting the most relevant part of the current sentence from the memory provides a natural solution for the requirement of modeling document-level context by document-level NMT. In this work, we propose a Transformer NMT system with associated memory network (AMN) to both capture the document-level context and select the most salient part related to the concerned translation from the memory. Experiments on several tasks show that the proposed method significantly improves the NMT performance over strong Transformer baselines and other related studies.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14528](https://arxiv.org/abs/1910.14528) [cs.CL] |
|           | (or [arXiv:1910.14528v1](https://arxiv.org/abs/1910.14528v1) [cs.CL] for this version) |





<h2 id="2019-11-01-3">3. Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019</h2>
Title: [Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019]( https://arxiv.org/abs/1910.14539 )

Authors: [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Ioan Calapodescu](https://arxiv.org/search/cs?searchtype=author&query=Calapodescu%2C+I), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 31 Oct 2019)*

> Recently, neural models led to significant improvements in both machine translation (MT) and natural language generation tasks (NLG). However, generation of long descriptive summaries conditioned on structured data remains an open challenge. Likewise, MT that goes beyond sentence-level context is still an open issue (e.g., document-level MT or MT with metadata). To address these challenges, we propose to leverage data from both tasks and do transfer learning between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of parallel data. Then, we adapt these models to pure NLG and MT+NLG tasks by fine-tuning with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the "Document Generation and Translation" task at WNGT 2019, and ranked first in all tracks.

| Comments: | WNGT 2019 - System Description Paper                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1910.14539](https://arxiv.org/abs/1910.14539) [cs.CL] |
|           | (or [arXiv:1910.14539v1](https://arxiv.org/abs/1910.14539v1) [cs.CL] for this version) |





<h2 id="2019-11-01-4">4. Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness</h2>
Title: [Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness]( https://arxiv.org/abs/1910.14589 )

Authors: [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Ioan Calapodescu](https://arxiv.org/search/cs?searchtype=author&query=Calapodescu%2C+I), [Marc Dymetman](https://arxiv.org/search/cs?searchtype=author&query=Dymetman%2C+M), [Claude Roux](https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+C), [Jean-Luc Meunier](https://arxiv.org/search/cs?searchtype=author&query=Meunier%2C+J), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

*(Submitted on 31 Oct 2019)*

> We share a French-English parallel corpus of Foursquare restaurant reviews ([this https URL](https://europe.naverlabs.com/research/natural-language-processing/machine-translation-of-restaurant-reviews)), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.

| Comments: | WNGT 2019 Paper                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:1910.14589](https://arxiv.org/abs/1910.14589) [cs.CL] |
|           | (or [arXiv:1910.14589v1](https://arxiv.org/abs/1910.14589v1) [cs.CL] for this version) |







<h2 id="2019-11-01-5">5. Adversarial NLI: A New Benchmark for Natural Language Understanding</h2>
Title: [Adversarial NLI: A New Benchmark for Natural Language Understanding]( https://arxiv.org/abs/1910.14599 )

Authors: [Yixin Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A), [Emily Dinan](https://arxiv.org/search/cs?searchtype=author&query=Dinan%2C+E), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Jason Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+J), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

*(Submitted on 31 Oct 2019)*

> We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1910.14599](https://arxiv.org/abs/1910.14599) [cs.CL] |
|           | (or [arXiv:1910.14599v1](https://arxiv.org/abs/1910.14599v1) [cs.CL] for this version) |