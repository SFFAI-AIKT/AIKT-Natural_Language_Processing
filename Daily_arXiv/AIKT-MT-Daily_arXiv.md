# Daily arXiv: Machine Translation - May, 2021

# Index


- [2021-06-07](#2021-06-07)

  - [1. Neural semi-Markov CRF for Monolingual Word Alignment](#2021-06-07-1)
  - [2. Human-Adversarial Visual Question Answering](#2021-06-07-2)
  - [3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages](#2021-06-07-3)
  - [4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer](#2021-06-07-4)
  - [5. Grounding 'Grounding' in NLP](#2021-06-07-5)
  - [6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](#2021-06-07-6)
  - [7. Scalable Transformers for Neural Machine Translation](#2021-06-07-7)
  - [8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](#2021-06-07-8)
  - [9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](#2021-06-07-9)
- [2021-06-04](#2021-06-04)
  - [1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](#2021-06-04-1)
  - [2. Representing Syntax and Composition with Geometric Transformations](#2021-06-04-2)
  - [3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](#2021-06-04-3)
  - [4. A Dataset and Baselines for Multilingual Reply Suggestion](#2021-06-04-4)
  - [5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](#2021-06-04-5)
  - [6. Lightweight Adapter Tuning for Multilingual Speech Translation](#2021-06-04-6)
  - [7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](#2021-06-04-7)
  - [8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](#2021-06-04-8)
  - [9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](#2021-06-04-9)
  - [10. Fingerprinting Fine-tuned Language Models in the Wild](#2021-06-04-10 )
  - [11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](#2021-06-04-11)
- [2021-06-03](#2021-06-03)

  - [1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation](#2021-06-03-1)
  - [2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](#2021-06-03-2)
  - [3. Discrete Cosine Transform as Universal Sentence Encoder](#2021-06-03-3)
  - [4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](#2021-06-03-4)
  - [5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](#2021-06-03-5)
  - [6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](#2021-06-03-6)
  - [7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](#2021-06-03-7)
  - [8. Evidence-based Factual Error Correction](#2021-06-03-8)
  - [9. Is Sparse Attention more Interpretable?](#2021-06-03-9)
  - [10. End-to-End NLP Knowledge Graph Construction](#2021-06-03-10)
  - [11. IrEne: Interpretable Energy Prediction for Transformers](#2021-06-03-11)
  - [12. Lower Perplexity is Not Always Human-Like](#2021-06-03-12)
  - [13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](#2021-06-03-13)
- [2021-06-02](#2021-06-02)

  - [1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](#2021-06-02-1)
  - [2. Language Model Evaluation Beyond Perplexity](#2021-06-02-2)
  - [3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](#2021-06-02-3)
  - [4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](#2021-06-02-4)
  - [5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](#2021-06-02-5)
  - [6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](#2021-06-02-6)
  - [7. ViTA: Visual-Linguistic Translation by Aligning Object Tags](#2021-06-02-7)
  - [8. An In-depth Study on Internal Structure of Chinese Words](#2021-06-02-8)
  - [9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](#2021-06-02-9)
  - [10. DoT: An efficient Double Transformer for NLP tasks with tables](#2021-06-02-10)
  - [11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](#2021-06-02-11)
  - [12. Incorporating Visual Layout Structures for Scientific Text Classification](#2021-06-02-12)
- [2021-06-01](#2021-06-01)
  - [1. An Attention Free Transformer](#2021-06-01-1)
  - [2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](#2021-06-01-2)
  - [3. Re-evaluating Word Mover's Distance](#2021-06-01-3)
  - [4. Memory-Efficient Differentiable Transformer Architecture Search](#2021-06-01-4)
  - [5. Why does CTC result in peaky behavior?](#2021-06-01-5)
  - [6. Grammatical Error Correction as GAN-like Sequence Labeling](#2021-06-01-6)
  - [7. Predictive Representation Learning for Language Modeling](#2021-06-01-7)
  - [8. Korean-English Machine Translation with Multiple Tokenization Strategy](#2021-06-01-8)
  - [9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](#2021-06-01-9)
  - [10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](#2021-06-01-10)
  - [11. Pre-training Universal Language Representation](#2021-06-01-11)
  - [12. Fast Nearest Neighbor Machine Translation](#2021-06-01-12)
  - [13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](#2021-06-01-13)
  - [14. Attention Flows are Shapley Value Explanations](#2021-06-01-14)
  - [15. G-Transformer for Document-level Machine Translation](#2021-06-01-15)
  - [16. On Compositional Generalization of Neural Machine Translation](#2021-06-01-16)
  - [17. Transfer Learning for Sequence Generation: from Single-source to Multi-source](#2021-06-01-17)
  - [18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](#2021-06-01-18)
  - [19. Effective Batching for Recurrent Neural Network Grammars](#2021-06-01-19)
  - [20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](#2021-06-01-20)
  - [21. Verdi: Quality Estimation and Error Detection for Bilingual](#2021-06-01-21)
  - [22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](#2021-06-01-22)
  - [23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](#2021-06-01-23)
  - [24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](#2021-06-01-24)
  - [25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](#2021-06-01-25)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-06-07

[Return to Index](#Index)



<h2 id="2021-06-07-1">1. Neural semi-Markov CRF for Monolingual Word Alignment
</h2>

Title: [Neural semi-Markov CRF for Monolingual Word Alignment](https://arxiv.org/abs/2106.02569)

Authors: [Wuwei Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+W), [Chao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+C), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W)

> Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02569](https://arxiv.org/abs/2106.02569) [cs.CL]** |
|           | (or **[arXiv:2106.02569v1](https://arxiv.org/abs/2106.02569v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-2">2. Human-Adversarial Visual Question Answering
</h2>

Title: [Human-Adversarial Visual Question Answering](https://arxiv.org/abs/2106.02280)

Authors: [Sasha Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+S), [Amanpreet Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A), [Vedanuj Goswami](https://arxiv.org/search/cs?searchtype=author&query=Goswami%2C+V), [Jose Alberto Lopez Magana](https://arxiv.org/search/cs?searchtype=author&query=Magana%2C+J+A+L), [Wojciech Galuba](https://arxiv.org/search/cs?searchtype=author&query=Galuba%2C+W), [Devi Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+D), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.

| Comments: | 22 pages, 13 figures. First two authors contributed equally  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.02280](https://arxiv.org/abs/2106.02280) [cs.CV]** |
|           | (or **[arXiv:2106.02280v1](https://arxiv.org/abs/2106.02280v1) [cs.CV]** for this version) |





<h2 id="2021-06-07-3">3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages
</h2>

Title: [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://arxiv.org/abs/2106.02124)

Authors: [Abteen Ebrahimi](https://arxiv.org/search/cs?searchtype=author&query=Ebrahimi%2C+A), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world's languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for over 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02124](https://arxiv.org/abs/2106.02124) [cs.CL]** |
|           | (or **[arXiv:2106.02124v1](https://arxiv.org/abs/2106.02124v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-4">4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer
</h2>

Title: [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://arxiv.org/abs/2106.02134)

Authors: [Wasi Uddin Ahmad](https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+W+U), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Yashar Mehdad](https://arxiv.org/search/cs?searchtype=author&query=Mehdad%2C+Y)

> In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT \cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the \emph{generalized} transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.

| Comments: | ACL 2021 (camera ready)                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02134](https://arxiv.org/abs/2106.02134) [cs.CL]** |
|           | (or **[arXiv:2106.02134v1](https://arxiv.org/abs/2106.02134v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-5">5. Grounding 'Grounding' in NLP
</h2>

Title: [Grounding 'Grounding' in NLP](https://arxiv.org/abs/2106.02192)

Authors: [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines "grounding" as the process of establishing what mutual information is required for successful communication between two interlocutors -- a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term "grounding" used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding.

| Comments: | 24 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02192](https://arxiv.org/abs/2106.02192) [cs.CL]** |
|           | (or **[arXiv:2106.02192v1](https://arxiv.org/abs/2106.02192v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-6">6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore
</h2>

Title: [BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://arxiv.org/abs/2106.02208)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Jacob Parnell](https://arxiv.org/search/cs?searchtype=author&query=Parnell%2C+J), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02208](https://arxiv.org/abs/2106.02208) [cs.CL]** |
|           | (or **[arXiv:2106.02208v1](https://arxiv.org/abs/2106.02208v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-7">7. Scalable Transformers for Neural Machine Translation
</h2>

Title: [Scalable Transformers for Neural Machine Translation](https://arxiv.org/abs/2106.02242)

Authors: [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Xiaogang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Jifeng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Hongsheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Transformer has been widely adopted in Neural Machine Translation (NMT) because of its large capacity and parallel training of sequence generation. However, the deployment of Transformer is challenging because different scenarios require models of different complexities and scales. Naively training multiple Transformers is redundant in terms of both computation and memory. In this paper, we propose a novel scalable Transformers, which naturally contains sub-Transformers of different scales and have shared parameters. Each sub-Transformer can be easily obtained by cropping the parameters of the largest Transformer. A three-stage training scheme is proposed to tackle the difficulty of training the scalable Transformers, which introduces additional supervisions from word-level and sequence-level self-distillation. Extensive experiments were conducted on WMT EN-De and En-Fr to validate our proposed scalable Transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02242](https://arxiv.org/abs/2106.02242) [cs.CL]** |
|           | (or **[arXiv:2106.02242v1](https://arxiv.org/abs/2106.02242v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-8">8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene
</h2>

Title: [Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](https://arxiv.org/abs/2106.02327)

Authors: [Ruikun Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Guanhuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Xiaojun Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+X)

> The major paradigm of applying a pre-trained language model to downstream tasks is to fine-tune it on labeled task data, which often suffers instability and low performance when the labeled examples are scarce.~One way to alleviate this problem is to apply post-training on unlabeled task data before fine-tuning, adapting the pre-trained model to target domains by contrastive learning that considers either token-level or sequence-level similarity. Inspired by the success of sequence masking, we argue that both token-level and sequence-level similarities can be captured with a pair of masked sequences.~Therefore, we propose complementary random masking (CRM) to generate a pair of masked sequences from an input sequence for sequence-level contrastive learning and then develop contrastive masked language modeling (CMLM) for post-training to integrate both token-level and sequence-level contrastive learnings.~Empirical results show that CMLM surpasses several recent post-training methods in few-shot settings without the need for data augmentation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02327](https://arxiv.org/abs/2106.02327) [cs.CL]** |
|           | (or **[arXiv:2106.02327v1](https://arxiv.org/abs/2106.02327v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-9">9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings
</h2>

Title: [Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](https://arxiv.org/abs/2106.02490)

Authors: [Thomas Conley](https://arxiv.org/search/cs?searchtype=author&query=Conley%2C+T), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Artificial Neural networks are mathematical models at their core. This truismpresents some fundamental difficulty when networks are tasked with Natural Language Processing. A key problem lies in measuring the similarity or distance among vectors in NLP embedding space, since the mathematical concept of distance does not always agree with the linguistic concept. We suggest that the best way to measure linguistic distance among vectors is by employing the Language Model (LM) that created them. We introduce Language Model Distance (LMD) for measuring accuracy of vector transformations based on the Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric by applying it to a simple neural network learning the Procrustes algorithm for bilingual word mapping.

| Subjects:          | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 17th International Conference on Natural Language Processing, pages 170-174, Patna, India, December 18-21, 2020 |
| Cite as:           | **[arXiv:2106.02490](https://arxiv.org/abs/2106.02490) [cs.CL]** |
|                    | (or **[arXiv:2106.02490v1](https://arxiv.org/abs/2106.02490v1) [cs.CL]** for this version) |








# 2021-06-04

[Return to Index](#Index)



<h2 id="2021-06-04-1">1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data
</h2>

Title: [TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](https://arxiv.org/abs/2106.01797)

Authors: [Pengda Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+P), [Yuhong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Among ubiquitous multimodal data in the real world, text is the modality generated by human, while image reflects the physical world honestly. In a visual understanding application, machines are expected to understand images like human. Inspired by this, we propose a novel self-supervised learning method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual representations by fully utilizing the naturally-existing multimodal data. Our core idea of self-supervised learning is to maximize the mutual information between features extracted from multiple views of a shared context to a rational degree. Different from previous methods which only consider multiple views from a single modality, our work produces multiple views from different modalities, and jointly optimizes the mutual information for features pairs of intra-modality and inter-modality. Considering the information gap between inter-modality features pairs from data noise, we adopt a \emph{ranking-based} contrastive learning to optimize the mutual information. During evaluation, we directly use the pre-trained visual representations to complete various image classification tasks. Experimental results show that, TVDIM significantly outperforms previous visual self-supervised methods when processing the same set of images.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01797](https://arxiv.org/abs/2106.01797) [cs.CL]** |
|           | (or **[arXiv:2106.01797v1](https://arxiv.org/abs/2106.01797v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-2">2. Representing Syntax and Composition with Geometric Transformations
</h2>

Title: [Representing Syntax and Composition with Geometric Transformations](https://arxiv.org/abs/2106.01904)

Authors: [Lorenzo Bertolini](https://arxiv.org/search/cs?searchtype=author&query=Bertolini%2C+L), [Julie Weeds](https://arxiv.org/search/cs?searchtype=author&query=Weeds%2C+J), [David Weir](https://arxiv.org/search/cs?searchtype=author&query=Weir%2C+D), [Qiwei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+Q)

> The exploitation of syntactic graphs (SyGs) as a word's context has been shown to be beneficial for distributional semantic models (DSMs), both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters (compared to conventional DSMs) and suffer from data sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic relations) has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation.

| Comments: | to appear in Findings of ACL 2021                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01904](https://arxiv.org/abs/2106.01904) [cs.CL]** |
|           | (or **[arXiv:2106.01904v1](https://arxiv.org/abs/2106.01904v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-3">3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models
</h2>

Title: [The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](https://arxiv.org/abs/2106.01950)

Authors: [Ulme Wennberg](https://arxiv.org/search/cs?searchtype=author&query=Wennberg%2C+U), [Gustav Eje Henter](https://arxiv.org/search/cs?searchtype=author&query=Henter%2C+G+E)

> Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.

| Comments: | 11 pages, 8 figures, Accepted to ACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01950](https://arxiv.org/abs/2106.01950) [cs.CL]** |
|           | (or **[arXiv:2106.01950v1](https://arxiv.org/abs/2106.01950v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-4">4. A Dataset and Baselines for Multilingual Reply Suggestion
</h2>

Title: [A Dataset and Baselines for Multilingual Reply Suggestion](https://arxiv.org/abs/2106.02017)

Authors: [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Budhaditya Deb](https://arxiv.org/search/cs?searchtype=author&query=Deb%2C+B), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Milad Shokouhi](https://arxiv.org/search/cs?searchtype=author&query=Shokouhi%2C+M), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at [this https URL](https://github.com/zhangmozhi/mrs).

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.02017](https://arxiv.org/abs/2106.02017) [cs.CL]** |
|           | (or **[arXiv:2106.02017v1](https://arxiv.org/abs/2106.02017v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-5">5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning
</h2>

Title: [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://arxiv.org/abs/2106.01804)

Authors: [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Wenming Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F)

> Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.

| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ACL2021 main conference                                      |
| Cite as:           | **[arXiv:2106.01804](https://arxiv.org/abs/2106.01804) [cs.CV]** |
|                    | (or **[arXiv:2106.01804v1](https://arxiv.org/abs/2106.01804v1) [cs.CV]** for this version) |





<h2 id="2021-06-04-6">6. Lightweight Adapter Tuning for Multilingual Speech Translation
</h2>

Title: [Lightweight Adapter Tuning for Multilingual Speech Translation](https://arxiv.org/abs/2106.01463)

Authors: [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.

| Comments: | Accepted at ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01463](https://arxiv.org/abs/2106.01463) [cs.CL]** |
|           | (or **[arXiv:2106.01463v1](https://arxiv.org/abs/2106.01463v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-7">7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?
</h2>

Title: [Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](https://arxiv.org/abs/2106.01561)

Authors: [Cunxiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Pai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.

| Comments: | Accepted By ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01561](https://arxiv.org/abs/2106.01561) [cs.CL]** |
|           | (or **[arXiv:2106.01561v1](https://arxiv.org/abs/2106.01561v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-8">8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction
</h2>

Title: [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://arxiv.org/abs/2106.01609)

Authors: [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.

| Comments: | Accepted in the main conference of ACL 2021. Code: [this https URL](https://github.com/lipiji/TtT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.01609](https://arxiv.org/abs/2106.01609) [cs.CL]** |
|           | (or **[arXiv:2106.01609v1](https://arxiv.org/abs/2106.01609v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-9">9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models
</h2>

Title: [Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](https://arxiv.org/abs/2106.01623)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Zhicheng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at [this https URL](https://github.com/RUCAIBox/Few-Shot-KG2Text).

| Comments: | Accepted to ACL 2021 Findings                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01623](https://arxiv.org/abs/2106.01623) [cs.CL]** |
|           | (or **[arXiv:2106.01623v1](https://arxiv.org/abs/2106.01623v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-10">10. Fingerprinting Fine-tuned Language Models in the Wild
</h2>

Title: [Fingerprinting Fine-tuned Language Models in the Wild](https://arxiv.org/abs/2106.01703)

Authors: [Nirav Diwan](https://arxiv.org/search/cs?searchtype=author&query=Diwan%2C+N), [Tanmoy Chakravorty](https://arxiv.org/search/cs?searchtype=author&query=Chakravorty%2C+T), [Zubair Shafiq](https://arxiv.org/search/cs?searchtype=author&query=Shafiq%2C+Z)

> There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda. Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic. While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin. Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually < 10) of pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text. It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios. To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild. Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches. Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01703](https://arxiv.org/abs/2106.01703) [cs.CL]** |
|           | (or **[arXiv:2106.01703v1](https://arxiv.org/abs/2106.01703v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-11">11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer
</h2>

Title: [Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](https://arxiv.org/abs/2106.01732)

Authors: [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Wentao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Jiani Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S)

> Multilingual pre-trained models have achieved remarkable transfer performance by pre-trained on rich kinds of languages. Most of the models such as mBERT are pre-trained on unlabeled corpora. The static and contextual embeddings from the models could not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by aligning the embeddings better. We propose a pre-training task named Alignment Language Model (AlignLM), which uses the statistical alignment information as the prior knowledge to guide bilingual word prediction. We evaluate our method on multilingual machine reading comprehension and natural language interface tasks. The results show AlignLM can improve the zero-shot performance significantly on MLQA and XNLI datasets.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01732](https://arxiv.org/abs/2106.01732) [cs.CL]** |
|           | (or **[arXiv:2106.01732v1](https://arxiv.org/abs/2106.01732v1) [cs.CL]** for this version) |









# 2021-06-03

[Return to Index](#Index)



<h2 id="2021-06-03-1">1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation
</h2>

Title: [Part of Speech and Universal Dependency effects on English Arabic Machine Translation](https://arxiv.org/abs/2106.00745)

Authors: [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O), [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Dmitry Nikolaev](https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+D), [Ofek Rafaeli](https://arxiv.org/search/cs?searchtype=author&query=Rafaeli%2C+O)

> In this research paper, I will elaborate on a method to evaluate machine translation models based on their performance on underlying syntactical phenomena between English and Arabic languages. This method is especially important as such "neural" and "machine learning" are hard to fine-tune and change. Thus, finding a way to evaluate them easily and diversely would greatly help the task of bettering them.

| Comments: | 19 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00745](https://arxiv.org/abs/2106.00745) [cs.CL]** |
|           | (or **[arXiv:2106.00745v1](https://arxiv.org/abs/2106.00745v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-2">2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation
</h2>

Title: [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://arxiv.org/abs/2106.00903)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at \url{[this https URL](https://github.com/longyuewangdcu/RLFW-NAT)}.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.00903](https://arxiv.org/abs/2106.00903) [cs.CL]** |
|           | (or **[arXiv:2106.00903v1](https://arxiv.org/abs/2106.00903v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-3">3. Discrete Cosine Transform as Universal Sentence Encoder
</h2>

Title: [Discrete Cosine Transform as Universal Sentence Encoder](https://arxiv.org/abs/2106.00934)

Authors: [Nada Almarwani](https://arxiv.org/search/cs?searchtype=author&query=Almarwani%2C+N), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets.

| Comments: | to be published in ACL-IJCNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00934](https://arxiv.org/abs/2106.00934) [cs.CL]** |
|           | (or **[arXiv:2106.00934v1](https://arxiv.org/abs/2106.00934v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-4">4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation
</h2>

Title: [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://arxiv.org/abs/2106.00941)

Authors: [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Irwin King](https://arxiv.org/search/cs?searchtype=author&query=King%2C+I)

> Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT EnglishGerman and EnglishChinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.

| Comments: | ACL 2021 main conference, long paper, 11 pages               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Theory (cs.IT) |
| Cite as:  | **[arXiv:2106.00941](https://arxiv.org/abs/2106.00941) [cs.CL]** |
|           | (or **[arXiv:2106.00941v1](https://arxiv.org/abs/2106.00941v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-5">5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers
</h2>

Title: [One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](https://arxiv.org/abs/2106.01023)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.

| Comments: | Findings of ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01023](https://arxiv.org/abs/2106.01023) [cs.CL]** |
|           | (or **[arXiv:2106.01023v1](https://arxiv.org/abs/2106.01023v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-6">6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling
</h2>

Title: [Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](https://arxiv.org/abs/2106.01040)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.

| Comments: | ACL-IJCNLP 2021                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01040](https://arxiv.org/abs/2106.01040) [cs.CL]** |
|           | (or **[arXiv:2106.01040v1](https://arxiv.org/abs/2106.01040v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-7">7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?
</h2>

Title: [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://arxiv.org/abs/2106.01045)

Authors: [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Alberto Martinelli](https://arxiv.org/search/cs?searchtype=author&query=Martinelli%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.

| Comments: | Accepted at ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01045](https://arxiv.org/abs/2106.01045) [cs.CL]** |
|           | (or **[arXiv:2106.01045v1](https://arxiv.org/abs/2106.01045v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-8">8. Evidence-based Factual Error Correction
</h2>

Title: [Evidence-based Factual Error Correction](https://arxiv.org/abs/2106.01072)

Authors: [James Thorne](https://arxiv.org/search/cs?searchtype=author&query=Thorne%2C+J), [Andreas Vlachos](https://arxiv.org/search/cs?searchtype=author&query=Vlachos%2C+A)

> This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.

| Comments: | To appear at ACL2021. arXiv admin note: text overlap with [arXiv:2012.15788](https://arxiv.org/abs/2012.15788) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01072](https://arxiv.org/abs/2106.01072) [cs.CL]** |
|           | (or **[arXiv:2106.01072v1](https://arxiv.org/abs/2106.01072v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-9">9. Is Sparse Attention more Interpretable?
</h2>

Title: [Is Sparse Attention more Interpretable?](https://arxiv.org/abs/2106.01087)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Stefan Lazov](https://arxiv.org/search/cs?searchtype=author&query=Lazov%2C+S), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists -- under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of ACL-IJCNLP 2021                               |
| Cite as:           | **[arXiv:2106.01087](https://arxiv.org/abs/2106.01087) [cs.CL]** |
|                    | (or **[arXiv:2106.01087v1](https://arxiv.org/abs/2106.01087v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-10">10. End-to-End NLP Knowledge Graph Construction
</h2>

Title: [End-to-End NLP Knowledge Graph Construction](https://arxiv.org/abs/2106.01167)

Authors: [Ishani Mondal](https://arxiv.org/search/cs?searchtype=author&query=Mondal%2C+I), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Charles Jochim](https://arxiv.org/search/cs?searchtype=author&query=Jochim%2C+C)

> This paper studies the end-to-end construction of an NLP Knowledge Graph (KG) from scientific papers. We focus on extracting four types of relations: evaluatedOn between tasks and datasets, evaluatedBy between tasks and evaluation metrics, as well as coreferent and related relations between the same type of entities. For instance, F1-score is coreferent with F-measure. We introduce novel methods for each of these relation types and apply our final framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a large-scale KG, which can facilitate automatically constructing scientific leaderboards for the NLP community. The results of our experiments indicate that the resulting KG contains high-quality information.

| Comments: | Accepted in ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01167](https://arxiv.org/abs/2106.01167) [cs.CL]** |
|           | (or **[arXiv:2106.01167v1](https://arxiv.org/abs/2106.01167v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-11">11. IrEne: Interpretable Energy Prediction for Transformers
</h2>

Title: [IrEne: Interpretable Energy Prediction for Transformers](https://arxiv.org/abs/2106.01199)

Authors: [Qingqing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Q), [Yash Kumar Lal](https://arxiv.org/search/cs?searchtype=author&query=Lal%2C+Y+K), [Harsh Trivedi](https://arxiv.org/search/cs?searchtype=author&query=Trivedi%2C+H), [Aruna Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at [this https URL](https://github.com/StonyBrookNLP/irene).

| Comments: | ACL 2021 camera ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01199](https://arxiv.org/abs/2106.01199) [cs.CL]** |
|           | (or **[arXiv:2106.01199v1](https://arxiv.org/abs/2106.01199v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-12">12. Lower Perplexity is Not Always Human-Like
</h2>

Title: [Lower Perplexity is Not Always Human-Like](https://arxiv.org/abs/2106.01229)

Authors: [Tatsuki Kuribayashi](https://arxiv.org/search/cs?searchtype=author&query=Kuribayashi%2C+T), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y), [Takumi Ito](https://arxiv.org/search/cs?searchtype=author&query=Ito%2C+T), [Ryo Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida%2C+R), [Masayuki Asahara](https://arxiv.org/search/cs?searchtype=author&query=Asahara%2C+M), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization -- the lower perplexity a language model has, the more human-like the language model is -- in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01229](https://arxiv.org/abs/2106.01229) [cs.CL]** |
|           | (or **[arXiv:2106.01229v1](https://arxiv.org/abs/2106.01229v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-13">13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers
</h2>

Title: [On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](https://arxiv.org/abs/2106.01335)

Authors: [Tianchu Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+T), [Shraddhan Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+S), [Michael Ferdman](https://arxiv.org/search/cs?searchtype=author&query=Ferdman%2C+M), [Peter Milder](https://arxiv.org/search/cs?searchtype=author&query=Milder%2C+P), [H. Andrew Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+H+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and log-scaled mapping which produces only a few (e.g. 23) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal (<1.0%) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01335](https://arxiv.org/abs/2106.01335) [cs.CL]** |
|           | (or **[arXiv:2106.01335v1](https://arxiv.org/abs/2106.01335v1) [cs.CL]** for this version) |









# 2021-06-02

[Return to Index](#Index)



<h2 id="2021-06-02-1">1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
</h2>

Title: [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> With large-scale pre-training, the past two years have witnessed significant performance boost on the Visual Question Answering (VQA) task. Though rapid progresses have been made, it remains unclear whether these state-of-the-art (SOTA) VQA models are robust when encountering test examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we present several interesting findings. (i) Surprisingly, during dataset collection, we find that non-expert annotators can successfully attack SOTA VQA models with relative ease. (ii) We test a variety of SOTA VQA models on our new dataset to highlight their fragility, and find that both large-scale pre-trained models and adversarial training methods can only achieve far lower performance than what they can achieve on the standard VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be used to improve the performance on other robust VQA benchmarks. (iv) We present a detailed analysis of the dataset, providing valuable insights on the challenges it brings to the community. We hope Adversarial VQA can serve as a valuable benchmark that will be used by future work to test the robustness of its developed VQA models. Our dataset is publicly available at https://adversarialvqa. [this http URL](http://github.io/).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.00245](https://arxiv.org/abs/2106.00245) [cs.CV]** |
|           | (or **[arXiv:2106.00245v1](https://arxiv.org/abs/2106.00245v1) [cs.CV]** for this version) |





<h2 id="2021-06-02-2">2. Language Model Evaluation Beyond Perplexity
</h2>

Title: [Language Model Evaluation Beyond Perplexity](https://arxiv.org/abs/2106.00085)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to certain statistical tendencies of natural language. We find that neural language models appear to learn only a subset of the statistical tendencies considered, but align much more closely with empirical trends than theoretical laws (when present). Further, the fit to different distributions is dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols suprisingly well.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00085](https://arxiv.org/abs/2106.00085) [cs.CL]** |
|           | (or **[arXiv:2106.00085v1](https://arxiv.org/abs/2106.00085v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-3">3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers
</h2>

Title: [An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](https://arxiv.org/abs/2106.00143)

Authors: [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Ruslan Mitkov](https://arxiv.org/search/cs?searchtype=author&query=Mitkov%2C+R)

> Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that these QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios.

| Comments: | Accepted to appear at the ACL-IJCNLP 2021 Main conference    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00143](https://arxiv.org/abs/2106.00143) [cs.CL]** |
|           | (or **[arXiv:2106.00143v1](https://arxiv.org/abs/2106.00143v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-4">4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation
</h2>

Title: [Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](https://arxiv.org/abs/2106.00169)

Authors: [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Denise Diaz](https://arxiv.org/search/cs?searchtype=author&query=Diaz%2C+D), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00169](https://arxiv.org/abs/2106.00169) [cs.CL]** |
|           | (or **[arXiv:2106.00169v1](https://arxiv.org/abs/2106.00169v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-5">5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives
</h2>

Title: [Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](https://arxiv.org/abs/2106.00181)

Authors: [Meichun Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+M), [Ziyang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z)

> Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human-scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people's attitudes.

| Comments: | Accepted at the 3rd Workshop on Gender Bias in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00181](https://arxiv.org/abs/2106.00181) [cs.CL]** |
|           | (or **[arXiv:2106.00181v1](https://arxiv.org/abs/2106.00181v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-6">6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021
</h2>

Title: [Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](https://arxiv.org/abs/2106.00197)

Authors: [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different tasks (i.e., Speech Recognition, Machine Translation, and Speech Translation) can be exploited to enhance the model's ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these features are processed by a shared encoder--decoder architecture. We apply several training techniques to improve the performance, including multi-task learning, task-level curriculum learning, data augmentation, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.00197](https://arxiv.org/abs/2106.00197) [cs.CL]** |
|           | (or **[arXiv:2106.00197v1](https://arxiv.org/abs/2106.00197v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-7">7. ViTA: Visual-Linguistic Translation by Aligning Object Tags
</h2>

Title: [ViTA: Visual-Linguistic Translation by Aligning Object Tags](https://arxiv.org/abs/2106.00250)

Authors: [Kshitij Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+K), [Devansh Gautam](https://arxiv.org/search/cs?searchtype=author&query=Gautam%2C+D), [Radhika Mamidi](https://arxiv.org/search/cs?searchtype=author&query=Mamidi%2C+R)

> Multimodal Machine Translation (MMT) enriches the source text with visual information for translation. It has gained popularity in recent years, and several pipelines have been proposed in the same direction. Yet, the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. In this paper, we propose our system for the Multimodal Translation Task of WAT 2021 from English to Hindi. We propose to use mBART, a pretrained multilingual sequence-to-sequence model, for the textual-only translations. Further, we bring the visual information to a textual domain by extracting object tags from the image and enhance the input for the multimodal task. We also explore the robustness of our system by systematically degrading the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test set and challenge set of the task.

| Comments: | 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00250](https://arxiv.org/abs/2106.00250) [cs.CL]** |
|           | (or **[arXiv:2106.00250v1](https://arxiv.org/abs/2106.00250v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-8">8. An In-depth Study on Internal Structure of Chinese Words
</h2>

Title: [An In-depth Study on Internal Structure of Chinese Words](https://arxiv.org/abs/2106.00334)

Authors: [Chen Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+C), [Saihao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Houquan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Zhenghua Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Zhefeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Baoxing Huai](https://arxiv.org/search/cs?searchtype=author&query=Huai%2C+B), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J)

> Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.

| Comments: | Accepted by ACL-IJCNLP 2021 (long paper)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00334](https://arxiv.org/abs/2106.00334) [cs.CL]** |
|           | (or **[arXiv:2106.00334v1](https://arxiv.org/abs/2106.00334v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-9">9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining
</h2>

Title: [SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](https://arxiv.org/abs/2106.00400)

Authors: [Chenglei Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+C), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yingfa Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Fanchao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Conventional tokenization methods for Chinese pretrained language models (PLMs) treat each character as an indivisible token (Devlin et al., 2019), which ignores the characteristics of the Chinese writing system. In this work, we comprehensively study the influences of three main factors on the Chinese tokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary. Correspondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk Word), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character), the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with Chinese word segmentation. To empirically compare the effectiveness of studied tokenizers, we pretrain BERT-style language models with them and evaluate the models on various downstream NLU tasks. We find that SHUOWEN and JIEZI tokenizers can generally outperform conventional single-character tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step. Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly better robustness in handling noisy texts. The code and pretrained models will be publicly released to facilitate linguistically informed Chinese NLP.

| Comments: | Work in progress. Feedback is welcome                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00400](https://arxiv.org/abs/2106.00400) [cs.CL]** |
|           | (or **[arXiv:2106.00400v1](https://arxiv.org/abs/2106.00400v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-10">10. DoT: An efficient Double Transformer for NLP tasks with tables
</h2>

Title: [DoT: An efficient Double Transformer for NLP tasks with tables](https://arxiv.org/abs/2106.00479)

Authors: [Syrine Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+S), [Thomas Mller](https://arxiv.org/search/cs?searchtype=author&query=Mller%2C+T), [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M)

> Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.

| Comments:    | 11 pages, 4 figures, to be published in Findings of ACL-IJCNLP 2021 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68-06                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2106.00479](https://arxiv.org/abs/2106.00479) [cs.CL]** |
|              | (or **[arXiv:2106.00479v1](https://arxiv.org/abs/2106.00479v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-11">11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations
</h2>

Title: [NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](https://arxiv.org/abs/2106.00590)

Authors: [Jialu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Tianqi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Cong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C)

> Effectively modeling text-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach to mine semantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting where texts in different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3447548.3467392](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467392&v=694504da) |
| Cite as:  | **[arXiv:2106.00590](https://arxiv.org/abs/2106.00590) [cs.CL]** |
|           | (or **[arXiv:2106.00590v1](https://arxiv.org/abs/2106.00590v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-12">12. Incorporating Visual Layout Structures for Scientific Text Classification
</h2>

Title: [Incorporating Visual Layout Structures for Scientific Text Classification](https://arxiv.org/abs/2106.00676)

Authors: [Zejiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Lucy Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L+L), [Bailey Kuehl](https://arxiv.org/search/cs?searchtype=author&query=Kuehl%2C+B), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D)

> Classifying the core textual components of a scientific paper-title, author, body text, etc.-is a critical first step in automated scientific document understanding. Previous work has shown how using elementary layout information, i.e., each token's 2D position on the page, leads to more accurate classification. We introduce new methods for incorporating VIsual LAyout structures (VILA), e.g., the grouping of page texts into text lines or text blocks, into language models to further improve performance. We show that the I-VILA approach, which simply adds special tokens denoting boundaries between layout structures into model inputs, can lead to +1~4.5 F1 Score improvements in token classification tasks. Moreover, we design a hierarchical model H-VILA that encodes these layout structures and record a up-to 70% efficiency boost without hurting prediction accuracy. The experiments are conducted on a newly curated evaluation suite, S2-VLUE, with a novel metric measuring VILA awareness and a new dataset covering 19 scientific disciplines with gold annotations. Pre-trained weights, benchmark datasets, and source code will be available at [this https URL](https://github.com/allenai/VILA)}{[this https URL](https://github.com/allenai/VILA).

| Comments: | 13 pages, 5 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00676](https://arxiv.org/abs/2106.00676) [cs.CL]** |
|           | (or **[arXiv:2106.00676v1](https://arxiv.org/abs/2106.00676v1) [cs.CL]** for this version) |








# 2021-06-01

[Return to Index](#Index)



<h2 id="2021-06-01-1">1. An Attention Free Transformer
</h2>

Title: [An Attention Free Transformer](https://arxiv.org/abs/2105.14103)

Authors: [Shuangfei Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+S), [Walter Talbott](https://arxiv.org/search/cs?searchtype=author&query=Talbott%2C+W), [Nitish Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+N), [Chen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Hanlin Goh](https://arxiv.org/search/cs?searchtype=author&query=Goh%2C+H), [Ruixiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Josh Susskind](https://arxiv.org/search/cs?searchtype=author&query=Susskind%2C+J)

> We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14103](https://arxiv.org/abs/2105.14103) [cs.LG]** |
|           | (or **[arXiv:2105.14103v1](https://arxiv.org/abs/2105.14103v1) [cs.LG]** for this version) |



<h2 id="2021-06-01-2">2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering
</h2>

Title: [LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](https://arxiv.org/abs/2105.14300)

Authors: [Zujie Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Haifeng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H), [Jiaying Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Most existing Visual Question Answering (VQA) systems tend to overly rely on language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer's loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.

| Comments: | Accepted by ACM SIGIR 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| DOI:      | [10.1145/3404835.3462981](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3404835.3462981&v=a1843a78) |
| Cite as:  | **[arXiv:2105.14300](https://arxiv.org/abs/2105.14300) [cs.CV]** |
|           | (or **[arXiv:2105.14300v1](https://arxiv.org/abs/2105.14300v1) [cs.CV]** for this version) |





<h2 id="2021-06-01-3">3. Re-evaluating Word Mover's Distance
</h2>

Title: [Re-evaluating Word Mover's Distance](https://arxiv.org/abs/2105.14403)

Authors: [Ryoma Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+R), [Makoto Yamada](https://arxiv.org/search/cs?searchtype=author&query=Yamada%2C+M), [Hisashi Kashima](https://arxiv.org/search/cs?searchtype=author&query=Kashima%2C+H)

> The word mover's distance (WMD) is a fundamental technique for measuring the similarity of two documents. As the crux of WMD, it can take advantage of the underlying geometry of the word space by employing an optimal transport formulation. The original study on WMD reported that WMD outperforms classical baselines such as bag-of-words (BOW) and TF-IDF by significant margins in various datasets. In this paper, we point out that the evaluation in the original study could be misleading. We re-evaluate the performances of WMD and the classical baselines and find that the classical baselines are competitive with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. However, this result is not intuitive. WMD should be superior to BOW because WMD can take the underlying geometry into account, whereas BOW cannot. Our analysis shows that this is due to the high-dimensional nature of the underlying metric. We find that WMD in high-dimensional spaces behaves more similarly to BOW than in low-dimensional spaces due to the curse of dimensionality.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14403](https://arxiv.org/abs/2105.14403) [cs.LG]** |
|           | (or **[arXiv:2105.14403v1](https://arxiv.org/abs/2105.14403v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-4">4. Memory-Efficient Differentiable Transformer Architecture Search
</h2>

Title: [Memory-Efficient Differentiable Transformer Architecture Search](https://arxiv.org/abs/2105.14669)

Authors: [Yuekai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Zhihua Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.

| Comments: | Accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.14669](https://arxiv.org/abs/2105.14669) [cs.LG]** |
|           | (or **[arXiv:2105.14669v1](https://arxiv.org/abs/2105.14669v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-5">5. Why does CTC result in peaky behavior?
</h2>

Title: [Why does CTC result in peaky behavior?](https://arxiv.org/abs/2105.14849)

Authors: [Albert Zeyer](https://arxiv.org/search/cs?searchtype=author&query=Zeyer%2C+A), [Ralf Schlter](https://arxiv.org/search/cs?searchtype=author&query=Schlter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Sound (cs.SD); Audio and Speech Processing (eess.AS); Statistics Theory (math.ST) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14849](https://arxiv.org/abs/2105.14849) [cs.LG]** |
|           | (or **[arXiv:2105.14849v1](https://arxiv.org/abs/2105.14849v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-6">6. Grammatical Error Correction as GAN-like Sequence Labeling
</h2>

Title: [Grammatical Error Correction as GAN-like Sequence Labeling](https://arxiv.org/abs/2105.14209)

Authors: [Kevin Parnow](https://arxiv.org/search/cs?searchtype=author&query=Parnow%2C+K), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> In Grammatical Error Correction (GEC), sequence labeling models enjoy fast inference compared to sequence-to-sequence models; however, inference in sequence labeling GEC models is an iterative process, as sentences are passed to the model for multiple rounds of correction, which exposes the model to sentences with progressively fewer errors at each round. Traditional GEC models learn from sentences with fixed error rates. Coupling this with the iterative correction process causes a mismatch between training and inference that affects final performance. In order to address this mismatch, we propose a GAN-like sequence labeling model, which consists of a grammatical error detector as a discriminator and a grammatical error labeler with Gumbel-Softmax sampling as a generator. By sampling from real error distributions, our errors are more genuine compared to traditional synthesized GEC errors, thus alleviating the aforementioned mismatch and allowing for better training. Our results on several evaluation benchmarks demonstrate that our proposed approach is effective and improves the previous state-of-the-art baseline.

| Comments: | Accepted by ACL21, Findings                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14209](https://arxiv.org/abs/2105.14209) [cs.CL]** |
|           | (or **[arXiv:2105.14209v1](https://arxiv.org/abs/2105.14209v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-7">7. Predictive Representation Learning for Language Modeling
</h2>

Title: [Predictive Representation Learning for Language Modeling](https://arxiv.org/abs/2105.14214)

Authors: [Qingfeng Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Q), [Luke Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+L), [Martha White](https://arxiv.org/search/cs?searchtype=author&query=White%2C+M), [Alona Fyshe](https://arxiv.org/search/cs?searchtype=author&query=Fyshe%2C+A)

> To effectively perform the task of next-word prediction, long short-term memory networks (LSTMs) must keep track of many types of information. Some information is directly related to the next word's identity, but some is more secondary (e.g. discourse-level features or features of downstream words). Correlates of secondary information appear in LSTM representations even though they are not part of an \emph{explicitly} supervised prediction task. In contrast, in reinforcement learning (RL), techniques that explicitly supervise representations to predict secondary information have been shown to be beneficial. Inspired by that success, we propose Predictive Representation Learning (PRL), which explicitly constrains LSTMs to encode specific predictions, like those that might need to be learned implicitly. We show that PRL 1) significantly improves two strong language modeling methods, 2) converges more quickly, and 3) performs better when data is limited. Our work shows that explicitly encoding a simple predictive task facilitates the search for a more effective language model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14214](https://arxiv.org/abs/2105.14214) [cs.CL]** |
|           | (or **[arXiv:2105.14214v1](https://arxiv.org/abs/2105.14214v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-8">8. Korean-English Machine Translation with Multiple Tokenization Strategy
</h2>

Title: [Korean-English Machine Translation with Multiple Tokenization Strategy](https://arxiv.org/abs/2105.14274)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> This study was conducted to find out how tokenization methods affect the training results of machine translation models. In this work, character tokenization, morpheme tokenization, and BPE tokenization were applied to Korean as the source language and English as the target language respectively, and the comparison experiment was conducted by repeating 50,000 epochs of each 9 models using the Transformer neural network. As a result of measuring the BLEU scores of the experimental models, the model that applied BPE tokenization to Korean and morpheme tokenization to English recorded 35.73, showing the best performance.

| Comments: | KCC2021 Undergraduate/Junior Thesis Competition              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14274](https://arxiv.org/abs/2105.14274) [cs.CL]** |
|           | (or **[arXiv:2105.14274v1](https://arxiv.org/abs/2105.14274v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-9">9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models
</h2>

Title: [Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](https://arxiv.org/abs/2105.14277)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> Intrinsic evaluation by humans for the performance of natural language generation models is conducted to overcome the fact that the quality of generated sentences cannot be fully represented by only extrinsic evaluation. Nevertheless, existing intrinsic evaluations have a large score deviation according to the evaluator's criteria. In this paper, we propose Grammar Accuracy Evaluation (GAE) that can provide specific evaluating criteria. As a result of analyzing the quality of machine translation by BLEU and GAE, it was confirmed that the BLEU score does not represent the absolute performance of machine translation models and that GAE compensates for the shortcomings of BLEU with a flexible evaluation on alternative synonyms and changes in sentence structure.

| Comments: | Journal of KIISE                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14277](https://arxiv.org/abs/2105.14277) [cs.CL]** |
|           | (or **[arXiv:2105.14277v1](https://arxiv.org/abs/2105.14277v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-10">10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search
</h2>

Title: [NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](https://arxiv.org/abs/2105.14444)

Authors: [Jin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Jian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> While pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.

| Comments: | Accepted by KDD 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| DOI:      | [10.1145/3447548.3467262](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467262&v=2e2b005f) |
| Cite as:  | **[arXiv:2105.14444](https://arxiv.org/abs/2105.14444) [cs.CL]** |
|           | (or **[arXiv:2105.14444v1](https://arxiv.org/abs/2105.14444v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-11">11. Pre-training Universal Language Representation
</h2>

Title: [Pre-training Universal Language Representation](https://arxiv.org/abs/2105.14478)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.

| Comments: | Accepted by ACL-IJCNLP 2021 main conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14478](https://arxiv.org/abs/2105.14478) [cs.CL]** |
|           | (or **[arXiv:2105.14478v1](https://arxiv.org/abs/2105.14478v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-12">12. Fast Nearest Neighbor Machine Translation
</h2>

Title: [Fast Nearest Neighbor Machine Translation](https://arxiv.org/abs/2105.14528)

Authors: [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiayu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Though nearest neighbor Machine Translation (kNN-MT) \cite{khandelwal2020nearest} has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-order slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-order faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications.\footnote{Code is available at \url{[this https URL](https://github.com/ShannonAI/fast-knn-nmt).}}

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14528](https://arxiv.org/abs/2105.14528) [cs.CL]** |
|           | (or **[arXiv:2105.14528v1](https://arxiv.org/abs/2105.14528v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-13">13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation
</h2>


Title: [HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](https://arxiv.org/abs/2105.14600)

Authors: [Ayan Sengupta](https://arxiv.org/search/cs?searchtype=author&query=Sengupta%2C+A), [Sourabh Kumar Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+S+K), [Tanmoy Chakraborty](https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+T), [Md Shad Akhtar](https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+M+S)

> Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentence-level semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning).

| Comments: | 15 pages, 13 tables, 6 Figures. Accepted at ACL-IJCNLP-2021 (Findings) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14600](https://arxiv.org/abs/2105.14600) [cs.CL]** |
|           | (or **[arXiv:2105.14600v1](https://arxiv.org/abs/2105.14600v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-14">14. Attention Flows are Shapley Value Explanations
</h2>


Title: [Attention Flows are Shapley Value Explanations](https://arxiv.org/abs/2105.14652)

Authors: [Kawin Ethayarajh](https://arxiv.org/search/cs?searchtype=author&query=Ethayarajh%2C+K), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D)

> Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that -- save for the degenerate case -- attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values -- which has driven their adoption among the ML community -- we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14652](https://arxiv.org/abs/2105.14652) [cs.CL]** |
|           | (or **[arXiv:2105.14652v1](https://arxiv.org/abs/2105.14652v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-15">15. G-Transformer for Document-level Machine Translation
</h2>


Title: [G-Transformer for Document-level Machine Translation](https://arxiv.org/abs/2105.14761)

Authors: [Guangsheng Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+G), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Zhiyang Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both non-pretraining and pre-training settings on three benchmark datasets.

| Comments: | Accepted by ACL2021 main track                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14761](https://arxiv.org/abs/2105.14761) [cs.CL]** |
|           | (or **[arXiv:2105.14761v1](https://arxiv.org/abs/2105.14761v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-16">16. On Compositional Generalization of Neural Machine Translation
</h2>


Title: [On Compositional Generalization of Neural Machine Translation](https://arxiv.org/abs/2105.14802)

Authors: [Yafu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Yulong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics.

| Comments: | To appear at the ACL 2021 main conference                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14802](https://arxiv.org/abs/2105.14802) [cs.CL]** |
|           | (or **[arXiv:2105.14802v1](https://arxiv.org/abs/2105.14802v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-17">17. Transfer Learning for Sequence Generation: from Single-source to Multi-source
</h2>


Title: [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://arxiv.org/abs/2105.14809)

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly.

| Comments: | ACL2021 main track long paper                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14809](https://arxiv.org/abs/2105.14809) [cs.CL]** |
|           | (or **[arXiv:2105.14809v1](https://arxiv.org/abs/2105.14809v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-18">18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models
</h2>


Title: [Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](https://arxiv.org/abs/2105.14813)

Authors: [Chong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Cenyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Xiaoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pretraining strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14813](https://arxiv.org/abs/2105.14813) [cs.CL]** |
|           | (or **[arXiv:2105.14813v1](https://arxiv.org/abs/2105.14813v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-19">19. Effective Batching for Recurrent Neural Network Grammars
</h2>


Title: [Effective Batching for Recurrent Neural Network Grammars](https://arxiv.org/abs/2105.14822)

Authors: [Hiroshi Noji](https://arxiv.org/search/cs?searchtype=author&query=Noji%2C+H), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y)

> As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our RNNG implementation is available at [this https URL](https://github.com/aistairc/rnng-pytorch/).

| Comments: | Findings of ACL: ACL-IJCNLP 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14822](https://arxiv.org/abs/2105.14822) [cs.CL]** |
|           | (or **[arXiv:2105.14822v1](https://arxiv.org/abs/2105.14822v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-20">20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models
</h2>


Title: [Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](https://arxiv.org/abs/2105.14839)

Authors: [David Peer](https://arxiv.org/search/cs?searchtype=author&query=Peer%2C+D), [Sebastian Stabinger](https://arxiv.org/search/cs?searchtype=author&query=Stabinger%2C+S), [Stefan Engl](https://arxiv.org/search/cs?searchtype=author&query=Engl%2C+S), [Antonio Rodriguez-Sanchez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez-Sanchez%2C+A)

> Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different NLP tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production and is a limiting factor for the deployment into embedded devices. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models, but as it needs an additional expensive pre-training phase, this solution is computationally expensive and can be financially prohibitive for smaller academic research groups. Another solution is to use layer-wise pruning methods, which reach high compression rates for transformer models and avoids the computational load of the pre-training distillation stage. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, greedy layer pruning (GLP) is introduced to (1) outperform current state-of-the-art for layer-wise pruning (2) close the performance gap when compared to knowledge distillation, while (3) using only a modest budget. More precisely, with the methodology presented it is possible to prune and evaluate competitive models on the whole GLUE benchmark with a budget of just $300. Our source code is available on [this https URL](https://github.com/deepopinion/greedy-layer-pruning).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14839](https://arxiv.org/abs/2105.14839) [cs.CL]** |
|           | (or **[arXiv:2105.14839v1](https://arxiv.org/abs/2105.14839v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-21">21. Verdi: Quality Estimation and Error Detection for Bilingual
</h2>


Title: [Verdi: Quality Estimation and Error Detection for Bilingual](https://arxiv.org/abs/2105.14878)

Authors: [Mingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Haijiang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Di Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+D), [Zixuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xiaoli Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Translation Quality Estimation is critical to reducing post-editing efforts in machine translation and to cross-lingual corpus cleaning. As a research problem, quality estimation (QE) aims to directly estimate the quality of translation in a given pair of source and target sentences, and highlight the words that need corrections, without referencing to golden translations. In this paper, we propose Verdi, a novel framework for word-level and sentence-level post-editing effort estimation for bilingual corpora. Verdi adopts two word predictors to enable diverse features to be extracted from a pair of sentences for subsequent quality estimation, including a transformer-based neural machine translation (NMT) model and a pre-trained cross-lingual language model (XLM). We exploit the symmetric nature of bilingual corpora and apply model-level dual learning in the NMT predictor, which handles a primal task and a dual task simultaneously with weight sharing, leading to stronger context prediction ability than single-direction NMT models. By taking advantage of the dual learning scheme, we further design a novel feature to directly encode the translated target information without relying on the source context. Extensive experiments conducted on WMT20 QE tasks demonstrate that our method beats the winner of the competition and outperforms other baseline methods by a great margin. We further use the sentence-level scores provided by Verdi to clean a parallel corpus and observe benefits on both model performance and training efficiency.

| Comments: | Accepted by The Web Conference 2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14878](https://arxiv.org/abs/2105.14878) [cs.CL]** |
|           | (or **[arXiv:2105.14878v1](https://arxiv.org/abs/2105.14878v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-22">22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation
</h2>


Title: [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://arxiv.org/abs/2105.14913)

Authors: [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.

| Comments: | Accepted into the main conference of ACL 2021. arXiv admin note: text overlap with [arXiv:2105.13072](https://arxiv.org/abs/2105.13072) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14913](https://arxiv.org/abs/2105.14913) [cs.CL]** |
|           | (or **[arXiv:2105.14913v1](https://arxiv.org/abs/2105.14913v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-23">23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?
</h2>


Title: [Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](https://arxiv.org/abs/2105.14940)

Authors: [Zae Myung Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Z+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D)

> Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of language-independent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a "black-box" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as "variance" or "confidence", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.

| Comments: | 10 pages, accepted at Findings of ACL 2021 (short)           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14940](https://arxiv.org/abs/2105.14940) [cs.CL]** |
|           | (or **[arXiv:2105.14940v1](https://arxiv.org/abs/2105.14940v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-24">24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
</h2>


Title: [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://arxiv.org/abs/2105.15071)

Authors: [Wei-Jen Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+W), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Francisco Guzmn](https://arxiv.org/search/cs?searchtype=author&query=Guzmn%2C+F), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15071](https://arxiv.org/abs/2105.15071) [cs.CL]** |
|           | (or **[arXiv:2105.15071v1](https://arxiv.org/abs/2105.15071v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-25">25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation
</h2>


Title: [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://arxiv.org/abs/2105.15087)

Authors: [Eleftheria Briakou](https://arxiv.org/search/cs?searchtype=author&query=Briakou%2C+E), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15087](https://arxiv.org/abs/2105.15087) [cs.CL]** |
|           | (or **[arXiv:2105.15087v1](https://arxiv.org/abs/2105.15087v1) [cs.CL]** for this version) |

