# Daily arXiv: Machine Translation - Oct., 2019

# Index

- [2019-10-31](#2019-10-31)
  - [1. A Latent Morphology Model for Open-Vocabulary Neural Machine Translation](#2019-10-31-1)
  - [2. Adapting Multilingual Neural Machine Translation to Unseen Languages](#2019-10-31-2)
  - [3. Ordered Memory](#2019-10-31-3)
  - [4. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](#2019-10-31-4)
  - [5. An Augmented Transformer Architecture for Natural Language Generation Tasks](#2019-10-31-5)
  - [6. ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task](#2019-10-31-6)
- [2019-10-30](#2019-10-30)
  - [1. Transformer-based Cascaded Multimodal Speech Translation](#2019-10-30-1)
  - [2. BPE-Dropout: Simple and Effective Subword Regularization](#2019-10-30-2)
  - [3. Findings of the Third Workshop on Neural Generation and Translation](#2019-10-30-3)
  - [4. An Empirical Study of Generation Order for Machine Translation](#2019-10-30-4)
- [2019-10-29](#2019-10-29)
  - [1. Multitask Learning For Different Subword Segmentations In Neural Machine Translation](#2019-10-29-1)
- [2019-10-28](#2019-10-28)
  - [1. DENS: A Dataset for Multi-class Emotion Analysis](#2019-10-28-1)
  - [2. On the Cross-lingual Transferability of Monolingual Representations](#2019-10-28-2)
  - [3. Analyzing ASR pretraining for low-resource speech-to-text translation](#2019-10-28-3)
  - [4. Pun-GAN: Generative Adversarial Network for Pun Generation](#2019-10-28-4)
  - [5. Wasserstein distances for evaluating cross-lingual embeddings](#2019-10-28-5)
  - [6. Promoting the Knowledge of Source Syntax in Transformer NMT Is Not Needed](#2019-10-28-6)
- [2019-10-25](#2019-10-25)
  - [1. A context sensitive real-time Spell Checker with language adaptability](#2019-10-25-1)
  - [2. Fast Structured Decoding for Sequence Models](#2019-10-25-2)
  - [3. Machine Translation from Natural Language to Code using Long-Short Term Memory](#2019-10-25-3)
  - [4. The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection](#2019-10-25-4)
- [2019-10-24](#2019-10-24)
  - [1. Robust Neural Machine Translation for Clean and Noisy Speech Transcripts](#2019-10-24-1)
  - [2. Controlling the Output Length of Neural Machine Translation](#2019-10-24-2)
  - [3. XL-Editor: Post-editing Sentences with XLNet](#2019-10-24-3)
  - [4. Fully Quantized Transformer for Improved Translation](#2019-10-24-4)
  - [5. Instance-Based Model Adaptation For Direct Speech Translation](#2019-10-24-5)
- [2019-10-23](#2019-10-23)
  - [1. Depth-Adaptive Transformer]( #2019-10-23-1 )
- [2019-10-22](#2019-10-22)
  - [1. Automatic Post-Editing for Machine Translation](#2019-10-22-1)
- [2019-10-21](#2019-10-21)
  - [1. Controlling Utterance Length in NMT-based Word Segmentation with Attention](#2019-10-21-1)
- [2019-10-18](#2019-10-18)
  - [1. LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition](#2019-10-18-1)
- [2019-10-17](#2019-10-17)
  - [1. Root Mean Square Layer Normalization](#2019-10-17-1)
  - [2. Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models](#2019-10-17-2)
  - [3. Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings](#2019-10-17-3)
  - [4. Fine-grained evaluation of German-English Machine Translation based on a Test Suite](#2019-10-17-4)
  - [5. Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite](#2019-10-17-5)
  - [6. Using Whole Document Context in Neural Machine Translation](#2019-10-17-6)
- [2019-10-16](#2019-10-16)
  - [1. In-training Matrix Factorization for Parameter-frugal Neural Machine Translation](#2019-10-16-1)
  - [2. Mapping Supervised Bilingual Word Embeddings from English to low-resource languages](#2019-10-16-2)
  - [3. Detecting Machine-Translated Text using Back Translation](#2019-10-16-3)
  - [4. Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation](#2019-10-16-4)
  - [5. On the Importance of Word Boundaries in Character-level Neural Machine Translation](#2019-10-16-5)
  - [6. Facebook AI's WAT19 Myanmar-English Translation Task Submission](#2019-10-16-6)
- [2019-10-15](#2019-10-15)
  - [1. From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction](#2019-10-15-1)
  - [2. Transformers without Tears: Improving the Normalization of Self-Attention](#2019-10-15-2)
  - [3. Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality](#2019-10-15-3)
  - [4. Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment](#2019-10-15-4)
- [2019-10-14](#2019-10-14)
  - [1. How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages](#2019-10-14-1)
- [2019-10-11](#2019-10-11)
  - [1. Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](#2019-10-11-1)
  - [2. Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)](#2019-10-11-2)
- [2019-10-10](#2019-10-10)
  - [1. Novel Applications of Factored Neural Machine Translation](#2019-10-10-1)
- [2019-10-09](#2019-10-09)
  - [1. Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation](#2019-10-09-1)
  - [2. Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](#2019-10-09-2)
  - [3. One-To-Many Multilingual End-to-end Speech Translation](#2019-10-09-3)
  - [4. An Interactive Machine Translation Framework for Modernizing Historical Documents](#2019-10-09-4)
  - [5. Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation](#2019-10-09-5)
  - [6. Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents](#2019-10-09-6)
- [2019-10-08](#2019-10-08)
  - [1. How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems](#2019-10-08-1)
  - [2. Domain Differential Adaptation for Neural Machine Translation](#2019-10-08-2)
  - [3. On Leveraging the Visual Modality for Neural Machine Translation](#2019-10-08-3)
  - [4. Adversarial reconstruction for Multi-modal Machine Translation](#2019-10-08-4)
- [2019-10-07](#2019-10-07)
  - [1. Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data](#2019-10-07-1)
  - [2. Modeling Confidence in Sequence-to-Sequence Models](#2019-10-07-2)
  - [3. Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods](#2019-10-07-3)
- [2019-10-04](#2019-10-04)
  - [1. Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations](#2019-10-04-1)
  - [2. Linking artificial and human neural representations of language](#2019-10-04-2)
- [2019-10-03](#2019-10-03)
  - [1. Speech-to-speech Translation between Untranscribed Unknown Languages](2019-10-03-1)
  - [2. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](2019-10-03-2)
- [2019-10-02](#2019-10-02)
  - [1. Interrogating the Explanatory Power of Attention in Neural Machine Translation](#2019-10-02-1)
  - [2. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations](#2019-10-02-2)
  - [3. Multilingual End-to-End Speech Translation](#2019-10-02-3)
  - [4. When and Why is Document-level Context Useful in Neural Machine Translation?](#2019-10-02-4)
  - [5. Grammatical Error Correction in Low-Resource Scenarios](#2019-10-02-5)
  - [6. Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair](#2019-10-02-6)
  - [7. A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation](#2019-10-02-7)
  - [8. Machine Translation for Machines: the Sentiment Classification Use Case](#2019-10-02-8)
  - [9. Putting Machine Translation in Context with the Noisy Channel Model](#2019-10-02-9)
- [2019-10-01](#2019-10-01)
  - [1. Revisiting Self-Training for Neural Sequence Generation](2019-10-01-1)
  - [2. The Source-Target Domain Mismatch Problem in Machine Translation](2019-10-01-2)
  - [3. Controllable Data Synthesis Method for Grammatical Error Correction](2019-10-01-3)
  - [4. Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation](2019-10-01-4)
  - [5. Simple and Effective Paraphrastic Similarity from Parallel Translations](2019-10-01-5)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)





# 2019-10-31

[Return to Index](#Index)



<h2 id="2019-10-31-1">1. A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</h2>

Title: [A Latent Morphology Model for Open-Vocabulary Neural Machine Translation]( https://arxiv.org/abs/1910.13890 )

Authors: [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Wilker Aziz](https://arxiv.org/search/cs?searchtype=author&query=Aziz%2C+W), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

*(Submitted on 30 Oct 2019)*

> Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, we propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. Our model generates words one character at a time by composing two latent representations: a continuous one, aimed at capturing the lexical semantics, and a set of (approximately) discrete features, aimed at capturing the morphosyntactic function, which are shared among different surface forms. Our model achieves better accuracy in translation into three morphologically-rich languages than conventional open-vocabulary NMT methods, while also demonstrating a better generalization capacity under low to mid-resource settings.

| Comments: | Submitted to ICLR 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.13890](https://arxiv.org/abs/1910.13890) [cs.CL]** |
|           | (or **[arXiv:1910.13890v1](https://arxiv.org/abs/1910.13890v1) [cs.CL]** for this version) |





<h2 id="2019-10-31-2">2. Adapting Multilingual Neural Machine Translation to Unseen Languages</h2>

Title: [Adapting Multilingual Neural Machine Translation to Unseen Languages]( https://arxiv.org/abs/1910.13998 )

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 30 Oct 2019)*

> Multilingual Neural Machine Translation (MNMT) for low-resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a LRL has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen LRL using data selection and model adaptation. In order to improve NMT for LRL, we employ perplexity to select HRL data that are most similar to the LRL on the basis of language distance. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRL-en). We further show that dynamic adaptation of the model's vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show reductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and data augmentation, on four LRL.

| Comments: | Accepted at the 16th International Workshop on Spoken Language Translation (IWSLT), November, 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.13998](https://arxiv.org/abs/1910.13998) [cs.CL]** |
|           | (or **[arXiv:1910.13998v1](https://arxiv.org/abs/1910.13998v1) [cs.CL]** for this version) |





<h2 id="2019-10-31-3">3. Ordered Memory</h2>

Title: [Ordered Memory]( https://arxiv.org/abs/1910.13466 )

Authors: [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Shawn Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Seyedarian Hosseini](https://arxiv.org/search/cs?searchtype=author&query=Hosseini%2C+S), [Zhouhan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Alessandro Sordoni](https://arxiv.org/search/cs?searchtype=author&query=Sordoni%2C+A), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville%2C+A)

*(Submitted on 29 Oct 2019)*

> Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some time. However, the difficulty of training memory models remains a problem obstructing the widespread use of such models. In this paper, we propose the Ordered Memory architecture. Inspired by Ordered Neurons (Shen et al., 2018), we introduce a new attention-based mechanism and use its cumulative probability to control the writing and erasing operation of the memory. We also introduce a new Gated Recursive Cell to compose lower-level representations into higher-level representation. We demonstrate that our model achieves strong performance on the logical inference task (Bowman et al., 2015)and the ListOps (Nangia and Bowman, 2018) task. We can also interpret the model to retrieve the induced tree structure, and find that these induced structures align with the ground truth. Finally, we evaluate our model on the Stanford SentimentTreebank tasks (Socher et al., 2013), and find that it performs comparatively with the state-of-the-art methods in the literature.

| Comments: | Published in NeurIPS 2019                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:1910.13466](https://arxiv.org/abs/1910.13466) [cs.LG]** |
|           | (or **[arXiv:1910.13466v1](https://arxiv.org/abs/1910.13466v1) [cs.LG]** for this version) |





<h2 id="2019-10-31-4">4. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h2>

Title: [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]( https://arxiv.org/abs/1910.13461 )

Authors: [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Yinhan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Abdelrahman Mohamed](https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+A), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 29 Oct 2019)*

> We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.13461](https://arxiv.org/abs/1910.13461) [cs.CL]** |
|           | (or **[arXiv:1910.13461v1](https://arxiv.org/abs/1910.13461v1) [cs.CL]** for this version) |





<h2 id="2019-10-31-5">5. An Augmented Transformer Architecture for Natural Language Generation Tasks</h2>

Title: [An Augmented Transformer Architecture for Natural Language Generation Tasks]( https://arxiv.org/abs/1910.13634 )

Authors: [Hailiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Adele Y.C. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+A+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Du Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Zhibin Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+Z), [Wenye Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W)

*(Submitted on 30 Oct 2019)*

> The Transformer based neural networks have been showing significant advantages on most evaluations of various natural language processing and other sequence-to-sequence tasks due to its inherent architecture based superiorities. Although the main architecture of the Transformer has been continuously being explored, little attention was paid to the positional encoding module. In this paper, we enhance the sinusoidal positional encoding algorithm by maximizing the variances between encoded consecutive positions to obtain additional promotion. Furthermore, we propose an augmented Transformer architecture encoded with additional linguistic knowledge, such as the Part-of-Speech (POS) tagging, to boost the performance on some natural language generation tasks, e.g., the automatic translation and summarization tasks. Experiments show that the proposed architecture attains constantly superior results compared to the vanilla Transformer.

| Comments: | This paper will be appeared in the conference workshop ICDM MLCS 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:1910.13634](https://arxiv.org/abs/1910.13634) [cs.CL]** |
|           | (or **[arXiv:1910.13634v1](https://arxiv.org/abs/1910.13634v1) [cs.CL]** for this version) |





<h2 id="2019-10-31-6">6. ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task</h2>

Title: [ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task]( https://arxiv.org/abs/1910.13689 )

Authors: [Ha Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Natalia Tomashenko](https://arxiv.org/search/cs?searchtype=author&query=Tomashenko%2C+N), [Marcely Zanon Boito](https://arxiv.org/search/cs?searchtype=author&query=Boito%2C+M+Z), [Antoine Caubriere](https://arxiv.org/search/cs?searchtype=author&query=Caubriere%2C+A), [Fethi Bougares](https://arxiv.org/search/cs?searchtype=author&query=Bougares%2C+F), [Mickael Rouvier](https://arxiv.org/search/cs?searchtype=author&query=Rouvier%2C+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Yannick Esteve](https://arxiv.org/search/cs?searchtype=author&query=Esteve%2C+Y)

*(Submitted on 30 Oct 2019)*

> This paper describes the ON-TRAC Consortium translation systems developed for the end-to-end model task of IWSLT Evaluation 2019 for the English-to-Portuguese language pair. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). A single end-to-end model built as a neural encoder-decoder architecture with attention mechanism was used for two primary submissions corresponding to the two EN-PT evaluations sets: (1) TED (MuST-C) and (2) How2. In this paper, we notably investigate impact of pooling heterogeneous corpora for training, impact of target tokenization (characters or BPEs), impact of speech input segmentation and we also compare our best end-to-end model (BLEU of 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT) approach.

| Comments: | IWSLT 2019 - First two authors contributed equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:1910.13689](https://arxiv.org/abs/1910.13689) [cs.CL]** |
|           | (or **[arXiv:1910.13689v1](https://arxiv.org/abs/1910.13689v1) [cs.CL]** for this version) |







# 2019-10-30

[Return to Index](#Index)



<h2 id="2019-10-30-1">1. Transformer-based Cascaded Multimodal Speech Translation</h2>
Title: [Transformer-based Cascaded Multimodal Speech Translation]( https://arxiv.org/abs/1910.13215 )

Authors: [Zixiu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Josiah Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 29 Oct 2019)*

> This paper describes the cascaded multimodal speech translation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture consists of an automatic speech recognition (ASR) system followed by a Transformer-based multimodal machine translation (MMT) system. While the ASR component is identical across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple conditioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer and its deliberation version with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes often harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation; (ii) the transformer and cascade deliberation integrate the visual modality better than the additive deliberation, as shown by the incongruence analysis.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.13215](https://arxiv.org/abs/1910.13215) [cs.CL]** |
|           | (or **[arXiv:1910.13215v1](https://arxiv.org/abs/1910.13215v1) [cs.CL]** for this version) |





<h2 id="2019-10-30-2">2. BPE-Dropout: Simple and Effective Subword Regularization</h2>
Title: [BPE-Dropout: Simple and Effective Subword Regularization]( https://arxiv.org/abs/1910.13267 )

Authors: [Ivan Provilkov](https://arxiv.org/search/cs?searchtype=author&query=Provilkov%2C+I), [Dmitrii Emelianenko](https://arxiv.org/search/cs?searchtype=author&query=Emelianenko%2C+D), [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E)

*(Submitted on 29 Oct 2019)*

> Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.13267](https://arxiv.org/abs/1910.13267) [cs.CL]** |
|           | (or **[arXiv:1910.13267v1](https://arxiv.org/abs/1910.13267v1) [cs.CL]** for this version) |





<h2 id="2019-10-30-3">3. Findings of the Third Workshop on Neural Generation and Translation</h2>
Title: [Findings of the Third Workshop on Neural Generation and Translation]( https://arxiv.org/abs/1910.13299 )

Authors: [Hiroaki Hayashi](https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+H) (1), [Yusuke Oda](https://arxiv.org/search/cs?searchtype=author&query=Oda%2C+Y) (2), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A) (3), [Ioannis Konstas](https://arxiv.org/search/cs?searchtype=author&query=Konstas%2C+I) (4), [Andrew Finch](https://arxiv.org/search/cs?searchtype=author&query=Finch%2C+A) (5), [Minh-Thang Luong](https://arxiv.org/search/cs?searchtype=author&query=Luong%2C+M) (2), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G) (1), [Katsuhito Sudoh](https://arxiv.org/search/cs?searchtype=author&query=Sudoh%2C+K) (6) ((1) Carnegie Mellon University, (2) Google Brain, (3) University of Edinburgh, (4) Heriot-Watt University, (5) Apple, (6) Nara Institute of Science and Technology)

*(Submitted on 29 Oct 2019)*

> This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.

| Comments: | Third Workshop on Neural Generation and Translation, EMNLP 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.13299](https://arxiv.org/abs/1910.13299) [cs.CL]** |
|           | (or **[arXiv:1910.13299v1](https://arxiv.org/abs/1910.13299v1) [cs.CL]** for this version) |





<h2 id="2019-10-30-4">4. An Empirical Study of Generation Order for Machine Translation</h2>
Title: [An Empirical Study of Generation Order for Machine Translation]( https://arxiv.org/abs/1910.13437 )

Authors: [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Jamie Kiros](https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J), [Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+J)

*(Submitted on 29 Oct 2019)*

> In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT'14 English → German translation task, order does not have a substantial impact on output quality, with unintuitive orderings such as alphabetical and shortest-first matching the performance of a standard Transformer. This demonstrates that traditional left-to-right generation is not strictly necessary to achieve high performance. On the other hand, results on the WMT'18 English → Chinese task tend to vary more widely, suggesting that translation for less well-aligned language pairs may be more sensitive to generation order.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.13437](https://arxiv.org/abs/1910.13437) [cs.CL]** |
|           | (or **[arXiv:1910.13437v1](https://arxiv.org/abs/1910.13437v1) [cs.CL]** for this version) |



# 2019-10-29

[Return to Index](#Index)



<h2 id="2019-10-29-1">1. Multitask Learning For Different Subword Segmentations In Neural Machine Translation</h2>
Title: [Multitask Learning For Different Subword Segmentations In Neural Machine Translation]( https://arxiv.org/abs/1910.12368 )

Authors: [Tejas Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+T), [Ramon Sanabria](https://arxiv.org/search/cs?searchtype=author&query=Sanabria%2C+R), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F)

*(Submitted on 27 Oct 2019)*

> In Neural Machine Translation (NMT) the usage of subwords and characters as source and target units offers a simple and flexible solution for translation of rare and unseen words. However, selecting the optimal subword segmentation involves a trade-off between expressiveness and flexibility, and is language and dataset-dependent. We present Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy. Our multi-task model exhibits improvements of up to 1.7 BLEU points on each decoder over single-task baseline models with the same number of parameters on datasets from two language pairs of IWSLT15 and one from IWSLT19. The multiple hypotheses generated at different granularities can be combined as a post-processing step to give better translations, which improves over hypothesis combination from baseline models while using substantially fewer parameters.

| Comments: | Accepted to 16th International Workshop on Spoken Language Translation (IWSLT) 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.12368](https://arxiv.org/abs/1910.12368) [cs.CL]** |
|           | (or **[arXiv:1910.12368v1](https://arxiv.org/abs/1910.12368v1) [cs.CL]** for this version) |



# 2019-10-28

[Return to Index](#Index)



<h2 id="2019-10-28-1">1. DENS: A Dataset for Multi-class Emotion Analysis</h2>
Title: [DENS: A Dataset for Multi-class Emotion Analysis]( https://arxiv.org/abs/1910.11769 )

Authors: [Chen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Muhammad Osama](https://arxiv.org/search/cs?searchtype=author&query=Osama%2C+M), [Anderson de Andrade](https://arxiv.org/search/cs?searchtype=author&query=de+Andrade%2C+A)

*(Submitted on 25 Oct 2019)*

> We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives available on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.

| Comments: | Accepted to EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.11769](https://arxiv.org/abs/1910.11769) [cs.CL]** |
|           | (or **[arXiv:1910.11769v1](https://arxiv.org/abs/1910.11769v1) [cs.CL]** for this version) |





<h2 id="2019-10-28-2">2. On the Cross-lingual Transferability of Monolingual Representations</h2>
Title: [On the Cross-lingual Transferability of Monolingual Representations]( https://arxiv.org/abs/1910.11856 )

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D)

*(Submitted on 25 Oct 2019)*

> State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective -freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.11856](https://arxiv.org/abs/1910.11856) [cs.CL]** |
|           | (or **[arXiv:1910.11856v1](https://arxiv.org/abs/1910.11856v1) [cs.CL]** for this version) |





<h2 id="2019-10-28-3">3. Analyzing ASR pretraining for low-resource speech-to-text translation</h2>
Title: [Analyzing ASR pretraining for low-resource speech-to-text translation]( https://arxiv.org/abs/1910.10762 )

Authors: [Mihaela C. Stoian](https://arxiv.org/search/cs?searchtype=author&query=Stoian%2C+M+C), [Sameer Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+S), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

*(Submitted on 23 Oct 2019)*

> Previous work has shown that for low-resource source languages, automatic speech-to-text translation (AST) can be improved by pretraining an end-to-end model on automatic speech recognition (ASR) data from a high-resource language. However, it is not clear what factors --e.g., language relatedness or size of the pretraining data-- yield the biggest improvements, or whether pretraining can be effectively combined with other methods such as data augmentation. Here, we experiment with pretraining on datasets of varying sizes, including languages related and unrelated to the AST source language. We find that the best predictor of final AST performance is the word error rate of the pretrained ASR model, and that differences in ASR/AST performance correlate with how phonetic information is encoded in the later RNN layers of our model. We also show that pretraining and data augmentation yield complementary benefits for AST.

| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.10762](https://arxiv.org/abs/1910.10762) [cs.CL]** |
|           | (or **[arXiv:1910.10762v1](https://arxiv.org/abs/1910.10762v1) [cs.CL]** for this version) |





<h2 id="2019-10-28-4">4. Pun-GAN: Generative Adversarial Network for Pun Generation</h2>
Title: [Pun-GAN: Generative Adversarial Network for Pun Generation]( https://arxiv.org/abs/1910.10950 )

Authors: [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Shunyao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Pengcheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+P), [Lei li](https://arxiv.org/search/cs?searchtype=author&query=li%2C+L), [Baobao Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

*(Submitted on 24 Oct 2019)*

> In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide the supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN), which does not require any pun corpus. It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences that can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.

| Comments: | EMNLP 2019 (short paper)                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.10950](https://arxiv.org/abs/1910.10950) [cs.CL]** |
|           | (or **[arXiv:1910.10950v1](https://arxiv.org/abs/1910.10950v1) [cs.CL]** for this version) |





<h2 id="2019-10-28-5">5. Wasserstein distances for evaluating cross-lingual embeddings</h2>
Title: [Wasserstein distances for evaluating cross-lingual embeddings]( https://arxiv.org/abs/1910.11005 )

Authors: [Georgios Balikas](https://arxiv.org/search/cs?searchtype=author&query=Balikas%2C+G), [Ioannis Partalas](https://arxiv.org/search/cs?searchtype=author&query=Partalas%2C+I)

*(Submitted on 24 Oct 2019)*

> Word embeddings are high dimensional vector representations of words that capture their semantic similarity in the vector space. There exist several algorithms for learning such embeddings both for a single language as well as for several languages jointly. In this work we propose to evaluate collections of embeddings by adapting downstream natural language tasks to the optimal transport framework. We show how the family of Wasserstein distances can be used to solve cross-lingual document retrieval and the cross-lingual document classification problems. We argue on the advantages of this approach compared to more traditional evaluation methods of embeddings like bilingual lexical induction. Our experimental results suggest that using Wasserstein distances on these problems out-performs several strong baselines and performs on par with state-of-the-art models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.11005](https://arxiv.org/abs/1910.11005) [cs.CL]** |
|           | (or **[arXiv:1910.11005v1](https://arxiv.org/abs/1910.11005v1) [cs.CL]** for this version) |





<h2 id="2019-10-28-6">6. Promoting the Knowledge of Source Syntax in Transformer NMT Is Not Needed</h2>
Title: [Promoting the Knowledge of Source Syntax in Transformer NMT Is Not Needed]( https://arxiv.org/abs/1910.11218 )

Authors: [Thuong-Hai Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+T), [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

*(Submitted on 24 Oct 2019)*

> The utility of linguistic annotation in neural machine translation seemed to had been established in past papers. The experiments were however limited to recurrent sequence-to-sequence architectures and relatively small data settings. We focus on the state-of-the-art Transformer model and use comparably larger corpora. Specifically, we try to promote the knowledge of source-side syntax using multi-task learning either through simple data manipulation techniques or through a dedicated model component. In particular, we train one of Transformer attention heads to produce source-side dependency tree. Overall, our results cast some doubt on the utility of multi-task setups with linguistic information. The data manipulation techniques, recommended in previous works, prove ineffective in large data settings. The treatment of self-attention as dependencies seems much more promising: it helps in translation and reveals that Transformer model can very easily grasp the syntactic structure. An important but curious result is, however, that identical gains are obtained by using trivial "linear trees" instead of true dependencies. The reason for the gain thus may not be coming from the added linguistic knowledge but from some simpler regularizing effect we induced on self-attention matrices.

| Comments:          | CICLING 2019                                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Computac\'ion y Sistemas, Vol. 23, No. 3, 2019, pp. 923-934  |
| DOI:               | [10.13053/CyS-23-3-3265](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.13053%2FCyS-23-3-3265&v=cb71f273) |
| Cite as:           | **[arXiv:1910.11218](https://arxiv.org/abs/1910.11218) [cs.CL]** |
|                    | (or **[arXiv:1910.11218v1](https://arxiv.org/abs/1910.11218v1) [cs.CL]** for this version) |



# 2019-10-25

[Return to Index](#Index)



<h2 id="2019-10-25-1">1. A context sensitive real-time Spell Checker with language adaptability</h2>
Title: [A context sensitive real-time Spell Checker with language adaptability]( https://arxiv.org/abs/1910.11242 )

Authors: [Prabhakar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P)

*(Submitted on 23 Oct 2019)*

> We present a novel language adaptable spell checking system which detects spelling errors and suggests context sensitive corrections in real-time. We show that our system can be extended to new languages with minimal language-specific processing. Available literature majorly discusses spell checkers for English but there are no publicly available systems which can be extended to work for other languages out of the box. Most of the systems do not work in real-time. We explain the process of generating a language's word dictionary and n-gram probability dictionaries using Wikipedia-articles data and manually curated video subtitles. We present the results of generating a list of suggestions for a misspelled word. We also propose three approaches to create noisy channel datasets of real-world typographic errors. We compare our system with industry-accepted spell checker tools for 11 languages. Finally, we show the performance of our system on synthetic datasets for 24 languages.

| Comments: | 7 pages, 6 images                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.11242](https://arxiv.org/abs/1910.11242) [cs.CL]** |
|           | (or **[arXiv:1910.11242v1](https://arxiv.org/abs/1910.11242v1) [cs.CL]** for this version) |





<h2 id="2019-10-25-2">2. Fast Structured Decoding for Sequence Models</h2>
Title: [Fast Structured Decoding for Sequence Models]( https://arxiv.org/abs/1910.11555 )

Authors: [Zhiqing Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Zhuohan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Haoqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Zi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Zhi-Hong Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Z)

*(Submitted on 25 Oct 2019)*

> Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to speed up the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve then decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8~14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.

| Comments: | Accepted to NeurIPS 2019 (Previous title: Structured Decoding for Non-Autoregressive Machine Translation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.11555](https://arxiv.org/abs/1910.11555) [cs.LG]** |
|           | (or **[arXiv:1910.11555v1](https://arxiv.org/abs/1910.11555v1) [cs.LG]** for this version) |





<h2 id="2019-10-25-3">3. Machine Translation from Natural Language to Code using Long-Short Term Memory</h2>
Title: [Machine Translation from Natural Language to Code using Long-Short Term Memory]( https://arxiv.org/abs/1910.11471 )

Authors: [K.M. Tahsin Hassan Rahit](https://arxiv.org/search/cs?searchtype=author&query=Rahit%2C+K+T+H), [Rashidul Hasan Nabil](https://arxiv.org/search/cs?searchtype=author&query=Nabil%2C+R+H), [Md Hasibul Huq](https://arxiv.org/search/cs?searchtype=author&query=Huq%2C+M+H)

*(Submitted on 25 Oct 2019)*

> Making computer programming language more understandable and easy for the human is a longstanding problem. From assembly language to present day's object-oriented programming, concepts came to make programming easier so that a programmer can focus on the logic and the architecture rather than the code and language itself. To go a step further in this journey of removing human-computer language barrier, this paper proposes machine learning approach using Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) to convert human language into programming language code. The programmer will write expressions for codes in layman's language, and the machine learning model will translate it to the targeted programming language. The proposed approach yields result with 74.40% accuracy. This can be further improved by incorporating additional techniques, which are also discussed in this paper.

| Comments:          | 8 pages, 3 figures, conference                               |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Programming Languages (cs.PL) |
| ACM classes:       | D.3.4; I.2.2                                                 |
| Journal reference: | Proceedings of the Future Technologies Conference (FTC) 2019. Advances in Intelligent Systems and Computing, vol 1069. Springer, Cham |
| DOI:               | [10.1007/978-3-030-32520-6_6](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-3-030-32520-6_6&v=e9c8ac7d) |
| Cite as:           | **[arXiv:1910.11471](https://arxiv.org/abs/1910.11471) [cs.CL]** |
|                    | (or **[arXiv:1910.11471v1](https://arxiv.org/abs/1910.11471v1) [cs.CL]** for this version) |





<h2 id="2019-10-25-4">4. The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</h2>
Title: [The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection]( https://arxiv.org/abs/1910.11493 )

Authors: [Arya D. McCarthy](https://arxiv.org/search/cs?searchtype=author&query=McCarthy%2C+A+D), [Ekaterina Vylomova](https://arxiv.org/search/cs?searchtype=author&query=Vylomova%2C+E), [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Chaitanya Malaviya](https://arxiv.org/search/cs?searchtype=author&query=Malaviya%2C+C), [Lawrence Wolf-Sonkin](https://arxiv.org/search/cs?searchtype=author&query=Wolf-Sonkin%2C+L), [Garrett Nicolai](https://arxiv.org/search/cs?searchtype=author&query=Nicolai%2C+G), [Christo Kirov](https://arxiv.org/search/cs?searchtype=author&query=Kirov%2C+C), [Miikka Silfverberg](https://arxiv.org/search/cs?searchtype=author&query=Silfverberg%2C+M), [Sebastian J. Mielke](https://arxiv.org/search/cs?searchtype=author&query=Mielke%2C+S+J), [Jeffrey Heinz](https://arxiv.org/search/cs?searchtype=author&query=Heinz%2C+J), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Mans Hulden](https://arxiv.org/search/cs?searchtype=author&query=Hulden%2C+M)

*(Submitted on 25 Oct 2019)*

> The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.

| Comments:          | Presented at SIGMORPHON 2019                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology (2019) 229-244 |
| DOI:               | [10.18653/v1/W19-4226](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FW19-4226&v=b11cd64b) |
| Cite as:           | **[arXiv:1910.11493](https://arxiv.org/abs/1910.11493) [cs.CL]** |
|                    | (or **[arXiv:1910.11493v1](https://arxiv.org/abs/1910.11493v1) [cs.CL]** for this version) |



# 2019-10-24

[Return to Index](#Index)



<h2 id="2019-10-24-1">1. Robust Neural Machine Translation for Clean and Noisy Speech Transcripts</h2>
Title: [Robust Neural Machine Translation for Clean and Noisy Speech Transcripts]( https://arxiv.org/abs/1910.10238 )

Authors: [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Robert Enyedi](https://arxiv.org/search/cs?searchtype=author&query=Enyedi%2C+R), [Alessandra Brusadin](https://arxiv.org/search/cs?searchtype=author&query=Brusadin%2C+A), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M)

*(Submitted on 22 Oct 2019)*

> Neural machine translation models have shown to achieve high quality when trained and fed with well structured and punctuated input texts. Unfortunately, the latter condition is not met in spoken language translation, where the input is generated by an automatic speech recognition (ASR) system. In this paper, we study how to adapt a strong NMT system to make it robust to typical ASR errors. As in our application scenarios transcripts might be post-edited by human experts, we propose adaptation strategies to train a single system that can translate either clean or noisy input with no supervision on the input type. Our experimental results on a public speech translation data set show that adapting a model on a significant amount of parallel data including ASR transcripts is beneficial with test data of the same type, but produces a small degradation when translating clean text. Adapting on both clean and noisy variants of the same data leads to the best results on both input types.

| Comments: | 6 pages, accepted at IWSLT 2019                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:1910.10238](https://arxiv.org/abs/1910.10238) [cs.CL]** |
|           | (or **[arXiv:1910.10238v1](https://arxiv.org/abs/1910.10238v1) [cs.CL]** for this version) |





<h2 id="2019-10-24-2">2. Controlling the Output Length of Neural Machine Translation</h2>
Title: [Controlling the Output Length of Neural Machine Translation]( https://arxiv.org/abs/1910.10408 )

Authors: [Surafel Melaku Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Mattia Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M)

*(Submitted on 23 Oct 2019)*

> The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of machine translation, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, quality should not only be measured in terms of adequacy and fluency, but also length. Exemplary cases are the translation of document files, subtitles, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This paper addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture: i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the network to generate shorter translations, as well as acquiring interpretable linguistic skills.

| Comments: | Accepted at the 16th International Workshop on Spoken Language Translation (IWSLT), 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.10408](https://arxiv.org/abs/1910.10408) [cs.CL]** |
|           | (or **[arXiv:1910.10408v1](https://arxiv.org/abs/1910.10408v1) [cs.CL]** for this version) |





<h2 id="2019-10-24-3">3. XL-Editor: Post-editing Sentences with XLNet</h2>
Title: [XL-Editor: Post-editing Sentences with XLNet]( https://arxiv.org/abs/1910.10479 )

Authors: [Yong-Siang Shih](https://arxiv.org/search/cs?searchtype=author&query=Shih%2C+Y), [Wei-Cheng Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+W), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

*(Submitted on 19 Oct 2019)*

> While neural sequence generation models achieve initial success for many NLP applications, the canonical decoding procedure with left-to-right generation order (i.e., autoregressive) in one-pass can not reflect the true nature of human revising a sentence to obtain a refined result. In this work, we propose XL-Editor, a novel training framework that enables state-of-the-art generalized autoregressive pretraining methods, XLNet specifically, to revise a given sentence by the variable-length insertion probability. Concretely, XL-Editor can (1) estimate the probability of inserting a variable-length sequence into a specific position of a given sentence; (2) execute post-editing operations such as insertion, deletion, and replacement based on the estimated variable-length insertion probability; (3) complement existing sequence-to-sequence models to refine the generated sequences. Empirically, we first demonstrate better post-editing capabilities of XL-Editor over XLNet on the text insertion and deletion tasks, which validates the effectiveness of our proposed framework. Furthermore, we extend XL-Editor to the unpaired text style transfer task, where transferring the target style onto a given sentence can be naturally viewed as post-editing the sentence into the target style. XL-Editor achieves significant improvement in style transfer accuracy and also maintains coherent semantic of the original sentence, showing the broad applicability of our method.

| Comments: | Under review                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.10479](https://arxiv.org/abs/1910.10479) [cs.CL]** |
|           | (or **[arXiv:1910.10479v1](https://arxiv.org/abs/1910.10479v1) [cs.CL]** for this version) |





<h2 id="2019-10-24-4">4. Fully Quantized Transformer for Improved Translation</h2>
Title: [Fully Quantized Transformer for Improved Translation]( https://arxiv.org/abs/1910.10485 )

Authors: [Gabriele Prato](https://arxiv.org/search/cs?searchtype=author&query=Prato%2C+G), [Ella Charlaix](https://arxiv.org/search/cs?searchtype=author&query=Charlaix%2C+E), [Mehdi Rezagholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Rezagholizadeh%2C+M)

*(Submitted on 17 Oct 2019)*

> State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsolved. In this work, we propose a quantization strategy tailored to the Transformer architecture. We evaluate our method on the WMT14 EN-FR and WMT14 EN-DE translation tasks and achieve state-of-the-art quantization results for the Transformer, obtaining no loss in BLEU scores compared to the non-quantized baseline. We further compress the Transformer by showing that, once the model is trained, a good portion of the nodes in the encoder can be removed without causing any loss in BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.10485](https://arxiv.org/abs/1910.10485) [cs.CL]** |
|           | (or **[arXiv:1910.10485v1](https://arxiv.org/abs/1910.10485v1) [cs.CL]** for this version) |





<h2 id="2019-10-24-5">5. Instance-Based Model Adaptation For Direct Speech Translation</h2>
Title: [Instance-Based Model Adaptation For Direct Speech Translation]( https://arxiv.org/abs/1910.10663 )

Authors: [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Viet-Nhat Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+V), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 23 Oct 2019)*

> Despite recent technology advancements, the effectiveness of neural approaches to end-to-end speech-to-text translation is still limited by the paucity of publicly available training corpora. We tackle this limitation with a method to improve data exploitation and boost the system's performance at inference time. Our approach allows us to customize "on the fly" an existing model to each incoming translation request. At its core, it exploits an instance selection procedure to retrieve, from a given pool of data, a small set of samples similar to the input query in terms of latent properties of its audio signal. The retrieved samples are then used for an instance-specific fine-tuning of the model. We evaluate our approach in three different scenarios. In all data conditions (different languages, in/out-of-domain adaptation), our instance-based adaptation yields coherent performance gains over static models.

| Comments: | 6 pages, under review at ICASSP 2020                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:1910.10663](https://arxiv.org/abs/1910.10663) [cs.CL]** |
|           | (or **[arXiv:1910.10663v1](https://arxiv.org/abs/1910.10663v1) [cs.CL]** for this version) |





# 2019-10-23

[Return to Index](#Index)



<h2 id="2019-10-23-1">1. Depth-Adaptive Transformer</h2>
Title: [Depth-Adaptive Transformer]( https://arxiv.org/abs/1910.10073 )

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

*(Submitted on 22 Oct 2019)*

> State of the art sequence-to-sequence models perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. Experiments on machine translation benchmarks show that this approach can match the accuracy of a baseline Transformer while using only half the number of decoder layers.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.10073](https://arxiv.org/abs/1910.10073) [cs.CL]** |
|           | (or **[arXiv:1910.10073v1](https://arxiv.org/abs/1910.10073v1) [cs.CL]** for this version) |



# 2019-10-22

[Return to Index](#Index)



<h2 id="2019-10-22-1">1. Automatic Post-Editing for Machine Translation</h2>
Title: [Automatic Post-Editing for Machine Translation]( https://arxiv.org/abs/1910.08592 )

Authors： [Rajen Chatterjee](https://arxiv.org/search/cs?searchtype=author&query=Chatterjee%2C+R)

*(Submitted on 18 Oct 2019)*

> Automatic Post-Editing (APE) aims to correct systematic errors in a machine translated text. This is primarily useful when the machine translation (MT) system is not accessible for improvement, leaving APE as a viable option to improve translation quality as a downstream task - which is the focus of this thesis. This field has received less attention compared to MT due to several reasons, which include: the limited availability of data to perform a sound research, contrasting views reported by different researchers about the effectiveness of APE, and limited attention from the industry to use APE in current production pipelines. In this thesis, we perform a thorough investigation of APE as a downstream task in order to: i) understand its potential to improve translation quality; ii) advance the core technology - starting from classical methods to recent deep-learning based solutions; iii) cope with limited and sparse data; iv) better leverage multiple input sources; v) mitigate the task-specific problem of over-correction; vi) enhance neural decoding to leverage external knowledge; and vii) establish an online learning framework to handle data diversity in real-time. All the above contributions are discussed across several chapters, and most of them are evaluated in the APE shared task organized each year at the Conference on Machine Translation. Our efforts in improving the technology resulted in the best system at the 2017 APE shared task, and our work on online learning received a distinguished paper award at the Italian Conference on Computational Linguistics. Overall, outcomes and findings of our work have boost interest among researchers and attracted industries to examine this technology to solve real-word problems.

| Comments: | PhD dissertation on Automatic Post-Editing for Machine Translation (this work has been done between 2014 and 2017) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.08592](https://arxiv.org/abs/1910.08592) [cs.CL]** |
|           | (or **[arXiv:1910.08592v1](https://arxiv.org/abs/1910.08592v1) [cs.CL]** for this version) |



# 2019-10-21

[Return to Index](#Index)



<h2 id="2019-10-21-1">1. Controlling Utterance Length in NMT-based Word Segmentation with Attention</h2>
Title: [Controlling Utterance Length in NMT-based Word Segmentation with Attention]( https://arxiv.org/abs/1910.08418 )

Authors: [Pierre Godard](https://arxiv.org/search/cs?searchtype=author&query=Godard%2C+P), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Francois Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

*(Submitted on 18 Oct 2019)*

> One of the basic tasks of computational language documentation (CLD) is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, well-resourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.

| Comments: | Accepted to IWSLT 2019 (Hong-Kong)                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.08418](https://arxiv.org/abs/1910.08418) [cs.CL]** |
|           | (or **[arXiv:1910.08418v1](https://arxiv.org/abs/1910.08418v1) [cs.CL]** for this version) |



# 2019-10-18

[Return to Index](#Index)



<h2 id="2019-10-18-1">1. LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition</h2>
Title: [LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition]( https://arxiv.org/abs/1910.07924 )

Authors: [Benjamin Beilharz](https://arxiv.org/search/cs?searchtype=author&query=Beilharz%2C+B), [Xin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Sariya Karimova](https://arxiv.org/search/cs?searchtype=author&query=Karimova%2C+S), [Stefan Riezler](https://arxiv.org/search/cs?searchtype=author&query=Riezler%2C+S)

*(Submitted on 17 Oct 2019)*

> We present a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material and over 50k parallel sentences. The audio data is read speech and thus low in disfluencies. The quality of audio and sentence alignments has been checked by a manual evaluation, showing that speech alignment quality is in general very high. The sentence alignment quality is comparable to well-used parallel translation data and can be adjusted by cutoffs on the automatic alignment score. To our knowledge, this corpus is to date the largest resource for end-to-end speech translation for German.

| Comments: | LREC 2020 submission                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.07924](https://arxiv.org/abs/1910.07924) [cs.CL]** |
|           | (or **[arXiv:1910.07924v1](https://arxiv.org/abs/1910.07924v1) [cs.CL]** for this version) |



# 2019-10-17

[Return to Index](#Index)



<h2 id="2019-10-17-1">1. Root Mean Square Layer Normalization</h2>
Title: [Root Mean Square Layer Normalization]( https://arxiv.org/abs/1910.07467 )

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

*(Submitted on 16 Oct 2019)*

> Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at [this https URL](https://github.com/bzhangGo/rmsnorm).

| Comments: | NeurIPS 2019                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.07467](https://arxiv.org/abs/1910.07467) [cs.LG]** |
|           | (or **[arXiv:1910.07467v1](https://arxiv.org/abs/1910.07467v1) [cs.LG]** for this version) |





<h2 id="2019-10-17-2">2. Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models</h2>
Title: [Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models]( https://arxiv.org/abs/1910.07117 )

Authors: [Tianxing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+T), [Jun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Bing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J), [Fuchun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+F)

*(Submitted on 16 Oct 2019)*

> In this work, we study how the large-scale pretrain-finetune framework changes the behavior of a neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. We find that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training. We demonstrate the forgetting phenomenon through a detailed behavior analysis from the perspectives of context sensitivity and knowledge transfer. Adopting the concept of data mixing, we propose an intuitive fine-tuning strategy named "mix-review". We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.07117](https://arxiv.org/abs/1910.07117) [cs.CL]** |
|           | (or **[arXiv:1910.07117v1](https://arxiv.org/abs/1910.07117v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-3">3. Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings</h2>
Title: [Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings]( https://arxiv.org/abs/1910.07221 )

Authors: [Yerai Doval](https://arxiv.org/search/cs?searchtype=author&query=Doval%2C+Y), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J), [Luis Espinosa-Anke](https://arxiv.org/search/cs?searchtype=author&query=Espinosa-Anke%2C+L), [Steven Schockaert](https://arxiv.org/search/cs?searchtype=author&query=Schockaert%2C+S)

*(Submitted on 16 Oct 2019)*

> Word embeddings have become a standard resource in the toolset of any Natural Language Processing practitioner. While monolingual word embeddings encode information about words in the context of a particular language, cross-lingual embeddings define a multilingual space where word embeddings from two or more languages are integrated together. Current state-of-the-art approaches learn these embeddings by aligning two disjoint monolingual vector spaces through an orthogonal transformation which preserves the structure of the monolingual counterparts. In this work, we propose to apply an additional transformation after this initial alignment step, which aims to bring the vector representations of a given word and its translations closer to their average. Since this additional transformation is non-orthogonal, it also affects the structure of the monolingual spaces. We show that our approach both improves the integration of the monolingual spaces as well as the quality of the monolingual spaces themselves. Furthermore, because our transformation can be applied to an arbitrary number of languages, we are able to effectively obtain a truly multilingual space. The resulting (monolingual and multilingual) spaces show consistent gains over the current state-of-the-art in standard intrinsic tasks, namely dictionary induction and word similarity, as well as in extrinsic tasks such as cross-lingual hypernym discovery and cross-lingual natural language inference.

| Comments:    | 32 pages, 2 figures, 7 tables. Preprint submitted to Journal of Information Processing and Management |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| Cite as:     | **[arXiv:1910.07221](https://arxiv.org/abs/1910.07221) [cs.CL]** |
|              | (or **[arXiv:1910.07221v1](https://arxiv.org/abs/1910.07221v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-4">4. Fine-grained evaluation of German-English Machine Translation based on a Test Suite</h2>
Title: [Fine-grained evaluation of German-English Machine Translation based on a Test Suite]( https://arxiv.org/abs/1910.07460 )

Authors: [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Eleftherios Avramidis](https://arxiv.org/search/cs?searchtype=author&query=Avramidis%2C+E), [Aljoscha Burchardt](https://arxiv.org/search/cs?searchtype=author&query=Burchardt%2C+A), [Hans Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+H)

*(Submitted on 16 Oct 2019)*

> We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The test suite has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the Third Conference on Machine Translation (WMT-2018) |
| DOI:               | [10.18653/v1/W18-6436](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FW18-6436&v=5a63db80) |
| Cite as:           | **[arXiv:1910.07460](https://arxiv.org/abs/1910.07460) [cs.CL]** |
|                    | (or **[arXiv:1910.07460v1](https://arxiv.org/abs/1910.07460v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-5">5. Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite</h2>
Title: [Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite]( https://arxiv.org/abs/1910.07468 )

Authors: [Avramidis Eleftherios](https://arxiv.org/search/cs?searchtype=author&query=Eleftherios%2C+A), [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Arle Lommel](https://arxiv.org/search/cs?searchtype=author&query=Lommel%2C+A), [Hans Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+H)

*(Submitted on 16 Oct 2019)*

> We present an alternative method of evaluating Quality Estimation systems, which is based on a linguistically-motivated Test Suite. We create a test-set consisting of 14 linguistic error categories and we gather for each of them a set of samples with both correct and erroneous translations. Then, we measure the performance of 5 Quality Estimation systems by checking their ability to distinguish between the correct and the erroneous translations. The detailed results are much more informative about the ability of each system. The fact that different Quality Estimation systems perform differently at various phenomena confirms the usefulness of the Test Suite.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the First Workshop on Translation Quality Estimation and Automatic Post-Editing (QEAPE-2018) |
| Cite as:           | **[arXiv:1910.07468](https://arxiv.org/abs/1910.07468) [cs.CL]** |
|                    | (or **[arXiv:1910.07468v1](https://arxiv.org/abs/1910.07468v1) [cs.CL]** for this version) |





<h2 id="2019-10-17-6">6. Using Whole Document Context in Neural Machine Translation</h2>
Title: [Using Whole Document Context in Neural Machine Translation]( https://arxiv.org/abs/1910.07481 )

Authors: [Valentin Macé](https://arxiv.org/search/cs?searchtype=author&query=Macé%2C+V), [Christophe Servan](https://arxiv.org/search/cs?searchtype=author&query=Servan%2C+C)

*(Submitted on 16 Oct 2019)*

> In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.

| Comments: | Accepted paper to IWSLT2019                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.07481](https://arxiv.org/abs/1910.07481) [cs.CL]** |
|           | (or **[arXiv:1910.07481v1](https://arxiv.org/abs/1910.07481v1) [cs.CL]** for this version) |



# 2019-10-16

[Return to Index](#Index)



<h2 id="2019-10-16-1">1. In-training Matrix Factorization for Parameter-frugal Neural Machine Translation</h2>
Title: [In-training Matrix Factorization for Parameter-frugal Neural Machine Translation]( https://arxiv.org/abs/1910.06393 )

Authors: [Zachary Kaden](https://arxiv.org/search/cs?searchtype=author&query=Kaden%2C+Z), [Teven Le Scao](https://arxiv.org/search/cs?searchtype=author&query=Scao%2C+T+L), [Raphael Olivier](https://arxiv.org/search/cs?searchtype=author&query=Olivier%2C+R)

*(Submitted on 27 Sep 2019)*

> In this paper, we propose the use of in-training matrix factorization to reduce the model size for neural machine translation. Using in-training matrix factorization, parameter matrices may be decomposed into the products of smaller matrices, which can compress large machine translation architectures by vastly reducing the number of learnable parameters. We apply in-training matrix factorization to different layers of standard neural architectures and show that in-training factorization is capable of reducing nearly 50% of learnable parameters without any associated loss in BLEU score. Further, we find that in-training matrix factorization is especially powerful on embedding layers, providing a simple and effective method to curtail the number of parameters with minimal impact on model performance, and, at times, an increase in performance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.06393](https://arxiv.org/abs/1910.06393) [cs.CL]** |
|           | (or **[arXiv:1910.06393v1](https://arxiv.org/abs/1910.06393v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-2">2. Mapping Supervised Bilingual Word Embeddings from English to low-resource languages</h2>
Title: [Mapping Supervised Bilingual Word Embeddings from English to low-resource languages]( https://arxiv.org/abs/1910.06411 )

Authors: [Sourav Dutta](https://arxiv.org/search/cs?searchtype=author&query=Dutta%2C+S) (1) ((1) Saarland University)

*(Submitted on 14 Oct 2019)*

> It is very challenging to work with low-resource languages due to the inadequate availability of data. Using a dictionary to map independently trained word embeddings into a shared vector space has proved to be very useful in learning bilingual embeddings in the past. Here we have tried to map individual embeddings of words in English and their corresponding translated words in low-resource languages like Estonian, Slovenian, Slovakian, and Hungarian. We have used a supervised learning approach. We report accuracy scores through various retrieval strategies which show that it is possible to approach challenging tasks in Natural Language Processing like machine translation for such languages, provided that we have at least some amount of proper bilingual data. We also conclude that we can follow an unsupervised learning path on monolingual text data as that is more suitable for low-resource languages.

| Comments: | 7 pages, 4 tables                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06411](https://arxiv.org/abs/1910.06411) [cs.CL]** |
|           | (or **[arXiv:1910.06411v1](https://arxiv.org/abs/1910.06411v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-3">3. Detecting Machine-Translated Text using Back Translation</h2>
Title: [Detecting Machine-Translated Text using Back Translation]( https://arxiv.org/abs/1910.06558 )

Authors: [Hoang-Quoc Nguyen-Son](https://arxiv.org/search/cs?searchtype=author&query=Nguyen-Son%2C+H), [Tran Phuong Thao](https://arxiv.org/search/cs?searchtype=author&query=Thao%2C+T+P), [Seira Hidano](https://arxiv.org/search/cs?searchtype=author&query=Hidano%2C+S), [Shinsaku Kiyomoto](https://arxiv.org/search/cs?searchtype=author&query=Kiyomoto%2C+S)

*(Submitted on 15 Oct 2019)*

> Machine-translated text plays a crucial role in the communication of people using different languages. However, adversaries can use such text for malicious purposes such as plagiarism and fake review. The existing methods detected a machine-translated text only using the text's intrinsic content, but they are unsuitable for classifying the machine-translated and human-written texts with the same meanings. We have proposed a method to extract features used to distinguish machine/human text based on the similarity between the intrinsic text and its back-translation. The evaluation of detecting translated sentences with French shows that our method achieves 75.0% of both accuracy and F-score. It outperforms the existing methods whose the best accuracy is 62.8% and the F-score is 62.7%. The proposed method even detects more efficiently the back-translated text with 83.4% of accuracy, which is higher than 66.7% of the best previous accuracy. We also achieve similar results not only with F-score but also with similar experiments related to Japanese. Moreover, we prove that our detector can recognize both machine-translated and machine-back-translated texts without the language information which is used to generate these machine texts. It demonstrates the persistence of our method in various applications in both low- and rich-resource languages.

| Comments: | INLG 2019, 9 pages                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06558](https://arxiv.org/abs/1910.06558) [cs.CL]** |
|           | (or **[arXiv:1910.06558v1](https://arxiv.org/abs/1910.06558v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-4">4. Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</h2>
Title: [Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation]( https://arxiv.org/abs/1910.06717 )

Authors: [Kenton Murray](https://arxiv.org/search/cs?searchtype=author&query=Murray%2C+K), [Jeffery Kinnison](https://arxiv.org/search/cs?searchtype=author&query=Kinnison%2C+J), [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Walter Scheirer](https://arxiv.org/search/cs?searchtype=author&query=Scheirer%2C+W), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

*(Submitted on 1 Oct 2019)*

> Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in machine translation. Yet these neural networks are very sensitive to architecture and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate architecture search into a single training run through auto-sizing, which uses regularization to delete neurons in a network over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the parameters from the model.

| Comments: | The 3rd Workshop on Neural Generation and Translation (WNGT 2019) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.06717](https://arxiv.org/abs/1910.06717) [cs.CL]** |
|           | (or **[arXiv:1910.06717v1](https://arxiv.org/abs/1910.06717v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-5">5. On the Importance of Word Boundaries in Character-level Neural Machine Translation</h2>
Title: [On the Importance of Word Boundaries in Character-level Neural Machine Translation]( https://arxiv.org/abs/1910.06753 )

Authors: [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Mattia A. Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

*(Submitted on 15 Oct 2019)*

> Neural Machine Translation (NMT) models generally perform translation using a fixed-size lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model.

| Comments: | To appear at the 3rd Workshop on Neural Generation and Translation (WNGT 2019) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06753](https://arxiv.org/abs/1910.06753) [cs.CL]** |
|           | (or **[arXiv:1910.06753v1](https://arxiv.org/abs/1910.06753v1) [cs.CL]** for this version) |





<h2 id="2019-10-16-6">6. Facebook AI's WAT19 Myanmar-English Translation Task Submission</h2>
Title: [Facebook AI's WAT19 Myanmar-English Translation Task Submission]( https://arxiv.org/abs/1910.06848 )

Authors: [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Matt Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+M), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 15 Oct 2019)*

> This paper describes Facebook AI's submission to the WAT 2019 Myanmar-English translation task. Our baseline systems are BPE-based transformer models. We explore methods to leverage monolingual data to improve generalization, including self-training, back-translation and their combination. We further improve results by using noisy channel re-ranking and ensembling. We demonstrate that these techniques can significantly improve not only a system trained with additional monolingual data, but even the baseline system trained exclusively on the provided small parallel dataset. Our system ranks first in both directions according to human evaluation and BLEU, with a gain of over 8 BLEU points above the second best system.

| Comments: | The 6th Workshop on Asian Translation                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06848](https://arxiv.org/abs/1910.06848) [cs.CL]** |
|           | (or **[arXiv:1910.06848v1](https://arxiv.org/abs/1910.06848v1) [cs.CL]** for this version) |





# 2019-10-15

[Return to Index](#Index)



<h2 id="2019-10-15-1">1. From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction</h2>
Title: [From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction]( https://arxiv.org/abs/1910.05535 )

Authors: [Mika Hämäläinen](https://arxiv.org/search/cs?searchtype=author&query=Hämäläinen%2C+M), [Simon Hengchen](https://arxiv.org/search/cs?searchtype=author&query=Hengchen%2C+S)

*(Submitted on 12 Oct 2019)*

> A great deal of historical corpora suffer from errors introduced by the OCR (optical character recognition) methods used in the digitization process. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We present a fully automatic unsupervised way of extracting parallel data for training a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of Recent Advances in Natural Language Processing. Angelova, G., Mitkov, R., Nikolova, I. & Temnikova, I. (eds.). Shoumen: INCOMA, p. 432-437 6 p (2019) |
| Cite as:           | **[arXiv:1910.05535](https://arxiv.org/abs/1910.05535) [cs.CL]** |
|                    | (or **[arXiv:1910.05535v1](https://arxiv.org/abs/1910.05535v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-2">2. Transformers without Tears: Improving the Normalization of Self-Attention</h2>
Title: [Transformers without Tears: Improving the Normalization of Self-Attention]( https://arxiv.org/abs/1910.05895 )

Authors: [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J)

*(Submitted on 14 Oct 2019)*

> We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose ℓ2 normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.

| Comments: | Accepted to IWSLT'19                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:1910.05895](https://arxiv.org/abs/1910.05895) [cs.CL]** |
|           | (or **[arXiv:1910.05895v1](https://arxiv.org/abs/1910.05895v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-3">3. Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality</h2>
Title: [Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality]( https://arxiv.org/abs/1910.06204 )

Authors: [Carolina Scarton](https://arxiv.org/search/cs?searchtype=author&query=Scarton%2C+C), [Mikel L. Forcada](https://arxiv.org/search/cs?searchtype=author&query=Forcada%2C+M+L), [Miquel Esplà-Gomis](https://arxiv.org/search/cs?searchtype=author&query=Esplà-Gomis%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 14 Oct 2019)*

> Devising metrics to assess translation quality has always been at the core of machine translation (MT) research. Traditional automatic reference-based metrics, such as BLEU, have shown correlations with human judgements of adequacy and fluency and have been paramount for the advancement of MT system development. Crowd-sourcing has popularised and enabled the scalability of metrics based on human judgements, such as subjective direct assessments (DA) of adequacy, that are believed to be more reliable than reference-based automatic metrics. Finally, task-based measurements, such as post-editing time, are expected to provide a more detailed evaluation of the usefulness of translations for a specific task. Therefore, while DA averages adequacy judgements to obtain an appraisal of (perceived) quality independently of the task, and reference-based automatic metrics try to objectively estimate quality also in a task-independent way, task-based metrics are measurements obtained either during or after performing a specific task. In this paper we argue that, although expensive, task-based measurements are the most reliable when estimating MT quality in a specific task; in our case, this task is post-editing. To that end, we report experiments on a dataset with newly-collected post-editing indicators and show their usefulness when estimating post-editing effort. Our results show that task-based metrics comparing machine-translated and post-edited versions are the best at tracking post-editing effort, as expected. These metrics are followed by DA, and then by metrics comparing the machine-translated version and independent references. We suggest that MT practitioners should be aware of these differences and acknowledge their implications when deciding how to evaluate MT for post-editing purposes.

| Comments: | IWSLT 2019, Hong Kong, November 2 and 3, 2019                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.06204](https://arxiv.org/abs/1910.06204) [cs.CL]** |
|           | (or **[arXiv:1910.06204v1](https://arxiv.org/abs/1910.06204v1) [cs.CL]** for this version) |





<h2 id="2019-10-15-4">4. Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment</h2>
Title: [Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment]( https://arxiv.org/abs/1910.06241 )

Authors: [Piotr Bojanowski](https://arxiv.org/search/cs?searchtype=author&query=Bojanowski%2C+P), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Tomas Mikolov](https://arxiv.org/search/cs?searchtype=author&query=Mikolov%2C+T), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A)

*(Submitted on 14 Oct 2019)*

> In this paper, we focus on the problem of adapting word vector-based models to new textual data. Given a model pre-trained on large reference data, how can we adapt it to a smaller piece of data with a slightly different language distribution? We frame the adaptation problem as a monolingual word vector alignment problem, and simply average models after alignment. We align vectors using the RCSLS criterion. Our formulation results in a simple and efficient algorithm that allows adapting general-purpose models to changing word distributions. In our evaluation, we consider applications to word embedding and text classification models. We show that the proposed approach yields good performance in all setups and outperforms a baseline consisting in fine-tuning the model on new data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1910.06241](https://arxiv.org/abs/1910.06241) [cs.CL]** |
|           | (or **[arXiv:1910.06241v1](https://arxiv.org/abs/1910.06241v1) [cs.CL]** for this version) |





# 2019-10-14

[Return to Index](#Index)

<h2 id="2019-10-14-1">1. How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages</h2>
Title: [How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages]( https://arxiv.org/abs/1910.05154 )

Authors: [Marcely Zanon Boito](https://arxiv.org/search/cs?searchtype=author&query=Boito%2C+M+Z), [Aline Villavicencio](https://arxiv.org/search/cs?searchtype=author&query=Villavicencio%2C+A), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

*(Submitted on 11 Oct 2019)*

> For language documentation initiatives, transcription is an expensive resource: one minute of audio is estimated to take one hour and a half on average of a linguist's work (Austin and Sallabank, 2013). Recently, collecting aligned translations in well-resourced languages became a popular solution for ensuring posterior interpretability of the recordings (Adda et al. 2016). In this paper we investigate language-related impact in automatic approaches for computational language documentation. We translate the bilingual Mboshi-French parallel corpus (Godard et al. 2017) into four other languages, and we perform bilingual-rooted unsupervised word discovery. Our results hint towards an impact of the well-resourced language in the quality of the output. However, by combining the information learned by different bilingual models, we are only able to marginally increase the quality of the segmentation.

| Comments: | 4 pages, workshop LIFT 2019                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:1910.05154](https://arxiv.org/abs/1910.05154) [cs.CL]** |
|           | (or **[arXiv:1910.05154v1](https://arxiv.org/abs/1910.05154v1) [cs.CL]** for this version) |





# 2019-10-11

[Return to Index](#Index)



<h2 id="2019-10-11-1">1. Novel Applications of Factored Neural Machine Translation</h2>
Title: [Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](https://arxiv.org/abs/1910.04708)

Authors: [Zirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Jiateng Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Ruochen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jaime Carbonell](https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J)

*(Submitted on 10 Oct 2019)*

> Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these methods across diverse cross-lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task-dependent. Stemming from this analysis, we propose a simple and novel framework that combines these two previously mutually-exclusive approaches. Extensive experiments on various tasks demonstrate that our proposed framework alleviates limitations of both approaches, and outperforms existing methods on the MUSE bilingual lexicon induction (BLI) benchmark. We further show that our proposed framework can generalize to contextualized representations and achieves state-of-the-art results on the CoNLL cross-lingual NER benchmark.

| Comments: | First two authors contributted equally. Source code is available at [this https URL](https://github.com/thespectrewithin/joint-align) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.04708 [cs.CL]**                                 |
|           | (or **arXiv:1910.04708v1 [cs.CL]** for this version)         |





<h2 id="2019-10-11-2">2. Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)</h2>
Title: [Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)](https://arxiv.org/abs/1910.04731)

Authors: [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Karin Sevegnani](https://arxiv.org/search/cs?searchtype=author&query=Sevegnani%2C+K), [Ioannis Konstas](https://arxiv.org/search/cs?searchtype=author&query=Konstas%2C+I), [Verena Rieser](https://arxiv.org/search/cs?searchtype=author&query=Rieser%2C+V)

*(Submitted on 10 Oct 2019)*

> We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network.
> We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: we synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12% increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dušek et al., 2019), where synthetic data lead to a 4% accuracy increase over the base model.

| Comments:    | Accepted as a short paper at INLG 2019               |
| ------------ | ---------------------------------------------------- |
| Subjects:    | **Computation and Language (cs.CL)**                 |
| ACM classes: | I.2.7                                                |
| Cite as:     | **arXiv:1910.04731 [cs.CL]**                         |
|              | (or **arXiv:1910.04731v1 [cs.CL]** for this version) |





# 2019-10-10

[Return to Index](#Index)

<h2 id="2019-10-10-1">1. Novel Applications of Factored Neural Machine Translation</h2>
Title: [Novel Applications of Factored Neural Machine Translation](https://arxiv.org/abs/1910.03912)

Authors: [Patrick Wilken](https://arxiv.org/search/cs?searchtype=author&query=Wilken%2C+P), [Evgeny Matusov](https://arxiv.org/search/cs?searchtype=author&query=Matusov%2C+E)

*(Submitted on 9 Oct 2019)*

> In this work, we explore the usefulness of target factors in neural machine translation (NMT) beyond their original purpose of predicting word lemmas and their inflections, as proposed by Garcìa-Martìnez et al., 2016. For this, we introduce three novel applications of the factored output architecture: In the first one, we use a factor to explicitly predict the word case separately from the target word itself. This allows for information to be shared between different casing variants of a word. In a second task, we use a factor to predict when two consecutive subwords have to be joined, eliminating the need for target subword joining markers. The third task is the prediction of special tokens of the operation sequence NMT model (OSNMT) of Stahlberg et al., 2018. Automatic evaluation on English-to-German and English-to-Turkish tasks showed that integration of such auxiliary prediction tasks into NMT is at least as good as the standard NMT approach. For the OSNMT, we observed a significant improvement in BLEU over the baseline OSNMT implementation due to a reduced output sequence length that resulted from the introduction of the target factors.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.03912 [cs.CL]**                                 |
|           | (or **arXiv:1910.03912v1 [cs.CL]** for this version)         |



# 2019-10-09

[Return to Index](#Index)



<h2 id="2019-10-09-1">1. Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation</h2>
Title: [Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation](https://arxiv.org/abs/1910.03009)

Authors: [Zhenhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 7 Oct 2019)*

> Neural Machine Translation (NMT) models have been proved strong when translating clean texts, but they are very sensitive to noise in the input. Improving NMT models robustness can be seen as a form of "domain'' adaption to noise. The recently created Machine Translation on Noisy Text task corpus provides noisy-clean parallel data for a few language pairs, but this data is very limited in size and diversity. The state-of-the-art approaches are heavily dependent on large volumes of back-translated data. This paper has two main contributions: Firstly, we propose new data augmentation methods to extend limited noisy data and further improve NMT robustness to noise while keeping the models small. Secondly, we explore the effect of utilizing noise from external data in the form of speech transcripts and show that it could help robustness.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.03009 [cs.CL]**                         |
|           | (or **arXiv:1910.03009v1 [cs.CL]** for this version) |





<h2 id="2019-10-09-2">2. Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations</h2>
Title: [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://arxiv.org/abs/1910.03065)

Authors: [Camburu Oana-Maria](https://arxiv.org/search/cs?searchtype=author&query=Oana-Maria%2C+C), [Shillingford Brendan](https://arxiv.org/search/cs?searchtype=author&query=Brendan%2C+S), [Minervini Pasquale](https://arxiv.org/search/cs?searchtype=author&query=Pasquale%2C+M), [Lukasiewicz Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+L), [Blunsom Phil](https://arxiv.org/search/cs?searchtype=author&query=Phil%2C+B)

*(Submitted on 7 Oct 2019)*

> To increase trust in artificial intelligence systems, a growing amount of works are enhancing these systems with the capability of producing natural language explanations that support their predictions. In this work, we show that such appealing frameworks are nonetheless prone to generating inconsistent explanations, such as "A dog is an animal" and "A dog is not an animal", which are likely to decrease users' trust in these systems. To detect such inconsistencies, we introduce a simple but effective adversarial framework for generating a complete target sequence, a scenario that has not been addressed so far. Finally, we apply our framework to a state-of-the-art neural model that provides natural language explanations on SNLI, and we show that this model is capable of generating a significant amount of inconsistencies.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | NeurIPS 2019 Workshop on Safety and Robustness in Decision Making, Vancouver, Canada |
| Cite as:           | **arXiv:1910.03065 [cs.CL]**                                 |
|                    | (or **arXiv:1910.03065v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-3">3. One-To-Many Multilingual End-to-end Speech Translation</h2>
Title: [One-To-Many Multilingual End-to-end Speech Translation](https://arxiv.org/abs/1910.03320)

Authors: [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 8 Oct 2019)*

> Nowadays, training end-to-end neural models for spoken language translation (SLT) still has to confront with extreme data scarcity conditions. The existing SLT parallel corpora are indeed orders of magnitude smaller than those available for the closely related tasks of automatic speech recognition (ASR) and machine translation (MT), which usually comprise tens of millions of instances. To cope with data paucity, in this paper we explore the effectiveness of transfer learning in end-to-end SLT by presenting a multilingual approach to the task. Multilingual solutions are widely studied in MT and usually rely on ``\textit{target forcing}'', in which multilingual parallel data are combined to train a single model by prepending to the input sequences a language token that specifies the target language. However, when tested in speech translation, our experiments show that MT-like \textit{target forcing}, used as is, is not effective in discriminating among the target languages. Thus, we propose a variant that uses target-language embeddings to shift the input representations in different portions of the space according to the language, so to better support the production of output in the desired target language. Our experiments on end-to-end SLT from English into six languages show important improvements when translating into similar languages, especially when these are supported by scarce data. Further improvements are obtained when using English ASR data as an additional language (up to *[Math Processing Error]* BLEU points).

| Comments: | 8 pages, one figure, version accepted at ASRU 2019           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.03320 [cs.CL]**                                 |
|           | (or **arXiv:1910.03320v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-4">4. An Interactive Machine Translation Framework for Modernizing Historical Documents</h2>
Title: [An Interactive Machine Translation Framework for Modernizing Historical Documents](https://arxiv.org/abs/1910.03355)

Authors: [Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F)

*(Submitted on 8 Oct 2019)*

> Due to the nature of human language, historical documents are hard to comprehend by contemporary people. This limits their accessibility to scholars specialized in the time period in which the documents were written. Modernization aims at breaking this language barrier by generating a new version of a historical document, written in the modern version of the document's original language. However, while it is able to increase the document's comprehension, modernization is still far from producing an error-free version. In this work, we propose a collaborative framework in which a scholar can work together with the machine to generate the new version. We tested our approach on a simulated environment, achieving significant reductions of the human effort needed to produce the modernized version of the document.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.03355 [cs.CL]**                         |
|           | (or **arXiv:1910.03355v1 [cs.CL]** for this version) |





<h2 id="2019-10-09-5">5. Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation</h2>
Title: [Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation](https://arxiv.org/abs/1910.03467)

Authors: [Thi-Vinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+T), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Phuong-Thai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P), [Le-Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L)

*(Submitted on 7 Oct 2019)*

> Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017), rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two low-resource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved significant improvements of up to roughly +1.0 BLEU points in both language pairs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.03467 [cs.CL]**                                 |
|           | (or **arXiv:1910.03467v1 [cs.CL]** for this version)         |





<h2 id="2019-10-09-6">6. Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents</h2>
Title: [Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents](https://arxiv.org/abs/1910.03487)

Authors: [Nikolaos Malandrakis](https://arxiv.org/search/cs?searchtype=author&query=Malandrakis%2C+N), [Minmin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+M), [Anuj Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+A), [Shuyang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+S), [Abhishek Sethi](https://arxiv.org/search/cs?searchtype=author&query=Sethi%2C+A), [Angeliki Metallinou](https://arxiv.org/search/cs?searchtype=author&query=Metallinou%2C+A)

*(Submitted on 4 Oct 2019)*

> Data availability is a bottleneck during early stages of development of new capabilities for intelligent artificial agents. We investigate the use of text generation techniques to augment the training data of a popular commercial artificial agent across categories of functionality, with the goal of faster development of new functionality. We explore a variety of encoder-decoder generative models for synthetic training data generation and propose using conditional variational auto-encoders. Our approach requires only direct optimization, works well with limited data and significantly outperforms the previous controlled text generation techniques. Further, the generated data are used as additional training samples in an extrinsic intent classification task, leading to improved performance by up to 5\% absolute f-score in low-resource cases, validating the usefulness of our approach.

| Comments: | EMNLP WNGT workshop                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1910.03487 [cs.CL]**                                 |
|           | (or **arXiv:1910.03487v1 [cs.CL]** for this version)         |



# 2019-10-08

[Return to Index](#Index)



<h2 id="2019-10-08-1">1. How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems</h2>
Title: [How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems](https://arxiv.org/abs/1910.02238)

Authors:[Thi-Vinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+T), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Phuong-Thai Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P), [Le-Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L)

*(Submitted on 5 Oct 2019)*

> While translating between Chinese-centric languages, many works have discovered clear advantages of using characters as the translation unit. Unfortunately, traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. They are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. In this paper, we demonstrate that the new transformer architecture can perform character-based translation better than the recurrent one. We conduct experiments on a low-resource language pair: Japanese-Vietnamese. Our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.02238 [cs.CL]**                         |
|           | (or **arXiv:1910.02238v1 [cs.CL]** for this version) |





<h2 id="2019-10-08-2">2. Domain Differential Adaptation for Neural Machine Translation</h2>
Title: [Domain Differential Adaptation for Neural Machine Translation](https://arxiv.org/abs/1910.02555)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 7 Oct 2019)*

> Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of {\it Domain Differential Adaptation (DDA)}, where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.

| Comments: | Workshop on Neural Generation and Translation (WNGT) at EMNLP 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.02555 [cs.CL]**                                 |
|           | (or **arXiv:1910.02555v1 [cs.CL]** for this version)         |





<h2 id="2019-10-08-3">3. On Leveraging the Visual Modality for Neural Machine Translation</h2>
Title: [On Leveraging the Visual Modality for Neural Machine Translation](https://arxiv.org/abs/1910.02754)

Authors:[Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Sang Keun Choe](https://arxiv.org/search/cs?searchtype=author&query=Choe%2C+S+K), [Quanyang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Q), [Yi Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F)

*(Submitted on 7 Oct 2019)*

> Leveraging the visual modality effectively for Neural Machine Translation (NMT) remains an open problem in computational linguistics. Recently, Caglayan et al. posit that the observed gains are limited mainly due to the very simple, short, repetitive sentences of the Multi30k dataset (the only multimodal MT dataset available at the time), which renders the source text sufficient for context. In this work, we further investigate this hypothesis on a new large scale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times longer mean sentence length than Multi30k and no repetition. We propose and evaluate three novel fusion techniques, each of which is designed to ensure the utilization of visual context at different stages of the Sequence-to-Sequence transduction pipeline, even under full linguistic context. However, we still obtain only marginal gains under full linguistic context and posit that visual embeddings extracted from deep vision models (ResNet for Multi30k, ResNext for How2) do not lend themselves to increasing the discriminativeness between the vocabulary elements at token level prediction in NMT. We demonstrate this qualitatively by analyzing attention distribution and quantitatively through Principal Component Analysis, arriving at the conclusion that it is the quality of the visual embeddings rather than the length of sentences, which need to be improved in existing MMT datasets.

| Comments: | Accepted to INLG 2019                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.02754 [cs.CL]**                                 |
|           | (or **arXiv:1910.02754v1 [cs.CL]** for this version)         |





<h2 id="2019-10-08-4">4. Adversarial reconstruction for Multi-modal Machine Translation</h2>
Title: [Adversarial reconstruction for Multi-modal Machine Translation](https://arxiv.org/abs/1910.02766)

Authors:[Jean-Benoit Delbrouck](https://arxiv.org/search/cs?searchtype=author&query=Delbrouck%2C+J), [Stéphane Dupont](https://arxiv.org/search/cs?searchtype=author&query=Dupont%2C+S)

*(Submitted on 7 Oct 2019)*

> Even with the growing interest in problems at the intersection of Computer Vision and Natural Language, grounding (i.e. identifying) the components of a structured description in an image still remains a challenging task. This contribution aims to propose a model which learns grounding by reconstructing the visual features for the Multi-modal translation task. Previous works have partially investigated standard approaches such as regression methods to approximate the reconstruction of a visual input. In this paper, we propose a different and novel approach which learns grounding by adversarial feedback. To do so, we modulate our network following the recent promising adversarial architectures and evaluate how the adversarial response from a visual reconstruction as an auxiliary task helps the model in its learning. We report the highest scores in term of BLEU and METEOR metrics on the different datasets.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.02766 [cs.CL]**                         |
|           | (or **arXiv:1910.02766v1 [cs.CL]** for this version) |







# 2019-10-07

[Return to Index](#Index)



<h2 id="2019-10-07-1">1. Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data</h2>
Title: [Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data](https://arxiv.org/abs/1910.01769)

Authors: [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

*(Submitted on 4 Oct 2019)*

> Recent advances in pre-training huge models on large amounts of text through self supervision have obtained state-of-the-art results in various natural language processing tasks. However, these huge and expensive models are difficult to use in practise for downstream tasks. Some recent efforts use knowledge distillation to compress these models. However, we see a gap between the performance of the smaller student models as compared to that of the large teacher. In this work, we leverage large amounts of in-domain unlabeled transfer data in addition to a limited amount of labeled training instances to bridge this gap. We show that simple RNN based student models even with hard distillation can perform at par with the huge teachers given the transfer set. The student performance can be further improved with soft distillation and leveraging teacher intermediate representations. We show that our student models can compress the huge teacher by up to 26x while still matching or even marginally exceeding the teacher performance in low-resource settings with small amount of labeled data.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1910.01769 [cs.CL]**                                 |
|           | (or **arXiv:1910.01769v1 [cs.CL]** for this version)         |





<h2 id="2019-10-07-2">2. Modeling Confidence in Sequence-to-Sequence Models</h2>
Title: [Modeling Confidence in Sequence-to-Sequence Models](https://arxiv.org/abs/1910.01859)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J), [Ngoc-Quan Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+N)

*(Submitted on 4 Oct 2019)*

> Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output.
> In this work, we propose to use the similarity between training and test conditions as a measure for models' confidence. We investigate methods solely using the similarity as well as methods combining it with the posterior probability. While traditionally only target tokens are annotated with confidence measures, we also investigate methods to annotate source tokens with confidence. By learning an internal alignment model, we can significantly improve confidence projection over using state-of-the-art external alignment tools. We evaluate the proposed methods on downstream confidence estimation for machine translation (MT). We show improvements on segment-level confidence estimation as well as on confidence estimation for source tokens. In addition, we show that the same methods can also be applied to other tasks using sequence-to-sequence models. On the automatic speech recognition (ASR) task, we are able to find 60% of the errors by looking at 20% of the data.

| Comments: | 8 pages; INLG 2019                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.01859 [cs.CL]**                         |
|           | (or **arXiv:1910.01859v1 [cs.CL]** for this version) |





<h2 id="2019-10-07-3">3. Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods</h2>
Title: [Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods](https://arxiv.org/abs/1910.02065)

Authors: [Camburu Oana-Maria](https://arxiv.org/search/cs?searchtype=author&query=Oana-Maria%2C+C), [Giunchiglia Eleonora](https://arxiv.org/search/cs?searchtype=author&query=Eleonora%2C+G), [Foerster Jakob](https://arxiv.org/search/cs?searchtype=author&query=Jakob%2C+F), [Lukasiewicz Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+L), [Blunsom Phil](https://arxiv.org/search/cs?searchtype=author&query=Phil%2C+B)

*(Submitted on 4 Oct 2019)*

> For AI systems to garner widespread public acceptance, we must develop methods capable of explaining the decisions of black-box models such as neural networks. In this work, we identify two issues of current explanatory methods. First, we show that two prevalent perspectives on explanations---feature-additivity and feature-selection---lead to fundamentally different instance-wise explanations. In the literature, explainers from different perspectives are currently being directly compared, despite their distinct explanation goals. The second issue is that current post-hoc explainers have only been thoroughly validated on simple models, such as linear regression, and, when applied to real-world neural networks, explainers are commonly evaluated under the assumption that the learned models behave reasonably. However, neural networks often rely on unreasonable correlations, even when producing correct decisions. We introduce a verification framework for explanatory methods under the feature-selection perspective. Our framework is based on a non-trivial neural network architecture trained on a real-world task, and for which we are able to provide guarantees on its inner workings. We validate the efficacy of our evaluation by showing the failure modes of current explainers. We aim for this framework to provide a publicly available, off-the-shelf evaluation when the feature-selection perspective on explanations is needed.

| Comments: | NeurIPS 2019 Workshop on Safety and Robustness in Decision Making, Vancouver, Canada |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.02065 [cs.CL]**                                 |
|           | (or **arXiv:1910.02065v1 [cs.CL]** for this version)         |





# 2019-10-04

[Return to Index](#Index)



<h2 id="2019-10-04-1">1. Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</h2>
Title: [Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations](https://arxiv.org/abs/1910.01157)

Authors: [Jeff Da](https://arxiv.org/search/cs?searchtype=author&query=Da%2C+J), [Jungo Kusai](https://arxiv.org/search/cs?searchtype=author&query=Kusai%2C+J)

*(Submitted on 2 Oct 2019)*

> Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT's commonsense representation abilities. First, we probe BERT's ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT's pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.

| Comments: | Accepted to EMNLP Commonsense (COIN)                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.01157 [cs.CL]**                         |
|           | (or **arXiv:1910.01157v1 [cs.CL]** for this version) |





<h2 id="2019-10-04-2">2. Linking artificial and human neural representations of language</h2>
Title: [Linking artificial and human neural representations of language](https://arxiv.org/abs/1910.01244)

Authors: [Jon Gauthier](https://arxiv.org/search/cs?searchtype=author&query=Gauthier%2C+J), [Roger Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+R)

*(Submitted on 2 Oct 2019)*

> What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance.
> We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.

| Comments: | EMNLP 2019                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neurons and Cognition (q-bio.NC) |
| Cite as:  | **arXiv:1910.01244 [cs.CL]**                                 |
|           | (or **arXiv:1910.01244v1 [cs.CL]** for this version)         |





# 2019-10-03

[Return to Index](#Index)



<h2 id="2019-10-03-1">1. Speech-to-speech Translation between Untranscribed Unknown Languages</h2>
Title: [Speech-to-speech Translation between Untranscribed Unknown Languages](https://arxiv.org/abs/1910.00795)

Authors: [Andros Tjandra](https://arxiv.org/search/cs?searchtype=author&query=Tjandra%2C+A), [Sakriani Sakti](https://arxiv.org/search/cs?searchtype=author&query=Sakti%2C+S), [Satoshi Nakamura](https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+S)

*(Submitted on 2 Oct 2019)*

> In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target language's discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.

| Comments: | Accepted in IEEE ASRU 2019. Web-page for more samples & details: [this https URL](https://sp2code-translation-v1.netlify.com/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.00795 [cs.CL]**                                 |
|           | (or **arXiv:1910.00795v1 [cs.CL]** for this version)         |





<h2 id="2019-10-03-2">2. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</h2>
Title: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

Authors: [Victor Sanh](https://arxiv.org/search/cs?searchtype=author&query=Sanh%2C+V), [Lysandre Debut](https://arxiv.org/search/cs?searchtype=author&query=Debut%2C+L), [Julien Chaumond](https://arxiv.org/search/cs?searchtype=author&query=Chaumond%2C+J), [Thomas Wolf](https://arxiv.org/search/cs?searchtype=author&query=Wolf%2C+T)

*(Submitted on 2 Oct 2019)*

> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remain challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.

| Comments: | 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.01108 [cs.CL]**                                 |
|           | (or **arXiv:1910.01108v1 [cs.CL]** for this version)         |



# 2019-10-02

[Return to Index](#Index)



<h2 id="2019-10-02-1">1. Interrogating the Explanatory Power of Attention in Neural Machine Translation</h2>
Title: [Interrogating the Explanatory Power of Attention in Neural Machine Translation](https://arxiv.org/abs/1910.00139)

Authors: [Pooya Moradi](https://arxiv.org/search/cs?searchtype=author&query=Moradi%2C+P), [Nishant Kambhatla](https://arxiv.org/search/cs?searchtype=author&query=Kambhatla%2C+N), [Anoop Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+A)

*(Submitted on 30 Sep 2019)*

> Attention models have become a crucial component in neural machine translation (NMT). They are often implicitly or explicitly used to justify the model's decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in NMT. To evaluate the explanatory power of attention for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained attention model. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68% of function words and 21% of content words in our German-English dataset. Our experiments demonstrate that attention models by themselves cannot reliably explain the decisions made by a NMT model.

| Comments: | Accepted at the 3rd Workshop on Neural Generation and Translation (WNGT 2019) held at EMNLP-IJCNLP 2019 (Camera ready) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1910.00139 [cs.CL]**                                 |
|           | (or **arXiv:1910.00139v1 [cs.CL]** for this version)         |





<h2 id="2019-10-02-2">2. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations</h2>
Title: [Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations](https://arxiv.org/abs/1910.00194)

Authors: [Christian Hadiwinoto](https://arxiv.org/search/cs?searchtype=author&query=Hadiwinoto%2C+C), [Hwee Tou Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+H+T), [Wee Chung Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+W+C)

*(Submitted on 1 Oct 2019)*

> Contextualized word representations are able to give different representations for the same word in different contexts, and they have been shown to be effective in downstream natural language processing tasks, such as question answering, named entity recognition, and sentiment analysis. However, evaluation on word sense disambiguation (WSD) in prior work shows that using contextualized word representations does not outperform the state-of-the-art approach that makes use of non-contextualized word embeddings. In this paper, we explore different strategies of integrating pre-trained contextualized word representations and our best strategy achieves accuracies exceeding the best prior published accuracies by significant margins on multiple benchmark WSD datasets.

| Comments: | 10 pages, 2 figures, EMNLP 2019                      |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00194 [cs.CL]**                         |
|           | (or **arXiv:1910.00194v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-3">3. Multilingual End-to-End Speech Translation</h2>
Title: [Multilingual End-to-End Speech Translation](https://arxiv.org/abs/1910.00254)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

*(Submitted on 1 Oct 2019)*

> In this paper, we propose a simple yet effective framework for multilingual end-to-end speech translation (ST), in which speech utterances in source languages are directly translated to the desired target languages with a universal sequence-to-sequence architecture. While multilingual models have shown to be useful for automatic speech recognition (ASR) and machine translation (MT), this is the first time they are applied to the end-to-end ST problem. We show the effectiveness of multilingual end-to-end ST in two scenarios: one-to-many and many-to-many translations with publicly available data. We experimentally confirm that multilingual end-to-end ST models significantly outperform bilingual ones in both scenarios. The generalization of multilingual training is also evaluated in a transfer learning scenario to a very low-resource language pair. All of our codes and the database are publicly available to encourage further research in this emergent multilingual ST topic.

| Comments: | Accepted to ASRU 2019                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1910.00254 [cs.CL]**                                 |
|           | (or **arXiv:1910.00254v1 [cs.CL]** for this version)         |





<h2 id="2019-10-02-4">4. When and Why is Document-level Context Useful in Neural Machine Translation?</h2>
Title: [When and Why is Document-level Context Useful in Neural Machine Translation?](https://arxiv.org/abs/1910.00294)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Duc Thanh Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+D+T), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 1 Oct 2019)*

> Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT.

| Comments: | DiscoMT 2019 camera-ready                            |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00294 [cs.CL]**                         |
|           | (or **arXiv:1910.00294v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-5">5. Grammatical Error Correction in Low-Resource Scenarios</h2>
Title: [Grammatical Error Correction in Low-Resource Scenarios](https://arxiv.org/abs/1910.00353)

Authors: [Jakub Náplava](https://arxiv.org/search/cs?searchtype=author&query=Náplava%2C+J), [Milan Straka](https://arxiv.org/search/cs?searchtype=author&query=Straka%2C+M)

*(Submitted on 1 Oct 2019 ([v1](https://arxiv.org/abs/1910.00353v1)), last revised 2 Oct 2019 (this version, v2))*

> Grammatical error correction in English is a long studied problem with many existing systems and datasets. However, there has been only a limited research on error correction of other languages. In this paper, we present a new dataset AKCES-GEC on grammatical error correction for Czech. We then make experiments on Czech, German and Russian and show that when utilizing synthetic parallel corpus, Transformer neural machine translation model can reach new state-of-the-art results on these datasets. AKCES-GEC is published under CC BY-NC-SA 4.0 license at [this https URL](https://hdl.handle.net/11234/1-3057) and the source code of the GEC model is available at [this https URL](https://github.com/ufal/low-resource-gec-wnut2019).

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.00353 [cs.CL]**                         |
|           | (or **arXiv:1910.00353v2 [cs.CL]** for this version) |





<h2 id="2019-10-02-6">6. Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair</h2>
Title: [Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair](https://arxiv.org/abs/1910.00368)

Authors: [Aidar Valeev](https://arxiv.org/search/cs?searchtype=author&query=Valeev%2C+A), [Ilshat Gibadullin](https://arxiv.org/search/cs?searchtype=author&query=Gibadullin%2C+I), [Albina Khusainova](https://arxiv.org/search/cs?searchtype=author&query=Khusainova%2C+A), [Adil Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A)

*(Submitted on 1 Oct 2019)*

> Neural machine translation is the current state-of-the-art in machine translation. Although it is successful in a resource-rich setting, its applicability for low-resource language pairs is still debatable. In this paper, we explore the effect of different techniques to improve machine translation quality when a parallel corpus is as small as 324 000 sentences, taking as an example previously unexplored Russian-Tatar language pair. We apply such techniques as transfer learning and semi-supervised learning to the base Transformer model, and empirically show that the resulting models improve Russian to Tatar and Tatar to Russian translation quality by +2.57 and +3.66 BLEU, respectively.

| Comments: | Presented on ICATHS'19                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00368 [cs.CL]**                         |
|           | (or **arXiv:1910.00368v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-7">7. A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation</h2>
Title: [A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation](https://arxiv.org/abs/1910.00373)

Authors: [Ilshat Gibadullin](https://arxiv.org/search/cs?searchtype=author&query=Gibadullin%2C+I), [Aidar Valeev](https://arxiv.org/search/cs?searchtype=author&query=Valeev%2C+A), [Albina Khusainova](https://arxiv.org/search/cs?searchtype=author&query=Khusainova%2C+A), [Adil Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A)

*(Submitted on 1 Oct 2019)*

> Neural machine translation has become the state-of-the-art for language pairs with large parallel corpora. However, the quality of machine translation for low-resource languages leaves much to be desired. There are several approaches to mitigate this problem, such as transfer learning, semi-supervised and unsupervised learning techniques. In this paper, we review the existing methods, where the main idea is to exploit the power of monolingual data, which, compared to parallel, is usually easier to obtain and significantly greater in amount.

| Comments: | Presented in ICATHS'19                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1910.00373 [cs.CL]**                         |
|           | (or **arXiv:1910.00373v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-8">8. Machine Translation for Machines: the Sentiment Classification Use Case</h2>
Title: [Machine Translation for Machines: the Sentiment Classification Use Case](https://arxiv.org/abs/1910.00478)

Authors: [Amirhossein Tebbifakhr](https://arxiv.org/search/cs?searchtype=author&query=Tebbifakhr%2C+A), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 1 Oct 2019)*

> We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency ("human-oriented" quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a "machine-oriented" criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1910.00478 [cs.CL]**                         |
|           | (or **arXiv:1910.00478v1 [cs.CL]** for this version) |





<h2 id="2019-10-02-9">9. Putting Machine Translation in Context with the Noisy Channel Model</h2>
Title: [Putting Machine Translation in Context with the Noisy Channel Model](https://arxiv.org/abs/1910.00553)

Authors: [Lei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Laurent Sartran](https://arxiv.org/search/cs?searchtype=author&query=Sartran%2C+L), [Wojciech Stokowiec](https://arxiv.org/search/cs?searchtype=author&query=Stokowiec%2C+W), [Wang Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+W), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Phil Blunsom](https://arxiv.org/search/cs?searchtype=author&query=Blunsom%2C+P), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C)

*(Submitted on 1 Oct 2019)*

> We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperforms comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1910.00553 [cs.CL]**                                 |
|           | (or **arXiv:1910.00553v1 [cs.CL]** for this version)         |





# 2019-10-01

[Return to Index](#Index)



<h2 id="2019-10-01-1">1. Revisiting Self-Training for Neural Sequence Generation</h2> 
Title: [Revisiting Self-Training for Neural Sequence Generation](https://arxiv.org/abs/1909.13788)

Authors:[Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 30 Sep 2019)*

> Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a "noisy" version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1909.13788 [cs.LG]**                                 |
|           | (or **arXiv:1909.13788v1 [cs.LG]** for this version)         |





<h2 id="2019-10-01-2">2. The Source-Target Domain Mismatch Problem in Machine Translation</h2> 
Title: [The Source-Target Domain Mismatch Problem in Machine Translation](https://arxiv.org/abs/1909.13151)

Authors:[Jiajun Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Matt Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+M), [Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M)

*(Submitted on 28 Sep 2019)*

> While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that particularly in low resource settings this causes the domains of the source and target language to greatly mismatch, as the two languages are often spoken in further apart regions of the world with more distinctive cultural traits and unrelated local events. In this work we first propose a controlled setting to carefully analyze the source-target domain mismatch, and its dependence on the amount of parallel and monolingual data. Second, we test both a model trained with back-translation and one trained with self-training. The latter leverages in-domain source monolingual data but uses potentially incorrect target references. We found that these two approaches are often complementary to each other. For instance, on a low-resource Nepali-English dataset the combined approach improves upon the baseline using just parallel data by 2.5 BLEU points, and by 0.6 BLEU point when compared to back-translation.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.13151 [cs.CL]**                         |
|           | (or **arXiv:1909.13151v1 [cs.CL]** for this version) |





<h2 id="2019-10-01-3">3. Controllable Data Synthesis Method for Grammatical Error Correction</h2> 
Title: [Controllable Data Synthesis Method for Grammatical Error Correction](https://arxiv.org/abs/1909.13302)

Authors:[Chencheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yongping Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Erhong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+E)

*(Submitted on 29 Sep 2019 ([v1](https://arxiv.org/abs/1909.13302v1)), last revised 2 Oct 2019 (this version, v2))*

> Due to the lack of parallel data in current Grammatical Error Correction (GEC) task, models based on Sequence to Sequence framework cannot be adequately trained to obtain higher performance. We propose two data synthesis methods which can control the error rate and the ratio of error types on synthetic data. The first approach is to corrupt each word in the monolingual corpus with a fixed probability, including replacement, insertion and deletion. Another approach is to train error generation models and further filtering the decoding results of the models. The experiments on different synthetic data show that the error rate is 40% and the ratio of error types is the same can improve the model performance better. Finally, we synthesize about 100 million data and achieve comparable performance as the state of the art, which uses twice as much data as we use.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1909.13302 [cs.CL]**                         |
|           | (or **arXiv:1909.13302v2 [cs.CL]** for this version) |





<h2 id="2019-10-01-4">4. Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation</h2> 
Title: [Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation](https://arxiv.org/abs/1909.13466)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Ehsan Zare Borzeshi](https://arxiv.org/search/cs?searchtype=author&query=Borzeshi%2C+E+Z), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

*(Submitted on 30 Sep 2019)*

> In recent years, neural machine translation (NMT) has become the dominant approach in automated translation. However, like many other deep learning approaches, NMT suffers from overfitting when the amount of training data is limited. This is a serious issue for low-resource language pairs and many specialized translation domains that are inherently limited in the amount of available supervised data. For this reason, in this paper we propose regressing word (ReWE) and sentence (ReSE) embeddings at training time as a way to regularize NMT models and improve their generalization. During training, our models are trained to jointly predict categorical (words in the vocabulary) and continuous (word and sentence embeddings) outputs. An extensive set of experiments over four language pairs of variable training set size has showed that ReWE and ReSE can outperform strong state-of-the-art baseline models, with an improvement that is larger for smaller training sets (e.g., up to +5:15 BLEU points in Basque-English translation). Visualizations of the decoder's output space show that the proposed regularizers improve the clustering of unique words, facilitating correct predictions. In a final experiment on unsupervised NMT, we show that ReWE and ReSE are also able to improve the quality of machine translation when no parallel data are available.

| Comments: | \c{opyright} 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1909.13466 [cs.CL]**                                 |
|           | (or **arXiv:1909.13466v1 [cs.CL]** for this version)         |





<h2 id="2019-10-01-5">5. Simple and Effective Paraphrastic Similarity from Parallel Translations</h2> 
Title: [Simple and Effective Paraphrastic Similarity from Parallel Translations](https://arxiv.org/abs/1909.13872)

Authors: [John Wieting](https://arxiv.org/search/cs?searchtype=author&query=Wieting%2C+J), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

*(Submitted on 30 Sep 2019)*

> We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.

| Comments: | Published as a short paper at ACL 2019               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1909.13872 [cs.CL]**                         |
|           | (or **arXiv:1909.13872v1 [cs.CL]** for this version) |

