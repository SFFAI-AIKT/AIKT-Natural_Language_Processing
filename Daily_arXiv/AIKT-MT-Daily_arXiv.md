# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-23](#2019-08-23)
  - [1. Denoising based Sequence-to-Sequence Pre-training for Text Generation](#2019-08-23-1)
  - [2. Dual Skew Divergence Loss for Neural Machine Translation](#2019-08-23-2)

- [2019-08-22](#2019-08-22)
  - [1. Improving Neural Machine Translation with Pre-trained Representation](#2019-08-22-1)
  - [2. On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning](#2019-08-22-2)
  - [3. An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing](#2019-08-22-3)
  - [4. A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation](#2019-08-22-4)
  - [5. Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples](#2019-08-22-5)

- [2019-08-21](#2019-08-21)
  - [1. Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior](#2019-08-21-1)
  - [2. ARAML: A Stable Adversarial Training Framework for Text Generation](#2019-08-21-2)
  - [3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers](#2019-08-21-3)

- [2019-08-20](#2019-08-20)
  - [1. UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task](#2019-08-20-1)
  - [2. Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation](#2019-08-20-2)
  - [3. The Transference Architecture for Automatic Post-Editing](#2019-08-20-3)
  - [4. Language Graph Distillation for Low-Resource Machine Translation](#2019-08-20-4)
  - [5. Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation](#2019-08-20-5)
  - [6. Recurrent Graph Syntax Encoder for Neural Machine Translation](#2019-08-20-6)
  - [7. Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces](#2019-08-20-7)

- [2019-08-19](#2019-08-19)
  - [1. Attending to Future Tokens For Bidirectional Sequence Generation](#2019-08-19-1)
  - [2. Towards Making the Most of BERT in Neural Machine Translation](#2019-08-19-2)
  - [3. Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs](#2019-08-19-3)
  - [4. Simple and Effective Noisy Channel Modeling for Neural Machine Translation](#2019-08-19-4)
  - [5. Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring](#2019-08-19-5)
- [2019-08-15](#2019-08-15)
  - [1. On The Evaluation of Machine Translation Systems Trained With Back-Translation](#2019-08-15-1)
- [2019-08-14](#2019-08-14)
  - [1. Neural Text Generation with Unlikelihood Training](#2019-08-14-1)
  - [2. LSTM vs. GRU vs. Bidirectional RNN for script generation](#2019-08-14-2)
  - [3. Attention is not not Explanation](#2019-08-14-3)
  - [4. Neural Machine Translation with Noisy Lexical Constraints](#2019-08-14-4)
- [2019-08-13](#2019-08-13)
  - [1. On the Validity of Self-Attention as Explanation in Transformer Models](#2019-08-13-1)
- [2019-08-12](#2019-08-12)
  - [1. Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling](#2019-08-12-1)
  - [2. UdS Submission for the WMT 19 Automatic Post-Editing Task](#2019-08-12-2)
- [2019-08-09](#2019-08-09)
  - [1. A Test Suite and Manual Evaluation of Document-Level NMT at WMT19](#2019-08-09-1)
- [2019-08-07](#2019-08-07)
  - [1. MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models](#2019-08-07-1)
  - [2. A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data](#2019-08-07-2)
  - [3. Self-Knowledge Distillation in Natural Language Processing](#2019-08-07-3)
- [2019-08-06](#2019-08-06)
  - [1. Invariance-based Adversarial Attack on Neural Machine Translation Systems](#2019-08-06-1)
  - [2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](#2019-08-06-2)
  - [3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](#2019-08-06-3)
  - [4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](#2019-08-06-4)
  - [5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](#2019-08-06-5)
  - [6. Predicting Actions to Help Predict Translations](#2019-08-06-6)
  - [7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](#2019-08-06-7)
- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-23

[Return to Index](#Index)



<h2 id="2019-08-23-1">1. Denoising based Sequence-to-Sequence Pre-training for Text Generation</h2> 

Title: [Denoising based Sequence-to-Sequence Pre-training for Text Generation](https://arxiv.org/abs/1908.08206)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ruoyu Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Sujian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

*(Submitted on 22 Aug 2019)*

> This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.

| Comments: | Accepted to EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08206 [cs.CL]**                         |
|           | (or **arXiv:1908.08206v1 [cs.CL]** for this version) |





<h2 id="2019-08-23-2">2. Dual Skew Divergence Loss for Neural Machine Translation</h2> 

Title: [Dual Skew Divergence Loss for Neural Machine Translation](https://arxiv.org/abs/1908.08399)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Yingting Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S)

*(Submitted on 22 Aug 2019)*

> For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.

| Comments: | 9pages                                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08399 [cs.CL]**                         |
|           | (or **arXiv:1908.08399v1 [cs.CL]** for this version) |