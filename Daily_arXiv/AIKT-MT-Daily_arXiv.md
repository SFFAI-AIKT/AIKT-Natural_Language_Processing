# Daily arXiv: Machine Translation - May, 2020

# Index

- [2020-05-01](#2020-05-01)
  - [1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation](2020-05-01-1)
  - [2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](2020-05-01-2)
  - [3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](2020-05-01-3)
  - [4. Capsule-Transformer for Neural Machine Translation](2020-05-01-4)
  - [5. End-to-End Neural Word Alignment Outperforms GIZA++](2020-05-01-5)
  - [6. Character-Level Translation with Self-attention](2020-05-01-6)
  - [7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](2020-05-01-7)
  - [8. Accurate Word Alignment Induction from Neural Machine Translation](2020-05-01-8)
  - [9. ](2020-05-01-9)
  - [10. ](2020-05-01-10)
  - [11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](2020-05-01-11)
  - [12. Language Model Prior for Low-Resource Neural Machine Translation](2020-05-01-12)
  - [13. A Call for More Rigor in Unsupervised Cross-lingual Learning](2020-05-01-13)
  - [14. ](2020-05-01-14)
  - [15. Investigating Transferability in Pretrained Language Models](2020-05-01-15)
  - [16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](2020-05-01-16)
  - [17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](2020-05-01-17)
  - [18. When does data augmentation help generalization in NLP?](2020-05-01-18)
  - [19. Imitation Attacks and Defenses for Black-box Machine Translation Systems](2020-05-01-19)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-05-10

[Return to Index](#Index)



<h2 id="2020-05-01-1">1. Simulated Multiple Reference Training Improves Low-Resource Machine Translation</h2>

Title: [Simulated Multiple Reference Training Improves Low-Resource Machine Translation](https://arxiv.org/abs/2004.14524)

Authors: [Huda Khayrallah](https://arxiv.org/search/cs?searchtype=author&query=Khayrallah%2C+H), [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Many valid translations exist for a given sentence, and yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce a novel MT training method that approximates the full space of possible translations by: sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser's distribution over possible tokens. With an English paraphraser, we demonstrate the effectiveness of our method in low-resource settings, with gains of 1.2 to 7 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14524](https://arxiv.org/abs/2004.14524) [cs.CL]** |
|           | (or **[arXiv:2004.14524v1](https://arxiv.org/abs/2004.14524v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-2">2. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</h2>

Title: [Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://arxiv.org/abs/2004.14564)

Authors: [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> We propose the use of a sequence-to-sequence paraphraser for automatic machine translation evaluation. The paraphraser takes a human reference as input and then force-decodes and scores an MT system output. We propose training the aforementioned paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot "language pair" (e.g., Russian to Russian). We denote our paraphraser "unbiased" because the mode of our model's output probability is centered around a copy of the input sequence, which in our case represent the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT19 segment-level shared metrics task in all languages, excluding Gujarati where the model had no training data. We also explore using our model conditioned on the source instead of the reference, and find that it outperforms every quality estimation as a metric system from the WMT19 shared task on quality estimation by a statistically significant margin in every language pair.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14564](https://arxiv.org/abs/2004.14564) [cs.CL]** |
|           | (or **[arXiv:2004.14564v1](https://arxiv.org/abs/2004.14564v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-3">3. Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation</h2>

Title: [Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation](https://arxiv.org/abs/2004.14607)

Authors: [Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Barbara Rychalska](https://arxiv.org/search/cs?searchtype=author&query=Rychalska%2C+B), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Dominika Basaj](https://arxiv.org/search/cs?searchtype=author&query=Basaj%2C+D)

> Despite increasing instances of machine translation (MT) systems including contextual information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark datasets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several baseline MT systems on the curated datasets. Surprisingly, we find that existing context-aware models do not improve discourse-related translations consistently across languages and phenomena.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14607](https://arxiv.org/abs/2004.14607) [cs.CL]** |
|           | (or **[arXiv:2004.14607v1](https://arxiv.org/abs/2004.14607v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-4">4. Capsule-Transformer for Neural Machine Translation</h2>

Title: [Capsule-Transformer for Neural Machine Translation](https://arxiv.org/abs/2004.14649)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Juncheng Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Transformer hugely benefits from its key design of the multi-head self-attention network (SAN), which extracts information from various perspectives through transforming the given input into different subspaces. However, its simple linear transformation aggregation strategy may still potentially fail to fully capture deeper contextualized information. In this paper, we thus propose the capsule-Transformer, which extends the linear transformation into a more general capsule routing algorithm by taking SAN as a special case of capsule network. So that the resulted capsule-Transformer is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words. Specifically, we see groups of attention weights in SAN as low layer capsules. By applying the iterative capsule routing algorithm they can be further aggregated into high layer capsules which contain deeper contextualized information. Experimental results on the widely-used machine translation datasets show our proposed capsule-Transformer outperforms strong Transformer baseline significantly.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14649](https://arxiv.org/abs/2004.14649) [cs.CL]** |
|           | (or **[arXiv:2004.14649v1](https://arxiv.org/abs/2004.14649v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-5">5. End-to-End Neural Word Alignment Outperforms GIZA++</h2>

Title: [End-to-End Neural Word Alignment Outperforms GIZA++](https://arxiv.org/abs/2004.14675)

Authors: [Thomas Zenkel](https://arxiv.org/search/cs?searchtype=author&query=Zenkel%2C+T), [Joern Wuebker](https://arxiv.org/search/cs?searchtype=author&query=Wuebker%2C+J), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J)

> Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14675](https://arxiv.org/abs/2004.14675) [cs.CL]** |
|           | (or **[arXiv:2004.14675v1](https://arxiv.org/abs/2004.14675v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-6">6. Character-Level Translation with Self-attention</h2>

Title: [Character-Level Translation with Self-attention](https://arxiv.org/abs/2004.14788)

Authors: [Yingqiang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Nikola I. Nikolov](https://arxiv.org/search/cs?searchtype=author&query=Nikolov%2C+N+I), [Yuhuang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Richard H.R. Hahnloser](https://arxiv.org/search/cs?searchtype=author&query=Hahnloser%2C+R+H)

> We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14788](https://arxiv.org/abs/2004.14788) [cs.CL]** |
|           | (or **[arXiv:2004.14788v1](https://arxiv.org/abs/2004.14788v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-7">7. Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation</h2>

Title: [Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine Translation](https://arxiv.org/abs/2004.14821)

Authors: [Shoetsu Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+S), [Jin Sakuma](https://arxiv.org/search/cs?searchtype=author&query=Sakuma%2C+J), [Naoki Yoshinaga](https://arxiv.org/search/cs?searchtype=author&query=Yoshinaga%2C+N), [Masashi Toyoda](https://arxiv.org/search/cs?searchtype=author&query=Toyoda%2C+M), [Masaru Kitsuregawa](https://arxiv.org/search/cs?searchtype=author&query=Kitsuregawa%2C+M)

> Neural machine translation (NMT) models do not work well in domains different from the training data. The standard approach to this problem is to build a small parallel data in the target domain and perform domain adaptation from a source domain where massive parallel data is available. However, domain adaptation between distant domains (e.g., subtitles and research papers) does not perform effectively because of mismatches in vocabulary; it will encounter many domain-specific unknown words (e.g., `angstrom') and words whose meanings shift across domains (e.g., `conductor'). In this study, aiming to solve these vocabulary mismatches in distant domain adaptation, we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pre-trained NMT model to the target domain. Prior to fine-tuning, our method replaces word embeddings in embedding layers of the NMT model, by projecting general word embeddings induced from monolingual data in the target domain onto the source-domain embedding space. Experimental results on distant domain adaptation for English-to-Japanese translation and German-to-English translation indicate that our vocabulary adaptation improves the performance of fine-tuning by 3.6 BLEU points.

| Comments: | 8pages + citations                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14821](https://arxiv.org/abs/2004.14821) [cs.CL]** |
|           | (or **[arXiv:2004.14821v1](https://arxiv.org/abs/2004.14821v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-8">8. Accurate Word Alignment Induction from Neural Machine Translation</h2>

Title: [Accurate Word Alignment Induction from Neural Machine Translation](https://arxiv.org/abs/2004.14837)

Authors: [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Guanhua Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Despite its original goal to jointly learn to align and translate, prior researches suggest that the state-of-the-art neural machine translation model Transformer captures poor word alignment through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignment, which could only be revealed if we choose the correct decoding step and layer to induce word alignment. We propose to induce alignment with the to-be-aligned target token as the decoder input and present two simple but effective interpretation methods for word alignment induction, either through the attention weights or the leave-one-out measures. In contrast to previous studies, we find that attention weights capture better word alignment than the leave-one-out measures under our setting. Using the proposed method with attention weights, we greatly improve over fast-align on word alignment induction. Finally, we present a multi-task learning framework to train the Transformer model and show that by incorporating GIZA++ alignments into our multi-task training, we can induce significantly better alignments than GIZA++.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14837](https://arxiv.org/abs/2004.14837) [cs.CL]** |
|           | (or **[arXiv:2004.14837v1](https://arxiv.org/abs/2004.14837v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-9">9. Neural Machine Translation for Low-Resourced Indian Languages</h2>

Title: [](https://arxiv.org/abs/2004.14911)

Authors: 



<h2 id="2020-05-01-10">10. Neural Machine Translation for Low-Resourced Indian Languages</h2>

Title: [](https://arxiv.org/abs/2004.14923)

Authors: 



<h2 id="2020-05-01-11">11. Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</h2>

Title: [Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation](https://arxiv.org/abs/2004.14927)

Authors: [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that document-level context can be used to capture domain generalities when in-domain parallel data is not available. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally present experiments showing the usefulness of large context when modeling multiple domains at once.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14927](https://arxiv.org/abs/2004.14927) [cs.CL]** |
|           | (or **[arXiv:2004.14927v1](https://arxiv.org/abs/2004.14927v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-12">12. Language Model Prior for Low-Resource Neural Machine Translation</h2>

Title: [Language Model Prior for Low-Resource Neural Machine Translation](https://arxiv.org/abs/2004.14928)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM "disagrees" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14928](https://arxiv.org/abs/2004.14928) [cs.CL]** |
|           | (or **[arXiv:2004.14928v1](https://arxiv.org/abs/2004.14928v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-13">13. A Call for More Rigor in Unsupervised Cross-lingual Learning</h2>

Title: [A Call for More Rigor in Unsupervised Cross-lingual Learning](https://arxiv.org/abs/2004.14958)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

> We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.14958](https://arxiv.org/abs/2004.14958) [cs.CL]** |
|           | (or **[arXiv:2004.14958v1](https://arxiv.org/abs/2004.14958v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-14">14. Neural Machine Translation for Low-Resourced Indian Languages</h2>

Title: [](https://arxiv.org/abs/2004.14963)

Authors: 



<h2 id="2020-05-01-15">15. Investigating Transferability in Pretrained Language Models</h2>

Title: [Investigating Transferability in Pretrained Language Models](https://arxiv.org/abs/2004.14975)

Authors: [Alex Tamkin](https://arxiv.org/search/cs?searchtype=author&query=Tamkin%2C+A), [Trisha Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+T), [Davide Giovanardi](https://arxiv.org/search/cs?searchtype=author&query=Giovanardi%2C+D), [Noah Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+N)

> While probing is a common technique for identifying knowledge in the representations of pretrained models, it is unclear whether this technique can explain the downstream success of models like BERT which are trained end-to-end during finetuning. To address this question, we compare probing with a different measure of transferability: the decrease in finetuning performance of a partially-reinitialized model. This technique reveals that in BERT, layers with high probing accuracy on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. In addition, dataset size impacts layer transferability: the less finetuning data one has, the more important the middle and later layers of BERT become. Furthermore, BERT does not simply find a better initializer for individual layers; instead, interactions between layers matter and reordering BERT's layers prior to finetuning significantly harms evaluation metrics. These results provide a way of understanding the transferability of parameters in pretrained language models, revealing the fluidity and complexity of transfer learning in these models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14975](https://arxiv.org/abs/2004.14975) [cs.CL]** |
|           | (or **[arXiv:2004.14975v1](https://arxiv.org/abs/2004.14975v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-16">16. Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation</h2>

Title: [Explicit Representation of the Translation Space: Automatic Paraphrasing for Machine Translation Evaluation](https://arxiv.org/abs/2004.14989)

Authors: [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Lisa Yankovskaya](https://arxiv.org/search/cs?searchtype=author&query=Yankovskaya%2C+L), [Andre Tättar](https://arxiv.org/search/cs?searchtype=author&query=Tättar%2C+A), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Following previous work on automatic paraphrasing, we assess the feasibility of improving BLEU (Papineni et al., 2002) using state-of-the-art neural paraphrasing techniques to generate additional references. We explore the extent to which diverse paraphrases can adequately cover the space of valid translations and compare to an alternative approach of generating paraphrases constrained by MT outputs. We compare both approaches to human-produced references in terms of diversity and the improvement in BLEU's correlation with human judgements of MT quality. Our experiments on the WMT19 metrics tasks for all into-English language directions show that somewhat surprisingly, the addition of diverse paraphrases, even those produced by humans, leads to only small, inconsistent changes in BLEU's correlation with human judgments, suggesting that BLEU's ability to correctly exploit multiple references is limited

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14989](https://arxiv.org/abs/2004.14989) [cs.CL]** |
|           | (or **[arXiv:2004.14989v1](https://arxiv.org/abs/2004.14989v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-17">17. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning</h2>

Title: [On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning](https://arxiv.org/abs/2004.15001)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J), [Vikas Bhardwaj](https://arxiv.org/search/cs?searchtype=author&query=Bhardwaj%2C+V)

> Pre-trained multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on some source language (typically English) and evaluated on a different target language. However, published results for baseline mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot cross-lingual performance varies greatly within the same fine-tuning run and between different fine-tuning runs. We recommend providing oracle scores alongside the zero-shot results: still fine-tune using English, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding the variation from bad checkpoints.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15001](https://arxiv.org/abs/2004.15001) [cs.CL]** |
|           | (or **[arXiv:2004.15001v1](https://arxiv.org/abs/2004.15001v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-18">18. When does data augmentation help generalization in NLP?</h2>

Title: [When does data augmentation help generalization in NLP?](https://arxiv.org/abs/2004.15012)

Authors: [Rohan Jha](https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+R), [Charles Lovering](https://arxiv.org/search/cs?searchtype=author&query=Lovering%2C+C), [Ellie Pavlick](https://arxiv.org/search/cs?searchtype=author&query=Pavlick%2C+E)

> Neural models often exploit superficial ("weak") features to achieve good performance, rather than deriving the more general ("strong") features that we'd prefer a model to use. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. Recent work has proposed using data augmentation--that is, generating training examples on which these weak features fail--as a means of encouraging models to prefer the stronger features. We design a series of toy learning problems to investigate the conditions under which such data augmentation is helpful. We show that augmenting with training examples on which the weak feature fails ("counterexamples") does succeed in preventing the model from relying on the weak feature, but often does not succeed in encouraging the model to use the stronger feature in general. We also find in many cases that the number of counterexamples needed to reach a given error rate is independent of the amount of training data, and that this type of data augmentation becomes less effective as the target strong feature becomes harder to learn.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15012](https://arxiv.org/abs/2004.15012) [cs.CL]** |
|           | (or **[arXiv:2004.15012v1](https://arxiv.org/abs/2004.15012v1) [cs.CL]** for this version) |



<h2 id="2020-05-01-19">19. Imitation Attacks and Defenses for Black-box Machine Translation Systems</h2>

Title: [Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://arxiv.org/abs/2004.15015)

Authors: [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Dawn Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.15015](https://arxiv.org/abs/2004.15015) [cs.CL]** |
|           | (or **[arXiv:2004.15015v1](https://arxiv.org/abs/2004.15015v1) [cs.CL]** for this version) |