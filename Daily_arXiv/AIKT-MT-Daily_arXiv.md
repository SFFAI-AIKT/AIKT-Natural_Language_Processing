# Daily arXiv: Machine Translation - May, 2021

# Index


- [2021-05-25](#2021-05-25)

  - [1. Prevent the Language Model from being Overconfident in Neural Machine Translation](#2021-05-25-1)
  - [2. Neural Machine Translation with Monolingual Translation Memory](#2021-05-25-2)
  - [3. True Few-Shot Learning with Language Models](#2021-05-25-3)
- [2021-05-24](2021-05-24)

  - [1. VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding](#2021-05-24-1)
  - [2. Pretrained Language Models for Text Generation: A Survey](#2021-05-24-2)
  - [3. Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](#2021-05-24-3)
- [2021-05-21](#2021-05-21-1)
- [1. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](#2021-05-21-1)
- [2021-05-20](#2021-05-20)
  - [1. Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing](#2021-05-20-1)
  - [2. Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder](#2021-05-20-2)
  - [3. Effective Attention Sheds Light On Interpretability](#2021-05-20-3)
  - [4. Investigating Math Word Problems using Pretrained Multilingual Language Models](#2021-05-20-4)
  - [5. Combining GCN and Transformer for Chinese Grammatical Error Detection](#2021-05-20-5)
  - [6. TableZa -- A classical Computer Vision approach to Tabular Extraction](#2021-05-20-6)
  - [7. Learning Language Specific Sub-network for Multilingual Machine Translation](#2021-05-20-7)
- [2021-05-19](#2021-05-19)
  - [1. Relative Positional Encoding for Transformers with Linear Complexity](#2021-05-19-1)
  - [2. Multi-Modal Image Captioning for the Visually Impaired](#2021-05-19-2)
  - [3. DRILL: Dynamic Representations for Imbalanced Lifelong Learning](#2021-05-19-3)
  - [4. Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](#2021-05-19-4)
- [2021-05-18](#2021-05-18)
  - [1. Rethinking Skip Connection with Layer Normalization in Transformers and ResNets](#2021-05-18-1)
  - [2. Conscious AI](#2021-05-18-2)
  - [3. Pay Attention to MLPs](#2021-05-18-3)
  - [4. DirectQE: Direct Pretraining for Machine Translation Quality Estimation](#2021-05-18-4)
  - [5. From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding](#2021-05-18-5)
  - [6. The Volctrans Neural Speech Translation System for IWSLT 2021](#2021-05-18-6 )
  - [7. Data Augmentation for Sign Language Gloss Translation](#2021-05-18-7)
  - [8. Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation](#2021-05-18-8)
  - [9. Stage-wise Fine-tuning for Graph-to-Text Generation](#2021-05-18-9)
- [2021-05-17](#2021-05-17)
  
  - [1. Distilling BERT for low complexity network training](#2021-05-17-1)
  - [2. Dynamic Multi-Branch Layers for On-Device Neural Machine Translation](#2021-05-17-2)
  - [3. Do Context-Aware Translation Models Pay the Right Attention?](#2021-05-17-3)
- [2021-05-14](#2021-05-14)
  - [1. Better than BERT but Worse than Baseline](#2021-05-14-1)
  - [2. Spelling Correction with Denoising Transformer](#2021-05-14-2)
  - [3. Designing Multimodal Datasets for NLP Challenges](#2021-05-14-3)
  - [4. Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](#2021-05-14-4)
- [2021-05-13]($2021-05-13)
  
  - [1. Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](#2021-05-13-1)
  - [2. Evaluating Gender Bias in Natural Language Inference](#2021-05-13-2)
  - [3. Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](#2021-05-13-3)
- [2021-05-12](#2021-05-21)
  
  - [1. Cross-Modal Generative Augmentation for Visual Question Answering](#2021-05-21-1)
  - [2. Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity](#2021-05-21-2)
  - [3. Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal](#2021-05-21-3)
  - [4. Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models](#2021-05-21-4)
  - [5. Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation](#2021-05-21-5)
  - [6. Can You Traducir This? Machine Translation for Code-Switched Input](#2021-05-21-6)
  - [7. BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](#2021-05-21-7)
  - [8. Towards transparency in NLP shared tasks](#2021-05-21-8)
  - [9. Including Signed Languages in Natural Language Processing](#2021-05-21-9)
- [2021-05-11](#2021-05-11-1)
  
  - [1. Duplex Sequence-to-Sequence Learning for Reversible Machine Translation](#2021-05-11-1)
  - [2. Measuring and Increasing Context Usage in Context-Aware Machine Translation](#2021-05-11-2)
  - [3. Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation](#2021-05-11-3)
  - [4. Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction](#2021-05-11-4)
  - [5. Self-Guided Curriculum Learning for Neural Machine Translation](#2021-05-11-5)
  - [6. UPC's Speech Translation System for IWSLT 2021](#2021-05-11-6)
- [2021-05-10](#2021-05-10)
  
  - [1. Adapting by Pruning: A Case Study on BERT](#2021-05-10-1)
  - [2. On-the-Fly Controlled Text Generation with Experts and Anti-Experts](#2021-05-10-2)
  - [3. Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](#2021-05-10-3)
  - [4. A Survey of Data Augmentation Approaches for NLP](#2021-05-10-4)
  - [5. Learning Shared Semantic Space for Speech-to-Text Translation](#2021-05-10-5)
  - [6. Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods](#2021-05-10-6)
  - [7. Are Pre-trained Convolutions Better than Pre-trained Transformers?](#2021-05-10-7)
  - [8. âˆ‚-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization](#2021-05-10-8)
- [2021-05-07](#2021-05-07)
  - [1. XeroAlign: Zero-Shot Cross-lingual Transformer Alignment](#2021-05-07-1)
  - [2. Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases](#2021-05-07-2)
  - [3. Content4All Open Research Sign Language Translation Datasets](#2021-05-07-3)
  - [4. Reliability Testing for Natural Language Processing Systems](#2021-05-07-4)
- [2021-05-06](#2021-05-06)
  
  - [1. Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution](#2021-05-06-1)
  - [2. Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy](#2021-05-06-2)
- [2021-05-04](#2021-05-04)	
  - [1. AlloST: Low-resource Speech Translation without Source Transcription](#2021-05-04-1)
  - [2. Larger-Scale Transformers for Multilingual Masked Language Modeling](#2021-05-04-2)
  - [3. Transformers: "The End of History" for NLP?](#2021-05-04-3)
  - [4. BERT memorisation and pitfalls in low-resource scenarios](#2021-05-04-4)
  - [5. Natural Language Generation Using Link Grammar for General Conversational Intelligence](#2021-05-04-5)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-05-25

[Return to Index](#Index)



<h2 id="2021-05-25-1">1. Prevent the Language Model from being Overconfident in Neural Machine Translation
</h2>

Title: [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://arxiv.org/abs/2105.11098)

Authors: [Mengqi Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+M), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xiao-Hua Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.

| Comments: | Accepted as a long paper at ACL 2021. Code is available at: [this https URL](https://github.com/Mlair77/nmt_adequacy) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.11098](https://arxiv.org/abs/2105.11098) [cs.CL]** |
|           | (or **[arXiv:2105.11098v1](https://arxiv.org/abs/2105.11098v1) [cs.CL]** for this version) |





<h2 id="2021-05-25-2">2. Neural Machine Translation with Monolingual Translation Memory
</h2>

Title: [Neural Machine Translation with Monolingual Translation Memory](https://arxiv.org/abs/2105.11269)

Authors: [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Wai Lam](https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+W), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L)

> Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.

| Comments: | ACL2021                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.11269](https://arxiv.org/abs/2105.11269) [cs.CL]** |
|           | (or **[arXiv:2105.11269v1](https://arxiv.org/abs/2105.11269v1) [cs.CL]** for this version) |





<h2 id="2021-05-25-3">3. True Few-Shot Learning with Language Models
</h2>

Title: [True Few-Shot Learning with Language Models](https://arxiv.org/abs/2105.11447)

Authors: [Ethan Perez](https://arxiv.org/search/cs?searchtype=author&query=Perez%2C+E), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates ("prompts"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.

| Comments: | Code at [this https URL](https://github.com/ethanjperez/true_few_shot) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2105.11447](https://arxiv.org/abs/2105.11447) [cs.CL]** |
|           | (or **[arXiv:2105.11447v1](https://arxiv.org/abs/2105.11447v1) [cs.CL]** for this version) |





# 2021-05-24

[Return to Index](#Index)



<h2 id="2021-05-24-1">1. VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding
</h2>

Title: [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding](https://arxiv.org/abs/2105.09996)

Authors: [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Gargi Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+G), [Po-Yao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Prahal Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+P), [Masoumeh Aminzadeh](https://arxiv.org/search/cs?searchtype=author&query=Aminzadeh%2C+M), [Christoph Feichtenhofer](https://arxiv.org/search/cs?searchtype=author&query=Feichtenhofer%2C+C), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

> We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training.

| Comments: | 9 pages, ACL Findings 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.09996](https://arxiv.org/abs/2105.09996) [cs.CV]** |
|           | (or **[arXiv:2105.09996v1](https://arxiv.org/abs/2105.09996v1) [cs.CV]** for this version) |





<h2 id="2021-05-24-2">2. Pretrained Language Models for Text Generation: A Survey
</h2>

Title: [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/abs/2105.10311)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.

| Comments: | Accepted by IJCAI 2021 Survey Track                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.10311](https://arxiv.org/abs/2105.10311) [cs.CL]** |
|           | (or **[arXiv:2105.10311v1](https://arxiv.org/abs/2105.10311v1) [cs.CL]** for this version) |





<h2 id="2021-05-24-3">3. Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining
</h2>

Title: [Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](https://arxiv.org/abs/2105.10419)

Authors: [Ivana KvapilÄ±kova](https://arxiv.org/search/cs?searchtype=author&query=KvapilÄ±kova%2C+I), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E), [OndÅ™ej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.

| Comments:          | ACL SRW 2020                                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics - Student Research Workshop, pages 255-262, Association for Computational Linguistics, 2020 |
| DOI:               | [10.18653/v1/2020.acl-srw.34](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2020.acl-srw.34&v=659f8833) |
| Cite as:           | **[arXiv:2105.10419](https://arxiv.org/abs/2105.10419) [cs.CL]** |
|                    | (or **[arXiv:2105.10419v1](https://arxiv.org/abs/2105.10419v1) [cs.CL]** for this version) |






# 2021-05-21

[Return to Index](#Index)



<h2 id="2021-05-21-1">1. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation
</h2>

Title: [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://arxiv.org/abs/2105.09501)

Authors: [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose \method, a training method to obtain a single unified multilingual translation model. mCOLT is empowered by two techniques: (i) a contrastive learning scheme to close the gap among representations of different languages, and (ii) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mCOLT achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mCOLT achieves an improvement of average 10+ BLEU compared with the multilingual baseline.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.09501](https://arxiv.org/abs/2105.09501) [cs.CL]** |
|           | (or **[arXiv:2105.09501v1](https://arxiv.org/abs/2105.09501v1) [cs.CL]** for this version) |










# 2021-05-20

[Return to Index](#Index)



<h2 id="2021-05-20-1">1. Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing
</h2>

Title: [Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing](https://arxiv.org/abs/2105.08807)

Authors: [Ganesh Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+G), [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Laks V.S. Lakshmanan](https://arxiv.org/search/cs?searchtype=author&query=Lakshmanan%2C+L+V)

> We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task.

| Comments: | Computational Approaches to Linguistic Code-Switching (CALCS 2021) workshop |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08807](https://arxiv.org/abs/2105.08807) [cs.CL]** |
|           | (or **[arXiv:2105.08807v1](https://arxiv.org/abs/2105.08807v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-2">2. Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder
</h2>

Title: [Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder](https://arxiv.org/abs/2105.08840)

Authors: [Yunhao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zhaokun Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+Z)

> Heterogeneity of sentences exists in sequence to sequence tasks such as machine translation. Sentences with largely varied meanings or grammatical structures may increase the difficulty of convergence while training the network. In this paper, we introduce a model to resolve the heterogeneity in the sequence to sequence task. The Multi-filter Gaussian Mixture Autoencoder (MGMAE) utilizes an autoencoder to learn the representations of the inputs. The representations are the outputs from the encoder, lying in the latent space whose dimension is the hidden dimension of the encoder. The representations of training data in the latent space are used to train Gaussian mixtures. The latent space representations are divided into several mixtures of Gaussian distributions. A filter (decoder) is tuned to fit the data in one of the Gaussian distributions specifically. Each Gaussian is corresponding to one filter so that the filter is responsible for the heterogeneity within this Gaussian. Thus the heterogeneity of the training data can be resolved. Comparative experiments are conducted on the Geo-query dataset and English-French translation. Our experiments show that compares to the traditional encoder-decoder model, this network achieves better performance on sequence to sequence tasks such as machine translation and question answering.

| Comments: | 7 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.08840](https://arxiv.org/abs/2105.08840) [cs.CL]** |
|           | (or **[arXiv:2105.08840v1](https://arxiv.org/abs/2105.08840v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-3">3. Effective Attention Sheds Light On Interpretability
</h2>

Title: [Effective Attention Sheds Light On Interpretability](https://arxiv.org/abs/2105.08855)

Authors: [Kaiser Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+K), [Ana MarasoviÄ‡](https://arxiv.org/search/cs?searchtype=author&query=MarasoviÄ‡%2C+A)

> An attention matrix of a transformer self-attention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer's behavior since it is more pertinent to the model output by design.

| Comments: | Accepted to Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08855](https://arxiv.org/abs/2105.08855) [cs.CL]** |
|           | (or **[arXiv:2105.08855v1](https://arxiv.org/abs/2105.08855v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-4">4. Investigating Math Word Problems using Pretrained Multilingual Language Models
</h2>

Title: [Investigating Math Word Problems using Pretrained Multilingual Language Models](https://arxiv.org/abs/2105.08928)

Authors: [Minghuan Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Lei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Lingxiao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J)

> In this paper, we revisit math word problems~(MWPs) from the cross-lingual and multilingual perspective. We construct our MWP solvers over pretrained multilingual language models using sequence-to-sequence model with copy mechanism. We compare how the MWP solvers perform in cross-lingual and multilingual scenarios. To facilitate the comparison of cross-lingual performance, we first adapt the large-scale English dataset MathQA as a counterpart of the Chinese dataset Math23K. Then we extend several English datasets to bilingual datasets through machine translation plus human annotation. Our experiments show that the MWP solvers may not be transferred to a different language even if the target expressions have the same operator set and constants. But for both cross-lingual and multilingual cases, it can be better generalized if problem types exist on both source language and target language.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08928](https://arxiv.org/abs/2105.08928) [cs.CL]** |
|           | (or **[arXiv:2105.08928v1](https://arxiv.org/abs/2105.08928v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-5">5. Combining GCN and Transformer for Chinese Grammatical Error Detection
</h2>

Title: [Combining GCN and Transformer for Chinese Grammatical Error Detection](https://arxiv.org/abs/2105.09085)

Authors: [Jinhong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

> This paper introduces our system at NLPTEA-2020 Task: Chinese Grammatical Error Diagnosis (CGED). CGED aims to diagnose four types of grammatical errors which are missing words (M), redundant words (R), bad word selection (S) and disordered words (W). The automatic CGED system contains two parts including error detection and error correction and our system is designed to solve the error detection problem. Our system is built on three models: 1) a BERT-based model leveraging syntactic information; 2) a BERT-based model leveraging contextual embeddings; 3) a lexicon-based graph neural network. We also design an ensemble mechanism to improve the performance of the single model. Finally, our system obtains the highest F1 scores at detection level and identification level among all teams participating in the CGED 2020 task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.09085](https://arxiv.org/abs/2105.09085) [cs.CL]** |
|           | (or **[arXiv:2105.09085v1](https://arxiv.org/abs/2105.09085v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-6">6. TableZa -- A classical Computer Vision approach to Tabular Extraction
</h2>

Title: [TableZa -- A classical Computer Vision approach to Tabular Extraction](https://arxiv.org/abs/2105.09137)

Authors: [Saumya Banthia](https://arxiv.org/search/cs?searchtype=author&query=Banthia%2C+S), [Anantha Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A), [Ravi Mangipudi](https://arxiv.org/search/cs?searchtype=author&query=Mangipudi%2C+R)

> Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).

| Comments:    | 14 pages, 16 figures, 1 table                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| ACM classes: | I.5.1; I.5.2; I.5.4                                          |
| Cite as:     | **[arXiv:2105.09137](https://arxiv.org/abs/2105.09137) [cs.CL]** |
|              | (or **[arXiv:2105.09137v1](https://arxiv.org/abs/2105.09137v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-7">7. Learning Language Specific Sub-network for Multilingual Machine Translation
</h2>

Title: [Learning Language Specific Sub-network for Multilingual Machine Translation](https://arxiv.org/abs/2105.09259)

Authors: [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradation on rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy extension to new language pairs and zero-shot translation.LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at [this https URL](https://github.com/NLP-Playground/LaSS).

| Comments: | To appear at ACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.09259](https://arxiv.org/abs/2105.09259) [cs.CL]** |
|           | (or **[arXiv:2105.09259v1](https://arxiv.org/abs/2105.09259v1) [cs.CL]** for this version) |








# 2021-05-19

[Return to Index](#Index)



<h2 id="2021-05-19-1">1. Relative Positional Encoding for Transformers with Linear Complexity
</h2>

Title: [Relative Positional Encoding for Transformers with Linear Complexity](https://arxiv.org/abs/2105.08399)

Authors: [Antoine Liutkus](https://arxiv.org/search/cs?searchtype=author&query=Liutkus%2C+A), [OndÅ™ej CÃ­fka](https://arxiv.org/search/cs?searchtype=author&query=CÃ­fka%2C+O), [Shih-Lun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Umut ÅžimÅŸekli](https://arxiv.org/search/cs?searchtype=author&query=ÅžimÅŸekli%2C+U), [Yi-Hsuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [GaÃ«l Richard](https://arxiv.org/search/cs?searchtype=author&query=Richard%2C+G)

> Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.

| Comments: | Accepted to ICML 2021 (long talk). 23 pages                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2105.08399](https://arxiv.org/abs/2105.08399) [cs.LG]** |
|           | (or **[arXiv:2105.08399v1](https://arxiv.org/abs/2105.08399v1) [cs.LG]** for this version) |





<h2 id="2021-05-19-2">2. Multi-Modal Image Captioning for the Visually Impaired
</h2>

Title: [Multi-Modal Image Captioning for the Visually Impaired](https://arxiv.org/abs/2105.08106)

Authors: [Hiba Ahsan](https://arxiv.org/search/cs?searchtype=author&query=Ahsan%2C+H), [Nikita Bhalla](https://arxiv.org/search/cs?searchtype=author&query=Bhalla%2C+N), [Daivat Bhatt](https://arxiv.org/search/cs?searchtype=author&query=Bhatt%2C+D), [Kaivankumar Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+K)

> One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image captioning systems. Current work on captioning images for the visually impaired do not use the textual data present in the image when generating captions. This problem is critical as many visual scenes contain text. Moreover, up to 21% of the questions asked by blind people about the images they click pertain to the text present in them. In this work, we propose altering AoANet, a state-of-the-art image captioning model, to leverage the text detected in the image as an input feature. In addition, we use a pointer-generator mechanism to copy the detected text to the caption when tokens need to be reproduced accurately. Our model outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2% performance improvement on CIDEr and SPICE scores, respectively.

| Comments: | 8 pages, 2 figures, 2 tables, accepted to NAACL-HLT SRW 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08106](https://arxiv.org/abs/2105.08106) [cs.CL]** |
|           | (or **[arXiv:2105.08106v1](https://arxiv.org/abs/2105.08106v1) [cs.CL]** for this version) |





<h2 id="2021-05-19-3">3. DRILL: Dynamic Representations for Imbalanced Lifelong Learning
</h2>

Title: [DRILL: Dynamic Representations for Imbalanced Lifelong Learning](https://arxiv.org/abs/2105.08445)

Authors: [Kyra Ahrens](https://arxiv.org/search/cs?searchtype=author&query=Ahrens%2C+K), [Fares Abawi](https://arxiv.org/search/cs?searchtype=author&query=Abawi%2C+F), [Stefan Wermter](https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S)

> Continual or lifelong learning has been a long-standing challenge in machine learning to date, especially in natural language processing (NLP). Although state-of-the-art language models such as BERT have ushered in a new era in this field due to their outstanding performance in multitask learning scenarios, they suffer from forgetting when being exposed to a continuous stream of data with shifting data distributions. In this paper, we introduce DRILL, a novel continual learning architecture for open-domain text classification. DRILL leverages a biologically inspired self-organizing neural architecture to selectively gate latent language representations from BERT in a task-incremental manner. We demonstrate in our experiments that DRILL outperforms current methods in a realistic scenario of imbalanced, non-stationary data without prior knowledge about task boundaries. To the best of our knowledge, DRILL is the first of its kind to use a self-organizing neural architecture for open-domain lifelong learning in NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08445](https://arxiv.org/abs/2105.08445) [cs.CL]** |
|           | (or **[arXiv:2105.08445v1](https://arxiv.org/abs/2105.08445v1) [cs.CL]** for this version) |





<h2 id="2021-05-19-4">4. Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation
</h2>

Title: [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://arxiv.org/abs/2105.08504)

Authors: [Mathias MÃ¼ller](https://arxiv.org/search/cs?searchtype=author&query=MÃ¼ller%2C+M), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search -- the de facto standard inference algorithm in NMT -- and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead.
> In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.

| Comments: | V1: ACL 2021 camera-ready                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.08504](https://arxiv.org/abs/2105.08504) [cs.CL]** |
|           | (or **[arXiv:2105.08504v1](https://arxiv.org/abs/2105.08504v1) [cs.CL]** for this version) |








# 2021-05-18

[Return to Index](#Index)



<h2 id="2021-05-18-1">1. Rethinking Skip Connection with Layer Normalization in Transformers and ResNets
</h2>

Title: [Rethinking Skip Connection with Layer Normalization in Transformers and ResNets](https://arxiv.org/abs/2105.07205)

Authors: [Fenglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Zhiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuexian Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y)

> Skip connection, is a widely-used technique to improve the performance and the convergence of deep neural networks, which is believed to relieve the difficulty in optimization due to non-linearity by propagating a linear component through the neural network layers. However, from another point of view, it can also be seen as a modulating mechanism between the input and the output, with the input scaled by a pre-defined value one. In this work, we investigate how the scale factors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale will lead to spurious gradient exploding or vanishing in line with the deepness of the models, which could be addressed by normalization, in particular, layer normalization, which induces consistent improvements over the plain skip connection. Inspired by the findings, we further propose to adaptively adjust the scale of the input by recursively applying skip connection with layer normalization, which promotes the performance substantially and generalizes well across diverse tasks including both machine translation and image classification datasets.

| Comments: | Accepted by COLING2020 (The 28th International Conference on Computational Linguistics (COLING 2020)) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2105.07205](https://arxiv.org/abs/2105.07205) [cs.LG]** |
|           | (or **[arXiv:2105.07205v1](https://arxiv.org/abs/2105.07205v1) [cs.LG]** for this version) |





<h2 id="2021-05-18-2">2. Conscious AI
</h2>

Title: [Conscious AI](https://arxiv.org/abs/2105.07879)

Authors: [Hadi Esmaeilzadeh](https://arxiv.org/search/cs?searchtype=author&query=Esmaeilzadeh%2C+H), [Reza Vaezi](https://arxiv.org/search/cs?searchtype=author&query=Vaezi%2C+R)

> Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07879](https://arxiv.org/abs/2105.07879) [cs.AI]** |
|           | (or **[arXiv:2105.07879v1](https://arxiv.org/abs/2105.07879v1) [cs.AI]** for this version) |





<h2 id="2021-05-18-3">3. Pay Attention to MLPs
</h2>

Title: [Pay Attention to MLPs](https://arxiv.org/abs/2105.08050)

Authors: [Hanxiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [David R. So](https://arxiv.org/search/cs?searchtype=author&query=So%2C+D+R), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08050](https://arxiv.org/abs/2105.08050) [cs.LG]** |
|           | (or **[arXiv:2105.08050v1](https://arxiv.org/abs/2105.08050v1) [cs.LG]** for this version) |





<h2 id="2021-05-18-4">4. DirectQE: Direct Pretraining for Machine Translation Quality Estimation
</h2>

Title: [DirectQE: Direct Pretraining for Machine Translation Quality Estimation](https://arxiv.org/abs/2105.07149)

Authors: [Qu Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Q), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiahuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xiang Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+X), [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Machine Translation Quality Estimation (QE) is a task of predicting the quality of machine translations without relying on any reference. Recently, the predictor-estimator framework trains the predictor as a feature extractor, which leverages the extra parallel corpora without QE labels, achieving promising QE performance. However, we argue that there are gaps between the predictor and the estimator in both data quality and training objectives, which preclude QE models from benefiting from a large number of parallel corpora more directly. We propose a novel framework called DirectQE that provides a direct pretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo data that is closer to the real QE data, and a detector is pretrained on these data with novel objectives that are akin to the QE task. Experiments on widely used benchmarks show that DirectQE outperforms existing methods, without using any pretraining models such as BERT. We also give extensive analyses showing how fixing the two gaps contributes to our improvements.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07149](https://arxiv.org/abs/2105.07149) [cs.CL]** |
|           | (or **[arXiv:2105.07149v1](https://arxiv.org/abs/2105.07149v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-5">5. From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding
</h2>

Title: [From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding](https://arxiv.org/abs/2105.07316)

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ibrahim Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+I), [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Ahmet ÃœstÃ¼n](https://arxiv.org/search/cs?searchtype=author&query=ÃœstÃ¼n%2C+A), [Marija StepanoviÄ‡](https://arxiv.org/search/cs?searchtype=author&query=StepanoviÄ‡%2C+M), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Siti Oryza Khairunnisa](https://arxiv.org/search/cs?searchtype=author&query=Khairunnisa%2C+S+O), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.

| Comments: | To appear in the proceedings of NAACL 2021                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.07316](https://arxiv.org/abs/2105.07316) [cs.CL]** |
|           | (or **[arXiv:2105.07316v1](https://arxiv.org/abs/2105.07316v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-6">6. The Volctrans Neural Speech Translation System for IWSLT 2021
</h2>

Title: [The Volctrans Neural Speech Translation System for IWSLT 2021](https://arxiv.org/abs/2105.07319)

Authors: [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Zhicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jian Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+J), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Rong Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+R), [Qianqian Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Jun Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> This paper describes the systems submitted to IWSLT 2021 by the Volctrans team. We participate in the offline speech translation and text-to-text simultaneous translation tracks. For offline speech translation, our best end-to-end model achieves 8.1 BLEU improvements over the benchmark on the MuST-C test set and is even approaching the results of a strong cascade solution. For text-to-text simultaneous translation, we explore the best practice to optimize the wait-k model. As a result, our final submitted systems exceed the benchmark at around 7 BLEU on the same latency regime. We will publish our code and model to facilitate both future research works and industrial applications.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07319](https://arxiv.org/abs/2105.07319) [cs.CL]** |
|           | (or **[arXiv:2105.07319v1](https://arxiv.org/abs/2105.07319v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-7">7. Data Augmentation for Sign Language Gloss Translation
</h2>

Title: [Data Augmentation for Sign Language Gloss Translation](https://arxiv.org/abs/2105.07476)

Authors: [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.

| Comments: | 4 pages, 1 page abstract                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.07476](https://arxiv.org/abs/2105.07476) [cs.CL]** |
|           | (or **[arXiv:2105.07476v1](https://arxiv.org/abs/2105.07476v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-8">8. Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation
</h2>

Title: [Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation](https://arxiv.org/abs/2105.07622)

Authors: [Ting-Wei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T), [Yung-An Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+Y), [Yi-Chieh Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Quality Estimation (QE) of Machine Translation (MT) is a task to estimate the quality scores for given translation outputs from an unknown MT system. However, QE scores for low-resource languages are usually intractable and hard to collect. In this paper, we focus on the Sentence-Level QE Shared Task of the Fifth Conference on Machine Translation (WMT20), but in a more challenging setting. We aim to predict QE scores of given translation outputs when barely none of QE scores of that paired languages are given during training. We propose an ensemble-based predictor-estimator QE model with transfer learning to overcome such QE data scarcity challenge by leveraging QE scores from other miscellaneous languages and translation results of targeted languages. Based on the evaluation results, we provide a detailed analysis of how each of our extension affects QE models on the reliability and the generalization ability to perform transfer learning under multilingual tasks. Finally, we achieve the best performance on the ensemble model combining the models pretrained by individual languages as well as different levels of parallel trained corpus with a Pearson's correlation of 0.298, which is 2.54 times higher than baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07622](https://arxiv.org/abs/2105.07622) [cs.CL]** |
|           | (or **[arXiv:2105.07622v1](https://arxiv.org/abs/2105.07622v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-9">9. Stage-wise Fine-tuning for Graph-to-Text Generation
</h2>

Title: [Stage-wise Fine-tuning for Graph-to-Text Generation](https://arxiv.org/abs/2105.08021)

Authors: [Qingyun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Semih Yavuz](https://arxiv.org/search/cs?searchtype=author&query=Yavuz%2C+S), [Victoria Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+V), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Nazneen Rajani](https://arxiv.org/search/cs?searchtype=author&query=Rajani%2C+N)

> Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.

| Comments: | 9 pages, Accepted by Proceedings of ACL-IJCNLP 2021 Student Research Workshop, Code and Resources at this [this https URL](https://github.com/EagleW/Stage-wise-Fine-tuning) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.08021](https://arxiv.org/abs/2105.08021) [cs.CL]** |
|           | (or **[arXiv:2105.08021v1](https://arxiv.org/abs/2105.08021v1) [cs.CL]** for this version) |







# 2021-05-17

[Return to Index](#Index)



<h2 id="2021-05-17-1">1. Distilling BERT for low complexity network training
</h2>

Title: [Distilling BERT for low complexity network training](https://arxiv.org/abs/2105.06514)

Authors: [Bansidhar Mangalwedhekar](https://arxiv.org/search/cs?searchtype=author&query=Mangalwedhekar%2C+B)

> This paper studies the efficiency of transferring BERT learnings to low complexity models like BiLSTM, BiLSTM with attention and shallow CNNs using sentiment analysis on SST-2 dataset. It also compares the complexity of inference of the BERT model with these lower complexity models and underlines the importance of these techniques in enabling high performance NLP models on edge devices like mobiles, tablets and MCU development boards like Raspberry Pi etc. and enabling exciting new applications.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.06514](https://arxiv.org/abs/2105.06514) [cs.CL]** |
|           | (or **[arXiv:2105.06514v1](https://arxiv.org/abs/2105.06514v1) [cs.CL]** for this version) |





<h2 id="2021-05-17-2">2. Dynamic Multi-Branch Layers for On-Device Neural Machine Translation
</h2>

Title: [Dynamic Multi-Branch Layers for On-Device Neural Machine Translation](https://arxiv.org/abs/2105.06679)

Authors: [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications such as neural machine translation (NMT) from cloud to mobile devices such as smartphones. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.6 times faster with the same number of parameters.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.06679](https://arxiv.org/abs/2105.06679) [cs.CL]** |
|           | (or **[arXiv:2105.06679v1](https://arxiv.org/abs/2105.06679v1) [cs.CL]** for this version) |





<h2 id="2021-05-17-3">3. Do Context-Aware Translation Models Pay the Right Attention?
</h2>

Title: [Do Context-Aware Translation Models Pay the Right Attention?](https://arxiv.org/abs/2105.06977)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Aditi Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+A), [AndrÃ© F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.

| Comments: | Accepted to ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.06977](https://arxiv.org/abs/2105.06977) [cs.CL]** |
|           | (or **[arXiv:2105.06977v1](https://arxiv.org/abs/2105.06977v1) [cs.CL]** for this version) |








# 2021-05-14

[Return to Index](#Index)



<h2 id="2021-05-14-1">1. Better than BERT but Worse than Baseline
</h2>

Title: [Better than BERT but Worse than Baseline](https://arxiv.org/abs/2105.05915)

Authors: [Boxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Jiaji Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Xingyu Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+X), [Kenneth Church](https://arxiv.org/search/cs?searchtype=author&query=Church%2C+K)

> This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition Identification (ADI) task. ADI inputs a text and outputs short forms (abbreviations/acronyms) and long forms (expansions). BERT with reranking improves over BERT without reranking but fails to reach the Ab3P rule-based baseline. What is BERT missing? Reranking introduces two new features: charmatch and freq. The first feature identifies opportunities to take advantage of character constraints in acronyms and the second feature identifies opportunities to take advantage of frequency constraints across documents.

| Comments: | 6 pages, 2 figures, 5 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05915](https://arxiv.org/abs/2105.05915) [cs.CL]** |
|           | (or **[arXiv:2105.05915v1](https://arxiv.org/abs/2105.05915v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-2">2. Spelling Correction with Denoising Transformer
</h2>

Title: [Spelling Correction with Denoising Transformer](https://arxiv.org/abs/2105.05977)

Authors: [Alex Kuznetsov](https://arxiv.org/search/cs?searchtype=author&query=Kuznetsov%2C+A), [Hector Urdiales](https://arxiv.org/search/cs?searchtype=author&query=Urdiales%2C+H)

> We present a novel method of performing spelling correction on short input strings, such as search queries or individual words. At its core lies a procedure for generating artificial typos which closely follow the error patterns manifested by humans. This procedure is used to train the production spelling correction model based on a transformer architecture. This model is currently served in the HubSpot product search. We show that our approach to typo generation is superior to the widespread practice of adding noise, which ignores human patterns. We also demonstrate how our approach may be extended to resource-scarce settings and train spelling correction models for Arabic, Greek, Russian, and Setswana languages, without using any labeled data.

| Comments: | 9 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05977](https://arxiv.org/abs/2105.05977) [cs.CL]** |
|           | (or **[arXiv:2105.05977v1](https://arxiv.org/abs/2105.05977v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-3">3. Designing Multimodal Datasets for NLP Challenges
</h2>

Title: [Designing Multimodal Datasets for NLP Challenges](https://arxiv.org/abs/2105.05999)

Authors: [James Pustejovsky](https://arxiv.org/search/cs?searchtype=author&query=Pustejovsky%2C+J), [Eben Holderness](https://arxiv.org/search/cs?searchtype=author&query=Holderness%2C+E), [Jingxuan Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+J), [Parker Glenn](https://arxiv.org/search/cs?searchtype=author&query=Glenn%2C+P), [Kyeongmin Rim](https://arxiv.org/search/cs?searchtype=author&query=Rim%2C+K), [Kelley Lynch](https://arxiv.org/search/cs?searchtype=author&query=Lynch%2C+K), [Richard Brutti](https://arxiv.org/search/cs?searchtype=author&query=Brutti%2C+R)

> In this paper, we argue that the design and development of multimodal datasets for natural language processing (NLP) challenges should be enhanced in two significant respects: to more broadly represent commonsense semantic inferences; and to better reflect the dynamics of actions and events, through a substantive alignment of textual and visual information. We identify challenges and tasks that are reflective of linguistic and cognitive competencies that humans have when speaking and reasoning, rather than merely the performance of systems on isolated tasks. We introduce the distinction between challenge-based tasks and competence-based performance, and describe a diagnostic dataset, Recipe-to-Video Questions (R2VQ), designed for testing competence-based comprehension over a multimodal recipe collection ([this http URL](http://r2vq.org/)). The corpus contains detailed annotation supporting such inferencing tasks and facilitating a rich set of question families that we use to evaluate NLP systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.05999](https://arxiv.org/abs/2105.05999) [cs.CL]** |
|           | (or **[arXiv:2105.05999v1](https://arxiv.org/abs/2105.05999v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-4">4. Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level
</h2>

Title: [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://arxiv.org/abs/2105.06020)

Authors: [Ruiqi Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+R), [Dhruba Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+D), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D), [Jacob Steinhardt](https://arxiv.org/search/cs?searchtype=author&query=Steinhardt%2C+J)

> Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-of-distribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%. We also find that finetuning noise increases with model size and that instance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium correlates with improvement from BERT-Medium to BERT-Large. Our findings suggest that instance-level predictions provide a rich source of information; we therefore, recommend that researchers supplement model weights with model predictions.

| Comments: | ACL 2021 Findings. Code and data: [this https URL](https://github.com/ruiqi-zhong/acl2021-instance-level) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.06020](https://arxiv.org/abs/2105.06020) [cs.CL]** |
|           | (or **[arXiv:2105.06020v1](https://arxiv.org/abs/2105.06020v1) [cs.CL]** for this version) |







# 2021-05-13

[Return to Index](#Index)



<h2 id="2021-05-13-1">1. Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction
</h2>

Title: [Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](https://arxiv.org/abs/2105.05498)

Authors: [Gyubok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+G), [Seongjun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Edward Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+E)

> Generating accurate terminology is a crucial component for the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases to appear in the translations. In many cases, however, those methods are evaluated on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. To encourage span-level representations in generation, we additionally impose a source-sentence conditioned masked span prediction loss in the decoder and observe improvements on both terminology translation as well as BLEU scores. Experimental results on three domain-specific corpora in two language pairs demonstrate that the proposed training scheme can improve the performance of existing lexically constrained methods that can operate both with or without a term dictionary at test time.

| Comments: | To appear in ACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05498](https://arxiv.org/abs/2105.05498) [cs.CL]** |
|           | (or **[arXiv:2105.05498v1](https://arxiv.org/abs/2105.05498v1) [cs.CL]** for this version) |





<h2 id="2021-05-13-2">2. Evaluating Gender Bias in Natural Language Inference
</h2>

Title: [Evaluating Gender Bias in Natural Language Inference](https://arxiv.org/abs/2105.05541)

Authors: [Shanya Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+S), [Manan Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+M), [Koustuv Sinha](https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+K)

> Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of gender bias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task that involves pairing gender-neutral premises against a gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are significantly prone to gender-induced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.

| Comments:    | NeurIPS 2020 Workshop on Dataset Curation and Security       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2105.05541](https://arxiv.org/abs/2105.05541) [cs.CL]** |
|              | (or **[arXiv:2105.05541v1](https://arxiv.org/abs/2105.05541v1) [cs.CL]** for this version) |





<h2 id="2021-05-13-3">3. Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders
</h2>

Title: [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://arxiv.org/abs/2105.05752)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Bojie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Yanyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yuhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [shen huang](https://arxiv.org/search/cs?searchtype=author&query=huang%2C+s), [Qi Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De show that our method achieves the state-of-the-art performance of 18.3 and 25.2 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2105.05752](https://arxiv.org/abs/2105.05752) [cs.CL]** |
|           | (or **[arXiv:2105.05752v1](https://arxiv.org/abs/2105.05752v1) [cs.CL]** for this version) |







# 2021-05-12

[Return to Index](#Index)



<h2 id="2021-05-12-1">1. Cross-Modal Generative Augmentation for Visual Question Answering
</h2>

Title: [Cross-Modal Generative Augmentation for Visual Question Answering](https://arxiv.org/abs/2105.04780)

Authors: [Zixu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yishu Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Data augmentation is an approach that can effectively improve the performance of multimodal machine learning. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low level operations with deterministic heuristics, our method proposes to learn an augmentation sampler that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly updated with a downstream pipeline. Experiments on Visual Question Answering tasks demonstrate the effectiveness of the proposed generative model, which is able to boost the strong UpDn-based models to the state-of-the-art performance.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.04780](https://arxiv.org/abs/2105.04780) [cs.CV]** |
|           | (or **[arXiv:2105.04780v1](https://arxiv.org/abs/2105.04780v1) [cs.CV]** for this version) |





<h2 id="2021-05-12-2">2. Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity
</h2>

Title: [Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity](https://arxiv.org/abs/2105.04616)

Authors: [Yingxue Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Mark-Jan Nederhof](https://arxiv.org/search/cs?searchtype=author&query=Nederhof%2C+M)

> By using a trigram model and fine-tuning a pretrained BERT model for sequence classification, we show that machine translation and human translation can be classified with an accuracy above chance level, which suggests that machine translation and human translation are different in a systematic way. The classification accuracy of machine translation is much higher than of human translation. We show that this may be explained by the difference in lexical diversity between machine translation and human translation. If machine translation has independent patterns from human translation, automatic metrics which measure the deviation of machine translation from human translation may conflate difference with quality. Our experiment with two different types of automatic metrics shows correlation with the result of the classification task. Therefore, we suggest the difference in lexical diversity between machine translation and human translation be given more attention in machine translation evaluation.

| Comments: | accepted by MoTra21, Nodalida 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04616](https://arxiv.org/abs/2105.04616) [cs.CL]** |
|           | (or **[arXiv:2105.04616v1](https://arxiv.org/abs/2105.04616v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-3">3. Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal
</h2>

Title: [Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal](https://arxiv.org/abs/2105.04633)

Authors: [Casey Kennington](https://arxiv.org/search/cs?searchtype=author&query=Kennington%2C+C)

> Humans' experience of the world is profoundly multimodal from the beginning, so why do existing state-of-the-art language models only use text as a modality to learn and represent semantic meaning? In this paper we review the literature on the role of embodiment and emotion in the interactive setting of spoken dialogue as necessary prerequisites for language learning for human children, including how words in child vocabularies are largely concrete, then shift to become more abstract as the children get older. We sketch a model of semantics that leverages current transformer-based models and a word-level grounded model, then explain the robot-dialogue system that will make use of our semantic model, the setting for the system to learn language, and existing benchmarks for evaluation.

| Comments: | 6 pages, ICLR 2021 Embodied Multimodal Learning Workshop     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04633](https://arxiv.org/abs/2105.04633) [cs.CL]** |
|           | (or **[arXiv:2105.04633v1](https://arxiv.org/abs/2105.04633v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-4">4. Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models
</h2>

Title: [Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models](https://arxiv.org/abs/2105.04688)

Authors: [Laura PÃ©rez-Mayos](https://arxiv.org/search/cs?searchtype=author&query=PÃ©rez-Mayos%2C+L), [Alba TÃ¡boas GarcÃ­a](https://arxiv.org/search/cs?searchtype=author&query=GarcÃ­a%2C+A+T), [Simon Mille](https://arxiv.org/search/cs?searchtype=author&query=Mille%2C+S), [Leo Wanner](https://arxiv.org/search/cs?searchtype=author&query=Wanner%2C+L)

> Multilingual Transformer-based language models, usually pretrained on more than 100 languages, have been shown to achieve outstanding results in a wide range of cross-lingual transfer tasks. However, it remains unknown whether the optimization for different languages conditions the capacity of the models to generalize over syntactic structures, and how languages with syntactic phenomena of different complexity are affected. In this work, we explore the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa. More specifically, we evaluate the syntactic generalization potential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform.

| Comments: | To be published in Findings of ACL 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04688](https://arxiv.org/abs/2105.04688) [cs.CL]** |
|           | (or **[arXiv:2105.04688v1](https://arxiv.org/abs/2105.04688v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-5">5. Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation
</h2>

Title: [Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation](https://arxiv.org/abs/2105.04840)

Authors: [Shun-Po Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+S), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Chih-Chiang Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.

| Comments: | Accepted in Findings of ACL-IJCNLP 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04840](https://arxiv.org/abs/2105.04840) [cs.CL]** |
|           | (or **[arXiv:2105.04840v1](https://arxiv.org/abs/2105.04840v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-6">6. Can You Traducir This? Machine Translation for Code-Switched Input
</h2>

Title: [Can You Traducir This? Machine Translation for Code-Switched Input](https://arxiv.org/abs/2105.04846)

Authors: [Jitao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J) (TLP), [FranÃ§ois Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F) (TLP)

> Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Workshop on Computational Approaches to Linguistic Code Switching, Jun 2021, Online, United States |
| Cite as:           | **[arXiv:2105.04846](https://arxiv.org/abs/2105.04846) [cs.CL]** |
|                    | (or **[arXiv:2105.04846v1](https://arxiv.org/abs/2105.04846v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-7">7. BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?
</h2>

Title: [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://arxiv.org/abs/2105.04949)

Authors: [Asahi Ushio](https://arxiv.org/search/cs?searchtype=author&query=Ushio%2C+A), [Luis Espinosa-Anke](https://arxiv.org/search/cs?searchtype=author&query=Espinosa-Anke%2C+L), [Steven Schockaert](https://arxiv.org/search/cs?searchtype=author&query=Schockaert%2C+S), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J)

> Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as eye is to seeing what ear is to hearing, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations\footnote{Source code and data to reproduce our experimental results are available in the following repository: \url{[this https URL](https://github.com/asahi417/analogy-language-model)}}.

| Comments: | Accepted by ACL 2021 main conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.04949](https://arxiv.org/abs/2105.04949) [cs.CL]** |
|           | (or **[arXiv:2105.04949v1](https://arxiv.org/abs/2105.04949v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-8">8. Towards transparency in NLP shared tasks
</h2>

Title: [Towards transparency in NLP shared tasks](https://arxiv.org/abs/2105.05020)

Authors: [Carla Parra EscartÃ­n](https://arxiv.org/search/cs?searchtype=author&query=EscartÃ­n%2C+C+P), [Teresa Lynn](https://arxiv.org/search/cs?searchtype=author&query=Lynn%2C+T), [Joss Moorkens](https://arxiv.org/search/cs?searchtype=author&query=Moorkens%2C+J), [Jane Dunne](https://arxiv.org/search/cs?searchtype=author&query=Dunne%2C+J)

> This article reports on a survey carried out across the Natural Language Processing (NLP) community. The survey aimed to capture the opinions of the research community on issues surrounding shared tasks, with respect to both participation and organisation. Amongst the 175 responses received, both positive and negative observations were made. We carried out and report on an extensive analysis of these responses, which leads us to propose a Shared Task Organisation Checklist that could support future participants and organisers. The proposed Checklist is flexible enough to accommodate the wide diversity of shared tasks in our field and its goal is not to be prescriptive, but rather to serve as a tool that encourages shared task organisers to foreground ethical behaviour, beginning with the common issues that the 175 respondents deemed important. Its usage would not only serve as an instrument to reflect on important aspects of shared tasks, but would also promote increased transparency around them.

| Comments: | 38 pages, 26 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.05020](https://arxiv.org/abs/2105.05020) [cs.CL]** |
|           | (or **[arXiv:2105.05020v1](https://arxiv.org/abs/2105.05020v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-9">9. Including Signed Languages in Natural Language Processing
</h2>

Title: [Including Signed Languages in Natural Language Processing](https://arxiv.org/abs/2105.05222)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Julie Hochgesang](https://arxiv.org/search/cs?searchtype=author&query=Hochgesang%2C+J), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Malihe Alikhani](https://arxiv.org/search/cs?searchtype=author&query=Alikhani%2C+M)

> Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.

| Comments: | Accepted as a Theme paper to ACL 2021                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05222](https://arxiv.org/abs/2105.05222) [cs.CL]** |
|           | (or **[arXiv:2105.05222v1](https://arxiv.org/abs/2105.05222v1) [cs.CL]** for this version) |





# 2021-05-11

[Return to Index](#Index)



<h2 id="2021-05-11-1">1. Duplex Sequence-to-Sequence Learning for Reversible Machine Translation
</h2>

Title: [Duplex Sequence-to-Sequence Learning for Reversible Machine Translation](https://arxiv.org/abs/2105.03458)

Authors: [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Sequence-to-sequence (seq2seq) problems such as machine translation are bidirectional, which naturally derive a pair of directional tasks and two directional learning signals. However, typical seq2seq neural networks are {\em simplex} that only model one unidirectional task, which cannot fully exploit the potential of bidirectional learning signals from parallel data. To address this issue, we propose a {\em duplex} seq2seq neural network, REDER (Reversible Duplex Transformer), and apply it to machine translation. The architecture of REDER has two ends, each of which specializes in a language so as to read and yield sequences in that language. As a result, REDER can simultaneously learn from the bidirectional signals, and enables {\em reversible machine translation} by simply flipping the input and output ends, Experiments on widely-used machine translation benchmarks verify that REDER achieves the first success of reversible machine translation, which helps obtain considerable gains over several strong baselines.

| Comments: | Under review, 10 pages                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03458](https://arxiv.org/abs/2105.03458) [cs.CL]** |
|           | (or **[arXiv:2105.03458v1](https://arxiv.org/abs/2105.03458v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-2">2. Measuring and Increasing Context Usage in Context-Aware Machine Translation
</h2>

Title: [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://arxiv.org/abs/2105.03482)

Authors: [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [AndrÃ© F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

> Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03482](https://arxiv.org/abs/2105.03482) [cs.CL]** |
|           | (or **[arXiv:2105.03482v1](https://arxiv.org/abs/2105.03482v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-3">3. Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation
</h2>

Title: [Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation](https://arxiv.org/abs/2105.03953)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Fine-tuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pre-training mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the fine-tuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at [this https URL](https://github.com/zliucr/cpt-nmt).

| Comments: | Accepted in Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.03953](https://arxiv.org/abs/2105.03953) [cs.CL]** |
|           | (or **[arXiv:2105.03953v1](https://arxiv.org/abs/2105.03953v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-4">4. Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction
</h2>

Title: [Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction](https://arxiv.org/abs/2105.04443)

Authors: [Zhenghao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Xiaoyuan Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Tat-Seng Chua](https://arxiv.org/search/cs?searchtype=author&query=Chua%2C+T)

> Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at [this https URL](https://github.com/thunlp/VERNet).

| Comments: | Accepted by NAACL2021, 9 pages, 5 figures                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04443](https://arxiv.org/abs/2105.04443) [cs.CL]** |
|           | (or **[arXiv:2105.04443v1](https://arxiv.org/abs/2105.04443v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-5">5. Self-Guided Curriculum Learning for Neural Machine Translation
</h2>

Title: [Self-Guided Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2105.04475)

Authors: [Lei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Ryohei Sasano](https://arxiv.org/search/cs?searchtype=author&query=Sasano%2C+R), [Koichi Takeda](https://arxiv.org/search/cs?searchtype=author&query=Takeda%2C+K)

> In the field of machine learning, the well-trained model is assumed to be able to recover the training labels, i.e. the synthetic labels predicted by the model should be as close to the ground-truth labels as possible. Inspired by this, we propose a self-guided curriculum strategy to encourage the learning of neural machine translation (NMT) models to follow the above recovery criterion, where we cast the recovery degree of each training example as its learning difficulty. Specifically, we adopt the sentence level BLEU score as the proxy of recovery degree. Different from existing curricula relying on linguistic prior knowledge or third-party language models, our chosen learning difficulty is more suitable to measure the degree of knowledge mastery of the NMT models. Experiments on translation benchmarks, including WMT14 Englishâ‡’German and WMT17 Chineseâ‡’English, demonstrate that our approach can consistently improve translation performance against strong baseline Transformer.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.04475](https://arxiv.org/abs/2105.04475) [cs.CL]** |
|           | (or **[arXiv:2105.04475v1](https://arxiv.org/abs/2105.04475v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-6">6. UPC's Speech Translation System for IWSLT 2021
</h2>

Title: [UPC's Speech Translation System for IWSLT 2021](https://arxiv.org/abs/2105.04512)

Authors: [Gerard I. GÃ¡llego](https://arxiv.org/search/cs?searchtype=author&query=GÃ¡llego%2C+G+I), [Ioannis Tsiamas](https://arxiv.org/search/cs?searchtype=author&query=Tsiamas%2C+I), [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [JosÃ© A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussÃ ](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussÃ %2C+M+R)

> This paper describes the submission to the IWSLT 2021 offline speech translation task by the UPC Machine Translation group. The task consists of building a system capable of translating English audio recordings extracted from TED talks into German text. Submitted systems can be either cascade or end-to-end and use a custom or given segmentation. Our submission is an end-to-end speech translation system, which combines pre-trained models (Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder, and uses an efficient fine-tuning technique, which trains only 20% of its total parameters. We show that adding an Adapter to the system and pre-training it, can increase the convergence speed and the final result, with which we achieve a BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble that obtains 28.22 BLEU score on the same set. Our submission also uses a custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the given segmentation.

| Comments: | Submitted to IWSLT 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04512](https://arxiv.org/abs/2105.04512) [cs.CL]** |
|           | (or **[arXiv:2105.04512v1](https://arxiv.org/abs/2105.04512v1) [cs.CL]** for this version) |







# 2021-05-10

[Return to Index](#Index)



<h2 id="2021-05-10-1">1. Adapting by Pruning: A Case Study on BERT
</h2>

Title: [Adapting by Pruning: A Case Study on BERT](https://arxiv.org/abs/2105.03343)

Authors: [Yang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Nicolo Colombo](https://arxiv.org/search/cs?searchtype=author&query=Colombo%2C+N), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W)

> Adapting pre-trained neural models to downstream tasks has become the standard practice for obtaining high-quality models. In this work, we propose a novel model adaptation paradigm, adapting by pruning, which prunes neural connections in the pre-trained model to optimise the performance on the target task; all remaining connections have their weights intact. We formulate adapting-by-pruning as an optimisation problem with a differentiable loss and propose an efficient algorithm to prune the model. We prove that the algorithm is near-optimal under standard assumptions and apply the algorithm to adapt BERT to some GLUE tasks. Results suggest that our method can prune up to 50% weights in BERT while yielding similar performance compared to the fine-tuned full model. We also compare our method with other state-of-the-art pruning methods and study the topological differences of their obtained sub-networks.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.03343](https://arxiv.org/abs/2105.03343) [cs.LG]** |
|           | (or **[arXiv:2105.03343v1](https://arxiv.org/abs/2105.03343v1) [cs.LG]** for this version) |





<h2 id="2021-05-10-2">2. On-the-Fly Controlled Text Generation with Experts and Anti-Experts
</h2>

Title: [On-the-Fly Controlled Text Generation with Experts and Anti-Experts](https://arxiv.org/abs/2105.03023)

Authors: [Alisa Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Maarten Sap](https://arxiv.org/search/cs?searchtype=author&query=Sap%2C+M), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Swabha Swayamdipta](https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S), [Chandra Bhagavatula](https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation which combines a pretrained language model with experts and/or anti-experts in an ensemble of language models. Intuitively, under our ensemble, output tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Our work highlights the promise of using LMs trained on text with (un)desired attributes for efficient decoding-time controlled language generation.

| Comments: | Accepted to ACL 2021, camera-ready version coming soon       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03023](https://arxiv.org/abs/2105.03023) [cs.CL]** |
|           | (or **[arXiv:2105.03023v1](https://arxiv.org/abs/2105.03023v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-3">3. Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates
</h2>

Title: [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://arxiv.org/abs/2105.03048)

Authors: [Yuqing Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y), [Yi-an Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Y), [Yuanjun Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+Y), [Yi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Stefano Soatto](https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S)

> Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.

| Comments: | 13 pages, 3 figures, Accepted at ACL 2021 main conference    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03048](https://arxiv.org/abs/2105.03048) [cs.CL]** |
|           | (or **[arXiv:2105.03048v1](https://arxiv.org/abs/2105.03048v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-4">4. A Survey of Data Augmentation Approaches for NLP
</h2>

Title: [A Survey of Data Augmentation Approaches for NLP](https://arxiv.org/abs/2105.03075)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Sarath Chandar](https://arxiv.org/search/cs?searchtype=author&query=Chandar%2C+S), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.

| Comments: | Accepted to ACL 2021 Findings                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.03075](https://arxiv.org/abs/2105.03075) [cs.CL]** |
|           | (or **[arXiv:2105.03075v1](https://arxiv.org/abs/2105.03075v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-5">5. Learning Shared Semantic Space for Speech-to-Text Translation
</h2>

Title: [Learning Shared Semantic Space for Speech-to-Text Translation](https://arxiv.org/abs/2105.03095)

Authors: [Chi Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Having numerous potential applications and great impact, end-to-end speech translation (ST) has long been treated as an independent task, failing to fully draw strength from the rapid advances of its sibling - text machine translation (MT). With text and audio inputs represented differently, the modality gap has rendered MT data and its end-to-end models incompatible with their ST counterparts. In observation of this obstacle, we propose to bridge this representation gap with Chimera. By projecting audio and text features to a common semantic representation, Chimera unifies MT and ST tasks and boosts the performance on ST benchmark, MuST-C, to a new state-of-the-art. Specifically, Chimera obtains 26.3 BLEU on EN-DE, improving the SOTA by a +2.7 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities.

| Comments: | 8 pages, 5 figures, Accepted by Findings of ACL 2021         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03095](https://arxiv.org/abs/2105.03095) [cs.CL]** |
|           | (or **[arXiv:2105.03095v1](https://arxiv.org/abs/2105.03095v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-6">6. Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods
</h2>

Title: [Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods](https://arxiv.org/abs/2105.03311)

Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Gareth J. F. Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+G+J+F), [Alan F. Smeaton](https://arxiv.org/search/cs?searchtype=author&query=Smeaton%2C+A+F)

> To facilitate effective translation modeling and translation studies, one of the crucial questions to address is how to assess translation quality. From the perspectives of accuracy, reliability, repeatability and cost, translation quality assessment (TQA) itself is a rich and challenging task. In this work, we present a high-level and concise survey of TQA methods, including both manual judgement criteria and automated evaluation metrics, which we classify into further detailed sub-categories. We hope that this work will be an asset for both translation model researchers and quality assessment researchers. In addition, we hope that it will enable practitioners to quickly develop a better understanding of the conventional TQA field, and to find corresponding closely relevant evaluation solutions for their own needs. This work may also serve inspire further development of quality assessment and evaluation methodologies for other natural language processing (NLP) tasks in addition to machine translation (MT), such as automatic text summarization (ATS), natural language understanding (NLU) and natural language generation (NLG).

| Comments: | Accepted to 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021): Workshop on Modelling Translation: Translatology in the Digital Age (MoTra21). arXiv admin note: substantial text overlap with [arXiv:1605.04515](https://arxiv.org/abs/1605.04515) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03311](https://arxiv.org/abs/2105.03311) [cs.CL]** |
|           | (or **[arXiv:2105.03311v1](https://arxiv.org/abs/2105.03311v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-7">7. Are Pre-trained Convolutions Better than Pre-trained Transformers?
</h2>

Title: [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://arxiv.org/abs/2105.03322)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [Jai Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+J), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Vamsi Aribandi](https://arxiv.org/search/cs?searchtype=author&query=Aribandi%2C+V), [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.03322](https://arxiv.org/abs/2105.03322) [cs.CL]** |
|           | (or **[arXiv:2105.03322v1](https://arxiv.org/abs/2105.03322v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-8">8. âˆ‚-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization
</h2>

Title: [âˆ‚-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization](https://arxiv.org/abs/2105.03417)

Authors: [Mokanarangan Thayaparan](https://arxiv.org/search/cs?searchtype=author&query=Thayaparan%2C+M), [Marco Valentino](https://arxiv.org/search/cs?searchtype=author&query=Valentino%2C+M), [Deborah Ferreira](https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+D), [Julia Rozanova](https://arxiv.org/search/cs?searchtype=author&query=Rozanova%2C+J), [AndrÃ© Freitas](https://arxiv.org/search/cs?searchtype=author&query=Freitas%2C+A)

> Constrained optimization solvers with Integer Linear programming (ILP) have been the cornerstone for explainable natural language inference during its inception. ILP based approaches provide a way to encode explicit and controllable assumptions casting natural language inference as an abductive reasoning problem, where the solver constructs a plausible explanation for a given hypothesis. While constrained based solvers provide explanations, they are often limited by the use of explicit constraints and cannot be integrated as part of broader deep neural architectures. In contrast, state-of-the-art transformer-based models can learn from data and implicitly encode complex constraints. However, these models are intrinsically black boxes. This paper presents a novel framework named âˆ‚-Explainer (Diff-Explainer) that combines the best of both worlds by casting the constrained optimization as part of a deep neural network via differentiable convex optimization and fine-tuning pre-trained transformers for downstream explainable NLP tasks. To demonstrate the efficacy of the framework, we transform the constraints presented by TupleILP and integrate them with sentence embedding transformers for the task of explainable science QA. Our experiments show up to â‰ˆ10% improvement over non-differentiable solver while still providing explanations for supporting its inference.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.03417](https://arxiv.org/abs/2105.03417) [cs.CL]** |
|           | (or **[arXiv:2105.03417v1](https://arxiv.org/abs/2105.03417v1) [cs.CL]** for this version) |






# 2021-05-07

[Return to Index](#Index)



<h2 id="2021-05-07-1">1. XeroAlign: Zero-Shot Cross-lingual Transformer Alignment
</h2>

Title: [XeroAlign: Zero-Shot Cross-lingual Transformer Alignment](https://arxiv.org/abs/2105.02472)

Authors: [Milan Gritta](https://arxiv.org/search/cs?searchtype=author&query=Gritta%2C+M), [Ignacio Iacobacci](https://arxiv.org/search/cs?searchtype=author&query=Iacobacci%2C+I)

> The introduction of pretrained cross-lingual language models brought decisive improvements to multilingual NLP tasks. However, the lack of labelled task data necessitates a variety of methods aiming to close the gap to high-resource languages. Zero-shot methods in particular, often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign, a simple method for task-specific alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows strong improvements over the baseline models to achieve state-of-the-art zero-shot results on three multilingual natural language understanding tasks. XLM-RA's text classification accuracy exceeds that of XLM-R trained with labelled data and performs on par with state-of-the-art models on a cross-lingual adversarial paraphrasing task.

| Comments: | Accepted as long paper at Findings of ACL 2021               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.02472](https://arxiv.org/abs/2105.02472) [cs.CL]** |
|           | (or **[arXiv:2105.02472v1](https://arxiv.org/abs/2105.02472v1) [cs.CL]** for this version) |





<h2 id="2021-05-07-2">2. Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases
</h2>

Title: [Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases](https://arxiv.org/abs/2105.02477)

Authors: [Li-Hsin Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+L), [Sampo Pyysalo](https://arxiv.org/search/cs?searchtype=author&query=Pyysalo%2C+S), [Jenna Kanerva](https://arxiv.org/search/cs?searchtype=author&query=Kanerva%2C+J), [Filip Ginter](https://arxiv.org/search/cs?searchtype=author&query=Ginter%2C+F)

> In this paper, we present a quantitative evaluation of differences between alternative translations in a large recently released Finnish paraphrase corpus focusing in particular on non-trivial variation in translation. We combine a series of automatic steps detecting systematic variation with manual analysis to reveal regularities and identify categories of translation differences. We find the paraphrase corpus to contain highly non-trivial translation variants difficult to recognize through automatic approaches.

| Comments: | Accepted to Workshop on MOdelling TRAnslation: Translatology in the Digital Age |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.02477](https://arxiv.org/abs/2105.02477) [cs.CL]** |
|           | (or **[arXiv:2105.02477v1](https://arxiv.org/abs/2105.02477v1) [cs.CL]** for this version) |





<h2 id="2021-05-07-3">3. Content4All Open Research Sign Language Translation Datasets
</h2>

Title: [Content4All Open Research Sign Language Translation Datasets](https://arxiv.org/abs/2105.02351)

Authors: [Necati Cihan Camgoz](https://arxiv.org/search/cs?searchtype=author&query=Camgoz%2C+N+C), [Ben Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+B), [Guillaume Rochette](https://arxiv.org/search/cs?searchtype=author&query=Rochette%2C+G), [Marco Giovanelli](https://arxiv.org/search/cs?searchtype=author&query=Giovanelli%2C+M), [Giacomo Inches](https://arxiv.org/search/cs?searchtype=author&query=Inches%2C+G), [Robin Nachtrab-Ribback](https://arxiv.org/search/cs?searchtype=author&query=Nachtrab-Ribback%2C+R), [Richard Bowden](https://arxiv.org/search/cs?searchtype=author&query=Bowden%2C+R)

> Computational sign language research lacks the large-scale datasets that enables the creation of useful reallife applications. To date, most research has been limited to prototype systems on small domains of discourse, e.g. weather forecasts. To address this issue and to push the field forward, we release six datasets comprised of 190 hours of footage on the larger domain of news. From this, 20 hours of footage have been annotated by Deaf experts and interpreters and is made publicly available for research purposes. In this paper, we share the dataset collection process and tools developed to enable the alignment of sign language video and subtitles, as well as baseline translation results to underpin future research.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.02351](https://arxiv.org/abs/2105.02351) [cs.CV]** |
|           | (or **[arXiv:2105.02351v1](https://arxiv.org/abs/2105.02351v1) [cs.CV]** for this version) |





<h2 id="2021-05-07-4">4. Reliability Testing for Natural Language Processing Systems
</h2>

Title: [Reliability Testing for Natural Language Processing Systems](https://arxiv.org/abs/2105.02590)

Authors: [Samson Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Kathy Baxter](https://arxiv.org/search/cs?searchtype=author&query=Baxter%2C+K), [Araz Taeihagh](https://arxiv.org/search/cs?searchtype=author&query=Taeihagh%2C+A), [Gregory A. Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+G+A), [Min-Yen Kan](https://arxiv.org/search/cs?searchtype=author&query=Kan%2C+M)

> Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing -- with an emphasis on interdisciplinary collaboration -- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.

| Comments: | Accepted to ACL-IJCNLP 2021 (main conference). Final camera-ready version to follow shortly |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.02590](https://arxiv.org/abs/2105.02590) [cs.LG]** |
|           | (or **[arXiv:2105.02590v1](https://arxiv.org/abs/2105.02590v1) [cs.LG]** for this version) |









# 2021-05-06

[Return to Index](#Index)



<h2 id="2021-05-06-1">1. Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution
</h2>

Title: [Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution](https://arxiv.org/abs/2105.01691)

Authors: [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Kenton Murray](https://arxiv.org/search/cs?searchtype=author&query=Murray%2C+K), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

> In this paper, we investigate the driving factors behind concatenation, a simple but effective data augmentation method for low-resource neural machine translation. Our experiments suggest that discourse context is unlikely the cause for the improvement of about +1 BLEU across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.01691](https://arxiv.org/abs/2105.01691) [cs.CL]** |
|           | (or **[arXiv:2105.01691v1](https://arxiv.org/abs/2105.01691v1) [cs.CL]** for this version) |



<h2 id="2021-05-06-2">2. Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy
</h2>

Title: [Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy](https://arxiv.org/abs/2105.01893)

Authors: [Zhengxin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> Simultaneous translation, which starts translating each sentence after receiving only a few words in source sentence, has a vital role in many scenarios. Although the previous prefix-to-prefix framework is considered suitable for simultaneous translation and achieves good performance, it still has two inevitable drawbacks: the high computational resource costs caused by the need to train a separate model for each latency k and the insufficient ability to encode information because each target token can only attend to a specific source prefix. We propose a novel framework that adopts a simple but effective decoding strategy which is designed for full-sentence models. Within this framework, training a single full-sentence model can achieve arbitrary given latency and save computational resources. Besides, with the competence of the full-sentence model to encode the whole sentence, our decoding strategy can enhance the information maintained in the decoded states in real time. Experimental results show that our method achieves better translation quality than baselines on 4 directions: Zhâ†’En, Enâ†’Ro and Enâ†”De.

| Comments: | 8 pages, 5 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.01893](https://arxiv.org/abs/2105.01893) [cs.CL]** |
|           | (or **[arXiv:2105.01893v1](https://arxiv.org/abs/2105.01893v1) [cs.CL]** for this version) |









# 2021-05-04

[Return to Index](#Index)



<h2 id="2021-05-04-1">1. AlloST: Low-resource Speech Translation without Source Transcription
</h2>


Title: [AlloST: Low-resource Speech Translation without Source Transcription](https://arxiv.org/abs/2105.00171)

Authors: [Yao-Fei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Hung-Shin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Hsin-Min Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> The end-to-end architecture has made promising progress in speech translation (ST). However, the ST task is still challenging under low-resource conditions. Most ST models have shown unsatisfactory results, especially in the absence of word information from the source speech utterance. In this study, we survey methods to improve ST performance without using source transcription, and propose a learning framework that utilizes a language-independent universal phone recognizer. The framework is based on an attention-based sequence-to-sequence model, where the encoder generates the phonetic embeddings and phone-aware acoustic representations, and the decoder controls the fusion of the two embedding streams to produce the target token sequence. In addition to investigating different fusion strategies, we explore the specific usage of byte pair encoding (BPE), which compresses a phone sequence into a syllable-like segmented sequence with semantic information. Experiments conducted on the Fisher Spanish-English and Taigi-Mandarin drama corpora show that our method outperforms the conformer-based baseline, and the performance is close to that of the existing best method using source transcription.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.00171](https://arxiv.org/abs/2105.00171) [cs.CL]** |
|           | (or **[arXiv:2105.00171v1](https://arxiv.org/abs/2105.00171v1) [cs.CL]** for this version) |



<h2 id="2021-05-04-2">2. Larger-Scale Transformers for Multilingual Masked Language Modeling
</h2>


Title: [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572)

Authors: [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Giri Anantharaman](https://arxiv.org/search/cs?searchtype=author&query=Anantharaman%2C+G), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.00572](https://arxiv.org/abs/2105.00572) [cs.CL]** |
|           | (or **[arXiv:2105.00572v1](https://arxiv.org/abs/2105.00572v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-3">3. Transformers: "The End of History" for NLP?
</h2>


Title: [Transformers: "The End of History" for NLP?](https://arxiv.org/abs/2105.00813)

Authors: [Anton Chernyavskiy](https://arxiv.org/search/cs?searchtype=author&query=Chernyavskiy%2C+A), [Dmitry Ilvovsky](https://arxiv.org/search/cs?searchtype=author&query=Ilvovsky%2C+D), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

> Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state-of-the-art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed some light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures.

| Comments:    | Transformers, NLP, BERT, RoBERTa, XLNet                      |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2105.00813](https://arxiv.org/abs/2105.00813) [cs.CL]** |
|              | (or **[arXiv:2105.00813v1](https://arxiv.org/abs/2105.00813v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-4">4. BERT memorisation and pitfalls in low-resource scenarios
</h2>


Title: [BERT memorisation and pitfalls in low-resource scenarios](https://arxiv.org/abs/2105.00828)

Authors: [Michael TÃ¤nzer](https://arxiv.org/search/cs?searchtype=author&query=TÃ¤nzer%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M)

> State-of-the-art pre-trained models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal performances even on extremely noisy datasets. Conversely, we also find that they completely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose a novel architecture based on BERT and prototypical networks that improves performance in low-resource named entity recognition tasks.

| Comments: | 14 pages, 24 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.00828](https://arxiv.org/abs/2105.00828) [cs.CL]** |
|           | (or **[arXiv:2105.00828v1](https://arxiv.org/abs/2105.00828v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-5">5. Natural Language Generation Using Link Grammar for General Conversational Intelligence
</h2>


Title: [Natural Language Generation Using Link Grammar for General Conversational Intelligence](https://arxiv.org/abs/2105.00830)

Authors: [Vignav Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+V), [Anton Kolonin](https://arxiv.org/search/cs?searchtype=author&query=Kolonin%2C+A)

> Many current artificial general intelligence (AGI) and natural language processing (NLP) architectures do not possess general conversational intelligence--that is, they either do not deal with language or are unable to convey knowledge in a form similar to the human language without manual, labor-intensive methods such as template-based customization. In this paper, we propose a new technique to automatically generate grammatically valid sentences using the Link Grammar database. This natural language generation method far outperforms current state-of-the-art baselines and may serve as the final component in a proto-AGI question answering pipeline that understandably handles natural language material.

| Comments: | 17 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.00830](https://arxiv.org/abs/2105.00830) [cs.CL]** |
|           | (or **[arXiv:2105.00830v1](https://arxiv.org/abs/2105.00830v1) [cs.CL]** for this version) |

