# Daily arXiv: Machine Translation - August, 2020

# Index


- [2020-08-28](#2020-08-28)

  - [1. Adaptable Filtering using Hierarchical Embeddings for Chinese Spell Check](#2020-08-28-1)
- [2020-08-27](#2020-08-27)

  - [1. The Impact of Indirect Machine Translation on Sentiment Classification](#2020-08-27-1)
- [2020-08-24](#2020-08-24)
- [1. Neural Machine Translation without Embeddings](#2020-08-24-1)
- [2020-08-21](#2020-08-21)

  - [1. Lite Training Strategies for Portuguese-English and English-Portuguese Translation](#2020-08-21-1)
  - [2. Inducing Language-Agnostic Multilingual Representations](#2020-08-21-2)
- [2020-08-20](#2020-08-20-1)

  - [1. Transformer based Multilingual document Embedding model](#2020-08-20-1)
- [2020-08-19](#2020-08-19)

  - [1. Word2vec Skip-gram Dimensionality Selection via Sequential Normalized Maximum Likelihood](#2020-08-19-1)
  - [2. Very Deep Transformers for Neural Machine Translation](#2020-08-19-2)
  - [3. Glancing Transformer for Non-Autoregressive Neural Machine Translation](#2020-08-19-3)
- [2020-08-18](#2020-08-18)
- [1. Lanfrica: A Participatory Approach to Documenting Machine Translation Research on African Languages](#2020-08-18-1)
  - [2. Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size](#2020-08-18-2)
- [2020-08-14](#2020-08-14)

  - [1. On the Importance of Local Information in Transformer Based Models](#2020-08-14-1)
- [2020-08-13](#2020-08-13)

  - [1. Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity](#2020-08-13-1)
  - [2. Approaching Neural Chinese Word Segmentation as a Low-Resource Machine Translation Task](#2020-08-13-2)
- [2020-08-12](#2020-08-12)

  - [1. On Learning Language-Invariant Representations for Universal Machine Translation](#2020-08-12-1)
  - [2. Revisiting Low Resource Status of Indian Languages in Machine Translation](#2020-08-12-2)
  - [3. The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020](#2020-08-12-3)
- [2020-08-11](#2020-08-11)

  - [1. Word Error Rate Estimation Without ASR Output: e-WER2](#2020-08-11-1)
  - [2. Navigating Language Models with Synthetic Agents](#2020-08-11-2)
  - [3. Distilling the Knowledge of BERT for Sequence-to-Sequence ASR](#2020-08-11-3)
  - [4. Does BERT Solve Commonsense Task via Commonsense Knowledge?](#2020-08-11-4)
- [2020-08-10](#2020-08-10)
- [1. A Multilingual Neural Machine Translation Model for Biomedical Data](#2020-08-10-1)
  - [2. Data Weighted Training Strategies for Grammatical Error Correction](#2020-08-10-2)
  - [3. SemEval-2020 Task 10: Emphasis Selection for Written Text in Visual Media](#2020-08-10-3)
- [2020-08-06](#2020-08-06)

  - [1. An exploration of the encoding of grammatical gender in word embeddings](#2020-08-06-1)
- [2020-08-05](#2020-08-05)
- [1. A Survey of Orthographic Information in Machine Translation](#2020-08-05-1)
  - [2. Defining and Evaluating Fair Natural Language Generation](#2020-08-05-2)
- [2020-08-04](#2020-08-04)

  - [1. Audiovisual Speech Synthesis using Tacotron2](#2020-08-04-1)
  - [2. DeLighT: Very Deep and Light-weight Transformer](#2020-08-04-2)
  - [3. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](#2020-08-04-3)
  - [4. LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?](#2020-08-04-4)
- [2020-08-03](#2020-08-03)
  - [1. Neural Language Generation: Formulation, Methods, and Evaluation](#2020-08-03-1)
  - [2. On Learning Universal Representations Across Languages](#2020-08-03-2)
  - [3. Word Embeddings: Stability and Semantic Change](#2020-08-03-3)
  - [4. Exploring Swedish & English fastText Embeddings with the Transformer](#2020-08-03-4)
  - [5. Multi-task learning for natural language processing in the 2020s: where are we going?](#2020-08-03-5)
  - [6. Toward Givenness Hierarchy Theoretic Natural Language Generation](#2020-08-03-6)
  - [7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](#2020-08-03-7)
  - [8. Neural Machine Translation model for University Email Application](#2020-08-03-8)
  - [9. Neural Composition: Learning to Generate from Multiple Models](#2020-08-03-9)
  - [10. SimulEval: An Evaluation Toolkit for Simultaneous Translation](#2020-08-03-10)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-08-28

[Return to Index](#Index)



<h2 id="2020-08-28-1">1. Adaptable Filtering using Hierarchical Embeddings for Chinese Spell Check</h2>

Title: [Adaptable Filtering using Hierarchical Embeddings for Chinese Spell Check](https://arxiv.org/abs/2008.12281)

Authors: [Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M), [Gia H. Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+G+H), [Nancy F. Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+N+F)

> Spell check is a useful application which involves processing noisy human-generated text. Compared to other languages like English, it is more challenging to detect and correct spelling errors in Chinese because it has more (up to 100k) characters. For Chinese spell check, using confusion sets narrows the search space and makes finding corrections easier. However, most, if not all, confusion sets used to date are fixed and thus do not include new, evolving error patterns. We propose a scalable approach to adapt confusion sets by exploiting hierarchical character embeddings to (1) obviate the need to handcraft confusion sets, and (2) resolve sparsity issues related to seldom-occurring errors. Our approach establishes new SOTA results in spelling error correction on the 2014 and 2015 Chinese Spelling Correction Bake-off datasets.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.12281](https://arxiv.org/abs/2008.12281) [cs.CL]** |
|           | (or **[arXiv:2008.12281v1](https://arxiv.org/abs/2008.12281v1) [cs.CL]** for this version) |









# 2020-08-27

[Return to Index](#Index)



<h2 id="2020-08-27-1">1. The Impact of Indirect Machine Translation on Sentiment Classification</h2>

Title: [The Impact of Indirect Machine Translation on Sentiment Classification](https://arxiv.org/abs/2008.11257)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Pintu Lohar](https://arxiv.org/search/cs?searchtype=author&query=Lohar%2C+P), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [James Hadley](https://arxiv.org/search/cs?searchtype=author&query=Hadley%2C+J)

> Sentiment classification has been crucial for many natural language processing (NLP) applications, such as the analysis of movie reviews, tweets, or customer feedback. A sufficiently large amount of data is required to build a robust sentiment classification system. However, such resources are not always available for all domains or for all languages.
> In this work, we propose employing a machine translation (MT) system to translate customer feedback into another language to investigate in which cases translated sentences can have a positive or negative impact on an automatic sentiment classifier. Furthermore, as performing a direct translation is not always possible, we explore the performance of automatic classifiers on sentences that have been translated using a pivot MT system.
> We conduct several experiments using the above approaches to analyse the performance of our proposed sentiment classification system and discuss the advantages and drawbacks of classifying translated sentences.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of Association for Machine Translation in the Americas, AMTA (2020) |
| Cite as:           | **[arXiv:2008.11257](https://arxiv.org/abs/2008.11257) [cs.CL]** |
|                    | (or **[arXiv:2008.11257v1](https://arxiv.org/abs/2008.11257v1) [cs.CL]** for this version) |







# 2020-08-24

[Return to Index](#Index)



<h2 id="2020-08-24-1">1. Neural Machine Translation without Embeddings</h2>

Title: [Neural Machine Translation without Embeddings](https://arxiv.org/abs/2008.09396)

Authors: [Uri Shaham](https://arxiv.org/search/cs?searchtype=author&query=Shaham%2C+U), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> Many NLP models follow the embed-contextualize-predict paradigm, in which each sequence token is represented as a dense vector via an embedding matrix, and fed into a contextualization component that aggregates the information from the entire sequence in order to make a prediction. Could NLP models work without the embedding component? To that end, we omit the input and output embeddings from a standard machine translation model, and represent text as a sequence of bytes via UTF-8 encoding, using a constant 256-dimension one-hot representation for each byte. Experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models, often outperforms character-to-character models, and sometimes even produces better translations than standard subword models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.09396](https://arxiv.org/abs/2008.09396) [cs.CL]** |
|           | (or **[arXiv:2008.09396v1](https://arxiv.org/abs/2008.09396v1) [cs.CL]** for this version) |







# 2020-08-21

[Return to Index](#Index)



<h2 id="2020-08-21-1">1. Lite Training Strategies for Portuguese-English and English-Portuguese Translation</h2>

Title: [Lite Training Strategies for Portuguese-English and English-Portuguese Translation](https://arxiv.org/abs/2008.08769)

Authors: [Alexandre Lopes](https://arxiv.org/search/cs?searchtype=author&query=Lopes%2C+A), [Rodrigo Nogueira](https://arxiv.org/search/cs?searchtype=author&query=Nogueira%2C+R), [Roberto Lotufo](https://arxiv.org/search/cs?searchtype=author&query=Lotufo%2C+R), [Helio Pedrini](https://arxiv.org/search/cs?searchtype=author&query=Pedrini%2C+H)

> Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available at [this https URL](https://github.com/unicamp-dl/Lite-T5-Translation).

| Comments: | for code and weights, visit [this https URL](https://github.com/unicamp-dl/Lite-T5-Translation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.08769](https://arxiv.org/abs/2008.08769) [cs.CL]** |
|           | (or **[arXiv:2008.08769v1](https://arxiv.org/abs/2008.08769v1) [cs.CL]** for this version) |





<h2 id="2020-08-21-2">2. Inducing Language-Agnostic Multilingual Representations</h2>

Title: [Inducing Language-Agnostic Multilingual Representations](https://arxiv.org/abs/2008.09112)

Authors: [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Steffen Eger](https://arxiv.org/search/cs?searchtype=author&query=Eger%2C+S), [Johannes Bjerva](https://arxiv.org/search/cs?searchtype=author&query=Bjerva%2C+J), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

> Multilingual representations have the potential to make cross-lingual systems available to the vast majority of languages in the world. However, they currently require large pretraining corpora, or assume access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: 1) re-aligning the vector spaces of target languages (all together) to a pivot source language; 2) removing languages-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and 3) normalizing input texts by removing morphological contractions and sentence reordering, thus yielding language-agnostic representations. We evaluate on the tasks of XNLI and reference-free MT evaluation of varying difficulty across 19 selected languages. Our experiments demonstrate the language-agnostic behavior of our multilingual representations, which manifest the potential of zero-shot cross-lingual transfer to distant and low-resource languages, and decrease the performance gap by 8.9 points (M-BERT) and 18.2 points (XLM-R) on average across all tasks and languages. We make our codes and models available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.09112](https://arxiv.org/abs/2008.09112) [cs.CL]** |
|           | (or **[arXiv:2008.09112v1](https://arxiv.org/abs/2008.09112v1) [cs.CL]** for this version) |





# 2020-08-20

[Return to Index](#Index)



<h2 id="2020-08-20-1">1. Transformer based Multilingual document Embedding model</h2>

Title: [Transformer based Multilingual document Embedding model](https://arxiv.org/abs/2008.08567)

Authors: [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Brian Mak](https://arxiv.org/search/cs?searchtype=author&query=Mak%2C+B)

> One of the current state-of-the-art multilingual document embedding model is the bidirectional LSTM-based multilingual neural machine translation model (LASER). This paper presents a transformer-based sentence/document embedding model, T-LASER, which makes three significant improvements. Firstly, the BiLSTM encoder is replaced by the attention-based transformer structure, which is more capable of learning sequential patterns in longer texts. Secondly, due to the absence of recurrence, T-LASER enables faster parallel computations in the encoder to generate the text embedding. Thirdly, we augment the NMT translation loss function with an additional novel distance constraint loss. This distance constraint loss would further bring the embeddings of parallel sentences close together in the vector space; we call the T-LASER model trained with distance constraint, cT-LASER. Our cT-LASER model significantly outperforms both BiLSTM-based LASER and the simpler transformer-based T-LASER.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.08567](https://arxiv.org/abs/2008.08567) [cs.CL]** |
|           | (or **[arXiv:2008.08567v1](https://arxiv.org/abs/2008.08567v1) [cs.CL]** for this version) |











# 2020-08-19

[Return to Index](#Index)



<h2 id="2020-08-19-1">1. Word2vec Skip-gram Dimensionality Selection via Sequential Normalized Maximum Likelihood</h2>

Title: [Word2vec Skip-gram Dimensionality Selection via Sequential Normalized Maximum Likelihood](https://arxiv.org/abs/2008.07720)

Title: [Pham Thuc Hung](https://arxiv.org/search/cs?searchtype=author&query=Hung%2C+P+T), [Kenji Yamanishi](https://arxiv.org/search/cs?searchtype=author&query=Yamanishi%2C+K)

> In this paper, we propose a novel information criteria-based approach to select the dimensionality of the word2vec Skip-gram (SG). From the perspective of the probability theory, SG is considered as an implicit probability distribution estimation under the assumption that there exists a true contextual distribution among words. Therefore, we apply information criteria with the aim of selecting the best dimensionality so that the corresponding model can be as close as possible to the true distribution. We examine the following information criteria for the dimensionality selection problem: the Akaike Information Criterion, Bayesian Information Criterion, and Sequential Normalized Maximum Likelihood (SNML) criterion. SNML is the total codelength required for the sequential encoding of a data sequence on the basis of the minimum description length. The proposed approach is applied to both the original SG model and the SG Negative Sampling model to clarify the idea of using information criteria. Additionally, as the original SNML suffers from computational disadvantages, we introduce novel heuristics for its efficient computation. Moreover, we empirically demonstrate that SNML outperforms both BIC and AIC. In comparison with other evaluation methods for word embedding, the dimensionality selected by SNML is significantly closer to the optimal dimensionality obtained by word analogy or word similarity tasks.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.07720](https://arxiv.org/abs/2008.07720) [cs.LG]** |
|           | (or **[arXiv:2008.07720v1](https://arxiv.org/abs/2008.07720v1) [cs.LG]** for this version) |





<h2 id="2020-08-19-2">2. Very Deep Transformers for Neural Machine Translation</h2>

Title: [Very Deep Transformers for Neural Machine Translation](https://arxiv.org/abs/2008.07772)

Title: [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Liyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: [this https URL](https://github.com/namisan/exdeep-nmt).

| Comments: | 6 pages, 3 figures and 3 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.07772](https://arxiv.org/abs/2008.07772) [cs.CL]** |
|           | (or **[arXiv:2008.07772v1](https://arxiv.org/abs/2008.07772v1) [cs.CL]** for this version) |





<h2 id="2020-08-19-3">3. Glancing Transformer for Non-Autoregressive Neural Machine Translation</h2>

Title: [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2008.07905)

Title: [Lihua Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+L), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Yu Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Y), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lin Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+L), [Weinan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Yong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Non-autoregressive neural machine translation achieves remarkable inference acceleration compared to autoregressive models. However, current non-autoregressive models still fall behind their autoregressive counterparts in prediction accuracy. We attribute the accuracy gaps to two disadvantages of non-autoregressive models: a) learning simultaneous generation under the overly strong conditional independence assumption; b) lacking explicit target language modeling. In this paper, we propose Glancing Transformer (GLAT) to address the above disadvantages, which reduces the difficulty of learning simultaneous generation and introduces explicit target language modeling in the non-autoregressive setting at the same time. Experiments on several benchmarks demonstrate that our approach significantly improves the accuracy of non-autoregressive models without sacrificing any inference efficiency. In particular, GLAT achieves 30.91 BLEU on WMT 2014 German-English, which narrows the gap between autoregressive models and non-autoregressive models to less than 0.5 BLEU score.

| Comments: | 11 pages, 3 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.07905](https://arxiv.org/abs/2008.07905) [cs.CL]** |
|           | (or **[arXiv:2008.07905v1](https://arxiv.org/abs/2008.07905v1) [cs.CL]** for this version) |





# 2020-08-18

[Return to Index](#Index)



<h2 id="2020-08-18-1">1. Lanfrica: A Participatory Approach to Documenting Machine Translation Research on African Languages</h2>

Title: [Lanfrica: A Participatory Approach to Documenting Machine Translation Research on African Languages](https://arxiv.org/abs/2008.07302)

Authors: [Chris C. Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C+C), [Bonaventure F.P. Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B+F)

> Over the years, there have been campaigns to include the African languages in the growing research on machine translation (MT) in particular, and natural language processing (NLP) in general. Africa has the highest language diversity, with 1500-2000 documented languages and many more undocumented or extinct languages(Lewis, 2009; Bendor-Samuel, 2017). This makes it hard to keep track of the MT research, models and dataset that have been developed for some of them. As the internet and social media make up the daily lives of more than half of the world(Lin, 2020), as well as over 40% of Africans(Campbell, 2019), online platforms can be useful in creating accessibility to researches, benchmarks and datasets in these African languages, thereby improving reproducibility and sharing of existing research and their results. In this paper, we introduce Lanfrica, a novel, on-going framework that employs a participatory approach to documenting researches, projects, benchmarks and dataset on African languages.

| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.07302](https://arxiv.org/abs/2008.07302) [cs.CY]** |
|           | (or **[arXiv:2008.07302v1](https://arxiv.org/abs/2008.07302v1) [cs.CY]** for this version) |





<h2 id="2020-08-18-2">2. Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size</h2>

Title: [Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size](https://arxiv.org/abs/2008.07027)

Authors: [Davis Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida%2C+D), [Allyson Ettinger](https://arxiv.org/search/cs?searchtype=author&query=Ettinger%2C+A), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> Fine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.

| Comments: | 12 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.07027](https://arxiv.org/abs/2008.07027) [cs.CL]** |
|           | (or **[arXiv:2008.07027v1](https://arxiv.org/abs/2008.07027v1) [cs.CL]** for this version) |



# 2020-08-14

[Return to Index](#Index)



<h2 id="2020-08-14-1">1. On the Importance of Local Information in Transformer Based Models</h2>

Title: [On the Importance of Local Information in Transformer Based Models](https://arxiv.org/abs/2008.05828)

Authors: [Madhura Pande](https://arxiv.org/search/cs?searchtype=author&query=Pande%2C+M), [Aakriti Budhraja](https://arxiv.org/search/cs?searchtype=author&query=Budhraja%2C+A), [Preksha Nema](https://arxiv.org/search/cs?searchtype=author&query=Nema%2C+P), [Pratyush Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+P), [Mitesh M. Khapra](https://arxiv.org/search/cs?searchtype=author&query=Khapra%2C+M+M)

> The self-attention module is a key component of Transformer-based models, wherein each token pays attention to every other token. Recent studies have shown that these heads exhibit syntactic, semantic, or local behaviour. Some studies have also identified promise in restricting this attention to be local, i.e., a token attending to other tokens only in a small neighbourhood around it. However, no conclusive evidence exists that such local attention alone is sufficient to achieve high accuracy on multiple NLP tasks. In this work, we systematically analyse the role of locality information in learnt models and contrast it with the role of syntactic information. More specifically, we first do a sensitivity analysis and show that, at every layer, the representation of a token is much more sensitive to tokens in a small neighborhood around it than to tokens which are syntactically related to it. We then define an attention bias metric to determine whether a head pays more attention to local tokens or to syntactically related tokens. We show that a larger fraction of heads have a locality bias as compared to a syntactic bias. Having established the importance of local attention heads, we train and evaluate models where varying fractions of the attention heads are constrained to be local. Such models would be more efficient as they would have fewer computations in the attention layer. We evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT datasets (En-De, En-Ru) and clearly demonstrate that such constrained models have comparable performance to the unconstrained models. Through this systematic evaluation we establish that attention in Transformer-based models can be constrained to be local without affecting performance.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2008.05828](https://arxiv.org/abs/2008.05828) [cs.CL]** |
|           | (or **[arXiv:2008.05828v1](https://arxiv.org/abs/2008.05828v1) [cs.CL]** for this version) |







# 2020-08-13

[Return to Index](#Index)



<h2 id="2020-08-13-1">1. Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity</h2>

Title: [Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity](https://arxiv.org/abs/2008.04935)

Authors: [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language; however, attempting to generate paraphrases from the model using beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the trade-off between semantic similarity and lexical/syntactic diversity between the input and output can be controlled at generation time. We conduct human evaluation to compare our method to a paraphraser trained on a large English synthetic paraphrase database and find that our model produces paraphrases that better preserve semantic meaning and grammatically, for the same level of lexical/syntactic diversity. Additional smaller human assessments demonstrate our approach also works in non-English languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.04935](https://arxiv.org/abs/2008.04935) [cs.CL]** |
|           | (or **[arXiv:2008.04935v1](https://arxiv.org/abs/2008.04935v1) [cs.CL]** for this version) |





<h2 id="2020-08-13-2">2. Approaching Neural Chinese Word Segmentation as a Low-Resource Machine Translation Task</h2>

Title: [Approaching Neural Chinese Word Segmentation as a Low-Resource Machine Translation Task](https://arxiv.org/abs/2008.05348)

Authors: [Pinzhen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

> Supervised Chinese word segmentation has been widely approached as sequence labeling or sequence modeling. Recently, some researchers attempted to treat it as character-level translation, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. When benchmarked on MSR corpus under closed test condition without additional data, our method achieves 97.6% F1, which is on a par with the state of the art.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.05348](https://arxiv.org/abs/2008.05348) [cs.CL]** |
|           | (or **[arXiv:2008.05348v1](https://arxiv.org/abs/2008.05348v1) [cs.CL]** for this version) |



# 2020-08-12

[Return to Index](#Index)



<h2 id="2020-08-12-1">1. On Learning Language-Invariant Representations for Universal Machine Translation</h2>

Title: [On Learning Language-Invariant Representations for Universal Machine Translation](https://arxiv.org/abs/2008.04510)

Authors:[Han Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Andrej Risteski](https://arxiv.org/search/cs?searchtype=author&query=Risteski%2C+A)

> The goal of universal machine translation is to learn to translate between any pair of languages, given a corpus of paired translated documents for \emph{a small subset} of all pairs of languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent. In this paper, we formally prove certain impossibilities of this endeavour in general, as well as prove positive results in the presence of additional (but natural) structure of data.
> For the former, we derive a lower bound on the translation error in the many-to-many translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made. For the latter, we show that if the paired documents in the corpus follow a natural \emph{encoder-decoder} generative process, we can expect a natural notion of ``generalization'': a linear number of language pairs, rather than quadratic, suffices to learn a good representation. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed. We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation.

| Comments: | Appeared in ICML 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2008.04510](https://arxiv.org/abs/2008.04510) [cs.LG]** |
|           | (or **[arXiv:2008.04510v1](https://arxiv.org/abs/2008.04510v1) [cs.LG]** for this version) |





<h2 id="2020-08-12-2">2. Revisiting Low Resource Status of Indian Languages in Machine Translation</h2>

Title: [Revisiting Low Resource Status of Indian Languages in Machine Translation](https://arxiv.org/abs/2008.04860)

Authors:[Jerin Philip](https://arxiv.org/search/cs?searchtype=author&query=Philip%2C+J), [Shashank Siripragada](https://arxiv.org/search/cs?searchtype=author&query=Siripragada%2C+S), [Vinay P. Namboodiri](https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P), [C.V. Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C)

> Indian language machine translation performance is hampered due to the lack of large scale multi-lingual sentence aligned corpora and robust benchmarks. Through this paper, we provide and analyse an automated framework to obtain such a corpus for Indian language neural machine translation (NMT) systems. Our pipeline consists of a baseline NMT system, a retrieval module, and an alignment module that is used to work with publicly available websites such as press releases by the government. The main contribution towards this effort is to obtain an incremental method that uses the above pipeline to iteratively improve the size of the corpus as well as improve each of the components of our system. Through our work, we also evaluate the design choices such as the choice of pivoting language and the effect of iterative incremental increase in corpus size. Our work in addition to providing an automated framework also results in generating a relatively larger corpus as compared to existing corpora that are available for Indian languages. This corpus helps us obtain substantially improved results on the publicly available WAT evaluation benchmark and other standard evaluation benchmarks.

| Comments: | 10 pages, few figures, Preprint under review                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.04860](https://arxiv.org/abs/2008.04860) [cs.CL]** |
|           | (or **[arXiv:2008.04860v1](https://arxiv.org/abs/2008.04860v1) [cs.CL]** for this version) |





<h2 id="2020-08-12-3">3. The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020</h2>

Title: [The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020](https://arxiv.org/abs/2008.04885)

Authors:[Tobias Domhan](https://arxiv.org/search/cs?searchtype=author&query=Domhan%2C+T), [Michael Denkowski](https://arxiv.org/search/cs?searchtype=author&query=Denkowski%2C+M), [David Vilar](https://arxiv.org/search/cs?searchtype=author&query=Vilar%2C+D), [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Felix Hieber](https://arxiv.org/search/cs?searchtype=author&query=Hieber%2C+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

> We present Sockeye 2, a modernized and streamlined version of the Sockeye neural machine translation (NMT) toolkit. New features include a simplified code base through the use of MXNet's Gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.04885](https://arxiv.org/abs/2008.04885) [cs.CL]** |
|           | (or **[arXiv:2008.04885v1](https://arxiv.org/abs/2008.04885v1) [cs.CL]** for this version) |





# 2020-08-11

[Return to Index](#Index)



<h2 id="2020-08-11-1">1. Word Error Rate Estimation Without ASR Output: e-WER2</h2>

Title: [Word Error Rate Estimation Without ASR Output: e-WER2](https://arxiv.org/abs/2008.03403)

Authors: [Ahmed Ali](https://arxiv.org/search/eess?searchtype=author&query=Ali%2C+A), [Steve Renals](https://arxiv.org/search/eess?searchtype=author&query=Renals%2C+S)

> Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we continue our effort in estimating WER using acoustic, lexical and phonotactic features. Our novel approach to estimate the WER uses a multistream end-to-end architecture. We report results for systems using internal speech decoder features (glass-box), systems without speech decoder features (black-box), and for systems without having access to the ASR system (no-box). The no-box system learns joint acoustic-lexical representation from phoneme recognition results along with MFCC acoustic features to estimate WER. Considering WER per sentence, our no-box system achieves 0.56 Pearson correlation with the reference evaluation and 0.24 root mean square error (RMSE) across 1,400 sentences. The estimated overall WER by e-WER2 is 30.9% for a three hours test set, while the WER computed using the reference transcriptions was 28.5%.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.03403](https://arxiv.org/abs/2008.03403) [eess.AS]** |
|           | (or **[arXiv:2008.03403v1](https://arxiv.org/abs/2008.03403v1) [eess.AS]** for this version) |





<h2 id="2020-08-11-2">2. Navigating Language Models with Synthetic Agents</h2>

Title: [Navigating Language Models with Synthetic Agents](https://arxiv.org/abs/2008.04162)

Authors: [Philip Feldman](https://arxiv.org/search/cs?searchtype=author&query=Feldman%2C+P)

> Modern natural language models such as the GPT-2/GPT-3 contain tremendous amounts of information about human belief in a consistently interrogatable form. If these models could be shown to accurately reflect the underlying beliefs of the human beings that produced the data used to train these models, then such models become a powerful sociological tool in ways that are distinct from traditional methods, such as interviews and surveys. In this study, We train a version of the GPT-2 on a corpora of historical chess games, and then compare the learned relationships of words in the model to the known ground truth of the chess board, move legality, and historical patterns of play. We find that the percentages of moves by piece using the model are substantially similar from human patterns. We further find that the model creates an accurate latent representation of the chessboard, and that it is possible to plot trajectories of legal moves across the board using this knowledge.

| Comments:    | 8 pages, 6 figures, 2 tables, 1 algorithm                    |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Multiagent Systems (cs.MA) |
| ACM classes: | I.2; I.6; J.4                                                |
| Cite as:     | **[arXiv:2008.04162](https://arxiv.org/abs/2008.04162) [cs.AI]** |
|              | (or **[arXiv:2008.04162v1](https://arxiv.org/abs/2008.04162v1) [cs.AI]** for this version) |





<h2 id="2020-08-11-3">3. Distilling the Knowledge of BERT for Sequence-to-Sequence ASR</h2>

Title: [Distilling the Knowledge of BERT for Sequence-to-Sequence ASR](https://arxiv.org/abs/2008.03822)

Authors: [Hayato Futami](https://arxiv.org/search/cs?searchtype=author&query=Futami%2C+H), [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Sei Ueno](https://arxiv.org/search/cs?searchtype=author&query=Ueno%2C+S), [Masato Mimura](https://arxiv.org/search/cs?searchtype=author&query=Mimura%2C+M), [Shinsuke Sakai](https://arxiv.org/search/cs?searchtype=author&query=Sakai%2C+S), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T)

> Attention-based sequence-to-sequence (seq2seq) models have achieved promising results in automatic speech recognition (ASR). However, as these models decode in a left-to-right way, they do not have access to context on the right. We leverage both left and right context by applying BERT as an external language model to seq2seq ASR through knowledge distillation. In our proposed method, BERT generates soft labels to guide the training of seq2seq ASR. Furthermore, we leverage context beyond the current utterance as input to BERT. Experimental evaluations show that our method significantly improves the ASR performance from the seq2seq baseline on the Corpus of Spontaneous Japanese (CSJ). Knowledge distillation from BERT outperforms that from a transformer LM that only looks at left context. We also show the effectiveness of leveraging context beyond the current utterance. Our method outperforms other LM application approaches such as n-best rescoring and shallow fusion, while it does not require extra inference cost.

| Comments: | Accepted in INTERSPEECH2020                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2008.03822](https://arxiv.org/abs/2008.03822) [cs.CL]** |
|           | (or **[arXiv:2008.03822v1](https://arxiv.org/abs/2008.03822v1) [cs.CL]** for this version) |





<h2 id="2020-08-11-4">4. Does BERT Solve Commonsense Task via Commonsense Knowledge?</h2>

Title: [Does BERT Solve Commonsense Task via Commonsense Knowledge?](https://arxiv.org/abs/2008.03945)

Authors: [Leyang Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Sijie Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> The success of pre-trained contextualized language models such as BERT motivates a line of work that investigates linguistic knowledge inside such models in order to explain the huge improvement in downstream tasks. While previous work shows syntactic, semantic and word sense knowledge in BERT, little work has been done on investigating how BERT solves CommonsenseQA tasks. In particular, it is an interesting research question whether BERT relies on shallow syntactic patterns or deeper commonsense knowledge for disambiguation. We propose two attention-based methods to analyze commonsense knowledge inside BERT, and the contribution of such knowledge for the model prediction. We find that attention heads successfully capture the structured commonsense knowledge encoded in ConceptNet, which helps BERT solve commonsense tasks directly. Fine-tuning further makes BERT learn to use the commonsense knowledge on higher layers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2008.03945](https://arxiv.org/abs/2008.03945) [cs.CL]** |
|           | (or **[arXiv:2008.03945v1](https://arxiv.org/abs/2008.03945v1) [cs.CL]** for this version) |







# 2020-08-10

[Return to Index](#Index)



<h2 id="2020-08-10-1">1. A Multilingual Neural Machine Translation Model for Biomedical Data</h2>

Title: [A Multilingual Neural Machine Translation Model for Biomedical Data](https://arxiv.org/abs/2008.02878)

Authors: [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Zae Myung Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Z+M), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V), [Eunjeong L. Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+E+L), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M)

> We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies.
> We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.

| Comments: | [this https URL](https://github.com/naver/covid19-nmt)       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2008.02878](https://arxiv.org/abs/2008.02878) [cs.CL]** |
|           | (or **[arXiv:2008.02878v1](https://arxiv.org/abs/2008.02878v1) [cs.CL]** for this version) |





<h2 id="2020-08-10-2">2. Data Weighted Training Strategies for Grammatical Error Correction</h2>

Title: [Data Weighted Training Strategies for Grammatical Error Correction](https://arxiv.org/abs/2008.02976)

Authors: [Jared Lichtarge](https://arxiv.org/search/cs?searchtype=author&query=Lichtarge%2C+J), [Chris Alberti](https://arxiv.org/search/cs?searchtype=author&query=Alberti%2C+C), [Shankar Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S)

> Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning data in the BEA-2019 shared task. Building upon recent work in Neural Machine Translation (NMT), we make use of both kinds of data by deriving example-level scores on our large pretraining data based on a smaller, higher-quality dataset. In this work, we perform an empirical study to discover how to best incorporate delta-log-perplexity, a type of example scoring, into a training schedule for GEC. In doing so, we perform experiments that shed light on the function and applicability of delta-log-perplexity. Models trained on scored data achieve state-of-the-art results on common GEC test sets.

| Comments: | Accepted to TACL (Transactions of the Association for Computational Linguistics) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2008.02976](https://arxiv.org/abs/2008.02976) [cs.CL]** |
|           | (or **[arXiv:2008.02976v1](https://arxiv.org/abs/2008.02976v1) [cs.CL]** for this version) |





<h2 id="2020-08-10-3">3. SemEval-2020 Task 10: Emphasis Selection for Written Text in Visual Media</h2>

Title: [SemEval-2020 Task 10: Emphasis Selection for Written Text in Visual Media](https://arxiv.org/abs/2008.03274)

Authors: [Amirreza Shirani](https://arxiv.org/search/cs?searchtype=author&query=Shirani%2C+A), [Franck Dernoncourt](https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F), [Nedim Lipka](https://arxiv.org/search/cs?searchtype=author&query=Lipka%2C+N), [Paul Asente](https://arxiv.org/search/cs?searchtype=author&query=Asente%2C+P), [Jose Echevarria](https://arxiv.org/search/cs?searchtype=author&query=Echevarria%2C+J), [Thamar Solorio](https://arxiv.org/search/cs?searchtype=author&query=Solorio%2C+T)

> In this paper, we present the main findings and compare the results of SemEval-2020 Task 10, Emphasis Selection for Written Text in Visual Media. The goal of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in textual content to enable automated design assistance in authoring. The main focus is on short text instances for social media, with a variety of examples, from social media posts to inspirational quotes. Participants were asked to model emphasis using plain text with no additional context from the user or other design considerations. SemEval-2020 Emphasis Selection shared task attracted 197 participants in the early phase and a total of 31 teams made submissions to this task. The highest-ranked submission achieved 0.823 Matchm score. The analysis of systems submitted to the task indicates that BERT and RoBERTa were the most common choice of pre-trained models used, and part of speech tag (POS) was the most useful feature. Full results can be found on the task's website.

| Comments: | Accepted at Proceedings of 14th International Workshop on Semantic Evaluation (SemEval-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2008.03274](https://arxiv.org/abs/2008.03274) [cs.CL]** |
|           | (or **[arXiv:2008.03274v1](https://arxiv.org/abs/2008.03274v1) [cs.CL]** for this version) |





# 2020-08-06

[Return to Index](#Index)



<h2 id="2020-08-05-1">1. An exploration of the encoding of grammatical gender in word embeddings</h2>

Title: [An exploration of the encoding of grammatical gender in word embeddings](https://arxiv.org/abs/2008.01946)

Authors: [Hartger Veeman](https://arxiv.org/search/cs?searchtype=author&query=Veeman%2C+H), [Ali Basirat](https://arxiv.org/search/cs?searchtype=author&query=Basirat%2C+A)

> The vector representation of words, known as word embeddings, has opened a new research approach in the study of languages. These representations can capture different types of information about words. The grammatical gender of nouns is a typical classification of nouns based on their formal and semantic properties. The study of grammatical gender based on word embeddings can give insight into discussions on how grammatical genders are determined. In this research, we compare different sets of word embeddings according to the accuracy of a neural classifier determining the grammatical gender of nouns. It is found that the information about grammatical gender is encoded differently in Swedish, Danish, and Dutch embeddings. Our experimental results on the contextualized embeddings pointed out that adding more contextual (semantic) information to embeddings is detrimental to the classifier's performance. We also observed that removing morpho-syntactic features such as articles from the training corpora of embeddings decreases the classification performance dramatically, indicating a large portion of the information is encoded in the relationship between nouns and articles.

| Comments: | Accepted by the 4th Swedish Symposium on Deep Learning (SSDL-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.01946](https://arxiv.org/abs/2008.01946) [cs.CL]** |
|           | (or **[arXiv:2008.01946v1](https://arxiv.org/abs/2008.01946v1) [cs.CL]** for this version) |







# 2020-08-05

[Return to Index](#Index)



<h2 id="2020-08-05-1">1. A Survey of Orthographic Information in Machine Translation</h2>

Title: [A Survey of Orthographic Information in Machine Translation](https://arxiv.org/abs/2008.01391)

Authors: [Bharathi Raja Chakravarthi](https://arxiv.org/search/cs?searchtype=author&query=Chakravarthi%2C+B+R), [Priya Rani](https://arxiv.org/search/cs?searchtype=author&query=Rani%2C+P), [Mihael Arcan](https://arxiv.org/search/cs?searchtype=author&query=Arcan%2C+M), [John P. McCrae](https://arxiv.org/search/cs?searchtype=author&query=McCrae%2C+J+P)

> Machine translation is one of the applications of natural language processing which has been explored in different languages. Recently researchers started paying attention towards machine translation for resource-poor languages and closely related languages. A widespread and underlying problem for these machine translation systems is the variation in orthographic conventions which causes many issues to traditional approaches. Two languages written in two different orthographies are not easily comparable, but orthographic information can also be used to improve the machine translation system. This article offers a survey of research regarding orthography's influence on machine translation of under-resourced languages. It introduces under-resourced languages in terms of machine translation and how orthographic information can be utilised to improve machine translation. We describe previous work in this area, discussing what underlying assumptions were made, and showing how orthographic knowledge improves the performance of machine translation of under-resourced languages. We discuss different types of machine translation and demonstrate a recent trend that seeks to link orthographic information with well-established machine translation methods. Considerable attention is given to current efforts of cognates information at different levels of machine translation and the lessons that can be drawn from this. Additionally, multilingual neural machine translation of closely related languages is given a particular focus in this survey. This article ends with a discussion of the way forward in machine translation with orthographic information, focusing on multilingual settings and bilingual lexicon induction.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.01391](https://arxiv.org/abs/2008.01391) [cs.CL]** |
|           | (or **[arXiv:2008.01391v1](https://arxiv.org/abs/2008.01391v1) [cs.CL]** for this version) |





<h2 id="2020-08-05-2">2. Defining and Evaluating Fair Natural Language Generation</h2>

Title: [Defining and Evaluating Fair Natural Language Generation](https://arxiv.org/abs/2008.01548)

Authors: [Catherine Yeo](https://arxiv.org/search/cs?searchtype=author&query=Yeo%2C+C), [Alyssa Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+A)

> Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.

| Comments: | 7 pages, 2 figures, to be published in Proceedings of the The Fourth Widening Natural Language Processing Workshop at ACL |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2008.01548](https://arxiv.org/abs/2008.01548) [cs.CL]** |
|           | (or **[arXiv:2008.01548v1](https://arxiv.org/abs/2008.01548v1) [cs.CL]** for this version) |



# 2020-08-04

[Return to Index](#Index)



<h2 id="2020-08-04-1">1. Audiovisual Speech Synthesis using Tacotron2</h2>

Title: [Audiovisual Speech Synthesis using Tacotron2](https://arxiv.org/abs/2008.00620)

Authors: [Ahmed Hussen Abdelaziz](https://arxiv.org/search/eess?searchtype=author&query=Abdelaziz%2C+A+H), [Anushree Prasanna Kumar](https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+A+P), [Chloe Seivwright](https://arxiv.org/search/eess?searchtype=author&query=Seivwright%2C+C), [Gabriele Fanelli](https://arxiv.org/search/eess?searchtype=author&query=Fanelli%2C+G), [Justin Binder](https://arxiv.org/search/eess?searchtype=author&query=Binder%2C+J), [Yannis Stylianou](https://arxiv.org/search/eess?searchtype=author&query=Stylianou%2C+Y), [Sachin Kajarekar](https://arxiv.org/search/eess?searchtype=author&query=Kajarekar%2C+S)

> Audiovisual speech synthesis is the problem of synthesizing a talking face while maximizing the coherency of the acoustic and visual speech. In this paper, we propose and compare two audiovisual speech synthesis systems for 3D face models. The first system is the AVTacotron2, which is an end-to-end text-to-audiovisual speech synthesizer based on the Tacotron2 architecture. AVTacotron2 converts a sequence of phonemes representing the sentence to synthesize into a sequence of acoustic features and the corresponding controllers of a face model. The output acoustic features are used to condition a WaveRNN to reconstruct the speech waveform, and the output facial controllers are used to generate the corresponding video of the talking face. The second audiovisual speech synthesis system is modular, where acoustic speech is synthesized from text using the traditional Tacotron2. The reconstructed acoustic speech signal is then used to drive the facial controls of the face model using an independently trained audio-to-facial-animation neural network. We further condition both the end-to-end and modular approaches on emotion embeddings that encode the required prosody to generate emotional audiovisual speech. We analyze the performance of the two systems and compare them to the ground truth videos using subjective evaluation tests. The end-to-end and modular systems are able to synthesize close to human-like audiovisual speech with mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS of 4.1 for the ground truth generated from professionally recorded videos. While the end-to-end system gives a better overall quality, the modular approach is more flexible and the quality of acoustic speech and visual speech synthesis is almost independent of each other.

| Comments: | This work has been submitted to the IEEE transactions on Multimedia for possible publication |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2008.00620](https://arxiv.org/abs/2008.00620) [eess.AS]** |
|           | (or **[arXiv:2008.00620v1](https://arxiv.org/abs/2008.00620v1) [eess.AS]** for this version) |





<h2 id="2020-08-04-2">2. DeLighT: Very Deep and Light-weight Transformer</h2>

Title: [DeLighT: Very Deep and Light-weight Transformer](https://arxiv.org/abs/2008.00623)

Authors: [Sachin Mehta](https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+S), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Srinivasan Iyer](https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+S), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Hannaneh Hajishirzi](https://arxiv.org/search/cs?searchtype=author&query=Hajishirzi%2C+H)

> We introduce a very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using DExTra, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on machine translation and language modeling tasks show that DeLighT matches the performance of baseline Transformers with significantly fewer parameters. On the WMT'14 En-Fr high resource dataset, DeLighT requires 1.8 times fewer parameters and 2 times fewer operations and achieves better performance (+0.4 BLEU score) than baseline transformers. On the WMT'16 En-Ro low resource dataset, DeLighT delivers similar performance with 2.8 times fewer parameters than baseline transformers.

| Comments: | 16 pages including references and appendix                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2008.00623](https://arxiv.org/abs/2008.00623) [cs.LG]** |
|           | (or **[arXiv:2008.00623v1](https://arxiv.org/abs/2008.00623v1) [cs.LG]** for this version) |





<h2 id="2020-08-04-3">3. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning</h2>

Title: [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401)

Authors: [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Chau Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+C), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.

| Comments: | 10 pages (main) + 5 pages (appendices). 9 tables and 2 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.00401](https://arxiv.org/abs/2008.00401) [cs.CL]** |
|           | (or **[arXiv:2008.00401v1](https://arxiv.org/abs/2008.00401v1) [cs.CL]** for this version) |





<h2 id="2020-08-04-4">4. LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?</h2>

Title: [LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?](https://arxiv.org/abs/2008.00805)

Authors: [Marc Pàmies](https://arxiv.org/search/cs?searchtype=author&query=Pàmies%2C+M), [Emily Öhman](https://arxiv.org/search/cs?searchtype=author&query=Öhman%2C+E), [Kaisla Kajava](https://arxiv.org/search/cs?searchtype=author&query=Kajava%2C+K), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

> This paper presents the different models submitted by the LT@Helsinki team for the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and C; titled offensive language identification and offense target identification, respectively. In both cases we used the so-called Bidirectional Encoder Representation from Transformer (BERT), a model pre-trained by Google and fine-tuned by us on the OLID and SOLID datasets. The results show that offensive tweet classification is one of several language-based tasks where BERT can achieve state-of-the-art results.

| Comments: | Accepted at SemEval-2020 Task 12. Identical to camera-ready version except where adjustments to fit arXiv requirements were necessary |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2008.00805](https://arxiv.org/abs/2008.00805) [cs.CL]** |
|           | (or **[arXiv:2008.00805v1](https://arxiv.org/abs/2008.00805v1) [cs.CL]** for this version) |







# 2020-08-03

[Return to Index](#Index)



<h2 id="2020-08-03-1">1. Neural Language Generation: Formulation, Methods, and Evaluation</h2>

Title: [Neural Language Generation: Formulation, Methods, and Evaluation](https://arxiv.org/abs/2007.15780)

Authors: [Cristina Garbacea](https://arxiv.org/search/cs?searchtype=author&query=Garbacea%2C+C), [Qiaozhu Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+Q)

> Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.

| Comments: | 70 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.15780](https://arxiv.org/abs/2007.15780) [cs.CL]** |
|           | (or **[arXiv:2007.15780v1](https://arxiv.org/abs/2007.15780v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-2">2. On Learning Universal Representations Across Languages</h2>

Title: [On Learning Universal Representations Across Languages](https://arxiv.org/abs/2007.15960)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations, and show the effectiveness on cross-lingual understanding and generation. We propose Hierarchical Contrastive Learning (HiCTL) to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on three benchmarks: language understanding tasks (QQP, QNLI, SST-2, MRPC, STS-B and MNLI) in the GLUE benchmark, cross-lingual natural language inference (XNLI) and machine translation. Experimental results show that the HiCTL obtains an absolute gain of 1.0%/2.2% accuracy on GLUE/XNLI as well as achieves substantial improvements of +1.7-+3.6 BLEU on both the high-resource and low-resource English-to-X translation tasks over strong baselines. We will release the source codes as soon as possible.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.15960](https://arxiv.org/abs/2007.15960) [cs.CL]** |
|           | (or **[arXiv:2007.15960v1](https://arxiv.org/abs/2007.15960v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-3">3. Word Embeddings: Stability and Semantic Change</h2>

Title: [Word Embeddings: Stability and Semantic Change](https://arxiv.org/abs/2007.16006)

Authors: [Lucas Rettenmeier](https://arxiv.org/search/cs?searchtype=author&query=Rettenmeier%2C+L)

> Word embeddings are computed by a class of techniques within natural language processing (NLP), that create continuous vector representations of words in a language from a large text corpus. The stochastic nature of the training process of most embedding techniques can lead to surprisingly strong instability, i.e. subsequently applying the same technique to the same data twice, can produce entirely different results. In this work, we present an experimental study on the instability of the training process of three of the most influential embedding techniques of the last decade: word2vec, GloVe and fastText. Based on the experimental results, we propose a statistical model to describe the instability of embedding techniques and introduce a novel metric to measure the instability of the representation of an individual word. Finally, we propose a method to minimize the instability - by computing a modified average over multiple runs - and apply it to a specific linguistic problem: The detection and quantification of semantic change, i.e. measuring changes in the meaning and usage of words over time.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16006](https://arxiv.org/abs/2007.16006) [cs.CL]** |
|           | (or **[arXiv:2007.16006v1](https://arxiv.org/abs/2007.16006v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-4">4. Exploring Swedish & English fastText Embeddings with the Transformer</h2>

Title: [Exploring Swedish & English fastText Embeddings with the Transformer](https://arxiv.org/abs/2007.16007)

Authors: [Tosin P. Adewumi](https://arxiv.org/search/cs?searchtype=author&query=Adewumi%2C+T+P), [Foteini Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F), [Marcus Liwicki](https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M)

> In this paper, our main contributions are that embeddings from relatively smaller corpora can outperform ones from far larger corpora and we present the new Swedish analogy test set. To achieve a good network performance in natural language processing (NLP) downstream tasks, several factors play important roles: dataset size, the right hyper-parameters, and well-trained embedding. We show that, with the right set of hyper-parameters, good network performance can be reached even on smaller datasets. We evaluate the embeddings at the intrinsic level and extrinsic level, by deploying them on the Transformer in named entity recognition (NER) task and conduct significance tests.This is done for both Swedish and English. We obtain better performance in both languages on the downstream task with far smaller training data, compared to recently released, common crawl versions and character n-grams appear useful for Swedish, a morphologically rich language.

| Comments: | 10 pages, 2 figures, 8 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2007.16007](https://arxiv.org/abs/2007.16007) [cs.CL]** |
|           | (or **[arXiv:2007.16007v1](https://arxiv.org/abs/2007.16007v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-5">5. Multi-task learning for natural language processing in the 2020s: where are we going?</h2>

Title: [Multi-task learning for natural language processing in the 2020s: where are we going?](https://arxiv.org/abs/2007.16008)

Authors: [Joseph Worsham](https://arxiv.org/search/cs?searchtype=author&query=Worsham%2C+J), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.

| Comments:          | 12 pages, 2 figures. Published in Elsevier Pattern Recognition Letters Volume 136. Accepted manuscript published here |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes:       | I.2.6; I.2.7                                                 |
| Journal reference: | Pattern Recognition Letters 136 (2020) 120-126               |
| DOI:               | [10.1016/j.patrec.2020.05.031](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1016%2Fj.patrec.2020.05.031&v=27506e7c) |
| Cite as:           | **[arXiv:2007.16008](https://arxiv.org/abs/2007.16008) [cs.CL]** |
|                    | (or **[arXiv:2007.16008v1](https://arxiv.org/abs/2007.16008v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-6">6. Toward Givenness Hierarchy Theoretic Natural Language Generation</h2>

Title: [Toward Givenness Hierarchy Theoretic Natural Language Generation](https://arxiv.org/abs/2007.16009)

Authors: [Poulomi Pal](https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+P), [Tom Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+T)

> Language-capable interactive robots participating in dialogues with human interlocutors must be able to naturally and efficiently communicate about the entities in their environment. A key aspect of such communication is the use of anaphoric language. The linguistic theory of the Givenness Hierarchy(GH) suggests that humans use anaphora based on the cognitive statuses their referents have in the minds of their interlocutors. In previous work, researchers presented GH-theoretic approaches to robot anaphora understanding. In this paper we describe how the GH might need to be used quite differently to facilitate robot anaphora generation.

| Comments: | Extended Abstract accepted for (non-archival) presentation at Advances in Cognitive Systems 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2007.16009](https://arxiv.org/abs/2007.16009) [cs.CL]** |
|           | (or **[arXiv:2007.16009v1](https://arxiv.org/abs/2007.16009v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-7">7. Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs</h2>

Title: [Exclusion and Inclusion -- A model agnostic approach to feature importance in DNNs](https://arxiv.org/abs/2007.16010)

Authors: [Subhadip Maji](https://arxiv.org/search/cs?searchtype=author&query=Maji%2C+S), [Arijit Ghosh Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+A+G), [Raghav Bali](https://arxiv.org/search/cs?searchtype=author&query=Bali%2C+R), [Vamsi M Bhandaru](https://arxiv.org/search/cs?searchtype=author&query=Bhandaru%2C+V+M)

> Deep Neural Networks in NLP have enabled systems to learn complex non-linear relationships. One of the major bottlenecks towards being able to use DNNs for real world applications is their characterization as black boxes. To solve this problem, we introduce a model agnostic algorithm which calculates phrase-wise importance of input features. We contend that our method is generalizable to a diverse set of tasks, by carrying out experiments for both Regression and Classification. We also observe that our approach is robust to outliers, implying that it only captures the essential aspects of the input.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2007.16010](https://arxiv.org/abs/2007.16010) [cs.CL]** |
|           | (or **[arXiv:2007.16010v1](https://arxiv.org/abs/2007.16010v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-8">8. Neural Machine Translation model for University Email Application</h2>

Title: [Neural Machine Translation model for University Email Application](https://arxiv.org/abs/2007.16011)

Authors: [Sandhya Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+S), [Siti Nur Afikah Bte Abdul Mazid](https://arxiv.org/search/cs?searchtype=author&query=Mazid%2C+S+N+A+B+A), [Nagender Aneja](https://arxiv.org/search/cs?searchtype=author&query=Aneja%2C+N)

> Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation (NMT) model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of EN -> ML of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.

| Comments:          | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Journal reference: | International Conference on Natural Language Processing (ICNLP 2020), July 11-13, 2020 |
| Cite as:           | **[arXiv:2007.16011](https://arxiv.org/abs/2007.16011) [cs.CL]** |
|                    | (or **[arXiv:2007.16011v1](https://arxiv.org/abs/2007.16011v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-9">9. Neural Composition: Learning to Generate from Multiple Models</h2>

Title: [Neural Composition: Learning to Generate from Multiple Models](https://arxiv.org/abs/2007.16013)

Authors: [Denis Filimonov](https://arxiv.org/search/cs?searchtype=author&query=Filimonov%2C+D), [Ravi Teja Gadde](https://arxiv.org/search/cs?searchtype=author&query=Gadde%2C+R+T), [Ariya Rastrow](https://arxiv.org/search/cs?searchtype=author&query=Rastrow%2C+A)

> Decomposing models into multiple components is critically important in many applications such as language modeling (LM) as it enables adapting individual components separately and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, which requires class-annotated data, or through biasing to individual phrases which is limited in scale. In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component, and how to combine probability distributions from each component, directly from unlabeled text data.

| Comments:    | submitted to NeurIPS'20 (under review)                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ACM classes: | I.2.6; I.2.7                                                 |
| Cite as:     | **[arXiv:2007.16013](https://arxiv.org/abs/2007.16013) [cs.CL]** |
|              | (or **[arXiv:2007.16013v1](https://arxiv.org/abs/2007.16013v1) [cs.CL]** for this version) |





<h2 id="2020-08-03-10">10. SimulEval: An Evaluation Toolkit for Simultaneous Translation</h2>

Title: [SimulEval: An Evaluation Toolkit for Simultaneous Translation](https://arxiv.org/abs/2007.16193)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2007.16193](https://arxiv.org/abs/2007.16193) [cs.CL]** |
|           | (or **[arXiv:2007.16193v1](https://arxiv.org/abs/2007.16193v1) [cs.CL]** for this version) |



