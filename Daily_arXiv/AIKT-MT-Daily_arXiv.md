# Daily arXiv: Machine Translation - May, 2021

# Index


- [2021-06-14](#2021-06-14)

  - [1. One Sense Per Translation](#2021-06-14-1)
  - [2. Graph Neural Networks for Natural Language Processing: A Survey](#2021-06-14-2)
  - [3. Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](#2021-06-14-3)
  - [4. Towards User-Driven Neural Machine Translation](#2021-06-14-4)
  - [5. A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation](#2021-06-14-5)
  - [6. Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](#2021-06-14-6)
  - [7. Zero-Shot Controlled Generation with Encoder-Decoder Transformers](#2021-06-14-7)
  - [8. Semi-Supervised and Unsupervised Sense Annotation via Translations](#2021-06-14-8)
- [2021-06-11](#2021-06-11)
  - [1. Data augmentation to improve robustness of image captioning solutions](#2021-06-11-1)
  - [2. Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021](#2021-06-11-2)
  - [3. Progressive Multi-Granularity Training for Non-Autoregressive Translation](#2021-06-11-3)
  - [4. AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](#2021-06-11-4)
  - [5. Exploring Unsupervised Pretraining Objectives for Machine Translation](#2021-06-11-5)
  - [6. Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](#2021-06-11-6)
  - [7. GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures](#2021-06-11-7)
  - [8. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation](#2021-06-11-8)
- [2021-06-10](#2021-06-10)

  - [1. PAM: Understanding Product Images in Cross Product Category Attribute Extraction](#2021-06-10-1)
  - [2. VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation](#2021-06-10-2)
  - [3. FastSeq: Make Sequence Generation Faster](#2021-06-10-3)
  - [4. Sentence Embeddings using Supervised Contrastive Learning](#2021-06-10-4)
  - [5. Probing Multilingual Language Models for Discourse](#2021-06-10-5)
  - [6. RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer](#2021-06-10-6)
  - [7. Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](#2021-06-10-7)
  - [8. Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study](#2021-06-10-8)
  - [9. Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation](#2021-06-10-9)
  - [10. AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT](#2021-06-10-10)
- [2021-06-09](#2021-06-09)

  - [1. A Survey of Transformers](#2021-06-09-1)
  - [2. Meta Learning for Knowledge Distillation](#2021-06-09-2)
  - [3. Lexicon Learning for Few-Shot Neural Sequence Modeling](#2021-06-09-3)
  - [4. Self-supervised and Supervised Joint Training for Resource-rich Machine Translation](#2021-06-09-4)
  - [5. Obtaining Better Static Word Embeddings Using Contextual Embedding Models](#2021-06-09-5)
  - [6. CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering](#2021-06-09-6)
  - [7. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](#2021-06-09-7)
- [2021-06-08](#2021-06-08)
  - [1. CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings](#2021-06-08-1)
  - [2. SelfDoc: Self-Supervised Document Representation Learning](#2021-06-08-2)
  - [3. Do Grammatical Error Correction Models Realize Grammatical Generalization?](#2021-06-08-3)
  - [4. The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](#2021-06-08-4)
  - [5. Itihasa: A large-scale corpus for Sanskrit to English translation](#2021-06-08-5)
  - [6. On the Language Coverage Bias for Neural Machine Translation](#2021-06-08-6)
  - [7. BERTGEN: Multi-task Generation through BERT](#2021-06-08-7)
  - [8. RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models](#2021-06-08-8)
  - [9. Diverse Pretrained Context Encodings Improve Document Translation](#2021-06-08-9)
  - [10. Encouraging Neural Machine Translation to Satisfy Terminology Constraints](#2021-06-08-10)
  - [11. A Simple Recipe for Multilingual Grammatical Error Correction](#2021-06-08-11)
- [2021-06-07](#2021-06-07)

  - [1. Neural semi-Markov CRF for Monolingual Word Alignment](#2021-06-07-1)
  - [2. Human-Adversarial Visual Question Answering](#2021-06-07-2)
  - [3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages](#2021-06-07-3)
  - [4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer](#2021-06-07-4)
  - [5. Grounding 'Grounding' in NLP](#2021-06-07-5)
  - [6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](#2021-06-07-6)
  - [7. Scalable Transformers for Neural Machine Translation](#2021-06-07-7)
  - [8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](#2021-06-07-8)
  - [9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](#2021-06-07-9)
- [2021-06-04](#2021-06-04)
  - [1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](#2021-06-04-1)
  - [2. Representing Syntax and Composition with Geometric Transformations](#2021-06-04-2)
  - [3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](#2021-06-04-3)
  - [4. A Dataset and Baselines for Multilingual Reply Suggestion](#2021-06-04-4)
  - [5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](#2021-06-04-5)
  - [6. Lightweight Adapter Tuning for Multilingual Speech Translation](#2021-06-04-6)
  - [7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](#2021-06-04-7)
  - [8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](#2021-06-04-8)
  - [9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](#2021-06-04-9)
  - [10. Fingerprinting Fine-tuned Language Models in the Wild](#2021-06-04-10 )
  - [11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](#2021-06-04-11)
- [2021-06-03](#2021-06-03)

  - [1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation](#2021-06-03-1)
  - [2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](#2021-06-03-2)
  - [3. Discrete Cosine Transform as Universal Sentence Encoder](#2021-06-03-3)
  - [4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](#2021-06-03-4)
  - [5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](#2021-06-03-5)
  - [6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](#2021-06-03-6)
  - [7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](#2021-06-03-7)
  - [8. Evidence-based Factual Error Correction](#2021-06-03-8)
  - [9. Is Sparse Attention more Interpretable?](#2021-06-03-9)
  - [10. End-to-End NLP Knowledge Graph Construction](#2021-06-03-10)
  - [11. IrEne: Interpretable Energy Prediction for Transformers](#2021-06-03-11)
  - [12. Lower Perplexity is Not Always Human-Like](#2021-06-03-12)
  - [13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](#2021-06-03-13)
- [2021-06-02](#2021-06-02)

  - [1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](#2021-06-02-1)
  - [2. Language Model Evaluation Beyond Perplexity](#2021-06-02-2)
  - [3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](#2021-06-02-3)
  - [4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](#2021-06-02-4)
  - [5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](#2021-06-02-5)
  - [6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](#2021-06-02-6)
  - [7. ViTA: Visual-Linguistic Translation by Aligning Object Tags](#2021-06-02-7)
  - [8. An In-depth Study on Internal Structure of Chinese Words](#2021-06-02-8)
  - [9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](#2021-06-02-9)
  - [10. DoT: An efficient Double Transformer for NLP tasks with tables](#2021-06-02-10)
  - [11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](#2021-06-02-11)
  - [12. Incorporating Visual Layout Structures for Scientific Text Classification](#2021-06-02-12)
- [2021-06-01](#2021-06-01)
  - [1. An Attention Free Transformer](#2021-06-01-1)
  - [2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](#2021-06-01-2)
  - [3. Re-evaluating Word Mover's Distance](#2021-06-01-3)
  - [4. Memory-Efficient Differentiable Transformer Architecture Search](#2021-06-01-4)
  - [5. Why does CTC result in peaky behavior?](#2021-06-01-5)
  - [6. Grammatical Error Correction as GAN-like Sequence Labeling](#2021-06-01-6)
  - [7. Predictive Representation Learning for Language Modeling](#2021-06-01-7)
  - [8. Korean-English Machine Translation with Multiple Tokenization Strategy](#2021-06-01-8)
  - [9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](#2021-06-01-9)
  - [10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](#2021-06-01-10)
  - [11. Pre-training Universal Language Representation](#2021-06-01-11)
  - [12. Fast Nearest Neighbor Machine Translation](#2021-06-01-12)
  - [13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](#2021-06-01-13)
  - [14. Attention Flows are Shapley Value Explanations](#2021-06-01-14)
  - [15. G-Transformer for Document-level Machine Translation](#2021-06-01-15)
  - [16. On Compositional Generalization of Neural Machine Translation](#2021-06-01-16)
  - [17. Transfer Learning for Sequence Generation: from Single-source to Multi-source](#2021-06-01-17)
  - [18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](#2021-06-01-18)
  - [19. Effective Batching for Recurrent Neural Network Grammars](#2021-06-01-19)
  - [20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](#2021-06-01-20)
  - [21. Verdi: Quality Estimation and Error Detection for Bilingual](#2021-06-01-21)
  - [22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](#2021-06-01-22)
  - [23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](#2021-06-01-23)
  - [24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](#2021-06-01-24)
  - [25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](#2021-06-01-25)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-06-14

[Return to Index](#Index)



<h2 id="2021-06-14-1">1. One Sense Per Translation
</h2>

Title: [One Sense Per Translation](https://arxiv.org/abs/2106.06082)

Authors: [Bradley Hauer](https://arxiv.org/search/cs?searchtype=author&query=Hauer%2C+B), [Grzegorz Kondrak](https://arxiv.org/search/cs?searchtype=author&query=Kondrak%2C+G)

> The idea of using lexical translations to define sense inventories has a long history in lexical semantics. We propose a theoretical framework which allows us to answer the question of why this apparently reasonable idea failed to produce useful results. We formally prove several propositions on how the translations of a word relate to its senses, as well as on the relationship between synonymy and polysemy. We empirically validate our theoretical findings on BabelNet, and demonstrate how they could be used to perform unsupervised word sense disambiguation of a substantial fraction of the lexicon.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06082](https://arxiv.org/abs/2106.06082) [cs.CL]** |
|           | (or **[arXiv:2106.06082v1](https://arxiv.org/abs/2106.06082v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-2">2. Graph Neural Networks for Natural Language Processing: A Survey
</h2>

Title: [Graph Neural Networks for Natural Language Processing: A Survey](https://arxiv.org/abs/2106.06090)

Authors: [Lingfei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Kai Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+K), [Xiaojie Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X), [Hanning Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Shucheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jian Pei](https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+J), [Bo Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+B)

> Deep learning has become the dominant approach in coping with various tasks in Natural LanguageProcessing (NLP). Although text inputs are typically represented as a sequence of tokens, there isa rich variety of NLP problems that can be best expressed with a graph structure. As a result, thereis a surge of interests in developing new deep learning techniques on graphs for a large numberof NLP tasks. In this survey, we present a comprehensive overview onGraph Neural Networks(GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, whichsystematically organizes existing research of GNNs for NLP along three axes: graph construction,graph representation learning, and graph based encoder-decoder models. We further introducea large number of NLP applications that are exploiting the power of GNNs and summarize thecorresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discussvarious outstanding challenges for making the full use of GNNs for NLP as well as future researchdirections. To the best of our knowledge, this is the first comprehensive overview of Graph NeuralNetworks for Natural Language Processing.

| Comments: | 127 pages                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.06090](https://arxiv.org/abs/2106.06090) [cs.CL]** |
|           | (or **[arXiv:2106.06090v1](https://arxiv.org/abs/2106.06090v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-3">3. Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation
</h2>

Title: [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://arxiv.org/abs/2106.06125)

Authors: [Xin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Haiying Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones. Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized. We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models.

| Comments: | Accepted by ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06125](https://arxiv.org/abs/2106.06125) [cs.CL]** |
|           | (or **[arXiv:2106.06125v1](https://arxiv.org/abs/2106.06125v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-4">4. Towards User-Driven Neural Machine Translation
</h2>

Title: [Towards User-Driven Neural Machine Translation](https://arxiv.org/abs/2106.06200)

Authors: [Huan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Liang Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Degen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+D), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset. To fill this gap, we introduce a novel framework called user-driven NMT. Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus. Experimental results confirm that the proposed user-driven NMT can generate user-specific translations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06200](https://arxiv.org/abs/2106.06200) [cs.CL]** |
|           | (or **[arXiv:2106.06200v1](https://arxiv.org/abs/2106.06200v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-5">5. A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation
</h2>

Title: [A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation](https://arxiv.org/abs/2106.06292)

Authors: [Sebastin Santy](https://arxiv.org/search/cs?searchtype=author&query=Santy%2C+S), [Prasanta Bhattacharya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+P)

> Recent advances in AI and ML applications have benefited from rapid progress in NLP research. Leaderboards have emerged as a popular mechanism to track and accelerate progress in NLP through competitive model development. While this has increased interest and participation, the over-reliance on single, and accuracy-based metrics have shifted focus from other important metrics that might be equally pertinent to consider in real-world contexts. In this paper, we offer a preliminary discussion of the risks associated with focusing exclusively on accuracy metrics and draw on recent discussions to highlight prescriptive suggestions on how to develop more practical and effective leaderboards that can better reflect the real-world utility of models.

| Comments: | pre-print: comments and suggestions welcome                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06292](https://arxiv.org/abs/2106.06292) [cs.CL]** |
|           | (or **[arXiv:2106.06292v1](https://arxiv.org/abs/2106.06292v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-6">6. Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment
</h2>

Title: [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://arxiv.org/abs/2106.06381)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Bo Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-labels word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rates on the alignment benchmarks. The code and pretrained parameters are available at [this https URL](https://github.com/CZWin32768/XLM-Align).

| Comments: | ACL-2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06381](https://arxiv.org/abs/2106.06381) [cs.CL]** |
|           | (or **[arXiv:2106.06381v1](https://arxiv.org/abs/2106.06381v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-7">7. Zero-Shot Controlled Generation with Encoder-Decoder Transformers
</h2>

Title: [Zero-Shot Controlled Generation with Encoder-Decoder Transformers](https://arxiv.org/abs/2106.06411)

Authors: [Devamanyu Hazarika](https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+D), [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [Dilek Hakkani-Tür](https://arxiv.org/search/cs?searchtype=author&query=Hakkani-Tür%2C+D)

> Controlling neural network-based models for natural language generation (NLG) has broad applications in numerous areas such as machine translation, document summarization, and dialog systems. Approaches that enable such control in a zero-shot manner would be of great importance as, among other reasons, they remove the need for additional annotated data and training. In this work, we propose novel approaches for controlling encoder-decoder transformer-based NLG models in a zero-shot manner. This is done by introducing three control knobs; namely, attention biasing, decoder mixing, and context augmentation, that are applied to these models at generation time. These knobs control the generation process by directly manipulating trained NLG models (e.g., biasing cross-attention layers) to realize the desired attributes in the generated outputs. We show that not only are these NLG models robust to such manipulations, but also their behavior could be controlled without an impact on their generation performance. These results, to the best of our knowledge, are the first of their kind. Through these control knobs, we also investigate the role of transformer decoder's self-attention module and show strong evidence that its primary role is maintaining fluency of sentences generated by these models. Based on this hypothesis, we show that alternative architectures for transformer decoders could be viable options. We also study how this hypothesis could lead to more efficient ways for training encoder-decoder transformer models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06411](https://arxiv.org/abs/2106.06411) [cs.CL]** |
|           | (or **[arXiv:2106.06411v1](https://arxiv.org/abs/2106.06411v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-8">8. Semi-Supervised and Unsupervised Sense Annotation via Translations
</h2>

Title: [Semi-Supervised and Unsupervised Sense Annotation via Translations](https://arxiv.org/abs/2106.06462)

Authors: [Bradley Hauer](https://arxiv.org/search/cs?searchtype=author&query=Hauer%2C+B), [Grzegorz Kondrak](https://arxiv.org/search/cs?searchtype=author&query=Kondrak%2C+G), [Yixing Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+Y), [Arnob Mallik](https://arxiv.org/search/cs?searchtype=author&query=Mallik%2C+A), [Lili Mou](https://arxiv.org/search/cs?searchtype=author&query=Mou%2C+L)

> Acquisition of multilingual training data continues to be a challenge in word sense disambiguation (WSD). To address this problem, unsupervised approaches have been developed in recent years that automatically generate sense annotations suitable for training supervised WSD systems. We present three new methods to creating sense-annotated corpora, which leverage translations, parallel corpora, lexical resources, and contextual and synset embeddings. Our semi-supervised method applies machine translation to transfer existing sense annotations to other languages. Our two unsupervised methods use a knowledge-based WSD system to annotate a parallel corpus, and refine the resulting sense annotations by identifying lexical translations. We obtain state-of-the-art results on standard WSD benchmarks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06462](https://arxiv.org/abs/2106.06462) [cs.CL]** |
|           | (or **[arXiv:2106.06462v1](https://arxiv.org/abs/2106.06462v1) [cs.CL]** for this version) |






# 2021-06-11

[Return to Index](#Index)



<h2 id="2021-06-11-1">1. Data augmentation to improve robustness of image captioning solutions
</h2>

Title: [Data augmentation to improve robustness of image captioning solutions](https://arxiv.org/abs/2106.05437)

Authors: [Shashank Bujimalla](https://arxiv.org/search/cs?searchtype=author&query=Bujimalla%2C+S), [Mahesh Subedar](https://arxiv.org/search/cs?searchtype=author&query=Subedar%2C+M), [Omesh Tickoo](https://arxiv.org/search/cs?searchtype=author&query=Tickoo%2C+O)

> In this paper, we study the impact of motion blur, a common quality flaw in real world images, on a state-of-the-art two-stage image captioning solution, and notice a degradation in solution performance as blur intensity increases. We investigate techniques to improve the robustness of the solution to motion blur using training data augmentation at each or both stages of the solution, i.e., object detection and captioning, and observe improved results. In particular, augmenting both the stages reduces the CIDEr-D degradation for high motion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to 6.8 on Vizwiz dataset.

| Comments: | CVPR VizWiz 2021 workshop                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.05437](https://arxiv.org/abs/2106.05437) [cs.CL]** |
|           | (or **[arXiv:2106.05437v1](https://arxiv.org/abs/2106.05437v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-2">2. Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021
</h2>

Title: [Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021](https://arxiv.org/abs/2106.05450)

Authors: [Katsuki Chousa](https://arxiv.org/search/cs?searchtype=author&query=Chousa%2C+K), [Makoto Morishita](https://arxiv.org/search/cs?searchtype=author&query=Morishita%2C+M)

> This paper describes our systems that were submitted to the restricted translation task at WAT 2021. In this task, the systems are required to output translated sentences that contain all given word constraints. Our system combined input augmentation and constrained beam search algorithms. Through experiments, we found that this combination significantly improves translation accuracy and can save inference time while containing all the constraints in the output. For both En->Ja and Ja->En, our systems obtained the best evaluation performances in automatic evaluation.

| Comments: | 9 pages, 4 figures, WAT 2021 Restricted Translation Task     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05450](https://arxiv.org/abs/2106.05450) [cs.CL]** |
|           | (or **[arXiv:2106.05450v1](https://arxiv.org/abs/2106.05450v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-3">3. Progressive Multi-Granularity Training for Non-Autoregressive Translation
</h2>

Title: [Progressive Multi-Granularity Training for Non-Autoregressive Translation](https://arxiv.org/abs/2106.05546)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Non-autoregressive translation (NAT) significantly accelerates the inference process via predicting the entire target sequence. However, recent studies show that NAT is weak at learning high-mode of knowledge such as one-to-many translations. We argue that modes can be divided into various granularities which can be learned from easy to hard. In this study, we empirically show that NAT models are prone to learn fine-grained lower-mode knowledge, such as words and phrases, compared with sentences. Based on this observation, we propose progressive multi-granularity training for NAT. More specifically, to make the most of the training data, we break down the sentence-level examples into three types, i.e. words, phrases, sentences, and with the training goes, we progressively increase the granularities. Experiments on Romanian-English, English-German, Chinese-English, and Japanese-English demonstrate that our approach improves the phrase translation accuracy and model reordering ability, therefore resulting in better translation quality against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance.

| Comments: | findings of ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05546](https://arxiv.org/abs/2106.05546) [cs.CL]** |
|           | (or **[arXiv:2106.05546v1](https://arxiv.org/abs/2106.05546v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-4">4. AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation
</h2>

Title: [AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://arxiv.org/abs/2106.05589)

Authors: [Xinnuo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Young-Bum Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Sungjin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes AUGNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts. The proposed system mostly outperforms the state-of-the-art methods on the FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FewShotSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at [this https URL](https://github.com/XinnuoXu/AugNLG).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL2021) |
| Cite as:           | **[arXiv:2106.05589](https://arxiv.org/abs/2106.05589) [cs.CL]** |
|                    | (or **[arXiv:2106.05589v1](https://arxiv.org/abs/2106.05589v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-5">5. Exploring Unsupervised Pretraining Objectives for Machine Translation
</h2>

Title: [Exploring Unsupervised Pretraining Objectives for Machine Translation](https://arxiv.org/abs/2106.05634)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English↔German, English↔Nepali and English↔Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.

| Comments: | Findings of ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05634](https://arxiv.org/abs/2106.05634) [cs.CL]** |
|           | (or **[arXiv:2106.05634v1](https://arxiv.org/abs/2106.05634v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-6">6. Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation
</h2>

Title: [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://arxiv.org/abs/2106.05691)

Authors: [Yuanxin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Zheng Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Weiping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher's soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student's performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher's hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher's hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student's performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x ~ 3.4x.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05691](https://arxiv.org/abs/2106.05691) [cs.CL]** |
|           | (or **[arXiv:2106.05691v1](https://arxiv.org/abs/2106.05691v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-7">7. GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures
</h2>

Title: [GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures](https://arxiv.org/abs/2106.05822)

Authors: [Ivan Chelombiev](https://arxiv.org/search/cs?searchtype=author&query=Chelombiev%2C+I), [Daniel Justus](https://arxiv.org/search/cs?searchtype=author&query=Justus%2C+D), [Douglas Orr](https://arxiv.org/search/cs?searchtype=author&query=Orr%2C+D), [Anastasia Dietrich](https://arxiv.org/search/cs?searchtype=author&query=Dietrich%2C+A), [Frithjof Gressmann](https://arxiv.org/search/cs?searchtype=author&query=Gressmann%2C+F), [Alexandros Koliousis](https://arxiv.org/search/cs?searchtype=author&query=Koliousis%2C+A), [Carlo Luschi](https://arxiv.org/search/cs?searchtype=author&query=Luschi%2C+C)

> Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we add a convolutional module to complement the self-attention module, decoupling the learning of local and global interactions. Secondly, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers and convolutions, while preserving the expressivity of the model. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.05822](https://arxiv.org/abs/2106.05822) [cs.CL]** |
|           | (or **[arXiv:2106.05822v1](https://arxiv.org/abs/2106.05822v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-8">8. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation
</h2>

Title: [ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation](https://arxiv.org/abs/2106.05970)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [An Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+A), [Miguel Eckstein](https://arxiv.org/search/cs?searchtype=author&query=Eckstein%2C+M), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> Automatic evaluations for natural language generation (NLG) conventionally rely on token-level or embedding-level comparisons with the text references. This is different from human language processing, for which visual imaginations often improve comprehension. In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation. With the help of CLIP and DALL-E, two cross-modal models pre-trained on large-scale image-text pairs, we automatically generate an image as the embodied imagination for the text snippet and compute the imagination similarity using contextual embeddings. Experiments spanning several text generation tasks demonstrate that adding imagination with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics' correlations with human similarity judgments in many circumstances.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.05970](https://arxiv.org/abs/2106.05970) [cs.CL]** |
|           | (or **[arXiv:2106.05970v1](https://arxiv.org/abs/2106.05970v1) [cs.CL]** for this version) |





# 2021-06-10

[Return to Index](#Index)



<h2 id="2021-06-10-1">1. PAM: Understanding Product Images in Cross Product Category Attribute Extraction
</h2>

Title: [PAM: Understanding Product Images in Cross Product Category Attribute Extraction](https://arxiv.org/abs/2106.04630)

Authors: [Rongmei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+R), [Xiang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Jie Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Nasser Zalmout](https://arxiv.org/search/cs?searchtype=author&query=Zalmout%2C+N), [Yan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Li Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+L), [Xin Luna Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+X+L)

> Understanding product attributes plays an important role in improving online shopping experience for customers and serves as an integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction. Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.

| Comments: | KDD 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| DOI:      | [10.1145/3447548.3467164](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467164&v=ffa91bdd) |
| Cite as:  | **[arXiv:2106.04630](https://arxiv.org/abs/2106.04630) [cs.CV]** |
|           | (or **[arXiv:2106.04630v1](https://arxiv.org/abs/2106.04630v1) [cs.CV]** for this version) |





<h2 id="2021-06-10-2">2. VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation
</h2>

Title: [VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation](https://arxiv.org/abs/2106.04632)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Licheng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Rohit Pillai](https://arxiv.org/search/cs?searchtype=author&query=Pillai%2C+R), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Luowei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y), [Tamara Lee Berg](https://arxiv.org/search/cs?searchtype=author&query=Berg%2C+T+L), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at [this https URL](https://value-leaderboard.github.io/).

| Comments: | VALUE is available at [this https URL](https://value-leaderboard.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.04632](https://arxiv.org/abs/2106.04632) [cs.CV]** |
|           | (or **[arXiv:2106.04632v1](https://arxiv.org/abs/2106.04632v1) [cs.CV]** for this version) |





<h2 id="2021-06-10-3">3. FastSeq: Make Sequence Generation Faster
</h2>

Title: [FastSeq: Make Sequence Generation Faster](https://arxiv.org/abs/2106.04718)

Authors: [Yu Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y), [Fei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+F), [Jiusheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Nikhil Bhendawade](https://arxiv.org/search/cs?searchtype=author&query=Bhendawade%2C+N), [Ting Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+T), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Desheng Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+D), [Bingyu Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+B), [Ruifei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R)

> Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop FastSeq framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, GPT2, and UniLM). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use with a simple one-line code change. The source code is available at [this https URL](https://github.com/microsoft/fastseq).

| Comments: | ACL 2021 Demo Track                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04718](https://arxiv.org/abs/2106.04718) [cs.CL]** |
|           | (or **[arXiv:2106.04718v1](https://arxiv.org/abs/2106.04718v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-4">4. Sentence Embeddings using Supervised Contrastive Learning
</h2>

Title: [Sentence Embeddings using Supervised Contrastive Learning](https://arxiv.org/abs/2106.04791)

Authors: [Danqi Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+D)

> Sentence embeddings encode sentences in fixed dense vectors and have played an important role in various NLP tasks and systems. Methods for building sentence embeddings include unsupervised learning such as Quick-Thoughts and supervised learning such as InferSent. With the success of pretrained NLP models, recent research shows that fine-tuning pretrained BERT on SNLI and Multi-NLI data creates state-of-the-art sentence embeddings, outperforming previous sentence embeddings methods on various evaluation benchmarks. In this paper, we propose a new method to build sentence embeddings by doing supervised contrastive learning. Specifically our method fine-tunes pretrained BERT on SNLI data, incorporating both supervised crossentropy loss and supervised contrastive loss. Compared with baseline where fine-tuning is only done with supervised cross-entropy loss similar to current state-of-the-art method SBERT, our supervised contrastive method improves 2.8% in average on Semantic Textual Similarity (STS) benchmarks and 1.05% in average on various sentence transfer tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04791](https://arxiv.org/abs/2106.04791) [cs.CL]** |
|           | (or **[arXiv:2106.04791v1](https://arxiv.org/abs/2106.04791v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-5">5. Probing Multilingual Language Models for Discourse
</h2>

Title: [Probing Multilingual Language Models for Discourse](https://arxiv.org/abs/2106.04832)

Authors: [Murathan Kurfalı](https://arxiv.org/search/cs?searchtype=author&query=Kurfalı%2C+M), [Robert Östling](https://arxiv.org/search/cs?searchtype=author&query=Östling%2C+R)

> Pre-trained multilingual language models have become an important building block in multilingual natural language processing. In the present paper, we investigate a range of such models to find out how well they transfer discourse-level knowledge across languages. This is done with a systematic evaluation on a broader set of discourse-level tasks than has been previously been assembled. We find that the XLM-RoBERTa family of models consistently show the best performance, by simultaneously being good monolingual models and degrading relatively little in a zero-shot setting. Our results also indicate that model distillation may hurt the ability of cross-lingual transfer of sentence representations, while language dissimilarity at most has a modest effect. We hope that our test suite, covering 5 tasks with a total of 22 languages in 10 distinct families, will serve as a useful evaluation platform for multilingual performance at and beyond the sentence level.

| Comments: | To be presented at RepL4NLP 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04832](https://arxiv.org/abs/2106.04832) [cs.CL]** |
|           | (or **[arXiv:2106.04832v1](https://arxiv.org/abs/2106.04832v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-6">6. RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer
</h2>

Title: [RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer](https://arxiv.org/abs/2106.04833)

Authors: [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in real-time, is useful in many scenarios but has not been fully investigated. In this work, we propose RealTranS, an end-to-end model for SST. To bridge the modality gap between speech and text, RealTranS gradually downsamples the input speech with interleaved convolution and unidirectional Transformer layers for acoustic modeling, and then maps speech features into text space with a weighted-shrinking operation and a semantic encoder. Besides, to improve the model performance in simultaneous scenarios, we propose a blank penalty to enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local reranking during decoding. Experiments on public and widely-used datasets show that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings.

| Comments: | Accepted by ACL2021 Findings                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.04833](https://arxiv.org/abs/2106.04833) [cs.CL]** |
|           | (or **[arXiv:2106.04833v1](https://arxiv.org/abs/2106.04833v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-7">7. Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding
</h2>

Title: [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://arxiv.org/abs/2106.04970)

Authors: [Xin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Houfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield the same predictions as greedy decoding but with a significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at [this https URL](https://github.com/AutoTemp/Shallow-Aggressive-Decoding).

| Comments: | Accepted by ACL2021 (main conference)                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04970](https://arxiv.org/abs/2106.04970) [cs.CL]** |
|           | (or **[arXiv:2106.04970v1](https://arxiv.org/abs/2106.04970v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-8">8. Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study
</h2>

Title: [Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study](https://arxiv.org/abs/2106.04995)

Authors: [Tamali Banerjee](https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+T), [Rudra Murthy V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+R+M), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

> Recent advances in Unsupervised Neural Machine Translation (UNMT) have minimized the gap between supervised and unsupervised machine translation performance for closely related language pairs. However, the situation is very different for distant language pairs. Lack of lexical overlap and low syntactic similarities such as between English and Indo-Aryan languages leads to poor translation quality in existing UNMT systems. In this paper, we show that initializing the embedding layer of UNMT models with cross-lingual embeddings shows significant improvements in BLEU score over existing approaches with embeddings randomly initialized. Further, static embeddings (freezing the embedding layer weights) lead to better gains compared to updating the embedding layer weights during training (non-static). We experimented using Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT approaches for three distant language pairs. The proposed cross-lingual embedding initialization yields BLEU score improvement of as much as ten times over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our analysis shows the importance of cross-lingual embedding, comparisons between approaches, and the scope of improvements in these systems.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04995](https://arxiv.org/abs/2106.04995) [cs.CL]** |
|           | (or **[arXiv:2106.04995v1](https://arxiv.org/abs/2106.04995v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-9">9. Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation
</h2>

Title: [Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2106.05093)

Authors: [Cunxiao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+C), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J)

> We propose a new training objective named order-agnostic cross entropy (OaXE) for fully non-autoregressive translation (NAT) models. OaXE improves the standard cross-entropy loss to ameliorate the effect of word reordering, which is a common source of the critical multimodality problem in NAT. Concretely, OaXE removes the penalty for word order errors, and computes the cross entropy loss based on the best possible alignment between model predictions and target tokens. Since the log loss is very sensitive to invalid references, we leverage cross entropy initialization and loss truncation to ensure the model focuses on a good part of the search space. Extensive experiments on major WMT benchmarks show that OaXE substantially improves translation performance, setting new state of the art for fully NAT models. Further analyses show that OaXE alleviates the multimodality problem by reducing token repetitions and increasing prediction confidence. Our code, data, and trained models are available at [this https URL](https://github.com/tencent-ailab/ICML21_OAXE).

| Comments: | ICML 2021 (Oral), Code at [this https URL](https://github.com/tencent-ailab/ICML21_OAXE) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.05093](https://arxiv.org/abs/2106.05093) [cs.CL]** |
|           | (or **[arXiv:2106.05093v1](https://arxiv.org/abs/2106.05093v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-10">10. AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT
</h2>

Title: [AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT](https://arxiv.org/abs/2106.05141)

Authors: [Tasnim Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+T), [M Saiful Bari](https://arxiv.org/search/cs?searchtype=author&query=Bari%2C+M+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AUGVIC, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AUGVIC with the ones from the extra monolingual data, we achieve further improvements. We show that AUGVIC helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AUGVIC, we perform an in-depth framework analysis.

| Comments: | ACL-2021 accepted paper                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05141](https://arxiv.org/abs/2106.05141) [cs.CL]** |
|           | (or **[arXiv:2106.05141v1](https://arxiv.org/abs/2106.05141v1) [cs.CL]** for this version) |





# 2021-06-09

[Return to Index](#Index)



<h2 id="2021-06-09-1">1. A Survey of Transformers
</h2>

Title: [A Survey of Transformers](https://arxiv.org/abs/2106.04554)

Authors: [Tianyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+T), [Yuxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xiangyang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X)

> Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04554](https://arxiv.org/abs/2106.04554) [cs.LG]** |
|           | (or **[arXiv:2106.04554v1](https://arxiv.org/abs/2106.04554v1) [cs.LG]** for this version) |





<h2 id="2021-06-09-2">2. Meta Learning for Knowledge Distillation
</h2>

Title: [Meta Learning for Knowledge Distillation](https://arxiv.org/abs/2106.04570)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> We present Meta Learning for Knowledge Distillation (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models. The code is available at [this https URL](https://github.com/JetRunner/MetaDistil)

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04570](https://arxiv.org/abs/2106.04570) [cs.LG]** |
|           | (or **[arXiv:2106.04570v1](https://arxiv.org/abs/2106.04570v1) [cs.LG]** for this version) |





<h2 id="2021-06-09-3">3. Lexicon Learning for Few-Shot Neural Sequence Modeling
</h2>

Title: [Lexicon Learning for Few-Shot Neural Sequence Modeling](https://arxiv.org/abs/2106.03993)

Authors: [Ekin Akyürek](https://arxiv.org/search/cs?searchtype=author&query=Akyürek%2C+E), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. Past work has shown that many failures of systematic generalization arise from neural models' inability to disentangle lexical phenomena from syntactic ones. To address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules. We describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.03993](https://arxiv.org/abs/2106.03993) [cs.CL]** |
|           | (or **[arXiv:2106.03993v1](https://arxiv.org/abs/2106.03993v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-4">4. Self-supervised and Supervised Joint Training for Resource-rich Machine Translation
</h2>

Title: [Self-supervised and Supervised Joint Training for Resource-rich Machine Translation](https://arxiv.org/abs/2106.04060)

Authors: [Yong Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Lu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

> Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, F2-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning, NMT models are trained on examples that are interbred from monolingual and parallel sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation benchmarks, WMT'14 English-German and WMT'14 English-French, demonstrate that our approach achieves substantial improvements over several strong baseline methods and obtains a new state of the art of 46.19 BLEU on English-French when incorporating back translation. Results also show that our approach is capable of improving model robustness to input perturbations such as code-switching noise which frequently appears on social media.

| Comments: | Accepted by ICML 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04060](https://arxiv.org/abs/2106.04060) [cs.CL]** |
|           | (or **[arXiv:2106.04060v1](https://arxiv.org/abs/2106.04060v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-5">5. Obtaining Better Static Word Embeddings Using Contextual Embedding Models
</h2>

Title: [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://arxiv.org/abs/2106.04302)

Authors: [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&query=Jaggi%2C+M)

> The advent of contextual word embeddings -- representations of words which incorporate semantic and syntactic information from their context -- has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.

| Comments: | ACL 2021 accept                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04302](https://arxiv.org/abs/2106.04302) [cs.CL]** |
|           | (or **[arXiv:2106.04302v1](https://arxiv.org/abs/2106.04302v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-6">6. CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering
</h2>

Title: [CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering](https://arxiv.org/abs/2106.04441)

Authors: [Feifei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+F), [Mustafa Canim](https://arxiv.org/search/cs?searchtype=author&query=Canim%2C+M), [Michael Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+M), [Alfio Gliozzo](https://arxiv.org/search/cs?searchtype=author&query=Gliozzo%2C+A), [Peter Fox](https://arxiv.org/search/cs?searchtype=author&query=Fox%2C+P)

> We present the first end-to-end, transformer-based table question answering (QA) system that takes natural language questions and massive table corpus as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, CLTR, extends the current state-of-the-art QA over tables model to build an end-to-end table QA architecture. This system has successfully tackled many real-world table QA problems with a simple, unified pipeline. Our proposed system can also generate a heatmap of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural language questions over 76,242 tables. The benchmarks are designed to validate CLTR as well as accommodate future table retrieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04441](https://arxiv.org/abs/2106.04441) [cs.CL]** |
|           | (or **[arXiv:2106.04441v1](https://arxiv.org/abs/2106.04441v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-7">7. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
</h2>

Title: [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://arxiv.org/abs/2106.04489)

Authors: [Rabeeh Karimi Mahabadi](https://arxiv.org/search/cs?searchtype=author&query=Mahabadi%2C+R+K), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J)

> State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in [this https URL](https://github.com/rabeehk/hyperformer).

| Comments: | accepted in ACL, 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04489](https://arxiv.org/abs/2106.04489) [cs.CL]** |
|           | (or **[arXiv:2106.04489v1](https://arxiv.org/abs/2106.04489v1) [cs.CL]** for this version) |



# 2021-06-08

[Return to Index](#Index)



<h2 id="2021-06-08-1">1. CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings
</h2>

Title: [CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings](https://arxiv.org/abs/2106.03143)

Authors: [Tatiana Likhomanenko](https://arxiv.org/search/cs?searchtype=author&query=Likhomanenko%2C+T), [Qiantong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Ronan Collobert](https://arxiv.org/search/cs?searchtype=author&query=Collobert%2C+R), [Gabriel Synnaeve](https://arxiv.org/search/cs?searchtype=author&query=Synnaeve%2C+G), [Alex Rogozhnikov](https://arxiv.org/search/cs?searchtype=author&query=Rogozhnikov%2C+A)

> Without positional information, attention-based transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed transformer models positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences of different length than those seen at training time. Relative positions are more robust to length change, but are more complex to implement and yield inferior model throughput. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative position embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, image and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03143](https://arxiv.org/abs/2106.03143) [cs.LG]** |
|           | (or **[arXiv:2106.03143v1](https://arxiv.org/abs/2106.03143v1) [cs.LG]** for this version) |





<h2 id="2021-06-08-2">2. SelfDoc: Self-Supervised Document Representation Learning
</h2>

Title: [SelfDoc: Self-Supervised Document Representation Learning](https://arxiv.org/abs/2106.03331)

Authors: [Peizhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jiuxiang Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Jason Kuen](https://arxiv.org/search/cs?searchtype=author&query=Kuen%2C+J), [Vlad I. Morariu](https://arxiv.org/search/cs?searchtype=author&query=Morariu%2C+V+I), [Handong Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rajiv Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+R), [Varun Manjunatha](https://arxiv.org/search/cs?searchtype=author&query=Manjunatha%2C+V), [Hongfu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H)

> We propose SelfDoc, a task-agnostic pre-training framework for document image understanding. Because documents are multimodal and are intended for sequential reading, our framework exploits the positional, textual, and visual information of every semantically meaningful component in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly fine-grained with excessive contextualization. Beyond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal information from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mechanism for multimodal feature fusion by adaptively emphasizing language and vision signals. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. It achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous works.

| Comments: | To appear in CVPR'2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.03331](https://arxiv.org/abs/2106.03331) [cs.CV]** |
|           | (or **[arXiv:2106.03331v1](https://arxiv.org/abs/2106.03331v1) [cs.CV]** for this version) |





<h2 id="2021-06-08-3">3. Do Grammatical Error Correction Models Realize Grammatical Generalization?
</h2>

Title: [Do Grammatical Error Correction Models Realize Grammatical Generalization?](https://arxiv.org/abs/2106.03031)

Authors: [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Hitomi Yanaka](https://arxiv.org/search/cs?searchtype=author&query=Yanaka%2C+H)

> There has been an increased interest in data generation approaches to grammatical error correction (GEC) using pseudo data. However, these approaches suffer from several issues that make them inconvenient for real-world deployment including a demand for large amounts of training data. On the other hand, some errors based on grammatical rules may not necessarily require a large amount of data if GEC models can realize grammatical generalization. This study explores to what extent GEC models generalize grammatical knowledge required for correcting errors. We introduce an analysis method using synthetic and real GEC datasets with controlled vocabularies to evaluate whether models can generalize to unseen errors. We found that a current standard Transformer-based GEC model fails to realize grammatical generalization even in simple settings with limited vocabulary and syntax, suggesting that it lacks the generalization ability required to correct errors from provided training examples.

| Comments: | ACL 2021 (Findings)                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03031](https://arxiv.org/abs/2106.03031) [cs.CL]** |
|           | (or **[arXiv:2106.03031v1](https://arxiv.org/abs/2106.03031v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-4">4. The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
</h2>

Title: [The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](https://arxiv.org/abs/2106.03193)

Authors: [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Cynthia Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Da Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+D), [Sanjana Krishnan](https://arxiv.org/search/cs?searchtype=author&query=Krishnan%2C+S), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03193](https://arxiv.org/abs/2106.03193) [cs.CL]** |
|           | (or **[arXiv:2106.03193v1](https://arxiv.org/abs/2106.03193v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-5">5. Itihasa: A large-scale corpus for Sanskrit to English translation
</h2>

Title: [Itihasa: A large-scale corpus for Sanskrit to English translation](https://arxiv.org/abs/2106.03269)

Authors: [Rahul Aralikatte](https://arxiv.org/search/cs?searchtype=author&query=Aralikatte%2C+R), [Miryam de Lhoneux](https://arxiv.org/search/cs?searchtype=author&query=de+Lhoneux%2C+M), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A)

> This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.

| Comments: | WAT 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03269](https://arxiv.org/abs/2106.03269) [cs.CL]** |
|           | (or **[arXiv:2106.03269v1](https://arxiv.org/abs/2106.03269v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-6">6. On the Language Coverage Bias for Neural Machine Translation
</h2>

Title: [On the Language Coverage Bias for Neural Machine Translation](https://arxiv.org/abs/2106.03297)

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation. We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants.

| Comments: | ACL 2021, Long Findings                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03297](https://arxiv.org/abs/2106.03297) [cs.CL]** |
|           | (or **[arXiv:2106.03297v1](https://arxiv.org/abs/2106.03297v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-7">7. BERTGEN: Multi-task Generation through BERT
</h2>

Title: [BERTGEN: Multi-task Generation through BERT](https://arxiv.org/abs/2106.03484)

Authors: [Faidon Mitzalis](https://arxiv.org/search/cs?searchtype=author&query=Mitzalis%2C+F), [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> We present BERTGEN, a novel generative, decoder-only model which extends BERT by fusing multimodal and multilingual pretrained models VL-BERT and M-BERT, respectively. BERTGEN is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multitask setting. With a comprehensive set of evaluations, we show that BERTGEN outperforms many strong baselines across the tasks explored. We also show BERTGEN's ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGEN substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.

| Comments: | Accepted to ACL 2021 Main Conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03484](https://arxiv.org/abs/2106.03484) [cs.CL]** |
|           | (or **[arXiv:2106.03484v1](https://arxiv.org/abs/2106.03484v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-8">8. RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models
</h2>

Title: [RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models](https://arxiv.org/abs/2106.03613)

Authors: [Xin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X), [Jianlei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Xucheng Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+X), [Jianxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7%~18% up to 45.8%~47.8% on different datasets with comparable weight compression ratio to existing distillation methods (4.6×~6.5× improvement from teacher model BERT_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.

| Comments: | 10 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.03613](https://arxiv.org/abs/2106.03613) [cs.CL]** |
|           | (or **[arXiv:2106.03613v1](https://arxiv.org/abs/2106.03613v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-9">9. Diverse Pretrained Context Encodings Improve Document Translation
</h2>

Title: [Diverse Pretrained Context Encodings Improve Document Translation](https://arxiv.org/abs/2106.03717)

Authors: [Domenic Donato](https://arxiv.org/search/cs?searchtype=author&query=Donato%2C+D), [Lei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C)

> We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pretrained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pretrained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multi-context model consistently outperforms the best existing context-aware transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03717](https://arxiv.org/abs/2106.03717) [cs.CL]** |
|           | (or **[arXiv:2106.03717v1](https://arxiv.org/abs/2106.03717v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-10">10. Encouraging Neural Machine Translation to Satisfy Terminology Constraints
</h2>

Title: [Encouraging Neural Machine Translation to Satisfy Terminology Constraints](https://arxiv.org/abs/2106.03730)

Authors: [Melissa Ailem](https://arxiv.org/search/cs?searchtype=author&query=Ailem%2C+M), [Jinghsu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Raheel Qader](https://arxiv.org/search/cs?searchtype=author&query=Qader%2C+R)

> We present a new approach to encourage neural machine translation to satisfy lexical constraints. Our method acts at the training step and thereby avoiding the introduction of any extra computational overhead at inference step. The proposed method combines three main ingredients. The first one consists in augmenting the training data to specify the constraints. Intuitively, this encourages the model to learn a copy behavior when it encounters constraint terms. Compared to previous work, we use a simplified augmentation strategy without source factors. The second ingredient is constraint token masking, which makes it even easier for the model to learn the copy behavior and generalize better. The third one, is a modification of the standard cross entropy loss to bias the model towards assigning high probabilities to constraint words. Empirical results show that our method improves upon related baselines in terms of both BLEU score and the percentage of generated constraint terms.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03730](https://arxiv.org/abs/2106.03730) [cs.CL]** |
|           | (or **[arXiv:2106.03730v1](https://arxiv.org/abs/2106.03730v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-11">11. A Simple Recipe for Multilingual Grammatical Error Correction
</h2>

Title: [A Simple Recipe for Multilingual Grammatical Error Correction](https://arxiv.org/abs/2106.03830)

Authors: [Sascha Rothe](https://arxiv.org/search/cs?searchtype=author&query=Rothe%2C+S), [Jonathan Mallinson](https://arxiv.org/search/cs?searchtype=author&query=Mallinson%2C+J), [Eric Malmi](https://arxiv.org/search/cs?searchtype=author&query=Malmi%2C+E), [Sebastian Krause](https://arxiv.org/search/cs?searchtype=author&query=Krause%2C+S), [Aliaksei Severyn](https://arxiv.org/search/cs?searchtype=author&query=Severyn%2C+A)

> This paper presents a simple recipe to train state-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a cLang-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages -- we demonstrate that performing a single fine-tuning step on cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03830](https://arxiv.org/abs/2106.03830) [cs.CL]** |
|           | (or **[arXiv:2106.03830v1](https://arxiv.org/abs/2106.03830v1) [cs.CL]** for this version) |






# 2021-06-07

[Return to Index](#Index)



<h2 id="2021-06-07-1">1. Neural semi-Markov CRF for Monolingual Word Alignment
</h2>

Title: [Neural semi-Markov CRF for Monolingual Word Alignment](https://arxiv.org/abs/2106.02569)

Authors: [Wuwei Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+W), [Chao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+C), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W)

> Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02569](https://arxiv.org/abs/2106.02569) [cs.CL]** |
|           | (or **[arXiv:2106.02569v1](https://arxiv.org/abs/2106.02569v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-2">2. Human-Adversarial Visual Question Answering
</h2>

Title: [Human-Adversarial Visual Question Answering](https://arxiv.org/abs/2106.02280)

Authors: [Sasha Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+S), [Amanpreet Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A), [Vedanuj Goswami](https://arxiv.org/search/cs?searchtype=author&query=Goswami%2C+V), [Jose Alberto Lopez Magana](https://arxiv.org/search/cs?searchtype=author&query=Magana%2C+J+A+L), [Wojciech Galuba](https://arxiv.org/search/cs?searchtype=author&query=Galuba%2C+W), [Devi Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+D), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.

| Comments: | 22 pages, 13 figures. First two authors contributed equally  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.02280](https://arxiv.org/abs/2106.02280) [cs.CV]** |
|           | (or **[arXiv:2106.02280v1](https://arxiv.org/abs/2106.02280v1) [cs.CV]** for this version) |





<h2 id="2021-06-07-3">3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages
</h2>

Title: [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://arxiv.org/abs/2106.02124)

Authors: [Abteen Ebrahimi](https://arxiv.org/search/cs?searchtype=author&query=Ebrahimi%2C+A), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world's languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for over 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02124](https://arxiv.org/abs/2106.02124) [cs.CL]** |
|           | (or **[arXiv:2106.02124v1](https://arxiv.org/abs/2106.02124v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-4">4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer
</h2>

Title: [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://arxiv.org/abs/2106.02134)

Authors: [Wasi Uddin Ahmad](https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+W+U), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Yashar Mehdad](https://arxiv.org/search/cs?searchtype=author&query=Mehdad%2C+Y)

> In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT \cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the \emph{generalized} transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.

| Comments: | ACL 2021 (camera ready)                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02134](https://arxiv.org/abs/2106.02134) [cs.CL]** |
|           | (or **[arXiv:2106.02134v1](https://arxiv.org/abs/2106.02134v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-5">5. Grounding 'Grounding' in NLP
</h2>

Title: [Grounding 'Grounding' in NLP](https://arxiv.org/abs/2106.02192)

Authors: [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines "grounding" as the process of establishing what mutual information is required for successful communication between two interlocutors -- a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term "grounding" used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding.

| Comments: | 24 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02192](https://arxiv.org/abs/2106.02192) [cs.CL]** |
|           | (or **[arXiv:2106.02192v1](https://arxiv.org/abs/2106.02192v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-6">6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore
</h2>

Title: [BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://arxiv.org/abs/2106.02208)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Jacob Parnell](https://arxiv.org/search/cs?searchtype=author&query=Parnell%2C+J), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02208](https://arxiv.org/abs/2106.02208) [cs.CL]** |
|           | (or **[arXiv:2106.02208v1](https://arxiv.org/abs/2106.02208v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-7">7. Scalable Transformers for Neural Machine Translation
</h2>

Title: [Scalable Transformers for Neural Machine Translation](https://arxiv.org/abs/2106.02242)

Authors: [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Xiaogang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Jifeng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Hongsheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Transformer has been widely adopted in Neural Machine Translation (NMT) because of its large capacity and parallel training of sequence generation. However, the deployment of Transformer is challenging because different scenarios require models of different complexities and scales. Naively training multiple Transformers is redundant in terms of both computation and memory. In this paper, we propose a novel scalable Transformers, which naturally contains sub-Transformers of different scales and have shared parameters. Each sub-Transformer can be easily obtained by cropping the parameters of the largest Transformer. A three-stage training scheme is proposed to tackle the difficulty of training the scalable Transformers, which introduces additional supervisions from word-level and sequence-level self-distillation. Extensive experiments were conducted on WMT EN-De and En-Fr to validate our proposed scalable Transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02242](https://arxiv.org/abs/2106.02242) [cs.CL]** |
|           | (or **[arXiv:2106.02242v1](https://arxiv.org/abs/2106.02242v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-8">8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene
</h2>

Title: [Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](https://arxiv.org/abs/2106.02327)

Authors: [Ruikun Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Guanhuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Xiaojun Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+X)

> The major paradigm of applying a pre-trained language model to downstream tasks is to fine-tune it on labeled task data, which often suffers instability and low performance when the labeled examples are scarce.~One way to alleviate this problem is to apply post-training on unlabeled task data before fine-tuning, adapting the pre-trained model to target domains by contrastive learning that considers either token-level or sequence-level similarity. Inspired by the success of sequence masking, we argue that both token-level and sequence-level similarities can be captured with a pair of masked sequences.~Therefore, we propose complementary random masking (CRM) to generate a pair of masked sequences from an input sequence for sequence-level contrastive learning and then develop contrastive masked language modeling (CMLM) for post-training to integrate both token-level and sequence-level contrastive learnings.~Empirical results show that CMLM surpasses several recent post-training methods in few-shot settings without the need for data augmentation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02327](https://arxiv.org/abs/2106.02327) [cs.CL]** |
|           | (or **[arXiv:2106.02327v1](https://arxiv.org/abs/2106.02327v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-9">9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings
</h2>

Title: [Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](https://arxiv.org/abs/2106.02490)

Authors: [Thomas Conley](https://arxiv.org/search/cs?searchtype=author&query=Conley%2C+T), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Artificial Neural networks are mathematical models at their core. This truismpresents some fundamental difficulty when networks are tasked with Natural Language Processing. A key problem lies in measuring the similarity or distance among vectors in NLP embedding space, since the mathematical concept of distance does not always agree with the linguistic concept. We suggest that the best way to measure linguistic distance among vectors is by employing the Language Model (LM) that created them. We introduce Language Model Distance (LMD) for measuring accuracy of vector transformations based on the Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric by applying it to a simple neural network learning the Procrustes algorithm for bilingual word mapping.

| Subjects:          | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 17th International Conference on Natural Language Processing, pages 170-174, Patna, India, December 18-21, 2020 |
| Cite as:           | **[arXiv:2106.02490](https://arxiv.org/abs/2106.02490) [cs.CL]** |
|                    | (or **[arXiv:2106.02490v1](https://arxiv.org/abs/2106.02490v1) [cs.CL]** for this version) |








# 2021-06-04

[Return to Index](#Index)



<h2 id="2021-06-04-1">1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data
</h2>

Title: [TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](https://arxiv.org/abs/2106.01797)

Authors: [Pengda Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+P), [Yuhong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Among ubiquitous multimodal data in the real world, text is the modality generated by human, while image reflects the physical world honestly. In a visual understanding application, machines are expected to understand images like human. Inspired by this, we propose a novel self-supervised learning method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual representations by fully utilizing the naturally-existing multimodal data. Our core idea of self-supervised learning is to maximize the mutual information between features extracted from multiple views of a shared context to a rational degree. Different from previous methods which only consider multiple views from a single modality, our work produces multiple views from different modalities, and jointly optimizes the mutual information for features pairs of intra-modality and inter-modality. Considering the information gap between inter-modality features pairs from data noise, we adopt a \emph{ranking-based} contrastive learning to optimize the mutual information. During evaluation, we directly use the pre-trained visual representations to complete various image classification tasks. Experimental results show that, TVDIM significantly outperforms previous visual self-supervised methods when processing the same set of images.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01797](https://arxiv.org/abs/2106.01797) [cs.CL]** |
|           | (or **[arXiv:2106.01797v1](https://arxiv.org/abs/2106.01797v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-2">2. Representing Syntax and Composition with Geometric Transformations
</h2>

Title: [Representing Syntax and Composition with Geometric Transformations](https://arxiv.org/abs/2106.01904)

Authors: [Lorenzo Bertolini](https://arxiv.org/search/cs?searchtype=author&query=Bertolini%2C+L), [Julie Weeds](https://arxiv.org/search/cs?searchtype=author&query=Weeds%2C+J), [David Weir](https://arxiv.org/search/cs?searchtype=author&query=Weir%2C+D), [Qiwei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+Q)

> The exploitation of syntactic graphs (SyGs) as a word's context has been shown to be beneficial for distributional semantic models (DSMs), both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters (compared to conventional DSMs) and suffer from data sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic relations) has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation.

| Comments: | to appear in Findings of ACL 2021                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01904](https://arxiv.org/abs/2106.01904) [cs.CL]** |
|           | (or **[arXiv:2106.01904v1](https://arxiv.org/abs/2106.01904v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-3">3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models
</h2>

Title: [The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](https://arxiv.org/abs/2106.01950)

Authors: [Ulme Wennberg](https://arxiv.org/search/cs?searchtype=author&query=Wennberg%2C+U), [Gustav Eje Henter](https://arxiv.org/search/cs?searchtype=author&query=Henter%2C+G+E)

> Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.

| Comments: | 11 pages, 8 figures, Accepted to ACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01950](https://arxiv.org/abs/2106.01950) [cs.CL]** |
|           | (or **[arXiv:2106.01950v1](https://arxiv.org/abs/2106.01950v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-4">4. A Dataset and Baselines for Multilingual Reply Suggestion
</h2>

Title: [A Dataset and Baselines for Multilingual Reply Suggestion](https://arxiv.org/abs/2106.02017)

Authors: [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Budhaditya Deb](https://arxiv.org/search/cs?searchtype=author&query=Deb%2C+B), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Milad Shokouhi](https://arxiv.org/search/cs?searchtype=author&query=Shokouhi%2C+M), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at [this https URL](https://github.com/zhangmozhi/mrs).

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.02017](https://arxiv.org/abs/2106.02017) [cs.CL]** |
|           | (or **[arXiv:2106.02017v1](https://arxiv.org/abs/2106.02017v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-5">5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning
</h2>

Title: [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://arxiv.org/abs/2106.01804)

Authors: [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Wenming Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F)

> Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.

| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ACL2021 main conference                                      |
| Cite as:           | **[arXiv:2106.01804](https://arxiv.org/abs/2106.01804) [cs.CV]** |
|                    | (or **[arXiv:2106.01804v1](https://arxiv.org/abs/2106.01804v1) [cs.CV]** for this version) |





<h2 id="2021-06-04-6">6. Lightweight Adapter Tuning for Multilingual Speech Translation
</h2>

Title: [Lightweight Adapter Tuning for Multilingual Speech Translation](https://arxiv.org/abs/2106.01463)

Authors: [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.

| Comments: | Accepted at ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01463](https://arxiv.org/abs/2106.01463) [cs.CL]** |
|           | (or **[arXiv:2106.01463v1](https://arxiv.org/abs/2106.01463v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-7">7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?
</h2>

Title: [Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](https://arxiv.org/abs/2106.01561)

Authors: [Cunxiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Pai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.

| Comments: | Accepted By ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01561](https://arxiv.org/abs/2106.01561) [cs.CL]** |
|           | (or **[arXiv:2106.01561v1](https://arxiv.org/abs/2106.01561v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-8">8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction
</h2>

Title: [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://arxiv.org/abs/2106.01609)

Authors: [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.

| Comments: | Accepted in the main conference of ACL 2021. Code: [this https URL](https://github.com/lipiji/TtT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.01609](https://arxiv.org/abs/2106.01609) [cs.CL]** |
|           | (or **[arXiv:2106.01609v1](https://arxiv.org/abs/2106.01609v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-9">9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models
</h2>

Title: [Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](https://arxiv.org/abs/2106.01623)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Zhicheng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at [this https URL](https://github.com/RUCAIBox/Few-Shot-KG2Text).

| Comments: | Accepted to ACL 2021 Findings                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01623](https://arxiv.org/abs/2106.01623) [cs.CL]** |
|           | (or **[arXiv:2106.01623v1](https://arxiv.org/abs/2106.01623v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-10">10. Fingerprinting Fine-tuned Language Models in the Wild
</h2>

Title: [Fingerprinting Fine-tuned Language Models in the Wild](https://arxiv.org/abs/2106.01703)

Authors: [Nirav Diwan](https://arxiv.org/search/cs?searchtype=author&query=Diwan%2C+N), [Tanmoy Chakravorty](https://arxiv.org/search/cs?searchtype=author&query=Chakravorty%2C+T), [Zubair Shafiq](https://arxiv.org/search/cs?searchtype=author&query=Shafiq%2C+Z)

> There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda. Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic. While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin. Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually < 10) of pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text. It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios. To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild. Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches. Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01703](https://arxiv.org/abs/2106.01703) [cs.CL]** |
|           | (or **[arXiv:2106.01703v1](https://arxiv.org/abs/2106.01703v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-11">11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer
</h2>

Title: [Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](https://arxiv.org/abs/2106.01732)

Authors: [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Wentao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Jiani Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S)

> Multilingual pre-trained models have achieved remarkable transfer performance by pre-trained on rich kinds of languages. Most of the models such as mBERT are pre-trained on unlabeled corpora. The static and contextual embeddings from the models could not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by aligning the embeddings better. We propose a pre-training task named Alignment Language Model (AlignLM), which uses the statistical alignment information as the prior knowledge to guide bilingual word prediction. We evaluate our method on multilingual machine reading comprehension and natural language interface tasks. The results show AlignLM can improve the zero-shot performance significantly on MLQA and XNLI datasets.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01732](https://arxiv.org/abs/2106.01732) [cs.CL]** |
|           | (or **[arXiv:2106.01732v1](https://arxiv.org/abs/2106.01732v1) [cs.CL]** for this version) |









# 2021-06-03

[Return to Index](#Index)



<h2 id="2021-06-03-1">1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation
</h2>

Title: [Part of Speech and Universal Dependency effects on English Arabic Machine Translation](https://arxiv.org/abs/2106.00745)

Authors: [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O), [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Dmitry Nikolaev](https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+D), [Ofek Rafaeli](https://arxiv.org/search/cs?searchtype=author&query=Rafaeli%2C+O)

> In this research paper, I will elaborate on a method to evaluate machine translation models based on their performance on underlying syntactical phenomena between English and Arabic languages. This method is especially important as such "neural" and "machine learning" are hard to fine-tune and change. Thus, finding a way to evaluate them easily and diversely would greatly help the task of bettering them.

| Comments: | 19 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00745](https://arxiv.org/abs/2106.00745) [cs.CL]** |
|           | (or **[arXiv:2106.00745v1](https://arxiv.org/abs/2106.00745v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-2">2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation
</h2>

Title: [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://arxiv.org/abs/2106.00903)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at \url{[this https URL](https://github.com/longyuewangdcu/RLFW-NAT)}.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.00903](https://arxiv.org/abs/2106.00903) [cs.CL]** |
|           | (or **[arXiv:2106.00903v1](https://arxiv.org/abs/2106.00903v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-3">3. Discrete Cosine Transform as Universal Sentence Encoder
</h2>

Title: [Discrete Cosine Transform as Universal Sentence Encoder](https://arxiv.org/abs/2106.00934)

Authors: [Nada Almarwani](https://arxiv.org/search/cs?searchtype=author&query=Almarwani%2C+N), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets.

| Comments: | to be published in ACL-IJCNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00934](https://arxiv.org/abs/2106.00934) [cs.CL]** |
|           | (or **[arXiv:2106.00934v1](https://arxiv.org/abs/2106.00934v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-4">4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation
</h2>

Title: [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://arxiv.org/abs/2106.00941)

Authors: [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Irwin King](https://arxiv.org/search/cs?searchtype=author&query=King%2C+I)

> Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.

| Comments: | ACL 2021 main conference, long paper, 11 pages               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Theory (cs.IT) |
| Cite as:  | **[arXiv:2106.00941](https://arxiv.org/abs/2106.00941) [cs.CL]** |
|           | (or **[arXiv:2106.00941v1](https://arxiv.org/abs/2106.00941v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-5">5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers
</h2>

Title: [One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](https://arxiv.org/abs/2106.01023)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.

| Comments: | Findings of ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01023](https://arxiv.org/abs/2106.01023) [cs.CL]** |
|           | (or **[arXiv:2106.01023v1](https://arxiv.org/abs/2106.01023v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-6">6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling
</h2>

Title: [Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](https://arxiv.org/abs/2106.01040)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.

| Comments: | ACL-IJCNLP 2021                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01040](https://arxiv.org/abs/2106.01040) [cs.CL]** |
|           | (or **[arXiv:2106.01040v1](https://arxiv.org/abs/2106.01040v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-7">7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?
</h2>

Title: [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://arxiv.org/abs/2106.01045)

Authors: [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Alberto Martinelli](https://arxiv.org/search/cs?searchtype=author&query=Martinelli%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.

| Comments: | Accepted at ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01045](https://arxiv.org/abs/2106.01045) [cs.CL]** |
|           | (or **[arXiv:2106.01045v1](https://arxiv.org/abs/2106.01045v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-8">8. Evidence-based Factual Error Correction
</h2>

Title: [Evidence-based Factual Error Correction](https://arxiv.org/abs/2106.01072)

Authors: [James Thorne](https://arxiv.org/search/cs?searchtype=author&query=Thorne%2C+J), [Andreas Vlachos](https://arxiv.org/search/cs?searchtype=author&query=Vlachos%2C+A)

> This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.

| Comments: | To appear at ACL2021. arXiv admin note: text overlap with [arXiv:2012.15788](https://arxiv.org/abs/2012.15788) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01072](https://arxiv.org/abs/2106.01072) [cs.CL]** |
|           | (or **[arXiv:2106.01072v1](https://arxiv.org/abs/2106.01072v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-9">9. Is Sparse Attention more Interpretable?
</h2>

Title: [Is Sparse Attention more Interpretable?](https://arxiv.org/abs/2106.01087)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Stefan Lazov](https://arxiv.org/search/cs?searchtype=author&query=Lazov%2C+S), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists -- under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of ACL-IJCNLP 2021                               |
| Cite as:           | **[arXiv:2106.01087](https://arxiv.org/abs/2106.01087) [cs.CL]** |
|                    | (or **[arXiv:2106.01087v1](https://arxiv.org/abs/2106.01087v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-10">10. End-to-End NLP Knowledge Graph Construction
</h2>

Title: [End-to-End NLP Knowledge Graph Construction](https://arxiv.org/abs/2106.01167)

Authors: [Ishani Mondal](https://arxiv.org/search/cs?searchtype=author&query=Mondal%2C+I), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Charles Jochim](https://arxiv.org/search/cs?searchtype=author&query=Jochim%2C+C)

> This paper studies the end-to-end construction of an NLP Knowledge Graph (KG) from scientific papers. We focus on extracting four types of relations: evaluatedOn between tasks and datasets, evaluatedBy between tasks and evaluation metrics, as well as coreferent and related relations between the same type of entities. For instance, F1-score is coreferent with F-measure. We introduce novel methods for each of these relation types and apply our final framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a large-scale KG, which can facilitate automatically constructing scientific leaderboards for the NLP community. The results of our experiments indicate that the resulting KG contains high-quality information.

| Comments: | Accepted in ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01167](https://arxiv.org/abs/2106.01167) [cs.CL]** |
|           | (or **[arXiv:2106.01167v1](https://arxiv.org/abs/2106.01167v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-11">11. IrEne: Interpretable Energy Prediction for Transformers
</h2>

Title: [IrEne: Interpretable Energy Prediction for Transformers](https://arxiv.org/abs/2106.01199)

Authors: [Qingqing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Q), [Yash Kumar Lal](https://arxiv.org/search/cs?searchtype=author&query=Lal%2C+Y+K), [Harsh Trivedi](https://arxiv.org/search/cs?searchtype=author&query=Trivedi%2C+H), [Aruna Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at [this https URL](https://github.com/StonyBrookNLP/irene).

| Comments: | ACL 2021 camera ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01199](https://arxiv.org/abs/2106.01199) [cs.CL]** |
|           | (or **[arXiv:2106.01199v1](https://arxiv.org/abs/2106.01199v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-12">12. Lower Perplexity is Not Always Human-Like
</h2>

Title: [Lower Perplexity is Not Always Human-Like](https://arxiv.org/abs/2106.01229)

Authors: [Tatsuki Kuribayashi](https://arxiv.org/search/cs?searchtype=author&query=Kuribayashi%2C+T), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y), [Takumi Ito](https://arxiv.org/search/cs?searchtype=author&query=Ito%2C+T), [Ryo Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida%2C+R), [Masayuki Asahara](https://arxiv.org/search/cs?searchtype=author&query=Asahara%2C+M), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization -- the lower perplexity a language model has, the more human-like the language model is -- in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01229](https://arxiv.org/abs/2106.01229) [cs.CL]** |
|           | (or **[arXiv:2106.01229v1](https://arxiv.org/abs/2106.01229v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-13">13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers
</h2>

Title: [On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](https://arxiv.org/abs/2106.01335)

Authors: [Tianchu Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+T), [Shraddhan Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+S), [Michael Ferdman](https://arxiv.org/search/cs?searchtype=author&query=Ferdman%2C+M), [Peter Milder](https://arxiv.org/search/cs?searchtype=author&query=Milder%2C+P), [H. Andrew Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+H+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and log-scaled mapping which produces only a few (e.g. 23) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal (<1.0%) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01335](https://arxiv.org/abs/2106.01335) [cs.CL]** |
|           | (or **[arXiv:2106.01335v1](https://arxiv.org/abs/2106.01335v1) [cs.CL]** for this version) |









# 2021-06-02

[Return to Index](#Index)



<h2 id="2021-06-02-1">1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
</h2>

Title: [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> With large-scale pre-training, the past two years have witnessed significant performance boost on the Visual Question Answering (VQA) task. Though rapid progresses have been made, it remains unclear whether these state-of-the-art (SOTA) VQA models are robust when encountering test examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we present several interesting findings. (i) Surprisingly, during dataset collection, we find that non-expert annotators can successfully attack SOTA VQA models with relative ease. (ii) We test a variety of SOTA VQA models on our new dataset to highlight their fragility, and find that both large-scale pre-trained models and adversarial training methods can only achieve far lower performance than what they can achieve on the standard VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be used to improve the performance on other robust VQA benchmarks. (iv) We present a detailed analysis of the dataset, providing valuable insights on the challenges it brings to the community. We hope Adversarial VQA can serve as a valuable benchmark that will be used by future work to test the robustness of its developed VQA models. Our dataset is publicly available at https://adversarialvqa. [this http URL](http://github.io/).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.00245](https://arxiv.org/abs/2106.00245) [cs.CV]** |
|           | (or **[arXiv:2106.00245v1](https://arxiv.org/abs/2106.00245v1) [cs.CV]** for this version) |





<h2 id="2021-06-02-2">2. Language Model Evaluation Beyond Perplexity
</h2>

Title: [Language Model Evaluation Beyond Perplexity](https://arxiv.org/abs/2106.00085)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to certain statistical tendencies of natural language. We find that neural language models appear to learn only a subset of the statistical tendencies considered, but align much more closely with empirical trends than theoretical laws (when present). Further, the fit to different distributions is dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols suprisingly well.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00085](https://arxiv.org/abs/2106.00085) [cs.CL]** |
|           | (or **[arXiv:2106.00085v1](https://arxiv.org/abs/2106.00085v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-3">3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers
</h2>

Title: [An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](https://arxiv.org/abs/2106.00143)

Authors: [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Ruslan Mitkov](https://arxiv.org/search/cs?searchtype=author&query=Mitkov%2C+R)

> Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that these QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios.

| Comments: | Accepted to appear at the ACL-IJCNLP 2021 Main conference    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00143](https://arxiv.org/abs/2106.00143) [cs.CL]** |
|           | (or **[arXiv:2106.00143v1](https://arxiv.org/abs/2106.00143v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-4">4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation
</h2>

Title: [Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](https://arxiv.org/abs/2106.00169)

Authors: [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Denise Diaz](https://arxiv.org/search/cs?searchtype=author&query=Diaz%2C+D), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00169](https://arxiv.org/abs/2106.00169) [cs.CL]** |
|           | (or **[arXiv:2106.00169v1](https://arxiv.org/abs/2106.00169v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-5">5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives
</h2>

Title: [Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](https://arxiv.org/abs/2106.00181)

Authors: [Meichun Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+M), [Ziyang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z)

> Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human-scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people's attitudes.

| Comments: | Accepted at the 3rd Workshop on Gender Bias in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00181](https://arxiv.org/abs/2106.00181) [cs.CL]** |
|           | (or **[arXiv:2106.00181v1](https://arxiv.org/abs/2106.00181v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-6">6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021
</h2>

Title: [Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](https://arxiv.org/abs/2106.00197)

Authors: [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different tasks (i.e., Speech Recognition, Machine Translation, and Speech Translation) can be exploited to enhance the model's ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these features are processed by a shared encoder--decoder architecture. We apply several training techniques to improve the performance, including multi-task learning, task-level curriculum learning, data augmentation, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.00197](https://arxiv.org/abs/2106.00197) [cs.CL]** |
|           | (or **[arXiv:2106.00197v1](https://arxiv.org/abs/2106.00197v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-7">7. ViTA: Visual-Linguistic Translation by Aligning Object Tags
</h2>

Title: [ViTA: Visual-Linguistic Translation by Aligning Object Tags](https://arxiv.org/abs/2106.00250)

Authors: [Kshitij Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+K), [Devansh Gautam](https://arxiv.org/search/cs?searchtype=author&query=Gautam%2C+D), [Radhika Mamidi](https://arxiv.org/search/cs?searchtype=author&query=Mamidi%2C+R)

> Multimodal Machine Translation (MMT) enriches the source text with visual information for translation. It has gained popularity in recent years, and several pipelines have been proposed in the same direction. Yet, the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. In this paper, we propose our system for the Multimodal Translation Task of WAT 2021 from English to Hindi. We propose to use mBART, a pretrained multilingual sequence-to-sequence model, for the textual-only translations. Further, we bring the visual information to a textual domain by extracting object tags from the image and enhance the input for the multimodal task. We also explore the robustness of our system by systematically degrading the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test set and challenge set of the task.

| Comments: | 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00250](https://arxiv.org/abs/2106.00250) [cs.CL]** |
|           | (or **[arXiv:2106.00250v1](https://arxiv.org/abs/2106.00250v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-8">8. An In-depth Study on Internal Structure of Chinese Words
</h2>

Title: [An In-depth Study on Internal Structure of Chinese Words](https://arxiv.org/abs/2106.00334)

Authors: [Chen Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+C), [Saihao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Houquan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Zhenghua Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Zhefeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Baoxing Huai](https://arxiv.org/search/cs?searchtype=author&query=Huai%2C+B), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J)

> Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.

| Comments: | Accepted by ACL-IJCNLP 2021 (long paper)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00334](https://arxiv.org/abs/2106.00334) [cs.CL]** |
|           | (or **[arXiv:2106.00334v1](https://arxiv.org/abs/2106.00334v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-9">9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining
</h2>

Title: [SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](https://arxiv.org/abs/2106.00400)

Authors: [Chenglei Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+C), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yingfa Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Fanchao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Conventional tokenization methods for Chinese pretrained language models (PLMs) treat each character as an indivisible token (Devlin et al., 2019), which ignores the characteristics of the Chinese writing system. In this work, we comprehensively study the influences of three main factors on the Chinese tokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary. Correspondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk Word), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character), the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with Chinese word segmentation. To empirically compare the effectiveness of studied tokenizers, we pretrain BERT-style language models with them and evaluate the models on various downstream NLU tasks. We find that SHUOWEN and JIEZI tokenizers can generally outperform conventional single-character tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step. Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly better robustness in handling noisy texts. The code and pretrained models will be publicly released to facilitate linguistically informed Chinese NLP.

| Comments: | Work in progress. Feedback is welcome                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00400](https://arxiv.org/abs/2106.00400) [cs.CL]** |
|           | (or **[arXiv:2106.00400v1](https://arxiv.org/abs/2106.00400v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-10">10. DoT: An efficient Double Transformer for NLP tasks with tables
</h2>

Title: [DoT: An efficient Double Transformer for NLP tasks with tables](https://arxiv.org/abs/2106.00479)

Authors: [Syrine Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+S), [Thomas Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+T), [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M)

> Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.

| Comments:    | 11 pages, 4 figures, to be published in Findings of ACL-IJCNLP 2021 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68-06                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2106.00479](https://arxiv.org/abs/2106.00479) [cs.CL]** |
|              | (or **[arXiv:2106.00479v1](https://arxiv.org/abs/2106.00479v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-11">11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations
</h2>

Title: [NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](https://arxiv.org/abs/2106.00590)

Authors: [Jialu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Tianqi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Cong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C)

> Effectively modeling text-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach to mine semantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting where texts in different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3447548.3467392](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467392&v=694504da) |
| Cite as:  | **[arXiv:2106.00590](https://arxiv.org/abs/2106.00590) [cs.CL]** |
|           | (or **[arXiv:2106.00590v1](https://arxiv.org/abs/2106.00590v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-12">12. Incorporating Visual Layout Structures for Scientific Text Classification
</h2>

Title: [Incorporating Visual Layout Structures for Scientific Text Classification](https://arxiv.org/abs/2106.00676)

Authors: [Zejiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Lucy Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L+L), [Bailey Kuehl](https://arxiv.org/search/cs?searchtype=author&query=Kuehl%2C+B), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D)

> Classifying the core textual components of a scientific paper-title, author, body text, etc.-is a critical first step in automated scientific document understanding. Previous work has shown how using elementary layout information, i.e., each token's 2D position on the page, leads to more accurate classification. We introduce new methods for incorporating VIsual LAyout structures (VILA), e.g., the grouping of page texts into text lines or text blocks, into language models to further improve performance. We show that the I-VILA approach, which simply adds special tokens denoting boundaries between layout structures into model inputs, can lead to +1~4.5 F1 Score improvements in token classification tasks. Moreover, we design a hierarchical model H-VILA that encodes these layout structures and record a up-to 70% efficiency boost without hurting prediction accuracy. The experiments are conducted on a newly curated evaluation suite, S2-VLUE, with a novel metric measuring VILA awareness and a new dataset covering 19 scientific disciplines with gold annotations. Pre-trained weights, benchmark datasets, and source code will be available at [this https URL](https://github.com/allenai/VILA)}{[this https URL](https://github.com/allenai/VILA).

| Comments: | 13 pages, 5 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00676](https://arxiv.org/abs/2106.00676) [cs.CL]** |
|           | (or **[arXiv:2106.00676v1](https://arxiv.org/abs/2106.00676v1) [cs.CL]** for this version) |








# 2021-06-01

[Return to Index](#Index)



<h2 id="2021-06-01-1">1. An Attention Free Transformer
</h2>

Title: [An Attention Free Transformer](https://arxiv.org/abs/2105.14103)

Authors: [Shuangfei Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+S), [Walter Talbott](https://arxiv.org/search/cs?searchtype=author&query=Talbott%2C+W), [Nitish Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+N), [Chen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Hanlin Goh](https://arxiv.org/search/cs?searchtype=author&query=Goh%2C+H), [Ruixiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Josh Susskind](https://arxiv.org/search/cs?searchtype=author&query=Susskind%2C+J)

> We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14103](https://arxiv.org/abs/2105.14103) [cs.LG]** |
|           | (or **[arXiv:2105.14103v1](https://arxiv.org/abs/2105.14103v1) [cs.LG]** for this version) |



<h2 id="2021-06-01-2">2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering
</h2>

Title: [LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](https://arxiv.org/abs/2105.14300)

Authors: [Zujie Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Haifeng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H), [Jiaying Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Most existing Visual Question Answering (VQA) systems tend to overly rely on language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer's loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.

| Comments: | Accepted by ACM SIGIR 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| DOI:      | [10.1145/3404835.3462981](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3404835.3462981&v=a1843a78) |
| Cite as:  | **[arXiv:2105.14300](https://arxiv.org/abs/2105.14300) [cs.CV]** |
|           | (or **[arXiv:2105.14300v1](https://arxiv.org/abs/2105.14300v1) [cs.CV]** for this version) |





<h2 id="2021-06-01-3">3. Re-evaluating Word Mover's Distance
</h2>

Title: [Re-evaluating Word Mover's Distance](https://arxiv.org/abs/2105.14403)

Authors: [Ryoma Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+R), [Makoto Yamada](https://arxiv.org/search/cs?searchtype=author&query=Yamada%2C+M), [Hisashi Kashima](https://arxiv.org/search/cs?searchtype=author&query=Kashima%2C+H)

> The word mover's distance (WMD) is a fundamental technique for measuring the similarity of two documents. As the crux of WMD, it can take advantage of the underlying geometry of the word space by employing an optimal transport formulation. The original study on WMD reported that WMD outperforms classical baselines such as bag-of-words (BOW) and TF-IDF by significant margins in various datasets. In this paper, we point out that the evaluation in the original study could be misleading. We re-evaluate the performances of WMD and the classical baselines and find that the classical baselines are competitive with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. However, this result is not intuitive. WMD should be superior to BOW because WMD can take the underlying geometry into account, whereas BOW cannot. Our analysis shows that this is due to the high-dimensional nature of the underlying metric. We find that WMD in high-dimensional spaces behaves more similarly to BOW than in low-dimensional spaces due to the curse of dimensionality.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14403](https://arxiv.org/abs/2105.14403) [cs.LG]** |
|           | (or **[arXiv:2105.14403v1](https://arxiv.org/abs/2105.14403v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-4">4. Memory-Efficient Differentiable Transformer Architecture Search
</h2>

Title: [Memory-Efficient Differentiable Transformer Architecture Search](https://arxiv.org/abs/2105.14669)

Authors: [Yuekai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Zhihua Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.

| Comments: | Accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.14669](https://arxiv.org/abs/2105.14669) [cs.LG]** |
|           | (or **[arXiv:2105.14669v1](https://arxiv.org/abs/2105.14669v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-5">5. Why does CTC result in peaky behavior?
</h2>

Title: [Why does CTC result in peaky behavior?](https://arxiv.org/abs/2105.14849)

Authors: [Albert Zeyer](https://arxiv.org/search/cs?searchtype=author&query=Zeyer%2C+A), [Ralf Schlüter](https://arxiv.org/search/cs?searchtype=author&query=Schlüter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Sound (cs.SD); Audio and Speech Processing (eess.AS); Statistics Theory (math.ST) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14849](https://arxiv.org/abs/2105.14849) [cs.LG]** |
|           | (or **[arXiv:2105.14849v1](https://arxiv.org/abs/2105.14849v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-6">6. Grammatical Error Correction as GAN-like Sequence Labeling
</h2>

Title: [Grammatical Error Correction as GAN-like Sequence Labeling](https://arxiv.org/abs/2105.14209)

Authors: [Kevin Parnow](https://arxiv.org/search/cs?searchtype=author&query=Parnow%2C+K), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> In Grammatical Error Correction (GEC), sequence labeling models enjoy fast inference compared to sequence-to-sequence models; however, inference in sequence labeling GEC models is an iterative process, as sentences are passed to the model for multiple rounds of correction, which exposes the model to sentences with progressively fewer errors at each round. Traditional GEC models learn from sentences with fixed error rates. Coupling this with the iterative correction process causes a mismatch between training and inference that affects final performance. In order to address this mismatch, we propose a GAN-like sequence labeling model, which consists of a grammatical error detector as a discriminator and a grammatical error labeler with Gumbel-Softmax sampling as a generator. By sampling from real error distributions, our errors are more genuine compared to traditional synthesized GEC errors, thus alleviating the aforementioned mismatch and allowing for better training. Our results on several evaluation benchmarks demonstrate that our proposed approach is effective and improves the previous state-of-the-art baseline.

| Comments: | Accepted by ACL21, Findings                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14209](https://arxiv.org/abs/2105.14209) [cs.CL]** |
|           | (or **[arXiv:2105.14209v1](https://arxiv.org/abs/2105.14209v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-7">7. Predictive Representation Learning for Language Modeling
</h2>

Title: [Predictive Representation Learning for Language Modeling](https://arxiv.org/abs/2105.14214)

Authors: [Qingfeng Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Q), [Luke Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+L), [Martha White](https://arxiv.org/search/cs?searchtype=author&query=White%2C+M), [Alona Fyshe](https://arxiv.org/search/cs?searchtype=author&query=Fyshe%2C+A)

> To effectively perform the task of next-word prediction, long short-term memory networks (LSTMs) must keep track of many types of information. Some information is directly related to the next word's identity, but some is more secondary (e.g. discourse-level features or features of downstream words). Correlates of secondary information appear in LSTM representations even though they are not part of an \emph{explicitly} supervised prediction task. In contrast, in reinforcement learning (RL), techniques that explicitly supervise representations to predict secondary information have been shown to be beneficial. Inspired by that success, we propose Predictive Representation Learning (PRL), which explicitly constrains LSTMs to encode specific predictions, like those that might need to be learned implicitly. We show that PRL 1) significantly improves two strong language modeling methods, 2) converges more quickly, and 3) performs better when data is limited. Our work shows that explicitly encoding a simple predictive task facilitates the search for a more effective language model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14214](https://arxiv.org/abs/2105.14214) [cs.CL]** |
|           | (or **[arXiv:2105.14214v1](https://arxiv.org/abs/2105.14214v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-8">8. Korean-English Machine Translation with Multiple Tokenization Strategy
</h2>

Title: [Korean-English Machine Translation with Multiple Tokenization Strategy](https://arxiv.org/abs/2105.14274)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> This study was conducted to find out how tokenization methods affect the training results of machine translation models. In this work, character tokenization, morpheme tokenization, and BPE tokenization were applied to Korean as the source language and English as the target language respectively, and the comparison experiment was conducted by repeating 50,000 epochs of each 9 models using the Transformer neural network. As a result of measuring the BLEU scores of the experimental models, the model that applied BPE tokenization to Korean and morpheme tokenization to English recorded 35.73, showing the best performance.

| Comments: | KCC2021 Undergraduate/Junior Thesis Competition              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14274](https://arxiv.org/abs/2105.14274) [cs.CL]** |
|           | (or **[arXiv:2105.14274v1](https://arxiv.org/abs/2105.14274v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-9">9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models
</h2>

Title: [Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](https://arxiv.org/abs/2105.14277)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> Intrinsic evaluation by humans for the performance of natural language generation models is conducted to overcome the fact that the quality of generated sentences cannot be fully represented by only extrinsic evaluation. Nevertheless, existing intrinsic evaluations have a large score deviation according to the evaluator's criteria. In this paper, we propose Grammar Accuracy Evaluation (GAE) that can provide specific evaluating criteria. As a result of analyzing the quality of machine translation by BLEU and GAE, it was confirmed that the BLEU score does not represent the absolute performance of machine translation models and that GAE compensates for the shortcomings of BLEU with a flexible evaluation on alternative synonyms and changes in sentence structure.

| Comments: | Journal of KIISE                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14277](https://arxiv.org/abs/2105.14277) [cs.CL]** |
|           | (or **[arXiv:2105.14277v1](https://arxiv.org/abs/2105.14277v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-10">10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search
</h2>

Title: [NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](https://arxiv.org/abs/2105.14444)

Authors: [Jin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Jian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> While pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.

| Comments: | Accepted by KDD 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| DOI:      | [10.1145/3447548.3467262](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467262&v=2e2b005f) |
| Cite as:  | **[arXiv:2105.14444](https://arxiv.org/abs/2105.14444) [cs.CL]** |
|           | (or **[arXiv:2105.14444v1](https://arxiv.org/abs/2105.14444v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-11">11. Pre-training Universal Language Representation
</h2>

Title: [Pre-training Universal Language Representation](https://arxiv.org/abs/2105.14478)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.

| Comments: | Accepted by ACL-IJCNLP 2021 main conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14478](https://arxiv.org/abs/2105.14478) [cs.CL]** |
|           | (or **[arXiv:2105.14478v1](https://arxiv.org/abs/2105.14478v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-12">12. Fast Nearest Neighbor Machine Translation
</h2>

Title: [Fast Nearest Neighbor Machine Translation](https://arxiv.org/abs/2105.14528)

Authors: [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiayu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Though nearest neighbor Machine Translation (kNN-MT) \cite{khandelwal2020nearest} has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-order slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-order faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications.\footnote{Code is available at \url{[this https URL](https://github.com/ShannonAI/fast-knn-nmt).}}

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14528](https://arxiv.org/abs/2105.14528) [cs.CL]** |
|           | (or **[arXiv:2105.14528v1](https://arxiv.org/abs/2105.14528v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-13">13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation
</h2>


Title: [HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](https://arxiv.org/abs/2105.14600)

Authors: [Ayan Sengupta](https://arxiv.org/search/cs?searchtype=author&query=Sengupta%2C+A), [Sourabh Kumar Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+S+K), [Tanmoy Chakraborty](https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+T), [Md Shad Akhtar](https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+M+S)

> Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentence-level semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning).

| Comments: | 15 pages, 13 tables, 6 Figures. Accepted at ACL-IJCNLP-2021 (Findings) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14600](https://arxiv.org/abs/2105.14600) [cs.CL]** |
|           | (or **[arXiv:2105.14600v1](https://arxiv.org/abs/2105.14600v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-14">14. Attention Flows are Shapley Value Explanations
</h2>


Title: [Attention Flows are Shapley Value Explanations](https://arxiv.org/abs/2105.14652)

Authors: [Kawin Ethayarajh](https://arxiv.org/search/cs?searchtype=author&query=Ethayarajh%2C+K), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D)

> Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that -- save for the degenerate case -- attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values -- which has driven their adoption among the ML community -- we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14652](https://arxiv.org/abs/2105.14652) [cs.CL]** |
|           | (or **[arXiv:2105.14652v1](https://arxiv.org/abs/2105.14652v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-15">15. G-Transformer for Document-level Machine Translation
</h2>


Title: [G-Transformer for Document-level Machine Translation](https://arxiv.org/abs/2105.14761)

Authors: [Guangsheng Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+G), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Zhiyang Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both non-pretraining and pre-training settings on three benchmark datasets.

| Comments: | Accepted by ACL2021 main track                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14761](https://arxiv.org/abs/2105.14761) [cs.CL]** |
|           | (or **[arXiv:2105.14761v1](https://arxiv.org/abs/2105.14761v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-16">16. On Compositional Generalization of Neural Machine Translation
</h2>


Title: [On Compositional Generalization of Neural Machine Translation](https://arxiv.org/abs/2105.14802)

Authors: [Yafu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Yulong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics.

| Comments: | To appear at the ACL 2021 main conference                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14802](https://arxiv.org/abs/2105.14802) [cs.CL]** |
|           | (or **[arXiv:2105.14802v1](https://arxiv.org/abs/2105.14802v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-17">17. Transfer Learning for Sequence Generation: from Single-source to Multi-source
</h2>


Title: [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://arxiv.org/abs/2105.14809)

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly.

| Comments: | ACL2021 main track long paper                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14809](https://arxiv.org/abs/2105.14809) [cs.CL]** |
|           | (or **[arXiv:2105.14809v1](https://arxiv.org/abs/2105.14809v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-18">18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models
</h2>


Title: [Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](https://arxiv.org/abs/2105.14813)

Authors: [Chong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Cenyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Xiaoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pretraining strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14813](https://arxiv.org/abs/2105.14813) [cs.CL]** |
|           | (or **[arXiv:2105.14813v1](https://arxiv.org/abs/2105.14813v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-19">19. Effective Batching for Recurrent Neural Network Grammars
</h2>


Title: [Effective Batching for Recurrent Neural Network Grammars](https://arxiv.org/abs/2105.14822)

Authors: [Hiroshi Noji](https://arxiv.org/search/cs?searchtype=author&query=Noji%2C+H), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y)

> As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our RNNG implementation is available at [this https URL](https://github.com/aistairc/rnng-pytorch/).

| Comments: | Findings of ACL: ACL-IJCNLP 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14822](https://arxiv.org/abs/2105.14822) [cs.CL]** |
|           | (or **[arXiv:2105.14822v1](https://arxiv.org/abs/2105.14822v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-20">20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models
</h2>


Title: [Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](https://arxiv.org/abs/2105.14839)

Authors: [David Peer](https://arxiv.org/search/cs?searchtype=author&query=Peer%2C+D), [Sebastian Stabinger](https://arxiv.org/search/cs?searchtype=author&query=Stabinger%2C+S), [Stefan Engl](https://arxiv.org/search/cs?searchtype=author&query=Engl%2C+S), [Antonio Rodriguez-Sanchez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez-Sanchez%2C+A)

> Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different NLP tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production and is a limiting factor for the deployment into embedded devices. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models, but as it needs an additional expensive pre-training phase, this solution is computationally expensive and can be financially prohibitive for smaller academic research groups. Another solution is to use layer-wise pruning methods, which reach high compression rates for transformer models and avoids the computational load of the pre-training distillation stage. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, greedy layer pruning (GLP) is introduced to (1) outperform current state-of-the-art for layer-wise pruning (2) close the performance gap when compared to knowledge distillation, while (3) using only a modest budget. More precisely, with the methodology presented it is possible to prune and evaluate competitive models on the whole GLUE benchmark with a budget of just $300. Our source code is available on [this https URL](https://github.com/deepopinion/greedy-layer-pruning).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14839](https://arxiv.org/abs/2105.14839) [cs.CL]** |
|           | (or **[arXiv:2105.14839v1](https://arxiv.org/abs/2105.14839v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-21">21. Verdi: Quality Estimation and Error Detection for Bilingual
</h2>


Title: [Verdi: Quality Estimation and Error Detection for Bilingual](https://arxiv.org/abs/2105.14878)

Authors: [Mingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Haijiang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Di Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+D), [Zixuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xiaoli Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Translation Quality Estimation is critical to reducing post-editing efforts in machine translation and to cross-lingual corpus cleaning. As a research problem, quality estimation (QE) aims to directly estimate the quality of translation in a given pair of source and target sentences, and highlight the words that need corrections, without referencing to golden translations. In this paper, we propose Verdi, a novel framework for word-level and sentence-level post-editing effort estimation for bilingual corpora. Verdi adopts two word predictors to enable diverse features to be extracted from a pair of sentences for subsequent quality estimation, including a transformer-based neural machine translation (NMT) model and a pre-trained cross-lingual language model (XLM). We exploit the symmetric nature of bilingual corpora and apply model-level dual learning in the NMT predictor, which handles a primal task and a dual task simultaneously with weight sharing, leading to stronger context prediction ability than single-direction NMT models. By taking advantage of the dual learning scheme, we further design a novel feature to directly encode the translated target information without relying on the source context. Extensive experiments conducted on WMT20 QE tasks demonstrate that our method beats the winner of the competition and outperforms other baseline methods by a great margin. We further use the sentence-level scores provided by Verdi to clean a parallel corpus and observe benefits on both model performance and training efficiency.

| Comments: | Accepted by The Web Conference 2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14878](https://arxiv.org/abs/2105.14878) [cs.CL]** |
|           | (or **[arXiv:2105.14878v1](https://arxiv.org/abs/2105.14878v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-22">22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation
</h2>


Title: [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://arxiv.org/abs/2105.14913)

Authors: [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.

| Comments: | Accepted into the main conference of ACL 2021. arXiv admin note: text overlap with [arXiv:2105.13072](https://arxiv.org/abs/2105.13072) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14913](https://arxiv.org/abs/2105.14913) [cs.CL]** |
|           | (or **[arXiv:2105.14913v1](https://arxiv.org/abs/2105.14913v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-23">23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?
</h2>


Title: [Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](https://arxiv.org/abs/2105.14940)

Authors: [Zae Myung Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Z+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D)

> Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of language-independent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a "black-box" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as "variance" or "confidence", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.

| Comments: | 10 pages, accepted at Findings of ACL 2021 (short)           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14940](https://arxiv.org/abs/2105.14940) [cs.CL]** |
|           | (or **[arXiv:2105.14940v1](https://arxiv.org/abs/2105.14940v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-24">24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
</h2>


Title: [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://arxiv.org/abs/2105.15071)

Authors: [Wei-Jen Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+W), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15071](https://arxiv.org/abs/2105.15071) [cs.CL]** |
|           | (or **[arXiv:2105.15071v1](https://arxiv.org/abs/2105.15071v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-25">25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation
</h2>


Title: [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://arxiv.org/abs/2105.15087)

Authors: [Eleftheria Briakou](https://arxiv.org/search/cs?searchtype=author&query=Briakou%2C+E), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15087](https://arxiv.org/abs/2105.15087) [cs.CL]** |
|           | (or **[arXiv:2105.15087v1](https://arxiv.org/abs/2105.15087v1) [cs.CL]** for this version) |

