# Daily arXiv: Machine Translation - October, 2020

# Index


- [2020-10-07](#2020-10-07)

  - [1. Multi-task Learning for Multilingual Neural Machine Translation](#2020-10-07-1)
  - [2. Do Explicit Alignments Robustly Improve Multilingual Encoders?](#2020-10-07-2)
  - [3. The Multilingual Amazon Reviews Corpus](#2020-10-07-3)
  - [4. On the Sparsity of Neural Machine Translation Models](#2020-10-07-4)
  - [5. On the Sub-Layer Functionalities of Transformer Decoder](#2020-10-07-5)
  - [6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](#2020-10-07-6)
  - [7. Analyzing Individual Neurons in Pre-trained Language Models](#2020-10-07-7)
  - [8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](#2020-10-07-8)
  - [9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](#2020-10-07-9)
  - [10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](#2020-10-07-10)
  - [11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](#2020-10-07-11)
  - [12. Inference Strategies for Machine Translation with Conditional Masking](#2020-10-07-12)
  - [13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](#2020-10-07-13)
  - [14. Guiding Attention for Self-Supervised Learning with Transformers](#2020-10-07-14)
  - [15. Adversarial Grammatical Error Correction](#2020-10-07-15)
  - [16. Efficient Inference For Neural Machine Translation](#2020-10-07-16)
  - [17. Iterative Domain-Repaired Back-Translation](#2020-10-07-17)
- [2020-10-06](#2020-10-06)

  - [1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](#2020-10-06-1)
  - [2. Transformer-Based Neural Text Generation with Syntactic Guidance](#2020-10-06-2)
  - [3. Second-Order NLP Adversarial Examples](#2020-10-06-3)
  - [4. GenAug: Data Augmentation for Finetuning Text Generators](#2020-10-06-4)
  - [5. Lifelong Language Knowledge Distillation](#2020-10-06-5)
  - [6. A Streaming Approach For Efficient Batched Beam Search](#2020-10-06-6)
  - [7. Self-training Improves Pre-training for Natural Language Understanding](#2020-10-06-7)
  - [8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](#2020-10-06-8)
- [2020-10-05](#2020-10-05)

  - [1. Nearest Neighbor Machine Translation](#2020-10-05-1)
  - [2. A Survey of the State of Explainable AI for Natural Language Processing](#2020-10-05-2)
  - [3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](#2020-10-05-3)
  - [4. Which *BERT? A Survey Organizing Contextualized Encoders](#2020-10-05-4)


- [2020-10-02](#2020-10-2)

  - [1. WeChat Neural Machine Translation Systems for WMT20](#2020-10-2-1)
- [2020-10-01](#2020-10-01)

  - [1. Rethinking Attention with Performers](#2020-10-01-1)
  - [2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](#2020-10-01-2)
  - [3. Can Automatic Post-Editing Improve NMT?](#2020-10-01-3)
  - [4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](#2020-10-01-4)
  - [5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation](#2020-10-01-5)
- [2020-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-09.md)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-10-07

[Return to Index](#Index)



<h2 id="2020-10-07-1">1. Multi-task Learning for Multilingual Neural Machine Translation</h2>

Title: [Multi-task Learning for Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.02523)

Authors: [Yiren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [ChengXiang Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+C), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H)

> While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02523](https://arxiv.org/abs/2010.02523) [cs.CL]** |
|           | (or **[arXiv:2010.02523v1](https://arxiv.org/abs/2010.02523v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-2">2. Do Explicit Alignments Robustly Improve Multilingual Encoders?</h2>

Title: [Do Explicit Alignments Robustly Improve Multilingual Encoders?](https://arxiv.org/abs/2010.02537)

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M)

> Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02537](https://arxiv.org/abs/2010.02537) [cs.CL]** |
|           | (or **[arXiv:2010.02537v1](https://arxiv.org/abs/2010.02537v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-3">3. The Multilingual Amazon Reviews Corpus</h2>

Title: [The Multilingual Amazon Reviews Corpus](https://arxiv.org/abs/2010.02573)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [György Szarvas](https://arxiv.org/search/cs?searchtype=author&query=Szarvas%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.

| Comments: | To appear in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02573](https://arxiv.org/abs/2010.02573) [cs.CL]** |
|           | (or **[arXiv:2010.02573v1](https://arxiv.org/abs/2010.02573v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-4">4. On the Sparsity of Neural Machine Translation Models</h2>

Title: [On the Sparsity of Neural Machine Translation Models](https://arxiv.org/abs/2010.02646)

Authors: [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02646](https://arxiv.org/abs/2010.02646) [cs.CL]** |
|           | (or **[arXiv:2010.02646v1](https://arxiv.org/abs/2010.02646v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-5">5. On the Sub-Layer Functionalities of Transformer Decoder</h2>

Title: [On the Sub-Layer Functionalities of Transformer Decoder](https://arxiv.org/abs/2010.02648)

Authors: [Yilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Prasad Tadepalli](https://arxiv.org/search/cs?searchtype=author&query=Tadepalli%2C+P), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages -- developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance -- a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.

| Comments: | Findings of the 2020 Conference on Empirical Methods in Natural Language Processing (Long) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02648](https://arxiv.org/abs/2010.02648) [cs.CL]** |
|           | (or **[arXiv:2010.02648v1](https://arxiv.org/abs/2010.02648v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-6">6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</h2>

Title: [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://arxiv.org/abs/2010.02684)

Authors: [Alvin Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+A), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A)

> This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a 'backdoor poisoning' attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier's predictions can be steered to the poison target class with success rates of >80% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.

| Comments: | Accepted in EMNLP-Findings 2020, Camera Ready Version        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2010.02684](https://arxiv.org/abs/2010.02684) [cs.CL]** |
|           | (or **[arXiv:2010.02684v1](https://arxiv.org/abs/2010.02684v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-7">7. Analyzing Individual Neurons in Pre-trained Language Models</h2>

Title: [Analyzing Individual Neurons in Pre-trained Language Models](https://arxiv.org/abs/2010.02695)

Authors: [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

> While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.

| Comments: | Accepted in EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02695](https://arxiv.org/abs/2010.02695) [cs.CL]** |
|           | (or **[arXiv:2010.02695v1](https://arxiv.org/abs/2010.02695v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-8">8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</h2>

Title: [Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](https://arxiv.org/abs/2010.02705)

Authors: [Minki Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+M), [Moonsu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Sung Ju Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J)

> We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.

| Comments: | 19 pages, 9 figures, EMNLP 2020                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02705](https://arxiv.org/abs/2010.02705) [cs.CL]** |
|           | (or **[arXiv:2010.02705v1](https://arxiv.org/abs/2010.02705v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-9">9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs</h2>

Title: [Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](https://arxiv.org/abs/2010.02847)

Authors: [Haiyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Alison Sneyd](https://arxiv.org/search/cs?searchtype=author&query=Sneyd%2C+A), [Mark Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+M)

> It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy "man is to computer-programmer as woman is to homemaker" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.

| Comments: | Accepted at AACL-IJCNLP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02847](https://arxiv.org/abs/2010.02847) [cs.CL]** |
|           | (or **[arXiv:2010.02847v1](https://arxiv.org/abs/2010.02847v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-10">10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</h2>

Title: [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://arxiv.org/abs/2010.02301)

Authors: [Xinyu Hua](https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X), [Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L)

> Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often "rambling" without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.

| Comments: | Accepted at EMNLP 2020 as a long paper                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02301](https://arxiv.org/abs/2010.02301) [cs.CL]** |
|           | (or **[arXiv:2010.02301v1](https://arxiv.org/abs/2010.02301v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-11">11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation</h2>

Title: [We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](https://arxiv.org/abs/2010.02339)

Authors: [Ashiqur R. KhudaBukhsh](https://arxiv.org/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R), [Rupak Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+R), [Mark S. Kamlet](https://arxiv.org/search/cs?searchtype=author&query=Kamlet%2C+M+S), [Tom M. Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell%2C+T+M)

> Polarization among US political parties, media and elites is a widely studied topic. Prominent lines of prior research across multiple disciplines have observed and analyzed growing polarization in social media. In this paper, we present a new methodology that offers a fresh perspective on interpreting polarization through the lens of machine translation. With a novel proposition that two sub-communities are speaking in two different \emph{languages}, we demonstrate that modern machine translation methods can provide a simple yet powerful and interpretable framework to understand the differences between two (or more) large-scale social media discussion data sets at the granularity of words. Via a substantial corpus of 86.6 million comments by 6.5 million users on over 200,000 news videos hosted by YouTube channels of four prominent US news networks, we demonstrate that simple word-level and phrase-level translation pairs can reveal deep insights into the current political divide -- what is \emph{black lives matter} to one can be \emph{all lives matter} to the other.

| Subjects: | **Computation and Language (cs.CL)**; Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02339](https://arxiv.org/abs/2010.02339) [cs.CL]** |
|           | (or **[arXiv:2010.02339v1](https://arxiv.org/abs/2010.02339v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-12">12. Inference Strategies for Machine Translation with Conditional Masking</h2>

Title: [Inference Strategies for Machine Translation with Conditional Masking](https://arxiv.org/abs/2010.02352)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard "mask-predict" algorithm, and provide analyses of its behavior on machine translation tasks.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02352](https://arxiv.org/abs/2010.02352) [cs.CL]** |
|           | (or **[arXiv:2010.02352v1](https://arxiv.org/abs/2010.02352v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-13">13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</h2>

Title: [Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](https://arxiv.org/abs/2010.02394)

Authors: [Lichao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L), [Congying Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+C), [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Tingting Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+T), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S), [Lifang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+L)

> Mixup is the latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT. To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named "mixup-transformer", for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUE benchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.

| Comments: | Accepted by COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02394](https://arxiv.org/abs/2010.02394) [cs.CL]** |
|           | (or **[arXiv:2010.02394v1](https://arxiv.org/abs/2010.02394v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-14">14. Guiding Attention for Self-Supervised Learning with Transformers</h2>

Title: [Guiding Attention for Self-Supervised Learning with Transformers](https://arxiv.org/abs/2010.02399)

Authors: [Ameet Deshpande](https://arxiv.org/search/cs?searchtype=author&query=Deshpande%2C+A), [Karthik Narasimhan](https://arxiv.org/search/cs?searchtype=author&query=Narasimhan%2C+K)

> In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.

| Comments: | Accepted to Findings of EMNLP, 2020                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02399](https://arxiv.org/abs/2010.02399) [cs.CL]** |
|           | (or **[arXiv:2010.02399v1](https://arxiv.org/abs/2010.02399v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-15">15. Adversarial Grammatical Error Correction</h2>

Title: [Adversarial Grammatical Error Correction](https://arxiv.org/abs/2010.02407)

Authors: [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V), [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D)

> Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs) have been successful in generating realistic texts across many different tasks by learning to directly minimize the difference between human-generated and synthetic text. In this work, we present an adversarial learning approach to GEC, using the generator-discriminator framework. The generator is a Transformer model, trained to produce grammatically correct sentences given grammatically incorrect ones. The discriminator is a sentence-pair classification model, trained to judge a given pair of grammatically incorrect-correct sentences on the quality of grammatical correction. We pre-train both the discriminator and the generator on parallel texts and then fine-tune them further using a policy gradient method that assigns high rewards to sentences which could be true corrections of the grammatically incorrect text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that Adversarial-GEC can achieve competitive GEC quality compared to NMT-based baselines.

| Comments: | 13 Pages, EMNLP 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02407](https://arxiv.org/abs/2010.02407) [cs.CL]** |
|           | (or **[arXiv:2010.02407v1](https://arxiv.org/abs/2010.02407v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-16">16. Efficient Inference For Neural Machine Translation</h2>

Title: [Efficient Inference For Neural Machine Translation](https://arxiv.org/abs/2010.02416)

Authors: [Yi-Te Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+Y), [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Yi-Hsiu Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Y), [Ilya Chatsviorkin](https://arxiv.org/search/cs?searchtype=author&query=Chatsviorkin%2C+I)

> Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02416](https://arxiv.org/abs/2010.02416) [cs.CL]** |
|           | (or **[arXiv:2010.02416v1](https://arxiv.org/abs/2010.02416v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-17">17. Iterative Domain-Repaired Back-Translation</h2>

Title: [Iterative Domain-Repaired Back-Translation](https://arxiv.org/abs/2010.02473)

Authors: [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02473](https://arxiv.org/abs/2010.02473) [cs.CL]** |
|           | (or **[arXiv:2010.02473v1](https://arxiv.org/abs/2010.02473v1) [cs.CL]** for this version) |







# 2020-10-06

[Return to Index](#Index)



<h2 id="2020-10-06-1">1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples</h2>

Title: [A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](https://arxiv.org/abs/2010.01345)

Authors: [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.

| Comments: | COLING 2020 Long Paper                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01345](https://arxiv.org/abs/2010.01345) [cs.CL]** |
|           | (or **[arXiv:2010.01345v1](https://arxiv.org/abs/2010.01345v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-2">2. Transformer-Based Neural Text Generation with Syntactic Guidance</h2>

Title: [Transformer-Based Neural Text Generation with Syntactic Guidance](https://arxiv.org/abs/2010.01737)

Authors: [Yinghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y) (Georgia Institute of Technology), [Rui Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+R) (Georgia Institute of Technology), [Isaac Rehg](https://arxiv.org/search/cs?searchtype=author&query=Rehg%2C+I) (Georgia Institute of Technology), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C) (Georgia Institute of Technology)

> We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.

| Comments: | 11 pages, 4 figures and 5 tables                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01737](https://arxiv.org/abs/2010.01737) [cs.CL]** |
|           | (or **[arXiv:2010.01737v1](https://arxiv.org/abs/2010.01737v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-3">3. Second-Order NLP Adversarial Examples</h2>

Title: [Second-Order NLP Adversarial Examples](https://arxiv.org/abs/2010.01770)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X)

> Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at [this https URL](https://github.com/jxmorris12/second-order-adversarial-examples).

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01770](https://arxiv.org/abs/2010.01770) [cs.CL]** |
|           | (or **[arXiv:2010.01770v2](https://arxiv.org/abs/2010.01770v2) [cs.CL]** for this version) |





<h2 id="2020-10-06-4">4. GenAug: Data Augmentation for Finetuning Text Generators</h2>

Title: [GenAug: Data Augmentation for Finetuning Text Generators](https://arxiv.org/abs/2010.01794)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Dongyeop Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+D), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.

| Comments: | EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available at [this https URL](https://github.com/styfeng/GenAug) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.01794](https://arxiv.org/abs/2010.01794) [cs.CL]** |
|           | (or **[arXiv:2010.01794v1](https://arxiv.org/abs/2010.01794v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-5">5. Lifelong Language Knowledge Distillation</h2>

Title: [Lifelong Language Knowledge Distillation](https://arxiv.org/abs/2010.02123)

Authors: [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Shang-Yu Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+S), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02123](https://arxiv.org/abs/2010.02123) [cs.CL]** |
|           | (or **[arXiv:2010.02123v1](https://arxiv.org/abs/2010.02123v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-6">6. A Streaming Approach For Efficient Batched Beam Search</h2>

Title: [A Streaming Approach For Efficient Batched Beam Search](https://arxiv.org/abs/2010.02164)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Violet Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+V), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically ``refills" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF) |
| Cite as:  | **[arXiv:2010.02164](https://arxiv.org/abs/2010.02164) [cs.CL]** |
|           | (or **[arXiv:2010.02164v1](https://arxiv.org/abs/2010.02164v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-7">7. Self-training Improves Pre-training for Natural Language Understanding</h2>

Title: [Self-training Improves Pre-training for Natural Language Understanding](https://arxiv.org/abs/2010.02194)

Authors: [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Beliz Gunel](https://arxiv.org/search/cs?searchtype=author&query=Gunel%2C+B), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02194](https://arxiv.org/abs/2010.02194) [cs.CL]** |
|           | (or **[arXiv:2010.02194v1](https://arxiv.org/abs/2010.02194v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-8">8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation</h2>

Title: [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.01667)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> To improve the performance of Neural Machine Translation~(NMT) for low-resource languages~(LRL), one effective strategy is to leverage parallel data from a related high-resource language~(HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate \emph{into} the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01667](https://arxiv.org/abs/2010.01667) [cs.CL]** |
|           | (or **[arXiv:2010.01667v1](https://arxiv.org/abs/2010.01667v1) [cs.CL]** for this version) |





# 2020-10-05

[Return to Index](#Index)



<h2 id="2020-10-05-1">1. Nearest Neighbor Machine Translation</h2>

Title: [Nearest Neighbor Machine Translation](https://arxiv.org/abs/2010.00710)

Authors: [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We introduce k-nearest-neighbor machine translation (kNN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.00710](https://arxiv.org/abs/2010.00710) [cs.CL]** |
|           | (or **[arXiv:2010.00710v1](https://arxiv.org/abs/2010.00710v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-2">2. A Survey of the State of Explainable AI for Natural Language Processing</h2>

Title: [A Survey of the State of Explainable AI for Natural Language Processing](https://arxiv.org/abs/2010.00711)

Authors: [Marina Danilevsky](https://arxiv.org/search/cs?searchtype=author&query=Danilevsky%2C+M), [Kun Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+K), [Ranit Aharonov](https://arxiv.org/search/cs?searchtype=author&query=Aharonov%2C+R), [Yannis Katsis](https://arxiv.org/search/cs?searchtype=author&query=Katsis%2C+Y), [Ban Kawas](https://arxiv.org/search/cs?searchtype=author&query=Kawas%2C+B), [Prithviraj Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+P)

> Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.

| Comments:    | To appear in AACL-IJCNLP 2020                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.00711](https://arxiv.org/abs/2010.00711) [cs.CL]** |
|              | (or **[arXiv:2010.00711v1](https://arxiv.org/abs/2010.00711v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-3">3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</h2>

Title: [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://arxiv.org/abs/2010.00784)

Authors: [Kristjan Arumae](https://arxiv.org/search/cs?searchtype=author&query=Arumae%2C+K), [Qing Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Parminder Bhatia](https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+P)

> Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.

| Comments: | arXiv admin note: text overlap with [arXiv:2004.03794](https://arxiv.org/abs/2004.03794) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.00784](https://arxiv.org/abs/2010.00784) [cs.CL]** |
|           | (or **[arXiv:2010.00784v1](https://arxiv.org/abs/2010.00784v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-4">4. Which *BERT? A Survey Organizing Contextualized Encoders</h2>

Title: [Which *BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/abs/2010.00854)

Authors: [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B)

> Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.00854](https://arxiv.org/abs/2010.00854) [cs.CL]** |
|           | (or **[arXiv:2010.00854v1](https://arxiv.org/abs/2010.00854v1) [cs.CL]** for this version) |



# 2020-10-02

[Return to Index](#Index)



<h2 id="2020-10-02-1">1. WeChat Neural Machine Translation Systems for WMT20</h2>

Title: [WeChat Neural Machine Translation Systems for WMT20](https://arxiv.org/abs/2010.00247)

Authors: [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Xianfeng Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Qinsong Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Q), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Sifan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H)

> We participate in the WMT 2020 shared news translation task on Chinese to English. Our system is based on the Transformer (Vaswani et al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese to English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.

| Comments: | Accepted at WMT 2020. Our Chinese to English system achieved the highest case-sensitive BLEU score among all submissions |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.00247](https://arxiv.org/abs/2010.00247) [cs.CL]** |
|           | (or **[arXiv:2010.00247v1](https://arxiv.org/abs/2010.00247v1) [cs.CL]** for this version) |



# 2020-10-01

[Return to Index](#Index)



<h2 id="2020-10-01-1">1. Rethinking Attention with Performers</h2>

Title: [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)

Authors: [Krzysztof Choromanski](https://arxiv.org/search/cs?searchtype=author&query=Choromanski%2C+K), [Valerii Likhosherstov](https://arxiv.org/search/cs?searchtype=author&query=Likhosherstov%2C+V), [David Dohan](https://arxiv.org/search/cs?searchtype=author&query=Dohan%2C+D), [Xingyou Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Andreea Gane](https://arxiv.org/search/cs?searchtype=author&query=Gane%2C+A), [Tamas Sarlos](https://arxiv.org/search/cs?searchtype=author&query=Sarlos%2C+T), [Peter Hawkins](https://arxiv.org/search/cs?searchtype=author&query=Hawkins%2C+P), [Jared Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+J), [Afroz Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+A), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [David Belanger](https://arxiv.org/search/cs?searchtype=author&query=Belanger%2C+D), [Lucy Colwell](https://arxiv.org/search/cs?searchtype=author&query=Colwell%2C+L), [Adrian Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+A)

> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.

| Comments: | 36 pages. This is an updated version of a previous submission which can be found at [arXiv:2006.03555](https://arxiv.org/abs/2006.03555). See [this https URL](https://github.com/google-research/google-research/tree/master/protein_lm) for protein language model code, and [this https URL](https://github.com/google-research/google-research/tree/master/performer) for Performer code |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.14794](https://arxiv.org/abs/2009.14794) [cs.LG]** |
|           | (or **[arXiv:2009.14794v1](https://arxiv.org/abs/2009.14794v1) [cs.LG]** for this version) |





<h2 id="2020-10-01-2">2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</h2>

Title: [Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](https://arxiv.org/abs/2009.14304)

Authors: [Saurabh Kulshreshtha](https://arxiv.org/search/cs?searchtype=author&query=Kulshreshtha%2C+S), [José Luis Redondo-García](https://arxiv.org/search/cs?searchtype=author&query=Redondo-García%2C+J+L), [Ching-Yun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C)

> Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work proposes several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs. dictionary-based supervision and rotational vs. fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.

| Comments: | Accepted as a long paper in Findings of EMNLP 2020           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14304](https://arxiv.org/abs/2009.14304) [cs.CL]** |
|           | (or **[arXiv:2009.14304v1](https://arxiv.org/abs/2009.14304v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-3">3. Can Automatic Post-Editing Improve NMT?</h2>

Title: [Can Automatic Post-Editing Improve NMT?](https://arxiv.org/abs/2009.14395)

Authors: [Shamil Chollampatt](https://arxiv.org/search/cs?searchtype=author&query=Chollampatt%2C+S), [Raymond Hendy Susanto](https://arxiv.org/search/cs?searchtype=author&query=Susanto%2C+R+H), [Liling Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L), [Ewa Szymanska](https://arxiv.org/search/cs?searchtype=author&query=Szymanska%2C+E)

> Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at [this https URL](https://github.com/shamilcm/pedra).

| Comments: | In EMNLP 2020                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14395](https://arxiv.org/abs/2009.14395) [cs.CL]** |
|           | (or **[arXiv:2009.14395v1](https://arxiv.org/abs/2009.14395v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-4">4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment</h2>

Title: [Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](https://arxiv.org/abs/2009.14510)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data.

| Comments: | EMNLP-2020 Long Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.14510](https://arxiv.org/abs/2009.14510) [cs.CL]** |
|           | (or **[arXiv:2009.14510v1](https://arxiv.org/abs/2009.14510v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-5">5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation</h2>

Title: [On Romanization for Model Transfer Between Scripts in Neural Machine Translation](https://arxiv.org/abs/2009.14824)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.

| Comments: | accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14824](https://arxiv.org/abs/2009.14824) [cs.CL]** |
|           | (or **[arXiv:2009.14824v1](https://arxiv.org/abs/2009.14824v1) [cs.CL]** for this version) |

