# Daily arXiv: Machine Translation - Mar., 2020

# Index

- [2020-04-21](#2020-04-21)
  - [1. The Cost of Training NLP Models: A Concise Overview](#2020-04-21-1)
  - [2. Adversarial Training for Large Neural Language Models](#2020-04-21-2)
  - [3. A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT](#2020-04-21-3)
  - [4. MPNet: Masked and Permuted Pre-training for Language Understanding](#2020-04-21-4)
  - [5. PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation](#2020-04-21-5)
- [2020-04-20](#2020-04-20)
  - [1. Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings](#2020-04-20-1)
  - [2. Understanding the Difficulty of Training Transformers](#2020-04-20-2)
  - [3. Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation](#2020-04-20-3)
  - [4. Batch Clustering for Multilingual News Streaming](#2020-04-20-4)
- [2020-04-17](#2020-04-17)
  - [1. Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation](#2020-04-17-1)
  - [2. Non-Autoregressive Machine Translation with Latent Alignments](#2020-04-17-2)
  - [3. Do sequence-to-sequence VAEs learn global features of sentences?](#2020-04-17-3)
  - [4. Cross-lingual Contextualized Topic Models with Zero-shot Learning](#2020-04-17-4)
- [2020-04-16](#2020-04-16)
  - [1. A hybrid classical-quantum workflow for natural language processing](#2020-04-16-1)
  - [2. lamBERT: Language and Action Learning Using Multimodal BERT](#2020-04-16-2)
  - [3. Balancing Training for Multilingual Neural Machine Translation](#2020-04-16-3)
  - [4. PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](#2020-04-16-4)
  - [5. Document-level Representation Learning using Citation-informed Transformers](#2020-04-16-5)
- [2020-04-15](#2020-04-15)
  - [1.Code Completion using Neural Attention and Byte Pair Encoding ](#2020-04-15-1)
  - [2. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](#2020-04-15-2)
  - [3. What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models](#2020-04-15-3)
  - [4. Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders](#2020-04-15-4)
- [2020-04-14](#2020-04-14)
  - [1. On the Language Neutrality of Pre-trained Multilingual Representations](#2020-04-14-1)
  - [2. Joint translation and unit conversion for end-to-end localization](#2020-04-14-2)
  - [3. When Does Unsupervised Machine Translation Work?](#2020-04-14-3)
- [2020-04-13](#2020-04-13)
  - [1. An In-depth Walkthrough on Evolution of Neural Machine Translation](#2020-04-13-1)
  - [2. Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data](#2020-04-13-2)
  - [3. Automated Spelling Correction for Clinical Text Mining in Russian](#2020-04-13-3)
  - [4. Longformer: The Long-Document Transformer](#2020-04-13-4)
- [2020-04-10](#2020-04-10-1)
  - [1. Learning to Scale Multilingual Representations for Vision-Language Tasks](#2020-04-10-1)
  - [2. On optimal transformer depth for low-resource language translation](#2020-04-10-2)
  - [3. Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](#2020-04-10-3)
  - [4. Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios](#2020-04-10-4)
  - [5. Translation Artifacts in Cross-lingual Transfer Learning](#2020-04-10-5)
- [2020-04-09](#2020-04-09)
  - [1. Re-translation versus Streaming for Simultaneous Translation](#2020-04-09-1)
  - [2. Dynamic Data Selection and Weighting for Iterative Back-Translation](#2020-04-09-2)
  - [3. Byte Pair Encoding is Suboptimal for Language Model Pretraining](#2020-04-09-3)
  - [4. Improving BERT with Self-Supervised Attention](#2020-04-09-4)
  - [5. Explicit Reordering for Neural Machine Translation](#2020-04-09-5)
  - [6. Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation](#2020-04-09-6)
- [2020-04-08](#2020-04-08)
  - [1. Multilingual enrichment of disease biomedical ontologies](#2020-04-08-1)
  - [2. Unsupervised Neural Machine Translation with Indirect Supervision](#2020-04-08-2)
  - [3. Self-Induced Curriculum Learning in Neural Machine Translation](#2020-04-08-3)
  - [4. Machine Translation with Unsupervised Length-Constraints](#2020-04-08-4)
  - [5. Towards Multimodal Simultaneous Neural Machine Translation](#2020-04-08-5)
  - [6. Improving Fluency of Non-Autoregressive Machine Translation](#2020-04-08-6)
- [2020-04-07](#2020-04-07)
  - [1. Neural Machine Translation with Imbalanced Classes](#2020-04-07-1)
  - [2. Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation](#2020-04-07-2)
  - [3. Meta-Learning for Few-Shot NMT Adaptation](#2020-04-07-3)
  - [4. Applying Cyclical Learning Rate to Neural Machine Translation](#2020-04-07-4)
  - [5. Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation](#2020-04-07-5)
  - [6. Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech](#2020-04-07-6)
  - [7. Reference Language based Unsupervised Neural Machine Translation](#2020-04-07-7)
  - [8. Detecting and Understanding Generalization Barriers for Neural Machine Translation](#2020-04-07-8)
  - [9. AR: Auto-Repair the Synthetic Data for Neural Machine Translation](#2020-04-07-9)
  - [10. Understanding Learning Dynamics for Neural Machine Translation](#2020-04-07-10)
- [2020-04-06](#2020-04-06)
  - [1. XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation](#2020-04-06-1)
  - [2. Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation](#2020-04-06-2)
  - [3. Aligned Cross Entropy for Non-Autoregressive Machine Translation](#2020-04-06-3)
  - [4. A Set of Recommendations for Assessing Human-Machine Parity in Language Translation](#2020-04-06-4)
- [2020-04-03](#2020-04-03)
  - [1. Igbo-English Machine Translation: An Evaluation Benchmark](#2020-04-03-1)
  - [2. Mapping Languages: The Corpus of Global Language Use](#2020-04-03-2)
- [2020-04-02](#2020-04-02)
  - [1. Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study](#2020-04-02-1)
  - [2. Sign Language Translation with Transformers](#2020-04-02-2)
- [2020-04-01](#2020-04-01)
  - [1. The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe](#2020-04-01-1)
  - [2. MULTEXT-East](#2020-04-01-2)
  - [3. Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks](#2020-04-01-3)
  - [4. On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation](#2020-04-01-4)
  - [5. Evaluating Amharic Machine Translation](#2020-04-01-5)
  - [6. Low Resource Neural Machine Translation: A Benchmark for Five African Languages](#2020-04-01-6)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-04-21

[Return to Index](#Index)



<h2 id="2020-04-21-1">1. The Cost of Training NLP Models: A Concise Overview</h2>

Title: [The Cost of Training NLP Models: A Concise Overview](https://arxiv.org/abs/2004.08900)

Authors: [Or Sharir](https://arxiv.org/search/cs?searchtype=author&query=Sharir%2C+O), [Barak Peleg](https://arxiv.org/search/cs?searchtype=author&query=Peleg%2C+B), [Yoav Shoham](https://arxiv.org/search/cs?searchtype=author&query=Shoham%2C+Y)

> We review the cost of training large-scale language models, and the drivers of these costs. The intended audience includes engineers and scientists budgeting their model-training experiments, as well as non-practitioners trying to make sense of the economics of modern-day Natural Language Processing (NLP).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.08900](https://arxiv.org/abs/2004.08900) [cs.CL]** |
|           | (or **[arXiv:2004.08900v1](https://arxiv.org/abs/2004.08900v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-2">2. Adversarial Training for Large Neural Language Models</h2>

Title: [Adversarial Training for Large Neural Language Models](https://arxiv.org/abs/2004.08994)

Authors: [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Yu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Hoifung Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code and pre-trained models will be made publicly available at [this https URL](https://github.com/namisan/mt-dnn).

| Comments: | 13 pages, 9 tables, 2 figures                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.08994](https://arxiv.org/abs/2004.08994) [cs.CL]** |
|           | (or **[arXiv:2004.08994v1](https://arxiv.org/abs/2004.08994v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-3">3. A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT</h2>

Title: [A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT](https://arxiv.org/abs/2004.09205)

Authors: [Chi-Liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Tsung-Yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Hung-Yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> Recently, multilingual BERT works remarkably well on cross-lingual transfer tasks, superior to static non-contextualized word embeddings. In this work, we provide an in-depth experimental study to supplement the existing literature of cross-lingual ability. We compare the cross-lingual ability of non-contextualized and contextualized representation model with the same data. We found that datasize and context window size are crucial factors to the transferability. We also observe the language-specific information in multilingual BERT. By manipulating the latent representations, we can control the output languages of multilingual BERT, and achieve unsupervised token translation. We further show that based on the observation, there is a computationally cheap but effective approach to improve the cross-lingual ability of multilingual BERT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09205](https://arxiv.org/abs/2004.09205) [cs.CL]** |
|           | (or **[arXiv:2004.09205v1](https://arxiv.org/abs/2004.09205v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-4">4. MPNet: Masked and Permuted Pre-training for Language Understanding</h2>

Title: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)

Authors: [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Jianfeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\footnote{\url{[this https URL](https://github.com/microsoft/MPNet)}}.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09297](https://arxiv.org/abs/2004.09297) [cs.CL]** |
|           | (or **[arXiv:2004.09297v1](https://arxiv.org/abs/2004.09297v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-5">5. PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation</h2>

Title: [PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation](https://arxiv.org/abs/2004.09447)

Authors: [Vivek Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+V), [Mayank Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M)

> Code-mixing is the phenomenon of using more than one language in a sentence. It is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, it adds to the challenge of processing and understanding natural language to a much larger extent. This paper presents a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators. We are releasing the parallel corpus to facilitate future research opportunities in code-mixed machine translation. The annotated corpus is available at [this https URL](https://doi.org/10.5281/zenodo.3605597).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09447](https://arxiv.org/abs/2004.09447) [cs.CL]** |
|           | (or **[arXiv:2004.09447v1](https://arxiv.org/abs/2004.09447v1) [cs.CL]** for this version) |







# 2020-04-20

[Return to Index](#Index)



<h2 id="2020-04-20-1">1. Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings</h2>

Title: [Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings](https://arxiv.org/abs/2004.08243)

Authors: [Pratik Jawanpuria](https://arxiv.org/search/cs?searchtype=author&query=Jawanpuria%2C+P), [Mayank Meghwanshi](https://arxiv.org/search/cs?searchtype=author&query=Meghwanshi%2C+M), [Bamdev Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+B)

> We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.

| Comments: | Accepted as a short paper in ACL 2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.08243](https://arxiv.org/abs/2004.08243) [cs.LG]** |
|           | (or **[arXiv:2004.08243v1](https://arxiv.org/abs/2004.08243v1) [cs.LG]** for this version) |





<h2 id="2020-04-20-2">2. Understanding the Difficulty of Training Transformers</h2>

Title: [Understanding the Difficulty of Training Transformers](https://arxiv.org/abs/2004.08249)

Authors: [Liyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> Transformers have been proved effective for many deep learning tasks. Training transformers, however, requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (the standard SGD fails to train Transformers effectively). In this paper, we study Transformer training from both theoretical and empirical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that substantially influences training. Specifically, we observe that for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable since it amplifies small parameter perturbations (e.g., parameter updates) and result in significant disturbances in the model output, yet a light dependency limits the potential of model training and can lead to an inferior trained model. Inspired by our analysis, we propose Admin (**A****d**aptive **m**odel **i****n**itialization) to stabilize the training in the early stage and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.08249](https://arxiv.org/abs/2004.08249) [cs.LG]** |
|           | (or **[arXiv:2004.08249v1](https://arxiv.org/abs/2004.08249v1) [cs.LG]** for this version) |





<h2 id="2020-04-20-3">3. Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation</h2>

Title: [Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation](https://arxiv.org/abs/2004.08053)

Authors: [Jordi Armengol-Estapé](https://arxiv.org/search/cs?searchtype=author&query=Armengol-Estapé%2C+J), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C)

> Introducing factors, that is to say, word features such as linguistic information referring to the source tokens, is known to improve the results of neural machine translation systems in certain settings, typically in recurrent architectures. This study proposes enhancing the current state-of-the-art neural machine translation architecture, the Transformer, so that it allows to introduce external knowledge. In particular, our proposed modification, the Factored Transformer, uses factors, either linguistic or semantic, that insert additional knowledge into the machine translation system. Apart from using different kinds of features, we study the effect of different architectural configurations. Specifically, we analyze the performance of combining words and features at the embedding level or at the encoder level, and we experiment with two different combination strategies. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both extremely low-resourced and very distant languages, and obtain an improvement of 1.2 BLEU. These improvements are achieved with linguistic and not with semantic information.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2004.08053](https://arxiv.org/abs/2004.08053) [cs.CL]** |
|              | (or **[arXiv:2004.08053v1](https://arxiv.org/abs/2004.08053v1) [cs.CL]** for this version) |





<h2 id="2020-04-20-4">4. Batch Clustering for Multilingual News Streaming</h2>

Title: [Batch Clustering for Multilingual News Streaming](https://arxiv.org/abs/2004.08123)

Authors: [Mathis Linger](https://arxiv.org/search/cs?searchtype=author&query=Linger%2C+M), [Mhamed Hajaiej](https://arxiv.org/search/cs?searchtype=author&query=Hajaiej%2C+M)

> Nowadays, digital news articles are widely available, published by various editors and often written in different languages. This large volume of diverse and unorganized information makes human reading very difficult or almost impossible. This leads to a need for algorithms able to arrange high amount of multilingual news into stories. To this purpose, we extend previous works on Topic Detection and Tracking, and propose a new system inspired from newsLens. We process articles per batch, looking for monolingual local topics which are then linked across time and languages. Here, we introduce a novel "replaying" strategy to link monolingual local topics into stories. Besides, we propose new fine tuned multilingual embedding using SBERT to create crosslingual stories. Our system gives monolingual state-of-the-art results on dataset of Spanish and German news and crosslingual state-of-the-art results on English, Spanish and German news.

| Comments:          | 7 pages, 2 figures                                           |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Journal reference: | Proceedings of Text2Story - Third Workshop on Narrative Extraction From Texts co-located with 42nd European Conference on Information Retrieval (ECIR 2020) Lisbon, Portugal, April 14th, 2020 |
| Cite as:           | **[arXiv:2004.08123](https://arxiv.org/abs/2004.08123) [cs.CL]** |
|                    | (or **[arXiv:2004.08123v1](https://arxiv.org/abs/2004.08123v1) [cs.CL]** for this version) |







# 2020-04-17

[Return to Index](#Index)



<h2 id="2020-04-17-1">1. Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation</h2>

Title: [Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation](https://arxiv.org/abs/2004.07324)

Authors: [Idriss Mghabbar](https://arxiv.org/search/cs?searchtype=author&query=Mghabbar%2C+I), [Pirashanth Ratnamogan](https://arxiv.org/search/cs?searchtype=author&query=Ratnamogan%2C+P)

> Lack of specialized data makes building a multi-domain neural machine translation tool challenging. Although emerging literature dealing with low resource languages starts to show promising results, most state-of-the-art models used millions of sentences. Today, the majority of multi-domain adaptation techniques are based on complex and sophisticated architectures that are not adapted for real-world applications. So far, no scalable method is performing better than the simple yet effective mixed-finetuning, i.e finetuning a generic model with a mix of all specialized data and generic data. In this paper, we propose a new training pipeline where knowledge distillation and multiple specialized teachers allow us to efficiently finetune a model without adding new costs at inference time. Our experiments demonstrated that our training pipeline allows improving the performance of multi-domain translation over finetuning in configurations with 2, 3, and 4 domains by up to 2 points in BLEU.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 24th European Conference on Artificial Intelligence (ECAI), 2020 |
| Cite as:           | **[arXiv:2004.07324](https://arxiv.org/abs/2004.07324) [cs.CL]** |
|                    | (or **[arXiv:2004.07324v1](https://arxiv.org/abs/2004.07324v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-2">2. Non-Autoregressive Machine Translation with Latent Alignments</h2>

Title: [Non-Autoregressive Machine Translation with Latent Alignments](https://arxiv.org/abs/2004.07437)

Authors: [Chitwan Saharia](https://arxiv.org/search/cs?searchtype=author&query=Saharia%2C+C), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W), [Saurabh Saxena](https://arxiv.org/search/cs?searchtype=author&query=Saxena%2C+S), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M)

> This paper investigates two latent alignment models for non-autoregressive machine translation, namely CTC and Imputer. CTC generates outputs in a single step, makes strong conditional independence assumptions about output variables, and marginalizes out latent alignments using dynamic programming. Imputer generates outputs in a constant number of steps, and approximately marginalizes out possible generation orders and latent alignments for training. These models are simpler than existing non-autoregressive methods, since they do not require output length prediction as a pre-process. In addition, our architecture is simpler than typical encoder-decoder architectures, since input-output cross attention is not used. On the competitive WMT'14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the baseline autoregressive Transformer with 27.8 BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07437](https://arxiv.org/abs/2004.07437) [cs.CL]** |
|           | (or **[arXiv:2004.07437v1](https://arxiv.org/abs/2004.07437v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-3">3. Do sequence-to-sequence VAEs learn global features of sentences?</h2>

Title: [Do sequence-to-sequence VAEs learn global features of sentences?](https://arxiv.org/abs/2004.07683)

Authors: [Tom Bosc](https://arxiv.org/search/cs?searchtype=author&query=Bosc%2C+T), [Pascal Vincent](https://arxiv.org/search/cs?searchtype=author&query=Vincent%2C+P)

> A longstanding goal in NLP is to compute global sentence representations. Such representations would be useful for sample-efficient semi-supervised learning and controllable text generation. To learn to represent global and local information separately, Bowman & al. (2016) proposed to train a sequence-to-sequence model with the variational auto-encoder (VAE) objective. What precisely is encoded in these latent variables expected to capture global features? We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we see that VAEs are prone to memorizing the first words and the sentence length, drastically limiting their usefulness. To alleviate this, we propose variants based on bag-of-words assumptions and language model pretraining. These variants learn latents that are more global: they are more predictive of topic or sentiment labels, and their reconstructions are more faithful to the labels of the original documents.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07683](https://arxiv.org/abs/2004.07683) [cs.CL]** |
|           | (or **[arXiv:2004.07683v1](https://arxiv.org/abs/2004.07683v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-4">4. Cross-lingual Contextualized Topic Models with Zero-shot Learning</h2>

Title: [Cross-lingual Contextualized Topic Models with Zero-shot Learning](https://arxiv.org/abs/2004.07737)

Authors: [Federico Bianchi](https://arxiv.org/search/cs?searchtype=author&query=Bianchi%2C+F), [Silvia Terragni](https://arxiv.org/search/cs?searchtype=author&query=Terragni%2C+S), [Dirk Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+D), [Debora Nozza](https://arxiv.org/search/cs?searchtype=author&query=Nozza%2C+D), [Elisabetta Fersini](https://arxiv.org/search/cs?searchtype=author&query=Fersini%2C+E)

> Many data sets in a domain (reviews, forums, news, etc.) exist in parallel languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model, i.e., our model learns topics on one language (here, English), and predicts them for documents in other languages. By using the text of the same document in different languages, we can evaluate the quality of the predictions. Our results show that topics are coherent and stable across languages, which suggests exciting future research directions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07737](https://arxiv.org/abs/2004.07737) [cs.CL]** |
|           | (or **[arXiv:2004.07737v1](https://arxiv.org/abs/2004.07737v1) [cs.CL]** for this version) |







# 2020-04-16

[Return to Index](#Index)



<h2 id="2020-04-16-1">1. A hybrid classical-quantum workflow for natural language processing</h2>

Title: [A hybrid classical-quantum workflow for natural language processing](https://arxiv.org/abs/2004.06800)

Authors: [Lee J. O'Riordan](https://arxiv.org/search/quant-ph?searchtype=author&query=O'Riordan%2C+L+J), [Myles Doyle](https://arxiv.org/search/quant-ph?searchtype=author&query=Doyle%2C+M), [Fabio Baruffa](https://arxiv.org/search/quant-ph?searchtype=author&query=Baruffa%2C+F), [Venkatesh Kannan](https://arxiv.org/search/quant-ph?searchtype=author&query=Kannan%2C+V)

> Natural language processing (NLP) problems are ubiquitous in classical computing, where they often require significant computational resources to infer sentence meanings. With the appearance of quantum computing hardware and simulators, it is worth developing methods to examine such problems on these platforms. In this manuscript we demonstrate the use of quantum computing models to perform NLP tasks, where we represent corpus meanings, and perform comparisons between sentences of a given structure. We develop a hybrid workflow for representing small and large scale corpus data sets to be encoded, processed, and decoded using a quantum circuit model. In addition, we provide our results showing the efficacy of the method, and release our developed toolkit as an open software suite.

| Comments: | For associated code, see [this https URL](https://github.com/ICHEC/QNLP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Quantum Physics (quant-ph)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.06800](https://arxiv.org/abs/2004.06800) [quant-ph]** |
|           | (or **[arXiv:2004.06800v1](https://arxiv.org/abs/2004.06800v1) [quant-ph]** for this version) |





<h2 id="2020-04-16-2">2. lamBERT: Language and Action Learning Using Multimodal BERT</h2>

Title: [lamBERT: Language and Action Learning Using Multimodal BERT](https://arxiv.org/abs/2004.07093)

Authors: [Kazuki Miyazawa](https://arxiv.org/search/cs?searchtype=author&query=Miyazawa%2C+K), [Tatsuya Aoki](https://arxiv.org/search/cs?searchtype=author&query=Aoki%2C+T), [Takato Horii](https://arxiv.org/search/cs?searchtype=author&query=Horii%2C+T), [Takayuki Nagai](https://arxiv.org/search/cs?searchtype=author&query=Nagai%2C+T)

> Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other models, such as the convolutional neural network-based model and the lamBERT model without pre-training.

| Comments: | 8 pages, 9 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.07093](https://arxiv.org/abs/2004.07093) [cs.LG]** |
|           | (or **[arXiv:2004.07093v1](https://arxiv.org/abs/2004.07093v1) [cs.LG]** for this version) |





<h2 id="2020-04-16-3">3. Balancing Training for Multilingual Neural Machine Translation</h2>

Title: [Balancing Training for Multilingual Neural Machine Translation](https://arxiv.org/abs/2004.06748)

Authors: [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.06748](https://arxiv.org/abs/2004.06748) [cs.CL]** |
|           | (or **[arXiv:2004.06748v2](https://arxiv.org/abs/2004.06748v2) [cs.CL]** for this version) |





<h2 id="2020-04-16-4">4. PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation</h2>

Title: [PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](https://arxiv.org/abs/2004.07159)

Authors: [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Chen Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W)

> Self-supervised pre-training has emerged as a powerful technique for natural language understanding and generation, such as BERT, MASS and BART. The existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. In this work, we present PALM which pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus especially for downstream generation conditioned on context, such as generative question answering and conversational response generation. PALM minimizes the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. With a novel pre-training scheme, PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on Gigaword and conversational response generation on Cornell Movie Dialogues.

| Comments: | arXiv admin note: text overlap with [arXiv:1910.10683](https://arxiv.org/abs/1910.10683), [arXiv:1910.13461](https://arxiv.org/abs/1910.13461) by other authors |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.07159](https://arxiv.org/abs/2004.07159) [cs.CL]** |
|           | (or **[arXiv:2004.07159v1](https://arxiv.org/abs/2004.07159v1) [cs.CL]** for this version) |





<h2 id="2020-04-16-5">5. Document-level Representation Learning using Citation-informed Transformers</h2>

Title: [Document-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2004.07180)

Authors: [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A), [Sergey Feldman](https://arxiv.org/search/cs?searchtype=author&query=Feldman%2C+S), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S)

> Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.07180](https://arxiv.org/abs/2004.07180) [cs.CL]** |
|           | (or **[arXiv:2004.07180v1](https://arxiv.org/abs/2004.07180v1) [cs.CL]** for this version) |









# 2020-04-15

[Return to Index](#Index)



<h2 id="2020-04-15-1">1. Code Completion using Neural Attention and Byte Pair Encoding</h2>

Title: [Code Completion using Neural Attention and Byte Pair Encoding](https://arxiv.org/abs/2004.06343)

Authors: [Youri Arkesteijn](https://arxiv.org/search/cs?searchtype=author&query=Arkesteijn%2C+Y), [Nikhil Saldanha](https://arxiv.org/search/cs?searchtype=author&query=Saldanha%2C+N), [Bastijn Kostense](https://arxiv.org/search/cs?searchtype=author&query=Kostense%2C+B)

*(Submitted on 14 Apr 2020)*

> In this paper, we aim to do code completion based on implementing a Neural Network from Li et. al.. Our contribution is that we use an encoding that is in-between character and word encoding called Byte Pair Encoding (BPE). We use this on the source code files treating them as natural text without first going through the abstract syntax tree (AST). We have implemented two models: an attention-enhanced LSTM and a pointer network, where the pointer network was originally introduced to solve out of vocabulary problems. We are interested to see if BPE can replace the need for the pointer network for code completion.

| Comments: | 4 pages, 4 figures, 1 table                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Software Engineering (cs.SE) |
| Cite as:  | [arXiv:2004.06343](https://arxiv.org/abs/2004.06343) [cs.CL] |
|           | (or [arXiv:2004.06343v1](https://arxiv.org/abs/2004.06343v1) [cs.CL] for this version) |





<h2 id="2020-04-15-2">2. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are</h2>

Title: [Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](https://arxiv.org/abs/2004.06358)

Authors: [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Matthias Paulik](https://arxiv.org/search/cs?searchtype=author&query=Paulik%2C+M)

*(Submitted on 14 Apr 2020)*

> Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.
> Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.

| Comments: | ACL 2020 theme track                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.06358](https://arxiv.org/abs/2004.06358) [cs.CL] |
|           | (or [arXiv:2004.06358v1](https://arxiv.org/abs/2004.06358v1) [cs.CL] for this version) |





<h2 id="2020-04-15-3">3. What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models</h2>

Title: [What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models](https://arxiv.org/abs/2004.06499)

Authors: [Wietse de Vries](https://arxiv.org/search/cs?searchtype=author&query=de+Vries%2C+W), [Andreas van Cranenburgh](https://arxiv.org/search/cs?searchtype=author&query=van+Cranenburgh%2C+A), [Malvina Nissim](https://arxiv.org/search/cs?searchtype=author&query=Nissim%2C+M)

*(Submitted on 14 Apr 2020)*

> Experiments with transfer learning on pre-trained language models such as BERT have shown that the layers of these models resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers of the network. We investigate to what extent these results also hold for a language other than English. For this we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, by considering the task of part-of-speech tagging in more detail, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations and it is therefore useful to combine information from different layers for best results, instead of selecting a single layer based on the best overall performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.06499](https://arxiv.org/abs/2004.06499) [cs.CL] |
|           | (or [arXiv:2004.06499v1](https://arxiv.org/abs/2004.06499v1) [cs.CL] for this version) |





<h2 id="2020-04-15-4">4. Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders</h2>

Title: [Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders](https://arxiv.org/abs/2004.06575)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

*(Submitted on 14 Apr 2020)*

> State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, and when adding new languages, without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:2004.06575](https://arxiv.org/abs/2004.06575) [cs.CL] |
|              | (or [arXiv:2004.06575v1](https://arxiv.org/abs/2004.06575v1) [cs.CL] for this version) |







# 2020-04-14

[Return to Index](#Index)



<h2 id="2020-04-14-1">1. On the Language Neutrality of Pre-trained Multilingual Representations</h2>

Title: [On the Language Neutrality of Pre-trained Multilingual Representations](https://arxiv.org/abs/2004.05160)

Authors: [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Rudolf Rosa](https://arxiv.org/search/cs?searchtype=author&query=Rosa%2C+R), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

*(Submitted on 9 Apr 2020)*

> Multilingual contextual embeddings, such as multilingual BERT (mBERT) and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the language-neutrality of mBERT with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. Contextual embeddings are still by default only moderately language-neutral, however, we show two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for languages, and second by fitting an explicit projection on small parallel data. In addition, we show how to reach state-of-the-art accuracy on language identification and word alignment in parallel sentences.

| Comments: | 11 pages, 3 figures. arXiv admin note: text overlap with [arXiv:1911.03310](https://arxiv.org/abs/1911.03310) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.05160](https://arxiv.org/abs/2004.05160) [cs.CL] |
|           | (or [arXiv:2004.05160v1](https://arxiv.org/abs/2004.05160v1) [cs.CL] for this version) |







<h2 id="2020-04-14-2">2. Joint translation and unit conversion for end-to-end localization</h2>

Title: [Joint translation and unit conversion for end-to-end localization](https://arxiv.org/abs/2004.05219)

Authors: [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Stanislas Lauly](https://arxiv.org/search/cs?searchtype=author&query=Lauly%2C+S), [Yaser Al-Onaizan](https://arxiv.org/search/cs?searchtype=author&query=Al-Onaizan%2C+Y)

*(Submitted on 10 Apr 2020)*

> A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which leads to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05219](https://arxiv.org/abs/2004.05219) [cs.CL] |
|           | (or [arXiv:2004.05219v1](https://arxiv.org/abs/2004.05219v1) [cs.CL] for this version) |







<h2 id="2020-04-14-3">3. When Does Unsupervised Machine Translation Work?</h2>

Title: [When Does Unsupervised Machine Translation Work?](https://arxiv.org/abs/2004.05516)

Authors: [Kelly Marchisio](https://arxiv.org/search/cs?searchtype=author&query=Marchisio%2C+K), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

*(Submitted on 12 Apr 2020)*

> Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which these methods succeed, and where they fail. We conduct an extensive empirical evaluation of unsupervised MT using dissimilar language pairs, dissimilar domains, diverse datasets, and authentic low-resource languages. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that random word embedding initialization can dramatically affect downstream translation performance. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05516](https://arxiv.org/abs/2004.05516) [cs.CL] |
|           | (or [arXiv:2004.05516v1](https://arxiv.org/abs/2004.05516v1) [cs.CL] for this version) |







# 2020-04-13

[Return to Index](#Index)



<h2 id="2020-04-13-1">1. An In-depth Walkthrough on Evolution of Neural Machine Translation</h2>

Title: [An In-depth Walkthrough on Evolution of Neural Machine Translation](https://arxiv.org/abs/2004.04902)

Authors: [Rohan Jagtap](https://arxiv.org/search/cs?searchtype=author&query=Jagtap%2C+R), [Dr. Sudhir N. Dhage](https://arxiv.org/search/cs?searchtype=author&query=Dhage%2C+D+S+N)

*(Submitted on 10 Apr 2020)*

> Neural Machine Translation (NMT) methodologies have burgeoned from using simple feed-forward architectures to the state of the art; viz. BERT model. The use cases of NMT models have been broadened from just language translations to conversational agents (chatbots), abstractive text summarization, image captioning, etc. which have proved to be a gem in their respective applications. This paper aims to study the major trends in Neural Machine Translation, the state of the art models in the domain and a high level comparison between them.

| Comments: | 10 pages, 10 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | [arXiv:2004.04902](https://arxiv.org/abs/2004.04902) [cs.CL] |
|           | (or [arXiv:2004.04902v1](https://arxiv.org/abs/2004.04902v1) [cs.CL] for this version) |





<h2 id="2020-04-13-2">2. Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data</h2>

Title: [Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data](https://arxiv.org/abs/2004.04972)

Authors: [Soumi Maiti](https://arxiv.org/search/cs?searchtype=author&query=Maiti%2C+S), [Erik Marchi](https://arxiv.org/search/cs?searchtype=author&query=Marchi%2C+E), [Alistair Conkie](https://arxiv.org/search/cs?searchtype=author&query=Conkie%2C+A)

*(Submitted on 10 Apr 2020)*

> We present progress towards bilingual Text-to-Speech which is able to transform a monolingual voice to speak a second language while preserving speaker voice quality. We demonstrate that a bilingual speaker embedding space contains a separate distribution for each language and that a simple transform in speaker space generated by the speaker embedding can be used to control the degree of accent of a synthetic voice in a language. The same transform can be applied even to monolingual speakers.
> In our experiments speaker data from an English-Spanish (Mexican) bilingual speaker was used, and the goal was to enable English speakers to speak Spanish and Spanish speakers to speak English. We found that the simple transform was sufficient to convert a voice from one language to the other with a high degree of naturalness. In one case the transformed voice outperformed a native language voice in listening tests. Experiments further indicated that the transform preserved many of the characteristics of the original voice. The degree of accent present can be controlled and naturalness is relatively consistent across a range of accent values.

| Comments: | Accepted to IEEE ICASSP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2004.04972](https://arxiv.org/abs/2004.04972) [cs.CL] |
|           | (or [arXiv:2004.04972v1](https://arxiv.org/abs/2004.04972v1) [cs.CL] for this version) |





<h2 id="2020-04-13-3">3. Automated Spelling Correction for Clinical Text Mining in Russian</h2>

Title: [Automated Spelling Correction for Clinical Text Mining in Russian](https://arxiv.org/abs/2004.04987)

Authors: [Ksenia Balabaeva](https://arxiv.org/search/cs?searchtype=author&query=Balabaeva%2C+K), [Anastasia Funkner](https://arxiv.org/search/cs?searchtype=author&query=Funkner%2C+A), [Sergey Kovalchuk](https://arxiv.org/search/cs?searchtype=author&query=Kovalchuk%2C+S)

*(Submitted on 10 Apr 2020)*

> The main goal of this paper is to develop a spell checker module for clinical text in Russian. The described approach combines string distance measure algorithms with technics of machine learning embedding methods. Our overall precision is 0.86, lexical precision - 0.975 and error precision is 0.74. We develop spell checker as a part of medical text mining tool regarding the problems of misspelling, negation, experiencer and temporality detection.

| Comments: | This paper is accepted for publication to MIE 2020 Conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04987](https://arxiv.org/abs/2004.04987) [cs.CL] |
|           | (or [arXiv:2004.04987v1](https://arxiv.org/abs/2004.04987v1) [cs.CL] for this version) |





<h2 id="2020-04-13-4">4. Longformer: The Long-Document Transformer</h2>

Title: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

Authors: [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E), [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A)

*(Submitted on 10 Apr 2020)*

> Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05150](https://arxiv.org/abs/2004.05150) [cs.CL] |
|           | (or [arXiv:2004.05150v1](https://arxiv.org/abs/2004.05150v1) [cs.CL] for this version) |





# 2020-04-10

[Return to Index](#Index)



<h2 id="2020-04-10-1">1. Learning to Scale Multilingual Representations for Vision-Language Tasks</h2>

Title: [Learning to Scale Multilingual Representations for Vision-Language Tasks](https://arxiv.org/abs/2004.04312)

Authors: [Andrea Burns](https://arxiv.org/search/cs?searchtype=author&query=Burns%2C+A), [Donghyun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Derry Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D), [Kate Saenko](https://arxiv.org/search/cs?searchtype=author&query=Saenko%2C+K), [Bryan A. Plummer](https://arxiv.org/search/cs?searchtype=author&query=Plummer%2C+B+A)

*(Submitted on 9 Apr 2020)*

> Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that represents many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for few. We use a novel masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04312](https://arxiv.org/abs/2004.04312) [cs.CV] |
|           | (or [arXiv:2004.04312v1](https://arxiv.org/abs/2004.04312v1) [cs.CV] for this version) |





<h2 id="2020-04-10-2">2. On optimal transformer depth for low-resource language translation</h2>

Title: [On optimal transformer depth for low-resource language translation](https://arxiv.org/abs/2004.04418)

Authors: [Elan van Biljon](https://arxiv.org/search/cs?searchtype=author&query=van+Biljon%2C+E), [Arnu Pretorius](https://arxiv.org/search/cs?searchtype=author&query=Pretorius%2C+A), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J)

*(Submitted on 9 Apr 2020)*

> Transformers have shown great promise as an approach to Neural Machine Translation (NMT) for low-resource languages. However, at the same time, transformer models remain difficult to optimize and require careful tuning of hyper-parameters to be useful in this setting. Many NMT toolkits come with a set of default hyper-parameters, which researchers and practitioners often adopt for the sake of convenience and avoiding tuning. These configurations, however, have been optimized for large-scale machine translation data sets with several millions of parallel sentences for European languages like English and French. In this work, we find that the current trend in the field to use very large models is detrimental for low-resource languages, since it makes training more difficult and hurts overall performance, confirming previous observations. We see our work as complementary to the Masakhane project ("Masakhane" means "We Build Together" in isiZulu.) In this spirit, low-resource NMT systems are now being built by the community who needs them the most. However, many in the community still have very limited access to the type of computational resources required for building extremely large models promoted by industrial research. Therefore, by showing that transformer models perform well (and often best) at low-to-moderate depth, we hope to convince fellow researchers to devote less computational resources, as well as time, to exploring overly large models during the development of these systems.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04418](https://arxiv.org/abs/2004.04418) [cs.CL] |
|           | (or [arXiv:2004.04418v1](https://arxiv.org/abs/2004.04418v1) [cs.CL] for this version) |





<h2 id="2020-04-10-3">3. Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</h2>

Title: [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://arxiv.org/abs/2004.04498)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 9 Apr 2020)*

> Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019).
> Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a `balanced' dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch.
> A known pitfall of transfer learning on new domains is `catastrophic forgetting', which we address both in adaptation and in inference. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. During inference we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of `black box` online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04498](https://arxiv.org/abs/2004.04498) [cs.CL] |
|           | (or [arXiv:2004.04498v1](https://arxiv.org/abs/2004.04498v1) [cs.CL] for this version) |





<h2 id="2020-04-10-4">4. Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios</h2>

Title: [Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios](https://arxiv.org/abs/2004.04507)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 9 Apr 2020)*

> Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not an adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04507](https://arxiv.org/abs/2004.04507) [cs.CL] |
|           | (or [arXiv:2004.04507v1](https://arxiv.org/abs/2004.04507v1) [cs.CL] for this version) |





<h2 id="2020-04-10-5">5. Translation Artifacts in Cross-lingual Transfer Learning</h2>

Title: [Translation Artifacts in Cross-lingual Transfer Learning](https://arxiv.org/abs/2004.04721)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

*(Submitted on 9 Apr 2020)*

> Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04721](https://arxiv.org/abs/2004.04721) [cs.CL] |
|           | (or [arXiv:2004.04721v1](https://arxiv.org/abs/2004.04721v1) [cs.CL] for this version) |



# 2020-04-09

[Return to Index](#Index)



<h2 id="2020-04-09-1">1. Re-translation versus Streaming for Simultaneous Translation</h2>

Title: [Re-translation versus Streaming for Simultaneous Translation](https://arxiv.org/abs/2004.03643)

Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G)

*(Submitted on 7 Apr 2020)*

> There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation's ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03643](https://arxiv.org/abs/2004.03643) [cs.CL] |
|           | (or [arXiv:2004.03643v1](https://arxiv.org/abs/2004.03643v1) [cs.CL] for this version) |





<h2 id="2020-04-09-2">2. Dynamic Data Selection and Weighting for Iterative Back-Translation</h2>

Title: [Dynamic Data Selection and Weighting for Iterative Back-Translation](https://arxiv.org/abs/2004.03672)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 7 Apr 2020)*

> Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality \textit{and} reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03672](https://arxiv.org/abs/2004.03672) [cs.CL] |
|           | (or [arXiv:2004.03672v1](https://arxiv.org/abs/2004.03672v1) [cs.CL] for this version) |





<h2 id="2020-04-09-3">3. Byte Pair Encoding is Suboptimal for Language Model Pretraining</h2>

Title: [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/abs/2004.03720)

Authors: [Kaj Bostrom](https://arxiv.org/search/cs?searchtype=author&query=Bostrom%2C+K), [Greg Durrett](https://arxiv.org/search/cs?searchtype=author&query=Durrett%2C+G)

*(Submitted on 7 Apr 2020)*

> The success of pretrained transformer language models in natural language processing has led to a wide range of different pretraining setups. These models employ a variety of subword tokenization methods, most notably byte pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE. We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.

| Comments:    | 4 pages, 2 figures                                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:2004.03720](https://arxiv.org/abs/2004.03720) [cs.CL] |
|              | (or [arXiv:2004.03720v1](https://arxiv.org/abs/2004.03720v1) [cs.CL] for this version) |





<h2 id="2020-04-09-4">4. Improving BERT with Self-Supervised Attention</h2>

Title: [Improving BERT with Self-Supervised Attention](https://arxiv.org/abs/2004.03808)

Authors: [Xiaoyu Kou](https://arxiv.org/search/cs?searchtype=author&query=Kou%2C+X), [Yaming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Yujing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Ce Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Yiren Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yunhai Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+Y), [Yan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jing Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+J)

*(Submitted on 8 Apr 2020)*

> One of the most popular paradigms of applying large, pre-trained NLP models such as BERT is to fine-tune it on a smaller dataset. However, one challenge remains as the fine-tuned model often overfits on smaller datasets. A symptom of this phenomenon is that irrelevant words in the sentences, even when they are obvious to humans, can substantially degrade the performance of these fine-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Specifically, SSA automatically generates weak, token-level attention labels iteratively by "probing" the fine-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their benefits. Empirically, on a variety of public datasets, we illustrate significant performance improvement using our SSA-enhanced BERT model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03808](https://arxiv.org/abs/2004.03808) [cs.CL] |
|           | (or [arXiv:2004.03808v1](https://arxiv.org/abs/2004.03808v1) [cs.CL] for this version) |





<h2 id="2020-04-09-5">5. Explicit Reordering for Neural Machine Translation</h2>

Title: [Explicit Reordering for Neural Machine Translation](https://arxiv.org/abs/2004.03818)

Authors: [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 8 Apr 2020)*

> In Transformer-based neural machine translation (NMT), the positional encoding mechanism helps the self-attention networks to learn the source representation with order dependency, which makes the Transformer-based NMT achieve state-of-the-art results for various translation tasks. However, Transformer-based NMT only adds representations of positions sequentially to word vectors in the input sentence and does not explicitly consider reordering information in this sentence. In this paper, we first empirically investigate the relationship between source reordering information and translation performance. The empirical findings show that the source input with the target order learned from the bilingual parallel dataset can substantially improve translation performance. Thus, we propose a novel reordering method to explicitly model this reordering information for the Transformer-based NMT. The empirical results on the WMT14 English-to-German, WAT ASPEC Japanese-to-English, and WMT17 Chinese-to-English translation tasks show the effectiveness of the proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03818](https://arxiv.org/abs/2004.03818) [cs.CL] |
|           | (or [arXiv:2004.03818v1](https://arxiv.org/abs/2004.03818v1) [cs.CL] for this version) |





<h2 id="2020-04-09-6">6. Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation</h2>

Title: [Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation](https://arxiv.org/abs/2004.04002)

Authors: [Stig-Arne Grönroos](https://arxiv.org/search/cs?searchtype=author&query=Grönroos%2C+S), [Sami Virpioja](https://arxiv.org/search/cs?searchtype=author&query=Virpioja%2C+S), [Mikko Kurimo](https://arxiv.org/search/cs?searchtype=author&query=Kurimo%2C+M)

*(Submitted on 8 Apr 2020)*

> There are several approaches for improving neural machine translation for low-resource languages: Monolingual data can be exploited via pretraining or data augmentation; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models; Subword segmentation and regularization techniques can be applied to ensure high coverage of the vocabulary. We review these approaches in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are related, with one being a very low-resource and the other a higher-resource language. We test various methods on three artificially restricted translation tasks---English to Estonian (low-resource) and Finnish (high-resource), English to Slovak and Czech, English to Danish and Swedish---and one real-world task, Norwegian to North Sámi and Finnish. The experiments show positive effects especially for scheduled multi-task learning, denoising autoencoder, and subword sampling.

| Comments: | 26 pages, 12 tables, 7 figures. Submitted (Mar 2020) to the Machine Translation journal Special Issue on Machine Translation for Low-Resource Languages (Springer) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04002](https://arxiv.org/abs/2004.04002) [cs.CL] |
|           | (or [arXiv:2004.04002v1](https://arxiv.org/abs/2004.04002v1) [cs.CL] for this version) |





# 2020-04-08

[Return to Index](#Index)



<h2 id="2020-04-08-1">1. Multilingual enrichment of disease biomedical ontologies</h2>

Title: [Multilingual enrichment of disease biomedical ontologies](https://arxiv.org/abs/2004.03181)

Authors: [Léo Bouscarrat](https://arxiv.org/search/q-bio?searchtype=author&query=Bouscarrat%2C+L) (QARMA, TALEP), [Antoine Bonnefoy](https://arxiv.org/search/q-bio?searchtype=author&query=Bonnefoy%2C+A), [Cécile Capponi](https://arxiv.org/search/q-bio?searchtype=author&query=Capponi%2C+C) (LIF, QARMA), [Carlos Ramisch](https://arxiv.org/search/q-bio?searchtype=author&query=Ramisch%2C+C) (TALEP)

*(Submitted on 7 Apr 2020)*

> Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects: coverage and quality. We look at the coverage of two biomedical ontologies focusing on diseases with respect to Wikidata for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both ontologies, plus Arabic, Chinese and Russian for the second one. We first use direct links between Wikidata and the studied ontologies and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to Wikidata with a commercial machine translation tool, here Google Cloud Translation.

| Subjects:          | **Quantitative Methods (q-bio.QM)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2nd workshop on MultilingualBIO: Multilingual Biomedical Text Processing, May 2020, Marseille, France |
| Cite as:           | [arXiv:2004.03181](https://arxiv.org/abs/2004.03181) [q-bio.QM] |
|                    | (or [arXiv:2004.03181v1](https://arxiv.org/abs/2004.03181v1) [q-bio.QM] for this version) |





<h2 id="2020-04-08-2">2. Unsupervised Neural Machine Translation with Indirect Supervision</h2>

Title: [Unsupervised Neural Machine Translation with Indirect Supervision](https://arxiv.org/abs/2004.03137)

Authors: [Hongxiao Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+H), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

*(Submitted on 7 Apr 2020)*

> Neural machine translation~(NMT) is ineffective for zero-resource languages. Recent works exploring the possibility of unsupervised neural machine translation (UNMT) with only monolingual data can achieve promising results. However, there are still big gaps between UNMT and NMT with parallel supervision. In this work, we introduce a multilingual unsupervised NMT (\method) framework to leverage weakly supervised signals from high-resource language pairs to zero-resource translation directions. More specifically, for unsupervised language pairs \texttt{En-De}, we can make full use of the information from parallel dataset \texttt{En-Fr} to jointly train the unsupervised translation directions all in one model. \method is based on multilingual models which require no changes to the standard unsupervised NMT. Empirical results demonstrate that \method significantly improves the translation quality by more than 3 BLEU score on six benchmark unsupervised translation directions.

| Subjects:      | **Computation and Language (cs.CL)**                         |
| -------------- | ------------------------------------------------------------ |
| Report number: | 10                                                           |
| Cite as:       | [arXiv:2004.03137](https://arxiv.org/abs/2004.03137) [cs.CL] |
|                | (or [arXiv:2004.03137v1](https://arxiv.org/abs/2004.03137v1) [cs.CL] for this version) |





<h2 id="2020-04-08-3">3. Self-Induced Curriculum Learning in Neural Machine Translation</h2>

Title: [Self-Induced Curriculum Learning in Neural Machine Translation](https://arxiv.org/abs/2004.03151)

Authors: [Dana Ruiter](https://arxiv.org/search/cs?searchtype=author&query=Ruiter%2C+D), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 7 Apr 2020)*

> Self-supervised neural machine translation (SS-NMT) learns how to extract/select suitable training data from comparable -- rather than parallel -- corpora and how to translate, in a way that the two tasks support each other in a virtuous circle. SS-NMT has been shown to be competitive with state-of-the-art unsupervised NMT. In this study we provide an in-depth analysis of the sampling choices the SS-NMT model takes during training. We show that, without it having been told to do so, the model selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) a denoising curriculum. We observe that the dynamics of the mutual-supervision of both system internal representation types is vital for the extraction and hence translation performance. We show that in terms of the human Gunning-Fog Readability index (GF), SS-NMT starts by extracting and learning from Wikipedia data suitable for high school (GF=10--11) and quickly moves towards content suitable for first year undergraduate students (GF=13).

| Comments: | 13 pages, 7 images                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.03151](https://arxiv.org/abs/2004.03151) [cs.CL] |
|           | (or [arXiv:2004.03151v1](https://arxiv.org/abs/2004.03151v1) [cs.CL] for this version) |





<h2 id="2020-04-08-4">4. Machine Translation with Unsupervised Length-Constraints</h2>

Title: [Machine Translation with Unsupervised Length-Constraints](https://arxiv.org/abs/2004.03176)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

*(Submitted on 7 Apr 2020)*

> We have seen significant improvements in machine translation due to the usage of deep learning. While the improvements in translation quality are impressive, the encoder-decoder architecture enables many more possibilities. In this paper, we explore one of these, the generation of constraint translation. We focus on length constraints, which are essential if the translation should be displayed in a given format. In this work, we propose an end-to-end approach for this task. Compared to a traditional method that first translates and then performs sentence compression, the text compression is learned completely unsupervised. By combining the idea with zero-shot multilingual machine translation, we are also able to perform unsupervised monolingual sentence compression. In order to fulfill the length constraints, we investigated several methods to integrate the constraints into the model. Using the presented technique, we are able to significantly improve the translation quality under constraints. Furthermore, we are able to perform unsupervised monolingual sentence compression.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.03176](https://arxiv.org/abs/2004.03176) [cs.CL] |
|           | (or [arXiv:2004.03176v1](https://arxiv.org/abs/2004.03176v1) [cs.CL] for this version) |





<h2 id="2020-04-08-5">5. Towards Multimodal Simultaneous Neural Machine Translation</h2>

Title: [Towards Multimodal Simultaneous Neural Machine Translation](https://arxiv.org/abs/2004.03180)

Authors: [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Tosho Hirasawa](https://arxiv.org/search/cs?searchtype=author&query=Hirasawa%2C+T), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

*(Submitted on 7 Apr 2020)*

> Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly harder than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT) which leverages visual information as an additional modality. Although the usefulness of images as an additional modality is moderate for full sentence translation, we verified, for the first time, its importance for simultaneous translation. Our experiments with the Multi30k dataset showed that MSNMT in a simultaneous setting significantly outperforms its text-only counterpart in situations where 5 or fewer input tokens are needed to begin translation. We then verified the importance of visual information during decoding by (a) performing an adversarial evaluation of MSNMT where we studied how models behave with incongruent input modality and (b) analyzing the image attention.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03180](https://arxiv.org/abs/2004.03180) [cs.CL] |
|           | (or [arXiv:2004.03180v1](https://arxiv.org/abs/2004.03180v1) [cs.CL] for this version) |





<h2 id="2020-04-08-6">6. Improving Fluency of Non-Autoregressive Machine Translation</h2>

Title: [Improving Fluency of Non-Autoregressive Machine Translation](https://arxiv.org/abs/2004.03227)

Authors: [Zdeněk Kasner](https://arxiv.org/search/cs?searchtype=author&query=Kasner%2C+Z), [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Jindřich Helcl](https://arxiv.org/search/cs?searchtype=author&query=Helcl%2C+J)

*(Submitted on 7 Apr 2020)*

> Non-autoregressive (nAR) models for machine translation (MT) manifest superior decoding speed when compared to autoregressive (AR) models, at the expense of impaired fluency of their outputs. We improve the fluency of a nAR model with connectionist temporal classification (CTC) by employing additional features in the scoring model used during beam search decoding. Since the beam search decoding in our model only requires to run the network in a single forward pass, the decoding speed is still notably higher than in standard AR models. We train models for three language pairs: German, Czech, and Romanian from and into English. The results show that our proposed models can be more efficient in terms of decoding speed and still achieve a competitive BLEU score relative to AR models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03227](https://arxiv.org/abs/2004.03227) [cs.CL] |
|           | (or [arXiv:2004.03227v1](https://arxiv.org/abs/2004.03227v1) [cs.CL] for this version) |





# 2020-04-07

[Return to Index](#Index)



<h2 id="2020-04-07-1">1. Neural Machine Translation with Imbalanced Classes</h2>

Title: [Neural Machine Translation with Imbalanced Classes](https://arxiv.org/abs/2004.02334)

Authors: [Thamme Gowda](https://arxiv.org/search/cs?searchtype=author&query=Gowda%2C+T), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

*(Submitted on 5 Apr 2020)*

> We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore the effect of class imbalance on NMT. We analyze the effect of vocabulary sizes on NMT performance and reveal an explanation for 'why' certain vocabulary sizes are better than others.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02334](https://arxiv.org/abs/2004.02334) [cs.CL] |
|           | (or [arXiv:2004.02334v1](https://arxiv.org/abs/2004.02334v1) [cs.CL] for this version) |





<h2 id="2020-04-07-2">2. Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation</h2>

Title: [Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation](https://arxiv.org/abs/2004.02577)

Authors: [Wei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W), [Chongxuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Tianhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 6 Apr 2020)*

> Existing data augmentation approaches for neural machine translation (NMT) have predominantly relied on back-translating in-domain (IND) monolingual corpora. These methods suffer from issues associated with a domain information gap, which leads to translation errors for low frequency and out-of-vocabulary terminology. This paper proposes a dictionary-based data augmentation (DDA) method for cross-domain NMT. DDA synthesizes a domain-specific dictionary with general domain corpora to automatically generate a large-scale pseudo-IND parallel corpus. The generated pseudo-IND data can be used to enhance a general domain trained baseline. The experiments show that the DDA-enhanced NMT models demonstrate consistent significant improvements, outperforming the baseline models by 3.75-11.53 BLEU. The proposed method is also able to further improve the performance of the back-translation based and IND-finetuned NMT models. The improvement is associated with the enhanced domain coverage produced by DDA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02577](https://arxiv.org/abs/2004.02577) [cs.CL] |
|           | (or [arXiv:2004.02577v1](https://arxiv.org/abs/2004.02577v1) [cs.CL] for this version) |





<h2 id="2020-04-07-3">3. Meta-Learning for Few-Shot NMT Adaptation</h2>

Title: [Meta-Learning for Few-Shot NMT Adaptation](https://arxiv.org/abs/2004.02745)

Authors: [Amr Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+A), [Hany Hassan](https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+H), [Hal Daumé III](https://arxiv.org/search/cs?searchtype=author&query=III%2C+H+D)

*(Submitted on 6 Apr 2020)*

> We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target domains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in-domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02745](https://arxiv.org/abs/2004.02745) [cs.CL] |
|           | (or [arXiv:2004.02745v1](https://arxiv.org/abs/2004.02745v1) [cs.CL] for this version) |



<h2 id="2020-04-07-4">4. Applying Cyclical Learning Rate to Neural Machine Translation</h2>

Title: [Applying Cyclical Learning Rate to Neural Machine Translation](https://arxiv.org/abs/2004.02401)

Authors: [Choon Meng Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+C+M), [Jianfeng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Wei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W)

*(Submitted on 6 Apr 2020)*

> In training deep learning networks, the optimizer and related learning rate are often used without much thought or with minimal tuning, even though it is crucial in ensuring a fast convergence to a good quality minimum of the loss function that can also generalize well on the test dataset. Drawing inspiration from the successful application of cyclical learning rate policy for computer vision related convolutional networks and datasets, we explore how cyclical learning rate can be applied to train transformer-based neural networks for neural machine translation. From our carefully designed experiments, we show that the choice of optimizers and the associated cyclical learning rate policy can have a significant impact on the performance. In addition, we establish guidelines when applying cyclical learning rates to neural machine translation tasks. Thus with our work, we hope to raise awareness of the importance of selecting the right optimizers and the accompanying learning rate policy, at the same time, encourage further research into easy-to-use learning rate policies.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02401](https://arxiv.org/abs/2004.02401) [cs.LG] |
|           | (or [arXiv:2004.02401v1](https://arxiv.org/abs/2004.02401v1) [cs.LG] for this version) |





<h2 id="2020-04-07-5">5. Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation</h2>

Title: [Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation](https://arxiv.org/abs/2004.02071)

Authors: [Sreyashi Nag](https://arxiv.org/search/cs?searchtype=author&query=Nag%2C+S), [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Varun Lakshminarasimhan](https://arxiv.org/search/cs?searchtype=author&query=Lakshminarasimhan%2C+V), [Swapnil Singhavi](https://arxiv.org/search/cs?searchtype=author&query=Singhavi%2C+S)

*(Submitted on 5 Apr 2020)*

> We explore ways of incorporating bilingual dictionaries to enable semi-supervised neural machine translation. Conventional back-translation methods have shown success in leveraging target side monolingual data. However, since the quality of back-translation models is tied to the size of the available parallel corpora, this could adversely impact the synthetically generated sentences in a low resource setting. We propose a simple data augmentation technique to address both this shortcoming. We incorporate widely available bilingual dictionaries that yield word-by-word translations to generate synthetic sentences. This automatically expands the vocabulary of the model while maintaining high quality content. Our method shows an appreciable improvement in performance over strong baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02071](https://arxiv.org/abs/2004.02071) [cs.CL] |
|           | (or [arXiv:2004.02071v1](https://arxiv.org/abs/2004.02071v1) [cs.CL] for this version) |







<h2 id="2020-04-07-6">6. Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech</h2>

Title: [Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech](https://arxiv.org/abs/2004.02077)

Authors: [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Scott Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+S)

*(Submitted on 5 Apr 2020)*

> While there is a large body of research studying deep learning methods for text generation from structured data, almost all of it focuses purely on English. In this paper, we study the effectiveness of machine translation based pre-training for data-to-text generation in non-English languages. Since the structured data is generally expressed in English, text generation into other languages involves elements of translation, transliteration and copying - elements already encoded in neural machine translation systems. Moreover, since data-to-text corpora are typically small, this task can benefit greatly from pre-training. Based on our experiments on Czech, a morphologically complex language, we find that pre-training lets us train end-to-end models with significantly improved performance, as judged by automatic metrics and human evaluation. We also show that this approach enjoys several desirable properties, including improved performance in low data scenarios and robustness to unseen slot values.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02077](https://arxiv.org/abs/2004.02077) [cs.CL] |
|           | (or [arXiv:2004.02077v1](https://arxiv.org/abs/2004.02077v1) [cs.CL] for this version) |







<h2 id="2020-04-07-7">7. Reference Language based Unsupervised Neural Machine Translation</h2>

Title: [Reference Language based Unsupervised Neural Machine Translation](https://arxiv.org/abs/2004.02127)

Authors: [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 5 Apr 2020)*

> Exploiting common language as an auxiliary for better translation has a long tradition in machine translation, which lets supervised learning based machine translation enjoy the enhancement delivered by the well-used pivot language, in case that the prerequisite of parallel corpus from source language to target language cannot be fully satisfied. The rising of unsupervised neural machine translation (UNMT) seems completely relieving the parallel corpus curse, though still subject to unsatisfactory performance so far due to vague clues available used for its core back-translation training. Further enriching the idea of pivot translation by freeing the use of parallel corpus other than its specified source and target, we propose a new reference language based UNMT framework, in which the reference language only shares parallel corpus with the source, indicating clear enough signal to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline in terms of only one auxiliary language, demonstrating the usefulness of the proposed reference language based UNMT with a good start.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02127](https://arxiv.org/abs/2004.02127) [cs.CL] |
|           | (or [arXiv:2004.02127v1](https://arxiv.org/abs/2004.02127v1) [cs.CL] for this version) |







<h2 id="2020-04-07-8">8. Detecting and Understanding Generalization Barriers for Neural Machine Translation</h2>

Title: [Detecting and Understanding Generalization Barriers for Neural Machine Translation](https://arxiv.org/abs/2004.02181)

Authors: [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Conghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 5 Apr 2020)*

> Generalization to unseen instances is our eternal pursuit for all data-driven models. However, for realistic task like machine translation, the traditional approach measuring generalization in an average sense provides poor understanding for the fine-grained generalization ability. As a remedy, this paper attempts to identify and understand generalization barrier words within an unseen input sentence that \textit{cause} the degradation of fine-grained generalization. We propose a principled definition of generalization barrier words and a modified version which is tractable in computation. Based on the modified one, we propose three simple methods for barrier detection by the search-aware risk estimation through counterfactual generation. We then conduct extensive analyses on those detected generalization barrier words on both Zh⇔En NIST benchmarks from various perspectives. Potential usage of the detected barrier words is also discussed.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.02181](https://arxiv.org/abs/2004.02181) [cs.CL] |
|           | (or [arXiv:2004.02181v1](https://arxiv.org/abs/2004.02181v1) [cs.CL] for this version) |







<h2 id="2020-04-07-9">9. AR: Auto-Repair the Synthetic Data for Neural Machine Translation</h2>

Title: [AR: Auto-Repair the Synthetic Data for Neural Machine Translation](https://arxiv.org/abs/2004.02196)

Authors: [Shanbo Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Shaohui Kuang](https://arxiv.org/search/cs?searchtype=author&query=Kuang%2C+S), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Changfeng Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

*(Submitted on 5 Apr 2020)*

> Compared with only using limited authentic parallel data as training corpus, many studies have proved that incorporating synthetic parallel data, which generated by back translation (BT) or forward translation (FT, or selftraining), into the NMT training process can significantly improve translation quality. However, as a well-known shortcoming, synthetic parallel data is noisy because they are generated by an imperfect NMT system. As a result, the improvements in translation quality bring by the synthetic parallel data are greatly diminished. In this paper, we propose a novel Auto- Repair (AR) framework to improve the quality of synthetic data. Our proposed AR model can learn the transformation from low quality (noisy) input sentence to high quality sentence based on large scale monolingual data with BT and FT techniques. The noise in synthetic parallel data will be sufficiently eliminated by the proposed AR model and then the repaired synthetic parallel data can help the NMT models to achieve larger improvements. Experimental results show that our approach can effective improve the quality of synthetic parallel data and the NMT model with the repaired synthetic data achieves consistent improvements on both WMT14 EN!DE and IWSLT14 DE!EN translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02196](https://arxiv.org/abs/2004.02196) [cs.CL] |
|           | (or [arXiv:2004.02196v1](https://arxiv.org/abs/2004.02196v1) [cs.CL] for this version) |







<h2 id="2020-04-07-10">10. Understanding Learning Dynamics for Neural Machine Translation</h2>

Title: [Understanding Learning Dynamics for Neural Machine Translation](https://arxiv.org/abs/2004.02199)

Authors: [Conghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 5 Apr 2020)*

> Despite the great success of NMT, there still remains a severe challenge: it is hard to interpret the internal dynamics during its training process. In this paper we propose to understand learning dynamics of NMT by using a recent proposed technique named Loss Change Allocation (LCA)~\citep{lan-2019-loss-change-allocation}. As LCA requires calculating the gradient on an entire dataset for each update, we instead present an approximate to put it into practice in NMT scenario. %motivated by the lesson from sgd. Our simulated experiment shows that such approximate calculation is efficient and is empirically proved to deliver consistent results to the brute-force implementation. In particular, extensive experiments on two standard translation benchmark datasets reveal some valuable findings.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.02199](https://arxiv.org/abs/2004.02199) [cs.CL] |
|           | (or [arXiv:2004.02199v1](https://arxiv.org/abs/2004.02199v1) [cs.CL] for this version) |







# 2020-04-06

[Return to Index](#Index)



<h2 id="2020-04-04-1">1. XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation</h2>

Title: [XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation](https://arxiv.org/abs/2004.01401)

Authors: [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Ning Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+N), [Fenfei Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+F), [Weizhen Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Guihong Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+G), [Xiaodong Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X), [Bruce Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Rahul Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+R), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Sining Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+S), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Jiun-Hung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Winnie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Shuguang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Fan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 3 Apr 2020)*

> In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01401](https://arxiv.org/abs/2004.01401) [cs.CL] |
|           | (or [arXiv:2004.01401v1](https://arxiv.org/abs/2004.01401v1) [cs.CL] for this version) |





<h2 id="2020-04-04-2">2. Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation</h2>

Title: [Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation](https://arxiv.org/abs/2004.01422)

Authors: [Felipe Sánchez-Martínez](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Martínez%2C+F), [Juan Antonio Pérez-Ortiz](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Ortiz%2C+J+A), [Rafael C. Carrasco](https://arxiv.org/search/cs?searchtype=author&query=Carrasco%2C+R+C)

*(Submitted on 3 Apr 2020)*

> Translation models based on hierarchical phrase-based statistical machine translation (HSMT) have shown better performances than the non-hierarchical phrase-based counterparts for some language pairs. The standard approach to HSMT learns and apply a synchronous context-free grammar with a single non-terminal. The hypothesis behind the grammar refinement algorithm presented in this work is that this single non-terminal is overloaded, and insufficiently discriminative, and therefore, an adequate split of it into more specialised symbols could lead to improved models. This paper presents a method to learn synchronous context-free grammars with a huge number of initial non-terminals, which are then grouped via a clustering algorithm. Our experiments show that the resulting smaller set of non-terminals correctly capture the contextual information that makes it possible to statistically significantly improve the BLEU score of the standard HSMT approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01422](https://arxiv.org/abs/2004.01422) [cs.CL] |
|           | (or [arXiv:2004.01422v1](https://arxiv.org/abs/2004.01422v1) [cs.CL] for this version) |





<h2 id="2020-04-04-3">3. Aligned Cross Entropy for Non-Autoregressive Machine Translation</h2>

Title: [Aligned Cross Entropy for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2004.01655)

Authors: [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Vladimir Karpukhin](https://arxiv.org/search/cs?searchtype=author&query=Karpukhin%2C+V), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

*(Submitted on 3 Apr 2020)*

> Non-autoregressive machine translation models significantly speed up decoding by allowing for parallel prediction of the entire target sequence. However, modeling word order is more challenging due to the lack of autoregressive factors in the model. This difficultly is compounded during training with cross entropy loss, which can highly penalize small shifts in word order. In this paper, we propose aligned cross entropy (AXE) as an alternative loss function for training of non-autoregressive models. AXE uses a differentiable dynamic program to assign loss based on the best possible monotonic alignment between target tokens and model predictions. AXE-based training of conditional masked language models (CMLMs) substantially improves performance on major WMT benchmarks, while setting a new state of the art for non-autoregressive models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01655](https://arxiv.org/abs/2004.01655) [cs.CL] |
|           | (or [arXiv:2004.01655v1](https://arxiv.org/abs/2004.01655v1) [cs.CL] for this version) |





<h2 id="2020-04-04-4">4. A Set of Recommendations for Assessing Human-Machine Parity in Language Translation</h2>

Title: [A Set of Recommendations for Assessing Human-Machine Parity in Language Translation](https://arxiv.org/abs/2004.01694)

Authors: [Samuel Läubli](https://arxiv.org/search/cs?searchtype=author&query=Läubli%2C+S), [Sheila Castilho](https://arxiv.org/search/cs?searchtype=author&query=Castilho%2C+S), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Qinlan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Q), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

*(Submitted on 3 Apr 2020)*

> The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a number of empirical investigations. We reassess Hassan et al.'s 2018 investigation into Chinese to English news translation, showing that the finding of human-machine parity was owed to weaknesses in the evaluation design - which is currently considered best practice in the field. We show that the professional human translations contained significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human-machine parity in particular, for which we offer a set of recommendations based on our empirical findings.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Journal of Artificial Intelligence Research 67 (2020) 653-672 |
| DOI:               | [10.1613/jair.1.11371](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1613%2Fjair.1.11371&v=de784ce7) |
| Cite as:           | [arXiv:2004.01694](https://arxiv.org/abs/2004.01694) [cs.CL] |
|                    | (or [arXiv:2004.01694v1](https://arxiv.org/abs/2004.01694v1) [cs.CL] for this version) |







# 2020-04-03

[Return to Index](#Index)



<h2 id="2020-04-03-1">1. Igbo-English Machine Translation: An Evaluation Benchmark</h2>

Title: [Igbo-English Machine Translation: An Evaluation Benchmark](https://arxiv.org/abs/2004.00648)

Authors: [Ignatius Ezeani](https://arxiv.org/search/cs?searchtype=author&query=Ezeani%2C+I), [Paul Rayson](https://arxiv.org/search/cs?searchtype=author&query=Rayson%2C+P), [Ikechukwu Onyenwe](https://arxiv.org/search/cs?searchtype=author&query=Onyenwe%2C+I), [Chinedu Uchechukwu](https://arxiv.org/search/cs?searchtype=author&query=Uchechukwu%2C+C), [Mark Hepple](https://arxiv.org/search/cs?searchtype=author&query=Hepple%2C+M)

*(Submitted on 1 Apr 2020)*

> Although researchers and practitioners are pushing the boundaries and enhancing the capacities of NLP tools and methods, works on African languages are lagging. A lot of focus on well resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese etc. Over 97% of the world's 7000 languages, including African languages, are low resourced for NLP i.e. they have little or no data, tools, and techniques for NLP research. For instance, only 5 out of 2965, 0.19% authors of full text papers in the ACL Anthology extracted from the 5 major conferences in 2018 ACL, NAACL, EMNLP, COLING and CoNLL, are affiliated to African institutions. In this work, we discuss our effort toward building a standard machine translation benchmark dataset for Igbo, one of the 3 major Nigerian languages. Igbo is spoken by more than 50 million people globally with over 50% of the speakers are in southeastern Nigeria. Igbo is low resourced although there have been some efforts toward developing IgboNLP such as part of speech tagging and diacritic restoration

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.00648](https://arxiv.org/abs/2004.00648) [cs.CL] |
|           | (or [arXiv:2004.00648v1](https://arxiv.org/abs/2004.00648v1) [cs.CL] for this version) |





<h2 id="2020-04-03-2">2. Mapping Languages: The Corpus of Global Language Use</h2>

Title: [Mapping Languages: The Corpus of Global Language Use](https://arxiv.org/abs/2004.00798)

Authors: [Jonathan Dunn](https://arxiv.org/search/cs?searchtype=author&query=Dunn%2C+J)

*(Submitted on 2 Apr 2020)*

> This paper describes a web-based corpus of global language use with a focus on how this corpus can be used for data-driven language mapping. First, the corpus provides a representation of where national varieties of major languages are used (e.g., English, Arabic, Russian) together with consistently collected data for each variety. Second, the paper evaluates a language identification model that supports more local languages with smaller sample sizes than alternative off-the-shelf models. Improved language identification is essential for moving beyond majority languages. Given the focus on language mapping, the paper analyzes how well this digital language data represents actual populations by (i) systematically comparing the corpus with demographic ground-truth data and (ii) triangulating the corpus with an alternate Twitter-based dataset. In total, the corpus contains 423 billion words representing 148 languages (with over 1 million words from each language) and 158 countries (again with over 1 million words from each country), all distilled from Common Crawl web data. The main contribution of this paper, in addition to describing this publicly-available corpus, is to provide a comprehensive analysis of the relationship between two sources of digital data (the web and Twitter) as well as their connection to underlying populations.

| Comments: | This is a pre-print of an article published in Language Resources and Evaluation. The final authenticated version is available online at: [this https URL](https://doi.org/10.1007/s10579-020-09489-2) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1007/s10579-020-09489-2](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2Fs10579-020-09489-2&v=6886dc37) |
| Cite as:  | [arXiv:2004.00798](https://arxiv.org/abs/2004.00798) [cs.CL] |
|           | (or [arXiv:2004.00798v1](https://arxiv.org/abs/2004.00798v1) [cs.CL] for this version) |





# 2020-04-02

[Return to Index](#Index)



<h2 id="2020-04-02-1">1. Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study</h2>

Title: [Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study](https://arxiv.org/abs/2004.00068)

Authors: [Michael Leventhal](https://arxiv.org/search/cs?searchtype=author&query=Leventhal%2C+M), [Allahsera Tapo](https://arxiv.org/search/cs?searchtype=author&query=Tapo%2C+A), [Sarah Luger](https://arxiv.org/search/cs?searchtype=author&query=Luger%2C+S), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M), [Christopher M. Homan](https://arxiv.org/search/cs?searchtype=author&query=Homan%2C+C+M)

*(Submitted on 31 Mar 2020)*

> We present novel methods for assessing the quality of human-translated aligned texts for learning machine translation models of under-resourced languages. Malian university students translated French texts, producing either written or oral translations to Bambara. Our results suggest that similar quality can be obtained from either written or spoken translations for certain kinds of texts. They also suggest specific instructions that human translators should be given in order to improve the quality of their work.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.00068](https://arxiv.org/abs/2004.00068) [cs.CL] |
|           | (or [arXiv:2004.00068v1](https://arxiv.org/abs/2004.00068v1) [cs.CL] for this version) |







<h2 id="2020-04-02-2">2. Sign Language Translation with Transformers</h2>

Title: [Sign Language Translation with Transformers](https://arxiv.org/abs/2004.00588)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K)

*(Submitted on 1 Apr 2020)*

> Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. Though SLT has gathered interest recently, little study has been performed on the translation system. This paper focuses on the translation system and improves performance by utilizing Transformer networks. We report a wide range of experimental results for various Transformer setups and introduce the use of Spatial-Temporal Multi-Cue (STMC) networks in an end-to-end SLT system with Transformer.
> We perform experiments on RWTH-PHOENIX-Weather 2014T, a challenging SLT benchmark dataset of German sign language, and ASLG-PC12, a dataset involving American Sign Language (ASL) recently used in gloss-to-text translation. Our methodology improves on the current state-of-the-art by over 5 and 7 points respectively in BLEU-4 score on ground truth glosses and by using an STMC network to predict glosses of the RWTH-PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an improvement of over 16 points in BLEU-4. Our findings also demonstrate that end-to-end translation on predicted glosses provides even better performance than translation on ground truth glosses. This shows potential for further improvement in SLT by either jointly training the SLR and translation systems or by revising the gloss annotation system.

| Comments: | 14 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.00588](https://arxiv.org/abs/2004.00588) [cs.CL] |
|           | (or [arXiv:2004.00588v1](https://arxiv.org/abs/2004.00588v1) [cs.CL] for this version) |









# 2020-04-01

[Return to Index](#Index)



<h2 id="2020-04-01-1">1. The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe</h2>

Title: [The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe](https://arxiv.org/abs/2003.13833)

Authors: [Georg Rehm](https://arxiv.org/search/cs?searchtype=author&query=Rehm%2C+G), [Katrin Marheinecke](https://arxiv.org/search/cs?searchtype=author&query=Marheinecke%2C+K), [Stefanie Hegele](https://arxiv.org/search/cs?searchtype=author&query=Hegele%2C+S), [Stelios Piperidis](https://arxiv.org/search/cs?searchtype=author&query=Piperidis%2C+S), [Kalina Bontcheva](https://arxiv.org/search/cs?searchtype=author&query=Bontcheva%2C+K), [Jan Hajič](https://arxiv.org/search/cs?searchtype=author&query=Hajič%2C+J), [Khalid Choukri](https://arxiv.org/search/cs?searchtype=author&query=Choukri%2C+K), [Andrejs Vasiļjevs](https://arxiv.org/search/cs?searchtype=author&query=Vasiļjevs%2C+A), [Gerhard Backfried](https://arxiv.org/search/cs?searchtype=author&query=Backfried%2C+G), [Christoph Prinz](https://arxiv.org/search/cs?searchtype=author&query=Prinz%2C+C), [José Manuel Gómez Pérez](https://arxiv.org/search/cs?searchtype=author&query=Pérez%2C+J+M+G), [Luc Meertens](https://arxiv.org/search/cs?searchtype=author&query=Meertens%2C+L), [Paul Lukowicz](https://arxiv.org/search/cs?searchtype=author&query=Lukowicz%2C+P), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Andrea Lösch](https://arxiv.org/search/cs?searchtype=author&query=Lösch%2C+A), [Philipp Slusallek](https://arxiv.org/search/cs?searchtype=author&query=Slusallek%2C+P), [Morten Irgens](https://arxiv.org/search/cs?searchtype=author&query=Irgens%2C+M), [Patrick Gatellier](https://arxiv.org/search/cs?searchtype=author&query=Gatellier%2C+P), [Joachim Köhler](https://arxiv.org/search/cs?searchtype=author&query=Köhler%2C+J), [Laure Le Bars](https://arxiv.org/search/cs?searchtype=author&query=Bars%2C+L+L), [Dimitra Anastasiou](https://arxiv.org/search/cs?searchtype=author&query=Anastasiou%2C+D), [Albina Auksoriūtė](https://arxiv.org/search/cs?searchtype=author&query=Auksoriūtė%2C+A), [Núria Bel](https://arxiv.org/search/cs?searchtype=author&query=Bel%2C+N), [António Branco](https://arxiv.org/search/cs?searchtype=author&query=Branco%2C+A), [Gerhard Budin](https://arxiv.org/search/cs?searchtype=author&query=Budin%2C+G), [Walter Daelemans](https://arxiv.org/search/cs?searchtype=author&query=Daelemans%2C+W), [Koenraad De Smedt](https://arxiv.org/search/cs?searchtype=author&query=De+Smedt%2C+K), [Radovan Garabík](https://arxiv.org/search/cs?searchtype=author&query=Garabík%2C+R), [Maria Gavriilidou](https://arxiv.org/search/cs?searchtype=author&query=Gavriilidou%2C+M), [Dagmar Gromann](https://arxiv.org/search/cs?searchtype=author&query=Gromann%2C+D), [Svetla Koeva](https://arxiv.org/search/cs?searchtype=author&query=Koeva%2C+S), [Simon Krek](https://arxiv.org/search/cs?searchtype=author&query=Krek%2C+S), [Cvetana Krstev](https://arxiv.org/search/cs?searchtype=author&query=Krstev%2C+C), [Krister Lindén](https://arxiv.org/search/cs?searchtype=author&query=Lindén%2C+K), [Bernardo Magnini](https://arxiv.org/search/cs?searchtype=author&query=Magnini%2C+B), [Jan Odijk](https://arxiv.org/search/cs?searchtype=author&query=Odijk%2C+J), [Maciej Ogrodniczuk](https://arxiv.org/search/cs?searchtype=author&query=Ogrodniczuk%2C+M), [Eiríkur Rögnvaldsson](https://arxiv.org/search/cs?searchtype=author&query=Rögnvaldsson%2C+E), [Mike Rosner](https://arxiv.org/search/cs?searchtype=author&query=Rosner%2C+M), [Bolette Sandford Pedersen](https://arxiv.org/search/cs?searchtype=author&query=Pedersen%2C+B+S), [Inguna Skadiņa](https://arxiv.org/search/cs?searchtype=author&query=Skadiņa%2C+I), [Marko Tadić](https://arxiv.org/search/cs?searchtype=author&query=Tadić%2C+M), [Dan Tufiş](https://arxiv.org/search/cs?searchtype=author&query=Tufiş%2C+D), [Tamás Váradi](https://arxiv.org/search/cs?searchtype=author&query=Váradi%2C+T), [Kadri Vider](https://arxiv.org/search/cs?searchtype=author&query=Vider%2C+K), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

*(Submitted on 30 Mar 2020)*

> Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe's specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI, including many opportunities, synergies but also misconceptions, has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.

| Comments: | Proceedings of the 12th Language Resources and Evaluation Conference (LREC 2020). To appear |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | [arXiv:2003.13833](https://arxiv.org/abs/2003.13833) [cs.CL] |
|           | (or [arXiv:2003.13833v1](https://arxiv.org/abs/2003.13833v1) [cs.CL] for this version) |





<h2 id="2020-04-01-2">2. MULTEXT-East</h2>

Title: [MULTEXT-East](https://arxiv.org/abs/2003.14026)

Authors: [Tomaž Erjavec](https://arxiv.org/search/cs?searchtype=author&query=Erjavec%2C+T)

*(Submitted on 31 Mar 2020)*

> MULTEXT-East language resources, a multilingual dataset for language engineering research, focused on the morphosyntactic level of linguistic description. The MULTEXT-East dataset includes the EAGLES-based morphosyntactic specifications, morphosyntactic lexicons, and an annotated multilingual corpora. The parallel corpus, the novel "1984" by George Orwell, is sentence aligned and contains hand-validated morphosyntactic descriptions and lemmas. The resources are uniformly encoded in XML, using the Text Encoding Initiative Guidelines, TEI P5, and cover 16 languages: Bulgarian, Croatian, Czech, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbian, Slovak, Slovene, and Ukrainian. This dataset is extensively documented, and freely available for research purposes. This case study gives a history of the development of the MULTEXT-East resources, presents their encoding and components, discusses related work and gives some conclusions.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Published in: Nancy Ide, James Pustejovsky, eds. 2007. Handbook of linguistic annotation. pp. 441-462. Springer |
| DOI:               | [10.1007/978-94-024-0881-2_17](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-94-024-0881-2_17&v=6fb4b8b1) |
| Cite as:           | [arXiv:2003.14026](https://arxiv.org/abs/2003.14026) [cs.CL] |
|                    | (or [arXiv:2003.14026v1](https://arxiv.org/abs/2003.14026v1) [cs.CL] for this version) |





<h2 id="2020-04-01-3">3. Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks</h2>

Title: [Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks](https://arxiv.org/abs/2003.14056)

Authors: [Prajit Dhar](https://arxiv.org/search/cs?searchtype=author&query=Dhar%2C+P), [Arianna Bisazza](https://arxiv.org/search/cs?searchtype=author&query=Bisazza%2C+A)

*(Submitted on 31 Mar 2020)*

> It is now established that modern neural language models can be successfully trained on multiple languages simultaneously without changes to the underlying architecture, providing an easy way to adapt a variety of NLP models to low-resource languages. But what kind of knowledge is really shared among languages within these models? Does multilingual training mostly lead to an alignment of the lexical representation spaces or does it also enable the sharing of purely grammatical knowledge? In this paper we dissect different forms of cross-lingual transfer and look for its most determining factors, using a variety of models and probing tasks. We find that exposing our language models to a related language does not always increase grammatical knowledge in the target language, and that optimal conditions for lexical-semantic transfer may not be optimal for syntactic transfer.

| Comments: | 9 pages single column with 6 figures                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.14056](https://arxiv.org/abs/2003.14056) [cs.CL] |
|           | (or [arXiv:2003.14056v1](https://arxiv.org/abs/2003.14056v1) [cs.CL] for this version) |





<h2 id="2020-04-01-4">4. On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation</h2>

Title: [On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation](https://arxiv.org/abs/2003.14324)

Authors: [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E)

*(Submitted on 31 Mar 2020)*

> New machine translations (MT) technologies are emerging rapidly and with them, bold claims of achieving human parity such as: (i) the results produced approach "accuracy achieved by average bilingual human translators" (Wu et al., 2017b) or (ii) the "translation quality is at human parity when compared to professional human translators" (Hassan et al., 2018) have seen the light of day (Laubli et al., 2018). Aside from the fact that many of these papers craft their own definition of human parity, these sensational claims are often not supported by a complete analysis of all aspects involved in translation. Establishing the discrepancies between the strengths of statistical approaches to MT and the way humans translate has been the starting point of our research. By looking at MT output and linguistic theory, we were able to identify some remaining issues. The problems range from simple number and gender agreement errors to more complex phenomena such as the correct translation of aspectual values and tenses. Our experiments confirm, along with other studies (Bentivogli et al., 2016), that neural MT has surpassed statistical MT in many aspects. However, some problems remain and others have emerged. We cover a series of problems related to the integration of specific linguistic features into statistical and neural MT, aiming to analyse and provide a solution to some of them. Our work focuses on addressing three main research questions that revolve around the complex relationship between linguistics and MT in general. We identify linguistic information that is lacking in order for automatic translation systems to produce more accurate translations and integrate additional features into the existing pipelines. We identify overgeneralization or 'algorithmic bias' as a potential drawback of neural MT and link it to many of the remaining linguistic issues.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.14324](https://arxiv.org/abs/2003.14324) [cs.CL] |
|           | (or [arXiv:2003.14324v1](https://arxiv.org/abs/2003.14324v1) [cs.CL] for this version) |





<h2 id="2020-04-01-5">5. Evaluating Amharic Machine Translation</h2>

Title: [Evaluating Amharic Machine Translation](https://arxiv.org/abs/2003.14386)

Authors: [Asmelash Teka Hadgu](https://arxiv.org/search/cs?searchtype=author&query=Hadgu%2C+A+T), [Adam Beaudoin](https://arxiv.org/search/cs?searchtype=author&query=Beaudoin%2C+A), [Abel Aregawi](https://arxiv.org/search/cs?searchtype=author&query=Aregawi%2C+A)

*(Submitted on 31 Mar 2020)*

> Machine translation (MT) systems are now able to provide very accurate results for high resource language pairs. However, for many low resource languages, MT is still under active research. In this paper, we develop and share a dataset to automatically evaluate the quality of MT systems for Amharic. We compare two commercially available MT systems that support translation of Amharic to and from English to assess the current state of MT for Amharic. The BLEU score results show that the results for Amharic translation are promising but still low. We hope that this dataset will be useful to the research community both in academia and industry as a benchmark to evaluate Amharic MT systems.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.14386](https://arxiv.org/abs/2003.14386) [cs.CL] |
|           | (or [arXiv:2003.14386v1](https://arxiv.org/abs/2003.14386v1) [cs.CL] for this version) |







<h2 id="2020-04-01-6">6. Low Resource Neural Machine Translation: A Benchmark for Five African Languages</h2>

Title: [Low Resource Neural Machine Translation: A Benchmark for Five African Languages](https://arxiv.org/abs/2003.14402)

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 31 Mar 2020)*

> Recent advents in Neural Machine Translation (NMT) have shown improvements in low-resource language (LRL) translation tasks. In this work, we benchmark NMT between English and five African LRL pairs (Swahili, Amharic, Tigrigna, Oromo, Somali [SATOS]). We collected the available resources on the SATOS languages to evaluate the current state of NMT for LRLs. Our evaluation, comparing a baseline single language pair NMT model against semi-supervised learning, transfer learning, and multilingual modeling, shows significant performance improvements both in the En-LRL and LRL-En directions. In terms of averaged BLEU score, the multilingual approach shows the largest gains, up to +5 points, in six out of ten translation directions. To demonstrate the generalization capability of each model, we also report results on multi-domain test sets. We release the standardized experimental data and the test sets for future works addressing the challenges of NMT in under-resourced settings, in particular for the SATOS languages.

| Comments: | Accepted for AfricaNLP workshop at ICLR 2020                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.14402](https://arxiv.org/abs/2003.14402) [cs.CL] |
|           | (or [arXiv:2003.14402v1](https://arxiv.org/abs/2003.14402v1) [cs.CL] for this version) |