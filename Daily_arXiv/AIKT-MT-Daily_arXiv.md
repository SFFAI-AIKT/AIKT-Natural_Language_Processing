# Daily arXiv: Machine Translation - January, 2021

# Index


- [2021-01-15](#2021-01-15)
	
  - [1. Structured Prediction as Translation between Augmented Natural Languages](#2021-01-15-1)
  - [2. Text Augmentation in a Multi-Task View](#2021-01-15-2)
  - [3. Persuasive Natural Language Generation -- A Literature Review](#2021-01-15-3)
- [2021-01-14](#2021-01-14)
	
  - [1. Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding](#2021-01-14-1)
  - [2. Latent Alignment of Procedural Concepts in Multimodal Recipes](#2021-01-14-2)
  - [3. Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration](#2021-01-14-3)
- [2021-01-13](#2021-01-13)
	
  - [1. Quantum Mathematics in Artificial Intelligence](#2021-01-13-1)
  - [2. Explain and Predict, and then Predict again](#2021-01-13-2)
  - [3. Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning](#2021-01-13-3)
  - [4. Transforming Multi-Conditioned Generation from Meaning Representation](#2021-01-13-4)
  - [5. Toward Effective Automated Content Analysis via Crowdsourcing](#2021-01-13-5)
- [2021-01-12](#2021-01-12)
	
  - [1. Misspelling Correction with Pre-trained Contextual Language Model](#2021-01-12-1)
  - [2. SDA: Improving Text Generation with Self Data Augmentation](#2021-01-12-2)
  - [3. Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing](#2021-01-12-3)
  - [4. Learning Better Sentence Representation with Syntax Information](#2021-01-12-4)
  - [5. Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation](#2021-01-12-5)
- [2021-01-11](#2021-01-11)
	
  - [1. MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets](#2021-01-11-1)
- [2021-01-08](#2021-01-08)
	- [1. User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics](#2021-01-08-1)
  - [2. Towards a Smart Data Processing and Storage Model](#2021-01-08-2)
- [2021-01-07](#2021-01-07)
	
  - [1. AutoDropout: Learning Dropout Patterns to Regularize Deep Networks](#2021-01-07-1)
- [2021-01-06](#2021-01-06)
	
  - [1. I-BERT: Integer-only BERT Quantization](#2021-01-06-1)
  - [2. Political Depolarization of News Articles Using Attribute-aware Word Embeddings](#2021-01-06-2)
  - [3. Local Translation Services for Neglected Languages](#2021-01-06-3)
- [2021-01-05](#2021-01-05)
	
  - [1. VinVL: Making Visual Representations Matter in Vision-Language Models](#2021-01-05-1)
  - [2. The Pile: An 800GB Dataset of Diverse Text for Language Modeling](#2021-01-05-2)
  - [3. EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](#2021-01-05-3)
  - [4. Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](#2021-01-05-4)
  - [5. A Graph Total Variation Regularized Softmax for Text Generation](#2021-01-05-5)
  - [6. BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding](#2021-01-05-6)
  - [7. Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](#2021-01-05-7)
  - [8. Understanding Few-Shot Commonsense Knowledge Models](#2021-01-05-8)
  - [9. On-the-Fly Attention Modularization for Neural Generation](#2021-01-05-9)
  - [10. Cross-Document Language Modeling](#2021-01-05-10)
  - [11. Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](#2021-01-05-11)
  - [12. KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](#2021-01-05-12)
  - [13. Decoding Time Lexical Domain Adaptationfor Neural Machine Translation](#2021-01-05-13)
  - [14. Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events](#2021-01-05-14)
  - [15. Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](#2021-01-05-15)
  - [16. How to Train Your Agent to Read and Write](#2021-01-05-16)
- [2021-01-01](#2021-01-01)
	- [1. Understanding and Improving Lexical Choice in Non-Autoregressive Translation](#2021-01-01-1)
  - [2. Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation](#2021-01-01-2)
  - [3. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](#2021-01-01-3)
  - [4. CMV-BERT: Contrastive multi-vocab pretraining of BERT](#2021-01-01-4)
  - [5. Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning](#2021-01-01-5)
  - [6. Transformer Feed-Forward Layers Are Key-Value Memories](#2021-01-01-6)
  - [7. Reservoir Transformer](#2021-01-01-7)
  - [8. Enhancing Pre-trained Language Model with Lexical Simplification](#2021-01-01-8)
  - [9. Accurate Word Representations with Universal Visual Guidance](#2021-01-01-9)
  - [10. Improving Zero-Shot Translation by Disentangling Positional Information](#2021-01-01-10)
  - [11. Improving BERT with Syntax-aware Local Attention](#2021-01-01-11)
  - [12. Synthetic Source Language Augmentation for Colloquial Neural Machine Translation](#2021-01-01-12)
  - [13. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?](#2021-01-01-13)
  - [14. SemGloVe: Semantic Co-occurrences for GloVe from BERT](#2021-01-01-14)
  - [15. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](#2021-01-01-15)
  - [16. Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation](#2021-01-01-16)
  - [17. Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation](#2021-01-01-17)
  - [18. CLEAR: Contrastive Learning for Sentence Representation](#2021-01-01-18)
  - [19. Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings](#2021-01-01-19)
  - [20. Towards Zero-Shot Knowledge Distillation for Natural Language Processing](#2021-01-01-20)
  - [21. Neural Machine Translation: A Review of Methods, Resources, and Tools](#2021-01-01-21)
  - [22. Linear-Time WordPiece Tokenization](#2021-01-01-22)
  - [23. XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders](#2021-01-01-23)
  - [24. How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](#2021-01-01-24)
  - [25. CoCoLM: COmplex COmmonsense Enhanced Language Model](#2021-01-01-25)
  - [26. VOLT: Improving Vocabularization via Optimal Transport for Machine Translation](#2021-01-01-26)
  - [27. ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](#2021-01-01-27)
  - [28. Revisiting Robust Neural Machine Translation: A Transformer Case Study](#2021-01-01-28)
  - [29. FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation](#2021-01-01-29)
  - [30. Making Pre-trained Language Models Better Few-shot Learners](#2021-01-01-30)
  - [31. Shortformer: Better Language Modeling using Shorter Inputs](#2021-01-01-31)
  - [32. Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade](#2021-01-01-32)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-01-15

[Return to Index](#Index)



<h2 id="2021-01-15-1">1. Structured Prediction as Translation between Augmented Natural Languages</h2>

Title: [Structured Prediction as Translation between Augmented Natural Languages](https://arxiv.org/abs/2101.05779)

Authors:[Giovanni Paolini](https://arxiv.org/search/cs?searchtype=author&query=Paolini%2C+G), [Ben Athiwaratkun](https://arxiv.org/search/cs?searchtype=author&query=Athiwaratkun%2C+B), [Jason Krone](https://arxiv.org/search/cs?searchtype=author&query=Krone%2C+J), [Jie Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J), [Alessandro Achille](https://arxiv.org/search/cs?searchtype=author&query=Achille%2C+A), [Rishita Anubhai](https://arxiv.org/search/cs?searchtype=author&query=Anubhai%2C+R), [Cicero Nogueira dos Santos](https://arxiv.org/search/cs?searchtype=author&query=Santos%2C+C+N+d), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B), [Stefano Soatto](https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S)

> We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.05779](https://arxiv.org/abs/2101.05779) [cs.LG]** |
|           | (or **[arXiv:2101.05779v1](https://arxiv.org/abs/2101.05779v1) [cs.LG]** for this version) |





<h2 id="2021-01-15-2">2. Text Augmentation in a Multi-Task View</h2>

Title: [Text Augmentation in a Multi-Task View](https://arxiv.org/abs/2101.05469)

Authors:[Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Chengyu Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Shiqi Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training. In this paper, we propose an alternative perspective -- a multi-task view (MTV) of data augmentation -- in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger levels of augmentation. In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that the MTV leads to higher and more robust performance improvements than traditional augmentation.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.05469](https://arxiv.org/abs/2101.05469) [cs.CL]** |
|           | (or **[arXiv:2101.05469v1](https://arxiv.org/abs/2101.05469v1) [cs.CL]** for this version) |





<h2 id="2021-01-15-3">3. Persuasive Natural Language Generation -- A Literature Review</h2>

Title: [Persuasive Natural Language Generation -- A Literature Review](https://arxiv.org/abs/2101.05786)

Authors:[Sebastian Duerr](https://arxiv.org/search/cs?searchtype=author&query=Duerr%2C+S), [Peter A. Gloor](https://arxiv.org/search/cs?searchtype=author&query=Gloor%2C+P+A)

> This literature review focuses on the use of Natural Language Generation (NLG) to automatically detect and generate persuasive texts. Extending previous research on automatic identification of persuasion in text, we concentrate on generative aspects through conceptualizing determinants of persuasion in five business-focused categories: benevolence, linguistic appropriacy, logical argumentation, trustworthiness, tools and datasets. These allow NLG to increase an existing message's persuasiveness. Previous research illustrates key aspects in each of the above mentioned five categories. A research agenda to further study persuasive NLG is developed. The review includes analysis of seventy-seven articles, outlining the existing body of knowledge and showing the steady progress in this research field.

| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7; J.4                                                   |
| Cite as:     | **[arXiv:2101.05786](https://arxiv.org/abs/2101.05786) [cs.CL]** |
|              | (or **[arXiv:2101.05786v1](https://arxiv.org/abs/2101.05786v1) [cs.CL]** for this version) |





# 2021-01-14

[Return to Index](#Index)



<h2 id="2021-01-14-1">1. Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding</h2>

Title: [Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding](https://arxiv.org/abs/2101.05208)

Authors: [Dexin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

> Visual context provides grounding information for multimodal machine translation (MMT). However, previous MMT models and probing studies on visual features suggest that visual information is less explored in MMT as it is often redundant to textual information. In this paper, we propose an object-level visual context modeling framework (OVC) to efficiently capture and explore visual information for multimodal machine translation. With detected objects, the proposed OVC encourages MMT to ground translation on desirable visual objects by masking irrelevant objects in the visual modality. We equip the proposed with an additional object-masking loss to achieve this goal. The object-masking loss is estimated according to the similarity between masked objects and the source texts so as to encourage masking source-irrelevant objects. Additionally, in order to generate vision-consistent target words, we further propose a vision-weighted translation loss for OVC. Experiments on MMT datasets demonstrate that the proposed OVC model outperforms state-of-the-art MMT models and analyses show that masking irrelevant objects helps grounding in MMT.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.05208](https://arxiv.org/abs/2101.05208) [cs.CV]** |
|           | (or **[arXiv:2101.05208v1](https://arxiv.org/abs/2101.05208v1) [cs.CV]** for this version) |





<h2 id="2021-01-14-2">2. Latent Alignment of Procedural Concepts in Multimodal Recipes</h2>

Title: [Latent Alignment of Procedural Concepts in Multimodal Recipes](https://arxiv.org/abs/2101.04727)

Authors: [Hossein Rajaby Faghihi](https://arxiv.org/search/cs?searchtype=author&query=Faghihi%2C+H+R), [Roshanak Mirzaee](https://arxiv.org/search/cs?searchtype=author&query=Mirzaee%2C+R), [Sudarshan Paliwal](https://arxiv.org/search/cs?searchtype=author&query=Paliwal%2C+S), [Parisa Kordjamshidi](https://arxiv.org/search/cs?searchtype=author&query=Kordjamshidi%2C+P)

> We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a reading comprehension on a recipe containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max-pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19\% improvement over the baselines.

| Comments:          | Published in ALVR 2020, a workshop in ACL 2020               |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Proceedings of the First Workshop on Advances in Language and Vision Research 2020 (26-31) |
| DOI:               | [10.18653/v1/2020.alvr-1.5](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2020.alvr-1.5&v=43bac969) |
| Cite as:           | **[arXiv:2101.04727](https://arxiv.org/abs/2101.04727) [cs.CL]** |
|                    | (or **[arXiv:2101.04727v1](https://arxiv.org/abs/2101.04727v1) [cs.CL]** for this version) |







<h2 id="2021-01-14-3">3. Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration</h2>

Title: [Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration](https://arxiv.org/abs/2101.05162)

Authors: [B. Mansurov](https://arxiv.org/search/cs?searchtype=author&query=Mansurov%2C+B), [A. Mansurov](https://arxiv.org/search/cs?searchtype=author&query=Mansurov%2C+A)

> In this paper, we introduce a data-driven approach to transliterating Uzbek dictionary words from the Cyrillic script into the Latin script, and vice versa. We heuristically align characters of words in the source script with sub-strings of the corresponding words in the target script and train a decision tree classifier that learns these alignments. On the test set, our Cyrillic to Latin model achieves a character level micro-averaged F1 score of 0.9992, and our Latin to Cyrillic model achieves the score of 0.9959. Our contribution is a novel method of producing machine transliterated texts for the low-resource Uzbek language.

| Comments: | 9 pages, 11 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.05162](https://arxiv.org/abs/2101.05162) [cs.CL]** |
|           | (or **[arXiv:2101.05162v1](https://arxiv.org/abs/2101.05162v1) [cs.CL]** for this version) |









# 2021-01-13

[Return to Index](#Index)



<h2 id="2021-01-13-1">1. Quantum Mathematics in Artificial Intelligence</h2>

Title: [Quantum Mathematics in Artificial Intelligence](https://arxiv.org/abs/2101.04255)

Authors:[Dominic Widdows](https://arxiv.org/search/cs?searchtype=author&query=Widdows%2C+D), [Kirsty Kitto](https://arxiv.org/search/cs?searchtype=author&query=Kitto%2C+K), [Trevor Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+T)

> In the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. At the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news.
> The mathematical techniques underlying both these areas have more in common than is sometimes realized. Vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. Quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks.
> This paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (AI), particularly in automated reasoning and natural language processing (NLP). Techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. Application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition.
> Some of these approaches can potentially be implemented on quantum hardware. Many of the practical steps in this implementation are in early stages, and some are already realized. Explaining some of the common mathematical tools can help researchers in both AI and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.04255](https://arxiv.org/abs/2101.04255) [cs.AI]** |
|           | (or **[arXiv:2101.04255v1](https://arxiv.org/abs/2101.04255v1) [cs.AI]** for this version) |





<h2 id="2021-01-13-2">2. Explain and Predict, and then Predict again</h2>

Title: [Explain and Predict, and then Predict again](https://arxiv.org/abs/2101.04109)

Authors:[Zijian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Koustav Rudra](https://arxiv.org/search/cs?searchtype=author&query=Rudra%2C+K), [Avishek Anand](https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+A)

> A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, that uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. And then we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- fact verification, sentiment classification, and QA -- and find that we substantially outperform existing approaches.

| Comments:    | Accepted in the WSDM 2021 and the camera-ready version will be there soon |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.m; I.2.7                                                 |
| DOI:         | [10.1145/3437963.3441758](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3437963.3441758&v=6e4daadf) |
| Cite as:     | **[arXiv:2101.04109](https://arxiv.org/abs/2101.04109) [cs.CL]** |
|              | (or **[arXiv:2101.04109v1](https://arxiv.org/abs/2101.04109v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-3">3. Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning</h2>

Title: [Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning](https://arxiv.org/abs/2101.04229)

Authors:[Evgeny Lagutin](https://arxiv.org/search/cs?searchtype=author&query=Lagutin%2C+E), [Daniil Gavrilov](https://arxiv.org/search/cs?searchtype=author&query=Gavrilov%2C+D), [Pavel Kalaidin](https://arxiv.org/search/cs?searchtype=author&query=Kalaidin%2C+P)

> Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019). Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li at al., 2020). In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.

| Comments: | accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.04229](https://arxiv.org/abs/2101.04229) [cs.CL]** |
|           | (or **[arXiv:2101.04229v1](https://arxiv.org/abs/2101.04229v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-4">4. Transforming Multi-Conditioned Generation from Meaning Representation</h2>

Title: [Transforming Multi-Conditioned Generation from Meaning Representation](https://arxiv.org/abs/2101.04257)

Authors:[Joosung Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J)

> In task-oriented conversation systems, natural language generation systems that generate sentences with specific information related to conversation flow are useful. Our study focuses on language generation by considering various information representing the meaning of utterances as multiple conditions of generation. NLG from meaning representations, the conditions for sentence meaning, generally goes through two steps: sentence planning and surface realization. However, we propose a simple one-stage framework to generate utterances directly from MR (Meaning Representation). Our model is based on GPT2 and generates utterances with flat conditions on slot and value pairs, which does not need to determine the structure of the sentence. We evaluate several systems in the E2E dataset with 6 automatic metrics. Our system is a simple method, but it demonstrates comparable performance to previous systems in automated metrics. In addition, using only 10\% of the data set without any other techniques, our model achieves comparable performance, and shows the possibility of performing zero-shot generation and expanding to other datasets.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.04257](https://arxiv.org/abs/2101.04257) [cs.CL]** |
|           | (or **[arXiv:2101.04257v1](https://arxiv.org/abs/2101.04257v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-5">5. Toward Effective Automated Content Analysis via Crowdsourcing</h2>

Title: [Toward Effective Automated Content Analysis via Crowdsourcing](https://arxiv.org/abs/2101.04615)

Authors:[Jiele Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Chau-Wai Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+C), [Xinyan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X), [Xianpeng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X)

> Many computer scientists use the aggregated answers of online workers to represent ground truth. Prior work has shown that aggregation methods such as majority voting are effective for measuring relatively objective features. For subjective features such as semantic connotation, online workers, known for optimizing their hourly earnings, tend to deteriorate in the quality of their responses as they work longer. In this paper, we aim to address this issue by proposing a quality-aware semantic data annotation system. We observe that with timely feedback on workers' performance quantified by quality scores, better informed online workers can maintain the quality of their labeling throughout an extended period of time. We validate the effectiveness of the proposed annotation system through i) evaluating performance based on an expert-labeled dataset, and ii) demonstrating machine learning tasks that can lead to consistent learning behavior with 70%-80% accuracy. Our results suggest that with our system, researchers can collect high-quality answers of subjective semantic features at a large scale.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.04615](https://arxiv.org/abs/2101.04615) [cs.CL]** |
|           | (or **[arXiv:2101.04615v1](https://arxiv.org/abs/2101.04615v1) [cs.CL]** for this version) |





# 2021-01-12

[Return to Index](#Index)



<h2 id="2021-01-12-1">1. Misspelling Correction with Pre-trained Contextual Language Model</h2>

Title: [Misspelling Correction with Pre-trained Contextual Language Model](https://arxiv.org/abs/2101.03204)

Authors: [Yifei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Xiaonan Jing](https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+X), [Youlim Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+Y), [Julia Taylor Rayz](https://arxiv.org/search/cs?searchtype=author&query=Rayz%2C+J+T)

> Spelling irregularities, known now as spelling mistakes, have been found for several centuries. As humans, we are able to understand most of the misspelled words based on their location in the sentence, perceived pronunciation, and context. Unlike humans, computer systems do not possess the convenient auto complete functionality of which human brains are capable. While many programs provide spelling correction functionality, many systems do not take context into account. Moreover, Artificial Intelligence systems function in the way they are trained on. With many current Natural Language Processing (NLP) systems trained on grammatically correct text data, many are vulnerable against adversarial examples, yet correctly spelled text processing is crucial for learning. In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model BERT. We present two experiments, based on BERT and the edit distance algorithm, for ranking and selecting candidate corrections. The results of our experiments demonstrated that when combined properly, contextual word embeddings of BERT and edit distance are capable of effectively correcting spelling errors.

| Comments: | Accepted by 2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC). IEEE |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.03204](https://arxiv.org/abs/2101.03204) [cs.CL]** |
|           | (or **[arXiv:2101.03204v1](https://arxiv.org/abs/2101.03204v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-2">2. SDA: Improving Text Generation with Self Data Augmentation</h2>

Title: [SDA: Improving Text Generation with Self Data Augmentation](https://arxiv.org/abs/2101.03236)

Authors: [Ping Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P), [Ruiyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Chunyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> Data augmentation has been widely used to improve deep neural networks in many research fields, such as computer vision. However, less work has been done in the context of text, partially due to its discrete nature and the complexity of natural languages. In this paper, we propose to improve the standard maximum likelihood estimation (MLE) paradigm by incorporating a self-imitation-learning phase for automatic data augmentation. Unlike most existing sentence-level augmentation strategies, which are only applied to specific models, our method is more general and could be easily adapted to any MLE-based training procedure. In addition, our framework allows task-specific evaluation metrics to be designed to flexibly control the generated sentences, for example, in terms of controlling vocabulary usage and avoiding nontrivial repetitions. Extensive experimental results demonstrate the superiority of our method on two synthetic and several standard real datasets, significantly improving related baselines.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03236](https://arxiv.org/abs/2101.03236) [cs.CL]** |
|           | (or **[arXiv:2101.03236v1](https://arxiv.org/abs/2101.03236v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-3">3. Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing</h2>

Title: [Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing](https://arxiv.org/abs/2101.03289)

Authors: [Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M), [Viet Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V), [Amir Pouran Ben Veyseh](https://arxiv.org/search/cs?searchtype=author&query=Veyseh%2C+A+P+B), [Thien Huu Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H)

> We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: [this https URL](https://github.com/nlp-uoregon/trankit). A demo website for our toolkit is also available at: [this http URL](http://nlp.uoregon.edu/trankit).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03289](https://arxiv.org/abs/2101.03289) [cs.CL]** |
|           | (or **[arXiv:2101.03289v1](https://arxiv.org/abs/2101.03289v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-4">4. Learning Better Sentence Representation with Syntax Information</h2>

Title: [Learning Better Sentence Representation with Syntax Information](https://arxiv.org/abs/2101.03343)

Authors: [Chen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C) (University of Science and Technology of China)

> Sentence semantic understanding is a key topic in the field of natural language processing. Recently, contextualized word representations derived from pre-trained language models such as ELMO and BERT have shown significant improvements for a wide range of semantic tasks, e.g. question answering, text classification and sentiment analysis. However, how to add external knowledge to further improve the semantic modeling capability of model is worth probing. In this paper, we propose a novel approach to combining syntax information with a pre-trained language model. In order to evaluate the effect of the pre-training model, first, we introduce RNN-based and Transformer-based pre-trained language models; secondly, to better integrate external knowledge, such as syntactic information integrate with the pre-training model, we propose a dependency syntax expansion (DSE) model. For evaluation, we have selected two subtasks: sentence completion task and biological relation extraction task. The experimental results show that our model achieves 91.2\% accuracy, outperforming the baseline model by 37.8\% on sentence completion task. And it also gets competitive performance by 75.1\% F1 score on relation extraction task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03343](https://arxiv.org/abs/2101.03343) [cs.CL]** |
|           | (or **[arXiv:2101.03343v1](https://arxiv.org/abs/2101.03343v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-5">5. Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation</h2>

Title: [Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation](https://arxiv.org/abs/2101.04030)

Authors: [Ritam Mallick](https://arxiv.org/search/cs?searchtype=author&query=Mallick%2C+R), [Seba Susan](https://arxiv.org/search/cs?searchtype=author&query=Susan%2C+S), [Vaibhaw Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+V), [Rizul Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+R), [Prateek Rawal](https://arxiv.org/search/cs?searchtype=author&query=Rawal%2C+P)

> Neural Machine Translation model is a sequence-to-sequence converter based on neural networks. Existing models use recurrent neural networks to construct both the encoder and decoder modules. In alternative research, the recurrent networks were substituted by convolutional neural networks for capturing the syntactic structure in the input sentence and decreasing the processing time. We incorporate the goodness of both approaches by proposing a convolutional-recurrent encoder for capturing the context information as well as the sequential information from the source sentence. Word embedding and position embedding of the source sentence is performed prior to the convolutional encoding layer which is basically a n-gram feature extractor capturing phrase-level context information. The rectified output of the convolutional encoding layer is added to the original embedding vector, and the sum is normalized by layer normalization. The normalized output is given as a sequential input to the recurrent encoding layer that captures the temporal information in the sequence. For the decoder, we use the attention-based recurrent neural network. Translation task on the German-English dataset verifies the efficacy of the proposed approach from the higher BLEU scores achieved as compared to the state of the art.

| Comments: | Accepted in 36th ACM/SIGAPP Symposium On Applied Computing 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.04030](https://arxiv.org/abs/2101.04030) [cs.CL]** |
|           | (or **[arXiv:2101.04030v1](https://arxiv.org/abs/2101.04030v1) [cs.CL]** for this version) |





# 2021-01-11

[Return to Index](#Index)



<h2 id="2021-01-11-1">1. MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets</h2>

Title: [MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets](https://arxiv.org/abs/2101.03028)

Authors:[Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Peng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Chenghao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C)

> Natural language processing (NLP) has been applied to various fields including text classification and sentiment analysis. In the shared task of sentiment analysis of code-mixed tweets, which is a part of the SemEval-2020 competition~\cite{patwa2020sentimix}, we preprocess datasets by replacing emoji and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03028](https://arxiv.org/abs/2101.03028) [cs.CL]** |
|           | (or **[arXiv:2101.03028v1](https://arxiv.org/abs/2101.03028v1) [cs.CL]** for this version) |









# 2021-01-08

[Return to Index](#Index)



<h2 id="2021-01-08-1">1. User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics</h2>

Title: [User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics](https://arxiv.org/abs/2101.02244)

Authors: [Anamaria Crisan](https://arxiv.org/search/cs?searchtype=author&query=Crisan%2C+A), [Michael Correll](https://arxiv.org/search/cs?searchtype=author&query=Correll%2C+M)

> Topic models are widely used analysis techniques for clustering documents and surfacing thematic elements of text corpora. These models remain challenging to optimize and often require a "human-in-the-loop" approach where domain experts use their knowledge to steer and adjust. However, the fragility, incompleteness, and opacity of these models means even minor changes could induce large and potentially undesirable changes in resulting model. In this paper we conduct a simulation-based analysis of human-centered interactions with topic models, with the objective of measuring the sensitivity of topic models to common classes of user actions. We find that user interactions have impacts that differ in magnitude but often negatively affect the quality of the resulting modelling in a way that can be difficult for the user to evaluate. We suggest the incorporation of sensitivity and "multiverse" analyses to topic model interfaces to surface and overcome these deficiencies.

| Comments:    | 16 Pages, 9 Figures, CHI 2021 Conference                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| MSC classes: | 68U15                                                        |
| ACM classes: | H.5.0                                                        |
| Cite as:     | **[arXiv:2101.02244](https://arxiv.org/abs/2101.02244) [cs.HC]** |
|              | (or **[arXiv:2101.02244v1](https://arxiv.org/abs/2101.02244v1) [cs.HC]** for this version) |





<h2 id="2021-01-08-2">2. Towards a Smart Data Processing and Storage Model</h2>

Title: [Towards a Smart Data Processing and Storage Model](https://arxiv.org/abs/2101.02522)

Authors: [Ronie Salgado](https://arxiv.org/search/cs?searchtype=author&query=Salgado%2C+R), [Marcus Denker](https://arxiv.org/search/cs?searchtype=author&query=Denker%2C+M) (RMOD), [Stéphane Ducasse](https://arxiv.org/search/cs?searchtype=author&query=Ducasse%2C+S) (RMOD), [Anne Etien](https://arxiv.org/search/cs?searchtype=author&query=Etien%2C+A) (RMOD), [Vincent Aranega](https://arxiv.org/search/cs?searchtype=author&query=Aranega%2C+V) (RMOD)

> In several domains it is crucial to store and manipulate data whose origin needs to be completely traceable to guarantee the consistency, trustworthiness and reliability on the data itself typically for ethical and legal reasons. It is also important to guarantee that such properties are also carried further when such data is composed and processed into new data. In this article we present the main requirements and theorethical problems that arise by the design of a system supporting data with such capabilities. We present an architecture for implementing a system as well as a prototype developed in Pharo.

| Subjects:          | **Computation and Language (cs.CL)**; Programming Languages (cs.PL); Software Engineering (cs.SE) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | IWST20: International Workshop on Smalltalk Technologies, Sep 2020, Novi Sad, Serbia |
| Cite as:           | **[arXiv:2101.02522](https://arxiv.org/abs/2101.02522) [cs.CL]** |
|                    | (or **[arXiv:2101.02522v1](https://arxiv.org/abs/2101.02522v1) [cs.CL]** for this version) |







# 2021-01-07

[Return to Index](#Index)



<h2 id="2021-01-07-1">1. AutoDropout: Learning Dropout Patterns to Regularize Deep Networks</h2>

Title: [AutoDropout: Learning Dropout Patterns to Regularize Deep Networks](https://arxiv.org/abs/2101.01761)

Authors: [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> Neural networks are often over-parameterized and hence benefit from aggressive regularization. Conventional regularization methods, such as Dropout or weight decay, do not leverage the structures of the network's inputs and hidden states. As a result, these conventional methods are less effective than methods that leverage the structures, such as SpatialDropout and DropBlock, which randomly drop the values at certain contiguous areas in the hidden states and setting them to zero. Although the locations of dropout areas random, the patterns of SpatialDropout and DropBlock are manually designed and fixed. Here we propose to learn the dropout patterns. In our method, a controller learns to generate a dropout pattern at every channel and layer of a target network, such as a ConvNet or a Transformer. The target network is then trained with the dropout pattern, and its resulting validation performance is used as a signal for the controller to learn from. We show that this method works well for both image recognition on CIFAR-10 and ImageNet, as well as language modeling on Penn Treebank and WikiText-2. The learned dropout patterns also transfers to different tasks and datasets, such as from language model on Penn Treebank to Engligh-French translation on WMT 2014. Our code will be available.

| Comments: | Accepted to AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2101.01761](https://arxiv.org/abs/2101.01761) [cs.LG]** |
|           | (or **[arXiv:2101.01761v1](https://arxiv.org/abs/2101.01761v1) [cs.LG]** for this version) |







# 2021-01-06

[Return to Index](#Index)



<h2 id="2021-01-06-1">1. I-BERT: Integer-only BERT Quantization</h2>

Title: [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)

Authors: [Sehoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Amir Gholami](https://arxiv.org/search/cs?searchtype=author&query=Gholami%2C+A), [Zhewei Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Michael W. Mahoney](https://arxiv.org/search/cs?searchtype=author&query=Mahoney%2C+M+W), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K)

> Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for many edge processors, and it has been a challenge to deploy these models for edge applications and devices that have resource constraints. While quantization can be a viable solution to this, previous work on quantizing Transformer based models uses floating-point arithmetic during inference, thus limiting model deployment on many edge processors. In this work, we propose a novel integer-only quantization scheme for Transformer based models that quantizes the entire inference process. In particular, we demonstrate how to approximate nonlinear operations in Transformer architectures, e.g., GELU, Softmax, and Layer Normalization, with lightweight integer computations. We use those approximations in our method, I-BERT, with an end-to-end integer-only inference, and without any floating point calculation. We test our approach on GLUE downstream tasks using RoBERTa-Base and RoBERTa-Large. For both cases, with an 8-bit integer-only quantization scheme, I-BERT achieves similar accuracy as compared to the full-precision baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.01321](https://arxiv.org/abs/2101.01321) [cs.CL]** |
|           | (or **[arXiv:2101.01321v1](https://arxiv.org/abs/2101.01321v1) [cs.CL]** for this version) |





<h2 id="2021-01-06-2">2. Political Depolarization of News Articles Using Attribute-aware Word Embeddings</h2>

Title: [Political Depolarization of News Articles Using Attribute-aware Word Embeddings](https://arxiv.org/abs/2101.01391)

Authors: [Ruibo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+R), [Lili Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Chenyan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+C), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> Political polarization in the US is on the rise. This polarization negatively affects the public sphere by contributing to the creation of ideological echo chambers. In this paper, we focus on addressing one of the factors that contributes to this polarity, polarized media. We introduce a framework for depolarizing news articles. Given an article on a certain topic with a particular ideological slant (eg., liberal or conservative), the framework first detects polar language in the article and then generates a new article with the polar language replaced with neutral expressions. To detect polar words, we train a multi-attribute-aware word embedding model that is aware of ideology and topics on 360k full-length media articles. Then, for text generation, we propose a new algorithm called Text Annealing Depolarization Algorithm (TADA). TADA retrieves neutral expressions from the word embedding model that not only decrease ideological polarity but also preserve the original argument of the text, while maintaining grammatical correctness. We evaluate our framework by comparing the depolarized output of our model in two modes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics. Based on feedback from 161 human testers, our framework successfully depolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs in fully-automatic mode. Furthermore, 81.2% of the testers agree that the non-polar content information is well-preserved and 79% agree that depolarization does not harm semantic correctness when they compare the original text and the depolarized text. Our work shows that data-driven methods can help to locate political polarity and aid in the depolarization of articles.

| Comments: | In Proceedings of the 15th International AAAI Conference on Weblogs and Social Media (ICWSM 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.01391](https://arxiv.org/abs/2101.01391) [cs.CL]** |
|           | (or **[arXiv:2101.01391v1](https://arxiv.org/abs/2101.01391v1) [cs.CL]** for this version) |





<h2 id="2021-01-06-3">3. Local Translation Services for Neglected Languages</h2>

Title: [Local Translation Services for Neglected Languages](https://arxiv.org/abs/2101.01628)

Authors: [David Noever](https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D), [Josh Kalin](https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J), [Matt Ciolino](https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M), [Dom Hambrick](https://arxiv.org/search/cs?searchtype=author&query=Hambrick%2C+D), [Gerry Dozier](https://arxiv.org/search/cs?searchtype=author&query=Dozier%2C+G)

> Taking advantage of computationally lightweight, but high-quality translators prompt consideration of new applications that address neglected languages. Locally run translators for less popular languages may assist data projects with protected or personal data that may require specific compliance checks before posting to a public translation API, but which could render reasonable, cost-effective solutions if done with an army of local, small-scale pair translators. Like handling a specialist's dialect, this research illustrates translating two historically interesting, but obfuscated languages: 1) hacker-speak ("l33t") and 2) reverse (or "mirror") writing as practiced by Leonardo da Vinci. The work generalizes a deep learning architecture to translatable variants of hacker-speak with lite, medium, and hard vocabularies. The original contribution highlights a fluent translator of hacker-speak in under 50 megabytes and demonstrates a generator for augmenting future datasets with greater than a million bilingual sentence pairs. The long short-term memory, recurrent neural network (LSTM-RNN) extends previous work demonstrating an English-to-foreign translation service built from as little as 10,000 bilingual sentence pairs. This work further solves the equivalent translation problem in twenty-six additional (non-obfuscated) languages and rank orders those models and their proficiency quantitatively with Italian as the most successful and Mandarin Chinese as the most challenging. For neglected languages, the method prototypes novel services for smaller niche translations such as Kabyle (Algerian dialect) which covers between 5-7 million speakers but one which for most enterprise translators, has not yet reached development. One anticipates the extension of this approach to other important dialects, such as translating technical (medical or legal) jargon and processing health records.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.01628](https://arxiv.org/abs/2101.01628) [cs.CL]** |
|           | (or **[arXiv:2101.01628v1](https://arxiv.org/abs/2101.01628v1) [cs.CL]** for this version) |







# 2021-01-05

[Return to Index](#Index)



<h2 id="2021-01-05-1">1. VinVL: Making Visual Representations Matter in Vision-Language Models</h2>

Title: [VinVL: Making Visual Representations Matter in Vision-Language Models](https://arxiv.org/abs/2101.00529)

Authors: [Pengchuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Xiujun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaowei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X), [Jianwei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used \emph{bottom-up and top-down} model \cite{anderson2018bottom}, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \oscar \cite{li2020oscar}, and utilize an improved approach \short\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00529](https://arxiv.org/abs/2101.00529) [cs.CV]** |
|           | (or **[arXiv:2101.00529v1](https://arxiv.org/abs/2101.00529v1) [cs.CV]** for this version) |





<h2 id="2021-01-05-2">2. The Pile: An 800GB Dataset of Diverse Text for Language Modeling</h2>

Title: [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)

Authors: [Leo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Stella Biderman](https://arxiv.org/search/cs?searchtype=author&query=Biderman%2C+S), [Sid Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+S), [Laurence Golding](https://arxiv.org/search/cs?searchtype=author&query=Golding%2C+L), [Travis Hoppe](https://arxiv.org/search/cs?searchtype=author&query=Hoppe%2C+T), [Charles Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+C), [Jason Phang](https://arxiv.org/search/cs?searchtype=author&query=Phang%2C+J), [Horace He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H), [Anish Thite](https://arxiv.org/search/cs?searchtype=author&query=Thite%2C+A), [Noa Nabeshima](https://arxiv.org/search/cs?searchtype=author&query=Nabeshima%2C+N), [Shawn Presser](https://arxiv.org/search/cs?searchtype=author&query=Presser%2C+S), [Connor Leahy](https://arxiv.org/search/cs?searchtype=author&query=Leahy%2C+C)

> Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00027](https://arxiv.org/abs/2101.00027) [cs.CL]** |
|           | (or **[arXiv:2101.00027v1](https://arxiv.org/abs/2101.00027v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-3">3. EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets</h2>

Title: [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://arxiv.org/abs/2101.00063)

Authors: [Xiaohan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Zhangyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Deep, heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focus on reducing inference cost/time, while still requiring expensive training process. Other works use extremely large batch sizes to shorten the pre-training time at the expense of high demand for computation resources. In this paper, inspired by the Early-Bird Lottery Tickets studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. We are the first to identify structured winning tickets in the early stage of BERT training, and use them for efficient training. Comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks show that EarlyBERT easily achieves comparable performance to standard BERT with 35~45% less training time.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00063](https://arxiv.org/abs/2101.00063) [cs.CL]** |
|           | (or **[arXiv:2101.00063v1](https://arxiv.org/abs/2101.00063v1) [cs.CL]** for this version) |



<h2 id="2021-01-05-4">4. Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment</h2>

Title: [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://arxiv.org/abs/2101.00148)

Authors: [Haoyue Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+H), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Sida I. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S+I)

> Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00148](https://arxiv.org/abs/2101.00148) [cs.CL]** |
|           | (or **[arXiv:2101.00148v1](https://arxiv.org/abs/2101.00148v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-5">5. A Graph Total Variation Regularized Softmax for Text Generation</h2>

Title: [A Graph Total Variation Regularized Softmax for Text Generation](https://arxiv.org/abs/2101.00153)

Authors: [Liu Bin](https://arxiv.org/search/cs?searchtype=author&query=Bin%2C+L), [Wang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+W), [Yin Guosheng](https://arxiv.org/search/cs?searchtype=author&query=Guosheng%2C+Y)

> The softmax operator is one of the most important functions in machine learning models. When applying neural networks to multi-category classification, the correlations among different categories are often ignored. For example, in text generation, a language model makes a choice of each new word based only on the former selection of its context. In this scenario, the link statistics information of concurrent words based on a corpus (an analogy of the natural way of expression) is also valuable in choosing the next word, which can help to improve the sentence's fluency and smoothness. To fully explore such important information, we propose a graph softmax function for text generation. It is expected that the final classification result would be dominated by both the language model and graphical text relationships among words. We use a graph total variation term to regularize softmax so as to incorporate the concurrent relationship into the language model. The total variation of the generated words should be small locally. We apply the proposed graph softmax to GPT2 for the text generation task. Experimental results demonstrate that the proposed graph softmax achieves better BLEU and perplexity than softmax. Human testers can also easily distinguish the text generated by the graph softmax or softmax.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00153](https://arxiv.org/abs/2101.00153) [cs.CL]** |
|           | (or **[arXiv:2101.00153v1](https://arxiv.org/abs/2101.00153v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-6">6. BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding</h2>

Title: [BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding](https://arxiv.org/abs/2101.00204)

Authors: [Abhik Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+A), [Tahmid Hasan](https://arxiv.org/search/cs?searchtype=author&query=Hasan%2C+T), [Kazi Samin](https://arxiv.org/search/cs?searchtype=author&query=Samin%2C+K), [M. Sohel Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+M+S), [Anindya Iqbal](https://arxiv.org/search/cs?searchtype=author&query=Iqbal%2C+A), [Rifat Shahriyar](https://arxiv.org/search/cs?searchtype=author&query=Shahriyar%2C+R)

> Pre-training language models on large volume of data with self-supervised objectives has become a standard practice in natural language processing. However, most such state-of-the-art models are available in only English and other resource-rich languages. Even in multilingual models, which are trained on hundreds of languages, low-resource ones still remain underrepresented. Bangla, the seventh most widely spoken language in the world, is still low in terms of resources. Few downstream task datasets for language understanding in Bangla are publicly available, and there is a clear shortage of good quality data for pre-training. In this work, we build a Bangla natural language understanding model pre-trained on 18.6 GB data we crawled from top Bangla sites on the internet. We introduce a new downstream task dataset and benchmark on four tasks on sentence classification, document classification, natural language understanding, and sequence tagging. Our model outperforms multilingual baselines and previous state-of-the-art results by 1-6%. In the process, we identify a major shortcoming of multilingual models that hurt performance for low-resource languages that don't share writing scripts with any high resource one, which we name the `Embedding Barrier'. We perform extensive experiments to study this barrier. We release all our datasets and pre-trained models to aid future NLP research on Bangla and other low-resource languages. Our code and data are available at [this https URL](https://github.com/csebuetnlp/banglabert).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00204](https://arxiv.org/abs/2101.00204) [cs.CL]** |
|           | (or **[arXiv:2101.00204v1](https://arxiv.org/abs/2101.00204v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-7">7. Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers</h2>

Title: [Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](https://arxiv.org/abs/2101.00234)

Authors: [Machel Reid](https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M), [Edison Marrese-Taylor](https://arxiv.org/search/cs?searchtype=author&query=Marrese-Taylor%2C+E), [Yutaka Matsuo](https://arxiv.org/search/cs?searchtype=author&query=Matsuo%2C+Y)

> The advent of the Transformer can arguably be described as a driving force behind many of the recent advances in natural language processing. However, despite their sizeable performance improvements, as recently shown, the model is severely over-parameterized, being parameter inefficient and computationally expensive to train. Inspired by the success of parameter-sharing in pretrained deep contextualized word representation encoders, we explore parameter-sharing methods in Transformers, with a specific focus on encoder-decoder models for sequence-to-sequence tasks such as neural machine translation. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer, a parameter efficient Transformer-based model which combines the newly proposed Sandwich-style parameter sharing technique - designed to overcome the deficiencies in naive cross-layer parameter sharing for generative models - and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization, and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.00234](https://arxiv.org/abs/2101.00234) [cs.CL]** |
|           | (or **[arXiv:2101.00234v1](https://arxiv.org/abs/2101.00234v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-8">8. Understanding Few-Shot Commonsense Knowledge Models</h2>

Title: [Understanding Few-Shot Commonsense Knowledge Models](https://arxiv.org/abs/2101.00297)

Authors: [Jeff Da](https://arxiv.org/search/cs?searchtype=author&query=Da%2C+J), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A)

> Providing natural language processing systems with commonsense knowledge is a critical challenge for achieving language understanding. Recently, commonsense knowledge models have emerged as a suitable approach for hypothesizing situation-relevant commonsense knowledge on-demand in natural language applications. However, these systems are limited by the fixed set of relations captured by schemas of the knowledge bases on which they're trained.
> To address this limitation, we investigate training commonsense knowledge models in a few-shot setting with limited tuples per commonsense relation in the graph. We perform five separate studies on different dimensions of few-shot commonsense knowledge learning, providing a roadmap on best practices for training these systems efficiently. Importantly, we find that human quality ratings for knowledge produced from a few-shot trained system can achieve performance within 6% of knowledge produced from fully supervised systems. This few-shot performance enables coverage of a wide breadth of relations in future commonsense systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00297](https://arxiv.org/abs/2101.00297) [cs.CL]** |
|           | (or **[arXiv:2101.00297v1](https://arxiv.org/abs/2101.00297v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-9">9. On-the-Fly Attention Modularization for Neural Generation</h2>

Title: [On-the-Fly Attention Modularization for Neural Generation](https://arxiv.org/abs/2101.00371)

Authors: [Yue Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y), [Chandra Bhagavatula](https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Jena D. Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+J+D), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A), [Jackie Chi Kit Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+J+C+K), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: generated text is repetitive, generic, self-inconsistent, and lacking commonsense. The empirical analyses on sentence-level attention patterns reveal that neural text degeneration may be associated with insufficient learning of inductive biases by the attention mechanism. Our findings motivate on-the-fly attention modularization, a simple but effective method for injecting inductive biases into attention computation during inference. The resulting text produced by the language model with attention modularization can yield enhanced diversity and commonsense reasoning while maintaining fluency and coherence.

| Comments: | 10 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.00371](https://arxiv.org/abs/2101.00371) [cs.CL]** |
|           | (or **[arXiv:2101.00371v1](https://arxiv.org/abs/2101.00371v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-10">10. Cross-Document Language Modeling</h2>

Title: [Cross-Document Language Modeling](https://arxiv.org/abs/2101.00406)

Authors: [Avi Caciularu](https://arxiv.org/search/cs?searchtype=author&query=Caciularu%2C+A), [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E), [Arie Cattan](https://arxiv.org/search/cs?searchtype=author&query=Cattan%2C+A), [Ido Dagan](https://arxiv.org/search/cs?searchtype=author&query=Dagan%2C+I)

> We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our cross-document language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including cross-document event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00406](https://arxiv.org/abs/2101.00406) [cs.CL]** |
|           | (or **[arXiv:2101.00406v1](https://arxiv.org/abs/2101.00406v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-11">11. Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</h2>

Title: [Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](https://arxiv.org/abs/2101.00416)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we generalize text infilling (e.g., masked language models) by proposing Sequence Span Rewriting (SSR) as a self-supervised sequence-to-sequence (seq2seq) pre-training objective. SSR provides more fine-grained learning signals for text representations by supervising the model to rewrite imperfect spans to ground truth, and it is more consistent than text infilling with many downstream seq2seq tasks that rewrite a source sentences into a target sentence. Our experiments with T5 models on various seq2seq tasks show that SSR can substantially improve seq2seq pre-training. Moreover, we observe SSR is especially helpful to improve pre-training a small-size seq2seq model with a powerful imperfect span generator, which indicates a new perspective of transferring knowledge from a large model to a smaller model for seq2seq pre-training.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00416](https://arxiv.org/abs/2101.00416) [cs.CL]** |
|           | (or **[arXiv:2101.00416v1](https://arxiv.org/abs/2101.00416v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-12">12. KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation</h2>

Title: [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://arxiv.org/abs/2101.00419)

Authors: [Yiran Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+Y), [Zai Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Z), [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Yunpu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We extend the popular BART architecture to a multi-modal model. We design a new pretraining task to improve the model performance on Visual Commonsense Generation task. Our pretraining task improves the Visual Commonsense Generation performance by leveraging knowledge from a large language model pretrained on an external knowledge graph. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on Visual Commonsense Generation. Experimental results show that by pretraining, our model reaches state-of-the-art performance on the Visual Commonsense Generation task.

| Comments: | Work in progress. The first three authors contribute equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.00419](https://arxiv.org/abs/2101.00419) [cs.CL]** |
|           | (or **[arXiv:2101.00419v1](https://arxiv.org/abs/2101.00419v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-13">13. Decoding Time Lexical Domain Adaptationfor Neural Machine Translation</h2>

Title: [Decoding Time Lexical Domain Adaptationfor Neural Machine Translation](https://arxiv.org/abs/2101.00421)

Authors: [Nikolay Bogoychev](https://arxiv.org/search/cs?searchtype=author&query=Bogoychev%2C+N), [Pinzhen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P)

> Machine translation systems are vulnerable to domain mismatch, especially when the task is low-resource. In this setting, out of domain translations are often of poor quality and prone to hallucinations, due to the translation model preferring to predict common words it has seen during training, as opposed to the more uncommon ones from a different domain. We present two simple methods for improving translation quality in this particular setting: First, we use lexical shortlisting in order to restrict the neural network predictions by IBM model computed alignments. Second, we perform n-best list reordering by reranking all translations based on the amount they overlap with each other. Our methods are computationally simpler and faster than alternative approaches, and show a moderate success on low-resource settings with explicit out of domain test sets. However, our methods lose their effectiveness when the domain mismatch is too great, or in high resource setting.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00421](https://arxiv.org/abs/2101.00421) [cs.CL]** |
|           | (or **[arXiv:2101.00421v1](https://arxiv.org/abs/2101.00421v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-14">14. Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events</h2>

Title: [Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events](https://arxiv.org/abs/2101.00822)

Authors: [Le Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L), [Tao Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+T), [Chaochun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Liefeng Bo](https://arxiv.org/search/cs?searchtype=author&query=Bo%2C+L), [Wen Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+W), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> Large-scale pretrained language models have shown thrilling generation capabilities, especially when they generate consistent long text in thousands of words with ease. However, users of these models can only control the prefix of sentences or certain global aspects of generated text. It is challenging to simultaneously achieve fine-grained controllability and preserve the state-of-the-art unconditional text generation capability. In this paper, we first propose a new task named "Outline to Story" (O2S) as a test bed for fine-grained controllable generation of long text, which generates a multi-paragraph story from cascaded events, i.e. a sequence of outline events that guide subsequent paragraph generation. We then create dedicate datasets for future benchmarks, built by state-of-the-art keyword extraction techniques. Finally, we propose an extremely simple yet strong baseline method for the O2S task, which fine tunes pre-trained language models on augmented sequences of outline-story pairs with simple language modeling objective. Our method does not introduce any new parameters or perform any architecture modification, except several special tokens as delimiters to build augmented sequences. Extensive experiments on various datasets demonstrate state-of-the-art conditional story generation performance with our model, achieving better fine-grained controllability and user flexibility. Our paper is among the first ones by our knowledge to propose a model and to create datasets for the task of "outline to story". Our work also instantiates research interest of fine-grained controllable generation of open-domain long text, where controlling inputs are represented by short text.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00822](https://arxiv.org/abs/2101.00822) [cs.CL]** |
|           | (or **[arXiv:2101.00822v1](https://arxiv.org/abs/2101.00822v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-15">15. Transformer-based Conditional Variational Autoencoder for Controllable Story Generation</h2>

Title: [Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](https://arxiv.org/abs/2101.00828)

Authors: [Le Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L), [Tao Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+T), [Chaochun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Liefeng Bo](https://arxiv.org/search/cs?searchtype=author&query=Bo%2C+L), [Wen Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+W), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> We investigate large-scale latent variable models (LVMs) for neural story generation -- an under-explored application for open-domain long text -- with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models -- GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00828](https://arxiv.org/abs/2101.00828) [cs.CL]** |
|           | (or **[arXiv:2101.00828v1](https://arxiv.org/abs/2101.00828v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-16">16. How to Train Your Agent to Read and Write</h2>

Title: [How to Train Your Agent to Read and Write](https://arxiv.org/abs/2101.00916)

Authors: [Li Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Mengge He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+M), [Guanghui Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G), [Mingkui Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q)

> Reading and writing research papers is one of the most privileged abilities that a qualified researcher should master. However, it is difficult for new researchers (\eg{students}) to fully {grasp} this ability. It would be fascinating if we could train an intelligent agent to help people read and summarize papers, and perhaps even discover and exploit the potential knowledge clues to write novel papers. Although there have been existing works focusing on summarizing (\emph{i.e.}, reading) the knowledge in a given text or generating (\emph{i.e.}, writing) a text based on the given knowledge, the ability of simultaneously reading and writing is still under development. Typically, this requires an agent to fully understand the knowledge from the given text materials and generate correct and fluent novel paragraphs, which is very challenging in practice. In this paper, we propose a Deep ReAder-Writer (DRAW) network, which consists of a \textit{Reader} that can extract knowledge graphs (KGs) from input paragraphs and discover potential knowledge, a graph-to-text \textit{Writer} that generates a novel paragraph, and a \textit{Reviewer} that reviews the generated paragraph from three different aspects. Extensive experiments show that our DRAW network outperforms considered baselines and several state-of-the-art methods on AGENDA and M-AGENDA datasets. Our code and supplementary are released at [this https URL](https://github.com/menggehe/DRAW).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00916](https://arxiv.org/abs/2101.00916) [cs.CL]** |
|           | (or **[arXiv:2101.00916v1](https://arxiv.org/abs/2101.00916v1) [cs.CL]** for this version) |



# 2021-01-01

[Return to Index](#Index)



<h2 id="2021-01-01-1">1. Understanding and Improving Lexical Choice in Non-Autoregressive Translation</h2>

Title: [Understanding and Improving Lexical Choice in Non-Autoregressive Translation](https://arxiv.org/abs/2012.14583)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14583](https://arxiv.org/abs/2012.14583) [cs.CL]** |
|           | (or **[arXiv:2012.14583v1](https://arxiv.org/abs/2012.14583v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-2">2. Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation</h2>

Title: [Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation](https://arxiv.org/abs/2012.14681)

Authors: [Hyojung Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+H), [Sathish Indurthi](https://arxiv.org/search/cs?searchtype=author&query=Indurthi%2C+S), [Mohd Abbas Zaidi](https://arxiv.org/search/cs?searchtype=author&query=Zaidi%2C+M+A), [Nikhil Kumar Lakumarapu](https://arxiv.org/search/cs?searchtype=author&query=Lakumarapu%2C+N+K), [Beomseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+B), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Chanwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C), [Inchul Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+I)

> Recently, simultaneous translation has gathered a lot of attention since it enables compelling applications such as subtitle translation for a live event or real-time video-call translation. Some of these translation applications allow editing of partial translation giving rise to re-translation approaches. The current re-translation approaches are based on autoregressive sequence generation models (ReTA), which generate tar-get tokens in the (partial) translation sequentially. The multiple re-translations with sequential generation inReTAmodelslead to an increased inference time gap between the incoming source input and the corresponding target output as the source input grows. Besides, due to the large number of inference operations involved, the ReTA models are not favorable for resource-constrained devices. In this work, we propose a faster re-translation system based on a non-autoregressive sequence generation model (FReTNA) to overcome the aforementioned limitations. We evaluate the proposed model on multiple translation tasks and our model reduces the inference times by several orders and achieves a competitive BLEUscore compared to the ReTA and streaming (Wait-k) models.The proposed model reduces the average computation time by a factor of 20 when compared to the ReTA model by incurring a small drop in the translation quality. It also outperforms the streaming-based Wait-k model both in terms of computation time (1.5 times lower) and translation quality.

| Comments: | work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.14681](https://arxiv.org/abs/2012.14681) [cs.CL]** |
|           | (or **[arXiv:2012.14681v1](https://arxiv.org/abs/2012.14681v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-3">3. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</h2>

Title: [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)

Authors: [Yang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Tengchao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+T), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Guoxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Yijuan Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Dinei Florencio](https://arxiv.org/search/cs?searchtype=author&query=Florencio%2C+D), [Cha Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Lidong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L)

> Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this paper, we present \textbf{LayoutLMv2} by pre-training text, layout and image in a multi-modal framework, where new model architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852), RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.14740](https://arxiv.org/abs/2012.14740) [cs.CL]** |
|           | (or **[arXiv:2012.14740v1](https://arxiv.org/abs/2012.14740v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-4">4. CMV-BERT: Contrastive multi-vocab pretraining of BERT</h2>

Title: [CMV-BERT: Contrastive multi-vocab pretraining of BERT](https://arxiv.org/abs/2012.14763)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Daniel Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+D)

> In this work, we represent CMV-BERT, which improves the pretraining of a language model via two ingredients: (a) contrastive learning, which is well studied in the area of computer vision; (b) multiple vocabularies, one of which is fine-grained and the other is coarse-grained. The two methods both provide different views of an original sentence, and both are shown to be beneficial. Downstream tasks demonstrate our proposed CMV-BERT are effective in improving the pretrained language models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14763](https://arxiv.org/abs/2012.14763) [cs.CL]** |
|           | (or **[arXiv:2012.14763v1](https://arxiv.org/abs/2012.14763v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-5">5. Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning</h2>

Title: [Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning](https://arxiv.org/abs/2012.14768)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder layers (instead of the uppermost layer) for sequence-to-sequence (Seq2Seq) models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers. In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction. It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. The source code will be released.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14768](https://arxiv.org/abs/2012.14768) [cs.CL]** |
|           | (or **[arXiv:2012.14768v1](https://arxiv.org/abs/2012.14768v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-6">6. Transformer Feed-Forward Layers Are Key-Value Memories</h2>

Title: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)

Authors: [Mor Geva](https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M), [Roei Schuster](https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+R), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14913](https://arxiv.org/abs/2012.14913) [cs.CL]** |
|           | (or **[arXiv:2012.14913v1](https://arxiv.org/abs/2012.14913v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-7">7. Reservoir Transformer</h2>

Title: [Reservoir Transformer](https://arxiv.org/abs/2012.15045)

Authors: [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Ari S. Morcos](https://arxiv.org/search/cs?searchtype=author&query=Morcos%2C+A+S), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear "reservoir" layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15045](https://arxiv.org/abs/2012.15045) [cs.CL]** |
|           | (or **[arXiv:2012.15045v1](https://arxiv.org/abs/2012.15045v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-8">8. Enhancing Pre-trained Language Model with Lexical Simplification</h2>

Title: [Enhancing Pre-trained Language Model with Lexical Simplification](https://arxiv.org/abs/2012.15070)

Authors: [Rongzhou Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+R), [Jiayi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> For both human readers and pre-trained language models (PrLMs), lexical diversity may lead to confusion and inaccuracy when understanding the underlying semantic meanings of given sentences. By substituting complex words with simple alternatives, lexical simplification (LS) is a recognized method to reduce such lexical diversity, and therefore to improve the understandability of sentences. In this paper, we leverage LS and propose a novel approach which can effectively improve the performance of PrLMs in text classification. A rule-based simplification process is applied to a given sentence. PrLMs are encouraged to predict the real label of the given sentence with auxiliary inputs from the simplified version. Using strong PrLMs (BERT and ELECTRA) as baselines, our approach can still further improve the performance in various text classification tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15070](https://arxiv.org/abs/2012.15070) [cs.CL]** |
|           | (or **[arXiv:2012.15070v1](https://arxiv.org/abs/2012.15070v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-9">9. Accurate Word Representations with Universal Visual Guidance</h2>

Title: [Accurate Word Representations with Universal Visual Guidance](https://arxiv.org/abs/2012.15086)

Authors: [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Haojie Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M)

> Word representation is a fundamental component in neural language understanding models. Recently, pre-trained language models (PrLMs) offer a new performant method of contextualized word representations by leveraging the sequence-level context for modeling. Although the PrLMs generally give more accurate contextualized word representations than non-contextualized models do, they are still subject to a sequence of text contexts without diverse hints for word representation from multimodality. This paper thus proposes a visual representation method to explicitly enhance conventional word embedding with multiple-aspect senses from visual guidance. In detail, we build a small-scale word-image dictionary from a multimodal seed dataset where each word corresponds to diverse related images. The texts and paired images are encoded in parallel, followed by an attention layer to integrate the multimodal representations. We show that the method substantially improves the accuracy of disambiguation. Experiments on 12 natural language understanding and machine translation tasks further verify the effectiveness and the generalization capability of the proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15086](https://arxiv.org/abs/2012.15086) [cs.CL]** |
|           | (or **[arXiv:2012.15086v1](https://arxiv.org/abs/2012.15086v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-1">10. Improving Zero-Shot Translation by Disentangling Positional Information</h2>

Title: [Improving Zero-Shot Translation by Disentangling Positional Information](https://arxiv.org/abs/2012.15127)

Authors: [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15127](https://arxiv.org/abs/2012.15127) [cs.CL]** |
|           | (or **[arXiv:2012.15127v1](https://arxiv.org/abs/2012.15127v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-11">11. Improving BERT with Syntax-aware Local Attention</h2>

Title: [Improving BERT with Syntax-aware Local Attention](https://arxiv.org/abs/2012.15150)

Authors: [Zhongli Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Qingyu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Q), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Yunbo Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y)

> Pre-trained Transformer-based neural language models, such as BERT, have achieved remarkable results on varieties of NLP tasks. Recent works have shown that attention-based models can benefit from more focused attention over local regions. Most of them restrict the attention scope within a linear span, or confine to certain tasks such as machine translation and question answering. In this paper, we propose a syntax-aware local attention, where the attention scopes are restrained based on the distances in the syntactic structure. The proposed syntax-aware local attention can be integrated with pretrained language models, such as BERT, to render the model to focus on syntactically relevant words. We conduct experiments on various single-sentence benchmarks, including sentence classification and sequence labeling tasks. Experimental results show consistent gains over BERT on all benchmark datasets. The extensive studies verify that our model achieves better performance owing to more focused attention over syntactically relevant words.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15150](https://arxiv.org/abs/2012.15150) [cs.CL]** |
|           | (or **[arXiv:2012.15150v1](https://arxiv.org/abs/2012.15150v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-12">12. Synthetic Source Language Augmentation for Colloquial Neural Machine Translation</h2>

Title: [Synthetic Source Language Augmentation for Colloquial Neural Machine Translation](https://arxiv.org/abs/2012.15178)

Authors: [Asrul Sani Ariesandy](https://arxiv.org/search/cs?searchtype=author&query=Ariesandy%2C+A+S), [Mukhlis Amien](https://arxiv.org/search/cs?searchtype=author&query=Amien%2C+M), [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Radityo Eko Prasojo](https://arxiv.org/search/cs?searchtype=author&query=Prasojo%2C+R+E)

> Neural machine translation (NMT) is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source of formal Indonesian language and show that it improves the baseline Id-En models (in BLEU) over the new test data.

| Comments:    | 5 pages                                                      |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2012.15178](https://arxiv.org/abs/2012.15178) [cs.CL]** |
|              | (or **[arXiv:2012.15178v1](https://arxiv.org/abs/2012.15178v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-13">13. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?</h2>

Title: [Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?](https://arxiv.org/abs/2012.15180)

Authors: [Thang M. Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+T+M), [Trung Bui](https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T), [Long Mai](https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+L), [Anh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A)

> Do state-of-the-art natural language understanding models care about word order - one of the most important characteristics of a sequence? Not always! We found 75% to 90% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Despite BERT embeddings are famously contextual, the contribution of each individual word to downstream tasks is almost unchanged even after the word's context is shuffled. BERT-based models are able to exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequence-pair inputs in natural language inference) to make correct decisions when tokens are arranged in random orders. Encouraging classifiers to capture word order information improves the performance on most GLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.

| Comments: | 23 pages, 13 figures. Preprint. Work in progress             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15180](https://arxiv.org/abs/2012.15180) [cs.CL]** |
|           | (or **[arXiv:2012.15180v1](https://arxiv.org/abs/2012.15180v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-14">14. SemGloVe: Semantic Co-occurrences for GloVe from BERT</h2>

Title: [SemGloVe: Semantic Co-occurrences for GloVe from BERT](https://arxiv.org/abs/2012.15197)

Authors: [Leilei Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+L), [Zhiyang Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Linchao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+L), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> GloVe learns word embeddings by leveraging statistical information from word co-occurrence matrices. However, word pairs in the matrices are extracted from a predefined local context window, which might lead to limited word pairs and potentially semantic irrelevant word pairs. In this paper, we propose SemGloVe, which distills semantic co-occurrences from BERT into static GloVe word embeddings. Particularly, we propose two models to extract co-occurrence statistics based on either the masked language model or the multi-head attention weights of BERT. Our methods can extract word pairs without limiting by the local window assumption and can define the co-occurrence weights by directly considering the semantic distance between word pairs. Experiments on several word similarity datasets and four external tasks show that SemGloVe can outperform GloVe.

| Comments: | 10 pages, 3 figures, 5 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.15197](https://arxiv.org/abs/2012.15197) [cs.CL]** |
|           | (or **[arXiv:2012.15197v1](https://arxiv.org/abs/2012.15197v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-15">15. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</h2>

Title: [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://arxiv.org/abs/2012.15409)

Authors: [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Can Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Guocheng Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+G), [Xinyan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X), [Hao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Jiachen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e. text or image) or limited multi-modal data (i.e. image-text pairs). In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15409](https://arxiv.org/abs/2012.15409) [cs.CL]** |
|           | (or **[arXiv:2012.15409v1](https://arxiv.org/abs/2012.15409v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-16">16. Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation</h2>

Title: [Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation](https://arxiv.org/abs/2012.15416)

Authors: [Damian Pascual](https://arxiv.org/search/cs?searchtype=author&query=Pascual%2C+D), [Beni Egressy](https://arxiv.org/search/cs?searchtype=author&query=Egressy%2C+B), [Florian Bolli](https://arxiv.org/search/cs?searchtype=author&query=Bolli%2C+F), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Large pre-trained language models are capable of generating realistic text. However, controlling these models so that the generated text satisfies lexical constraints, i.e., contains specific words, is a challenging problem. Given that state-of-the-art language models are too large to be trained from scratch in a manageable time, it is desirable to control these models without re-training them. Methods capable of doing this are called plug-and-play. Recent plug-and-play methods have been successful in constraining small bidirectional language models as well as forward models in tasks with a restricted search space, e.g., machine translation. However, controlling large transformer-based models to meet lexical constraints without re-training them remains a challenge. In this work, we propose Directed Beam Search (DBS), a plug-and-play method for lexically constrained language generation. Our method can be applied to any language model, is easy to implement and can be used for general language generation. In our experiments we use DBS to control GPT-2. We demonstrate its performance on keyword-to-phrase generation and we obtain comparable results as a state-of-the-art non-plug-and-play model for lexically constrained story generation.

| Comments: | Preprint. Work in progress                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15416](https://arxiv.org/abs/2012.15416) [cs.CL]** |
|           | (or **[arXiv:2012.15416v1](https://arxiv.org/abs/2012.15416v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-17">17. Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation</h2>

Title: [Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/2012.15455)

Authors: [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

> We explore two types of monolingual data that can be included in knowledge distillation training for neural machine translation (NMT). The first is the source-side monolingual data. Second, is the target-side monolingual data that is used as back-translation data. Both datasets are (forward-)translated by a teacher model from source-language to target-language, which are then combined into a dataset for smaller student models. We find that source-side monolingual data improves model performance when evaluated by test-set originated from source-side. Likewise, target-side data has a positive effect on the test-set in the opposite direction. We also show that it is not required to train the student model with the same data used by the teacher, as long as the domains are the same. Finally, we find that combining source-side and target-side yields in better performance than relying on just one side of the monolingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15455](https://arxiv.org/abs/2012.15455) [cs.CL]** |
|           | (or **[arXiv:2012.15455v1](https://arxiv.org/abs/2012.15455v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-18">18. CLEAR: Contrastive Learning for Sentence Representation</h2>

Title: [CLEAR: Contrastive Learning for Sentence Representation](https://arxiv.org/abs/2012.15466)

Authors: [Zhuofeng Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Sinong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Madian Khabsa](https://arxiv.org/search/cs?searchtype=author&query=Khabsa%2C+M), [Fei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Hao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+H)

> Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks. Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks.

| Comments: | 10 pages, 2 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15466](https://arxiv.org/abs/2012.15466) [cs.CL]** |
|           | (or **[arXiv:2012.15466v1](https://arxiv.org/abs/2012.15466v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-19">19. Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings</h2>

Title: [Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings](https://arxiv.org/abs/2012.15484)

Authors: [Kiran Ramnath](https://arxiv.org/search/cs?searchtype=author&query=Ramnath%2C+K), [Mark Hasegawa-Johnson](https://arxiv.org/search/cs?searchtype=author&query=Hasegawa-Johnson%2C+M)

> Fact-based Visual Question Answering (FVQA), a challenging variant of VQA, requires a QA-system to include facts from a diverse knowledge graph (KG) in its reasoning process to produce an answer. Large KGs, especially common-sense KGs, are known to be incomplete, i.e. not all non-existent facts are always incorrect. Therefore, being able to reason over incomplete KGs for QA is a critical requirement in real-world applications that has not been addressed extensively in the literature. We develop a novel QA architecture that allows us to reason over incomplete KGs, something current FVQA state-of-the-art (SOTA) approaches lack.We use KG Embeddings, a technique widely used for KG completion, for the downstream task of FVQA. We also employ a new image representation technique we call "Image-as-Knowledge" to enable this capability, alongside a simple one-step co-Attention mechanism to attend to text and image during QA. Our FVQA architecture is faster during inference time, being O(m), as opposed to existing FVQA SOTA methods which are O(N logN), where m is number of vertices, N is number of edges (which is O(m^2)). We observe that our architecture performs comparably in the standard answer-retrieval baseline with existing methods; while for missing-edge reasoning, our KG representation outperforms the SOTA representation by 25%, and image representation outperforms the SOTA representation by 2.6%.

| Comments: | 9 pages, 10 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15484](https://arxiv.org/abs/2012.15484) [cs.CL]** |
|           | (or **[arXiv:2012.15484v1](https://arxiv.org/abs/2012.15484v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-20">20. Towards Zero-Shot Knowledge Distillation for Natural Language Processing</h2>

Title: [Towards Zero-Shot Knowledge Distillation for Natural Language Processing](https://arxiv.org/abs/2012.15495)

Authors: [Ahmad Rashid](https://arxiv.org/search/cs?searchtype=author&query=Rashid%2C+A), [Vasileios Lioutas](https://arxiv.org/search/cs?searchtype=author&query=Lioutas%2C+V), [Abbas Ghaddar](https://arxiv.org/search/cs?searchtype=author&query=Ghaddar%2C+A), [Mehdi Rezagholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Rezagholizadeh%2C+M)

> Knowledge Distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher's training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-Shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out of domain data and adversarial training to learn the teacher's output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher's classification score (accuracy or F1) while compressing the model 30 times.

| Comments: | 13 pages, 8 tables, 2 algorithms and 1 figure                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15495](https://arxiv.org/abs/2012.15495) [cs.CL]** |
|           | (or **[arXiv:2012.15495v1](https://arxiv.org/abs/2012.15495v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-21">21. Neural Machine Translation: A Review of Methods, Resources, and Tools</h2>

Title: [Neural Machine Translation: A Review of Methods, Resources, and Tools](https://arxiv.org/abs/2012.15515)

Authors: [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zonghan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Gang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions.

| Comments: | Accepted by AI Open                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15515](https://arxiv.org/abs/2012.15515) [cs.CL]** |
|           | (or **[arXiv:2012.15515v1](https://arxiv.org/abs/2012.15515v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-22">22. Linear-Time WordPiece Tokenization</h2>

Title: [Linear-Time WordPiece Tokenization](https://arxiv.org/abs/2012.15524)

Authors: [Xinying Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Alex Salcianu](https://arxiv.org/search/cs?searchtype=author&query=Salcianu%2C+A), [Yang Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Dave Dopson](https://arxiv.org/search/cs?searchtype=author&query=Dopson%2C+D), [Denny Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+D)

> WordPiece tokenization is a subword-based tokenization schema adopted by BERT: it segments the input text via a longest-match-first tokenization strategy, known as Maximum Matching or MaxMatch. To the best of our knowledge, all published MaxMatch algorithms are quadratic (or higher). In this paper, we propose LinMaxMatch, a novel linear-time algorithm for MaxMatch and WordPiece tokenization. Inspired by the Aho-Corasick algorithm, we introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. Experimental results show that our algorithm is 3x faster on average than two production systems by HuggingFace and TensorFlow Text. Regarding long-tail inputs, our algorithm is 4.5x faster at the 95 percentile. This work has immediate practical value (reducing inference latency, saving compute resources, etc.) and is of theoretical interest by providing an optimal complexity solution to the decades-old MaxMatch problem.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15524](https://arxiv.org/abs/2012.15524) [cs.CL]** |
|           | (or **[arXiv:2012.15524v1](https://arxiv.org/abs/2012.15524v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-23">23. XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders</h2>

Title: [XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders](https://arxiv.org/abs/2012.15547)

Authors: [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Akiko Eriguchi](https://arxiv.org/search/cs?searchtype=author&query=Eriguchi%2C+A), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Arul Menezes](https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+A), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Multilingual machine translation enables a single model to translate between different languages. Most existing multilingual machine translation systems adopt a randomly initialized Transformer backbone. In this work, inspired by the recent success of language model pre-training, we present XLM-T, which initializes the model with an off-the-shelf pretrained cross-lingual Transformer encoder and fine-tunes it with multilingual parallel data. This simple method achieves significant improvements on a WMT dataset with 10 language pairs and the OPUS-100 corpus with 94 pairs. Surprisingly, the method is also effective even upon the strong baseline with back-translation. Moreover, extensive analysis of XLM-T on unsupervised syntactic parsing, word alignment, and multilingual classification explains its effectiveness for machine translation. The code will be at [this https URL](https://aka.ms/xlm-t).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15547](https://arxiv.org/abs/2012.15547) [cs.CL]** |
|           | (or **[arXiv:2012.15547v1](https://arxiv.org/abs/2012.15547v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-24">24. How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</h2>

Title: [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://arxiv.org/abs/2012.15613)

Authors: [Phillip Rust](https://arxiv.org/search/cs?searchtype=author&query=Rust%2C+P), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> In this work we provide a \textit{systematic empirical comparison} of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first establish if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for a performance difference. To disentangle the impacting variables, we train new monolingual models on the same data, but with different tokenizers, both the monolingual and the multilingual version. We find that while the pretraining data size is an important factor, the designated tokenizer of the monolingual model plays an equally important role in the downstream performance. Our results show that languages which are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15613](https://arxiv.org/abs/2012.15613) [cs.CL]** |
|           | (or **[arXiv:2012.15613v1](https://arxiv.org/abs/2012.15613v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-25">25. CoCoLM: COmplex COmmonsense Enhanced Language Model</h2>

Title: [CoCoLM: COmplex COmmonsense Enhanced Language Model](https://arxiv.org/abs/2012.15643)

Authors: [Changlong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C), [Hongming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yangqiu Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Wilfred Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+W)

> Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contains rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with the complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address this problem, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Different from existing fine-tuning approaches, we do not focus on a specific task and propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graphs ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense knowledge among eventualities. Experiments on multiple downstream commonsense tasks that requires the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15643](https://arxiv.org/abs/2012.15643) [cs.CL]** |
|           | (or **[arXiv:2012.15643v1](https://arxiv.org/abs/2012.15643v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-26">26. VOLT: Improving Vocabularization via Optimal Transport for Machine Translation</h2>

Title: [VOLT: Improving Vocabularization via Optimal Transport for Machine Translation](https://arxiv.org/abs/2012.15671)

Authors: [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Chun Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+C), [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> It is well accepted that the choice of token vocabulary largely affects the performance of machine translation. However, due to expensive trial costs, most studies only conduct simple trials with dominant approaches (e.g BPE) and commonly used vocabulary sizes. In this paper, we find an exciting relation between an information-theoretic feature and BLEU scores. With this observation, we formulate the quest of vocabularization -- finding the best token dictionary with a proper size -- as an optimal transport problem. We then propose VOLT, a simple and efficient vocabularization solution without the full and costly trial training. We evaluate our approach on multiple machine translation tasks, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. Empirical results show that VOLT beats widely-used vocabularies on diverse scenarios. For example, VOLT achieves 70% vocabulary size reduction and 0.6 BLEU gain on English-German translation. Also, one advantage of VOLT lies in its low resource consumption. Compared to naive BPE-search, VOLT reduces the search time from 288 GPU hours to 0.5 CPU hours.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15671](https://arxiv.org/abs/2012.15671) [cs.CL]** |
|           | (or **[arXiv:2012.15671v1](https://arxiv.org/abs/2012.15671v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-27">27. ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</h2>

Title: [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674)

Authors: [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance on downstream cross-lingual tasks. This improvement stems from the learning of a large amount of monolingual and parallel corpora. While it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for the low-resource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to break the constraint of parallel corpus size on the model performance. Our key insight is to integrate the idea of back translation in the pre-training process. We generate pseudo-parallel sentences pairs on a monolingual corpus to enable the learning of semantic alignment between different languages, which enhances the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results on various cross-lingual downstream tasks. The codes and pre-trained models will be made publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15674](https://arxiv.org/abs/2012.15674) [cs.CL]** |
|           | (or **[arXiv:2012.15674v1](https://arxiv.org/abs/2012.15674v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-28">28. Revisiting Robust Neural Machine Translation: A Transformer Case Study</h2>

Title: [Revisiting Robust Neural Machine Translation: A Transformer Case Study](https://arxiv.org/abs/2012.15710)

Authors: [Peyman Passban](https://arxiv.org/search/cs?searchtype=author&query=Passban%2C+P), [Puneeth S.M. Saladi](https://arxiv.org/search/cs?searchtype=author&query=Saladi%2C+P+S), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Transformers (Vaswani et al., 2017) have brought a remarkable improvement in the performance of neural machine translation (NMT) systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.
> Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15710](https://arxiv.org/abs/2012.15710) [cs.CL]** |
|           | (or **[arXiv:2012.15710v1](https://arxiv.org/abs/2012.15710v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-29">29. FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation</h2>

Title: [FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation](https://arxiv.org/abs/2012.15717)

Authors: [Wenhao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Tong Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+T), [Xu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Jian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Wei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Yanfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Previous domain adaptation research usually neglect the diversity in translation within a same domain, which is a core problem for adapting a general neural machine translation (NMT) model into a specific domain in real-world scenarios. One representative of such challenging scenarios is to deploy a translation system for a conference with a specific topic, e.g. computer networks or natural language processing, where there is usually extremely less resources due to the limited time schedule. To motivate a wide investigation in such settings, we present a real-world fine-grained domain adaptation task in machine translation (FDMT). The FDMT dataset (Zh-En) consists of four sub-domains of information technology: autonomous vehicles, AI education, real-time networks and smart phone. To be closer to reality, FDMT does not employ any in-domain bilingual training data. Instead, each sub-domain is equipped with monolingual data, bilingual dictionary and knowledge base, to encourage in-depth exploration of these available resources. Corresponding development set and test set are provided for evaluation purpose. We make quantitative experiments and deep analyses in this new setting, which benchmarks the fine-grained domain adaptation task and reveals several challenging problems that need to be addressed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15717](https://arxiv.org/abs/2012.15717) [cs.CL]** |
|           | (or **[arXiv:2012.15717v1](https://arxiv.org/abs/2012.15717v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-30">30. Making Pre-trained Language Models Better Few-shot Learners</h2>

Title: [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723)

Authors: [Tianyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Adam Fisch](https://arxiv.org/search/cs?searchtype=author&query=Fisch%2C+A), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15723](https://arxiv.org/abs/2012.15723) [cs.CL]** |
|           | (or **[arXiv:2012.15723v1](https://arxiv.org/abs/2012.15723v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-31">31. Shortformer: Better Language Modeling using Shorter Inputs</h2>

Title: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)

Authors: [Ofir Press](https://arxiv.org/search/cs?searchtype=author&query=Press%2C+O), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We explore the benefits of decreasing the input length of transformers. First, we show that initially training the model on short subsequences, before moving on to longer ones, both reduces overall training time and, surprisingly, gives a large improvement in perplexity. We then show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens (when generating sequences that are larger than the maximal length that the transformer can handle at once). Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. By combining these techniques, we increase training speed by 65%, make generation nine times faster, and substantially improve perplexity on WikiText-103, without adding any parameters.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15832](https://arxiv.org/abs/2012.15832) [cs.CL]** |
|           | (or **[arXiv:2012.15832v1](https://arxiv.org/abs/2012.15832v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-32">32. Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade</h2>

Title: [Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade](https://arxiv.org/abs/2012.15833)

Authors: [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Xiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X)

> Fully non-autoregressive neural machine translation (NAT) is proposed to simultaneously predict tokens with single forward of neural networks, which significantly reduces the inference latency at the expense of quality drop compared to the Transformer baseline. In this work, we target on closing the performance gap while maintaining the latency advantage. We first inspect the fundamental issues of fully NAT models, and adopt dependency reduction in the learning space of output tokens as the basic guidance. Then, we revisit methods in four different aspects that have been proven effective for improving NAT models, and carefully combine these techniques with necessary modifications. Our extensive experiments on three translation benchmarks show that the proposed system achieves the new state-of-the-art results for fully NAT models, and obtains comparable performance with the autoregressive and iterative NAT systems. For instance, one of the proposed models achieves 27.49 BLEU points on WMT14 En-De with approximately 16.5X speed up at inference time.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15833](https://arxiv.org/abs/2012.15833) [cs.CL]** |
|           | (or **[arXiv:2012.15833v1](https://arxiv.org/abs/2012.15833v1) [cs.CL]** for this version) |

