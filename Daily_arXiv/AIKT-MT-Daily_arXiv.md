# Daily arXiv: Machine Translation - October, 2020

# Index


- [2020-10-12](#2020-10-12)

  - [1. Query-Key Normalization for Transformers](#2020-10-12-1)
  - [2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](#2020-10-12-2)
  - [3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](#2020-10-12-3)
  - [4. Token-level Adaptive Training for Neural Machine Translation](#2020-10-12-4)
  - [5. A Survey of Knowledge-Enhanced Text Generation](#2020-10-12-5)
  - [6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](#2020-10-12-6)
  - [7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](#2020-10-12-7)
  - [8. Self-Paced Learning for Neural Machine Translation](#2020-10-12-8)
  - [9. Recursive Top-Down Production for Sentence Generation with Latent Trees](#2020-10-12-9)
- [2020-10-09](#2020-10-09)
- [1. Shallow-to-Deep Training for Neural Machine Translation](#2020-10-09-1)
  - [2. Improving Attention Mechanism with Query-Value Interaction](#2020-10-09-2)
  - [3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](#2020-10-09-3)
  - [4. What Can We Do to Improve Peer Review in NLP?](#2020-10-09-4)
  - [5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks](#2020-10-09-5)
  - [6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](#2020-10-09-6)
  - [7. Cross-Thought for Sentence Encoder Pre-training](#2020-10-09-7)
  - [8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation](#2020-10-09-8)
  - [9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](#2020-10-09-9)
- [2020-10-08](#2020-10-08)
- [1. Plug and Play Autoencoders for Conditional Text Generation](#2020-10-08-1)
  - [2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](#2020-10-08-2)
  - [3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](#2020-10-08-3)
  - [4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](#2020-10-08-4)
  - [5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](#2020-10-08-5)
  - [6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](#2020-10-08-6)
  - [7. WER we are and WER we think we are](#2020-10-08-7)
  - [8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](#2020-10-08-8)
  - [9. TeaForN: Teacher-Forcing with N-grams](#2020-10-08-9)
  - [10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](#2020-10-08-10)
- [2020-10-07](#2020-10-07)
- [1. Multi-task Learning for Multilingual Neural Machine Translation](#2020-10-07-1)
  - [2. Do Explicit Alignments Robustly Improve Multilingual Encoders?](#2020-10-07-2)
  - [3. The Multilingual Amazon Reviews Corpus](#2020-10-07-3)
  - [4. On the Sparsity of Neural Machine Translation Models](#2020-10-07-4)
  - [5. On the Sub-Layer Functionalities of Transformer Decoder](#2020-10-07-5)
  - [6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](#2020-10-07-6)
  - [7. Analyzing Individual Neurons in Pre-trained Language Models](#2020-10-07-7)
  - [8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](#2020-10-07-8)
  - [9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](#2020-10-07-9)
  - [10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](#2020-10-07-10)
  - [11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](#2020-10-07-11)
  - [12. Inference Strategies for Machine Translation with Conditional Masking](#2020-10-07-12)
  - [13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](#2020-10-07-13)
  - [14. Guiding Attention for Self-Supervised Learning with Transformers](#2020-10-07-14)
  - [15. Adversarial Grammatical Error Correction](#2020-10-07-15)
  - [16. Efficient Inference For Neural Machine Translation](#2020-10-07-16)
  - [17. Iterative Domain-Repaired Back-Translation](#2020-10-07-17)
- [2020-10-06](#2020-10-06)

  - [1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](#2020-10-06-1)
  - [2. Transformer-Based Neural Text Generation with Syntactic Guidance](#2020-10-06-2)
  - [3. Second-Order NLP Adversarial Examples](#2020-10-06-3)
  - [4. GenAug: Data Augmentation for Finetuning Text Generators](#2020-10-06-4)
  - [5. Lifelong Language Knowledge Distillation](#2020-10-06-5)
  - [6. A Streaming Approach For Efficient Batched Beam Search](#2020-10-06-6)
  - [7. Self-training Improves Pre-training for Natural Language Understanding](#2020-10-06-7)
  - [8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](#2020-10-06-8)
- [2020-10-05](#2020-10-05)

  - [1. Nearest Neighbor Machine Translation](#2020-10-05-1)
  - [2. A Survey of the State of Explainable AI for Natural Language Processing](#2020-10-05-2)
  - [3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](#2020-10-05-3)
  - [4. Which *BERT? A Survey Organizing Contextualized Encoders](#2020-10-05-4)


- [2020-10-02](#2020-10-2)

  - [1. WeChat Neural Machine Translation Systems for WMT20](#2020-10-2-1)
- [2020-10-01](#2020-10-01)

  - [1. Rethinking Attention with Performers](#2020-10-01-1)
  - [2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](#2020-10-01-2)
  - [3. Can Automatic Post-Editing Improve NMT?](#2020-10-01-3)
  - [4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](#2020-10-01-4)
  - [5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation](#2020-10-01-5)
- [2020-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-09.md)
- [2020-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-08.md)
- [2020-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-07.md)
- [2020-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-06.md)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-10-12

[Return to Index](#Index)



<h2 id="2020-10-12-1">1. Query-Key Normalization for Transformers</h2>

Title: [Query-Key Normalization for Transformers](https://arxiv.org/abs/2010.04245)

Authors: [Alex Henry](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+A), [Prudhvi Raj Dachapally](https://arxiv.org/search/cs?searchtype=author&query=Dachapally%2C+P+R), [Shubham Pawar](https://arxiv.org/search/cs?searchtype=author&query=Pawar%2C+S), [Yuxuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer's normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply ℓ2 normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT'15.

| Comments: | 8 pages, 2 figures, accepted at Findings of EMNLP 2020       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04245](https://arxiv.org/abs/2010.04245) [cs.CL]** |
|           | (or **[arXiv:2010.04245v1](https://arxiv.org/abs/2010.04245v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-2">2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task</h2>

Title: [Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](https://arxiv.org/abs/2010.04297)

Authors: [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Amy Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+A), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Qijun Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Q), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Dipanjan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

> The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. Our submission is based on BLEURT, a previously published metric based on transfer learning. We extend the metric beyond English and evaluate it on 12 languages for which training examples are available, as well as four "zero-shot" languages, for which we have no fine-tuning data. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that BLEURT achieves competitive results on the WMT Metrics 2019 Shared Task, indicating its promise for the 2020 edition.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04297](https://arxiv.org/abs/2010.04297) [cs.CL]** |
|           | (or **[arXiv:2010.04297v1](https://arxiv.org/abs/2010.04297v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-3">3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning</h2>

Title: [Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](https://arxiv.org/abs/2010.04314)

Authors: [Xiaomian Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+X), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.

| Comments: | Accepted to EMNLP 2020 long paper                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04314](https://arxiv.org/abs/2010.04314) [cs.CL]** |
|           | (or **[arXiv:2010.04314v1](https://arxiv.org/abs/2010.04314v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-4">4. Token-level Adaptive Training for Neural Machine Translation</h2>

Title: [Token-level Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2010.04380)

Authors: [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.

| Comments: | 12 pages; Accepted by EMNLP 2020                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04380](https://arxiv.org/abs/2010.04380) [cs.CL]** |
|           | (or **[arXiv:2010.04380v1](https://arxiv.org/abs/2010.04380v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-5">5. A Survey of Knowledge-Enhanced Text Generation</h2>

Title: [A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/abs/2010.04389)

Authors: [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Zaitang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhiting Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Z), [Qingyun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M)

> The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.

| Comments: | 44 pages; Preprint; A paper and code collection is available at [this https URL](https://github.com/wyu97/KENLG-Reading) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04389](https://arxiv.org/abs/2010.04389) [cs.CL]** |
|           | (or **[arXiv:2010.04389v1](https://arxiv.org/abs/2010.04389v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-6">6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation</h2>

Title: [Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](https://arxiv.org/abs/2010.04411)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.

| Comments: | Accepted to EMNLP 2020, 12 pages, 2 figures, 9 tables        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04411](https://arxiv.org/abs/2010.04411) [cs.CL]** |
|           | (or **[arXiv:2010.04411v1](https://arxiv.org/abs/2010.04411v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-7">7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels</h2>

Title: [Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](https://arxiv.org/abs/2010.04438)

Authors: [Harris Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+H), [Jamie Kiros](https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W)

> A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.

| Comments: | 10 pages (+3 appendix), 11 figures, 5 tables. Accepted to Findings of EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2010.04438](https://arxiv.org/abs/2010.04438) [cs.CL]** |
|           | (or **[arXiv:2010.04438v1](https://arxiv.org/abs/2010.04438v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-8">8. Self-Paced Learning for Neural Machine Translation</h2>

Title: [Self-Paced Learning for Neural Machine Translation](https://arxiv.org/abs/2010.04505)

Authors: [Yu Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Yikai Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B)

> Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.

| Comments: | Accepted by EMNLP2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04505](https://arxiv.org/abs/2010.04505) [cs.CL]** |
|           | (or **[arXiv:2010.04505v1](https://arxiv.org/abs/2010.04505v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-9">9. Recursive Top-Down Production for Sentence Generation with Latent Trees</h2>

Title: [Recursive Top-Down Production for Sentence Generation with Latent Trees](https://arxiv.org/abs/2010.04704)

Authors: [Shawn Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Timothy J. O'Donnell](https://arxiv.org/search/cs?searchtype=author&query=O'Donnell%2C+T+J), [Alessandro Sordoni](https://arxiv.org/search/cs?searchtype=author&query=Sordoni%2C+A), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville%2C+A)

> We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017), where it outperforms previous models on the LENGTH split, and English question formation (McCoy et al., 2020), where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset (Elliott et al., 2016), and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04704](https://arxiv.org/abs/2010.04704) [cs.CL]** |
|           | (or **[arXiv:2010.04704v1](https://arxiv.org/abs/2010.04704v1) [cs.CL]** for this version) |







# 2020-10-09

[Return to Index](#Index)



<h2 id="2020-10-09-1">1. Shallow-to-Deep Training for Neural Machine Translation</h2>

Title: [Shallow-to-Deep Training for Neural Machine Translation](https://arxiv.org/abs/2010.03737)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Huizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is 1.4 × faster than training from scratch, and achieves a BLEU score of 30.33 and 43.29 on two tasks. The code is publicly available at [this https URL](https://github.com/libeineu/SDT-Training/).

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03737](https://arxiv.org/abs/2010.03737) [cs.CL]** |
|           | (or **[arXiv:2010.03737v1](https://arxiv.org/abs/2010.03737v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-2">2. Improving Attention Mechanism with Query-Value Interaction</h2>

Title: [Improving Attention Mechanism with Query-Value Interaction](https://arxiv.org/abs/2010.03766)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Attention mechanism has played critical roles in various state-of-the-art NLP models such as Transformer and BERT. It can be formulated as a ternary function that maps the input queries, keys and values into an output by using a summation of values weighted by the attention weights derived from the interactions between queries and keys. Similar with query-key interactions, there is also inherent relatedness between queries and values, and incorporating query-value interactions has the potential to enhance the output by learning customized values according to the characteristics of queries. However, the query-value interactions are ignored by existing attention methods, which may be not optimal. In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions. We propose a query-value interaction function which can learn query-aware attention values, and combine them with the original values and attention weights to form the final output. Extensive experiments on four datasets for different tasks show that our approach can consistently improve the performance of many attention-based models by incorporating query-value interactions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03766](https://arxiv.org/abs/2010.03766) [cs.CL]** |
|           | (or **[arXiv:2010.03766v1](https://arxiv.org/abs/2010.03766v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-3">3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning</h2>

Title: [ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768)

Authors: [Mohit Shridhar](https://arxiv.org/search/cs?searchtype=author&query=Shridhar%2C+M), [Xingdi Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+X), [Marc-Alexandre Côté](https://arxiv.org/search/cs?searchtype=author&query=Côté%2C+M), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Adam Trischler](https://arxiv.org/search/cs?searchtype=author&query=Trischler%2C+A), [Matthew Hausknecht](https://arxiv.org/search/cs?searchtype=author&query=Hausknecht%2C+M)

> Given a simple request (e.g., Put a washed apple in the kitchen fridge), humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, visual scene understanding, and so forth).

| Comments: | Data, code, and videos are available at [this http URL](http://alfworld.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO) |
| Cite as:  | **[arXiv:2010.03768](https://arxiv.org/abs/2010.03768) [cs.CL]** |
|           | (or **[arXiv:2010.03768v1](https://arxiv.org/abs/2010.03768v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-4">4. What Can We Do to Improve Peer Review in NLP?</h2>

Title: [What Can We Do to Improve Peer Review in NLP?](https://arxiv.org/abs/2010.03863)

Authors: [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

> Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.

| Comments: | To appear at Findings of EMNLP                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.03863](https://arxiv.org/abs/2010.03863) [cs.CL]** |
|           | (or **[arXiv:2010.03863v1](https://arxiv.org/abs/2010.03863v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-5">5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks</h2>

Title: [Dense Relational Image Captioning via Multi-task Triple-Stream Networks](https://arxiv.org/abs/2010.03855)

Authors: [Dong-Jin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Tae-Hyun oh](https://arxiv.org/search/cs?searchtype=author&query=oh%2C+T), [Jinsoo Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J), [In So Kweon](https://arxiv.org/search/cs?searchtype=author&query=Kweon%2C+I+S)

> We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions of each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS, i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to not only learn to generate captions but also predict the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. We additionally extend analysis to an ablation study, applications on holistic image captioning, scene graph generation, and retrieval tasks

| Comments: | Journal extension of our CVPR 2019 paper [arXiv:1903.05942](https://arxiv.org/abs/1903.05942) . Source code : [this https URL](https://github.com/Dong-JinKim/DenseRelationalCaptioning) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2010.03855](https://arxiv.org/abs/2010.03855) [cs.CV]** |
|           | (or **[arXiv:2010.03855v1](https://arxiv.org/abs/2010.03855v1) [cs.CV]** for this version) |





<h2 id="2020-10-09-6">6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations</h2>

Title: [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://arxiv.org/abs/2010.03644)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [Pradyumna Narayana](https://arxiv.org/search/cs?searchtype=author&query=Narayana%2C+P), [Kazoo Sone](https://arxiv.org/search/cs?searchtype=author&query=Sone%2C+K), [Sugato Basu](https://arxiv.org/search/cs?searchtype=author&query=Basu%2C+S), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2010.03644](https://arxiv.org/abs/2010.03644) [cs.CL]** |
|           | (or **[arXiv:2010.03644v1](https://arxiv.org/abs/2010.03644v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-7">7. Cross-Thought for Sentence Encoder Pre-training</h2>

Title: [Cross-Thought for Sentence Encoder Pre-training](https://arxiv.org/abs/2010.03652)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03652](https://arxiv.org/abs/2010.03652) [cs.CL]** |
|           | (or **[arXiv:2010.03652v1](https://arxiv.org/abs/2010.03652v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-8">8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation</h2>

Title: [Leveraging Discourse Rewards for Document-Level Neural Machine Translation](https://arxiv.org/abs/2010.03732)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Nazanin Esmaili](https://arxiv.org/search/cs?searchtype=author&query=Esmaili%2C+N), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion (LC) and coherence (COH), by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F_BERT.

| Comments: | Accepted at COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03732](https://arxiv.org/abs/2010.03732) [cs.CL]** |
|           | (or **[arXiv:2010.03732v1](https://arxiv.org/abs/2010.03732v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-9">9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</h2>

Title: [Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](https://arxiv.org/abs/2010.03790)

Authors: [Keerthiram Murugesan](https://arxiv.org/search/cs?searchtype=author&query=Murugesan%2C+K), [Mattia Atzeni](https://arxiv.org/search/cs?searchtype=author&query=Atzeni%2C+M), [Pavan Kapanipathi](https://arxiv.org/search/cs?searchtype=author&query=Kapanipathi%2C+P), [Pushkar Shukla](https://arxiv.org/search/cs?searchtype=author&query=Shukla%2C+P), [Sadhana Kumaravel](https://arxiv.org/search/cs?searchtype=author&query=Kumaravel%2C+S), [Gerald Tesauro](https://arxiv.org/search/cs?searchtype=author&query=Tesauro%2C+G), [Kartik Talamadupula](https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K), [Mrinmaya Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+M), [Murray Campbell](https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M)

> Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03790](https://arxiv.org/abs/2010.03790) [cs.AI]** |
|           | (or **[arXiv:2010.03790v1](https://arxiv.org/abs/2010.03790v1) [cs.AI]** for this version) |





# 2020-10-08

[Return to Index](#Index)



<h2 id="2020-10-08-1">1. Plug and Play Autoencoders for Conditional Text Generation</h2>

Title: [Plug and Play Autoencoders for Conditional Text Generation](https://arxiv.org/abs/2010.02983)

Authors: [Florian Mai](https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+F) (1 and 2), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N) (3), [Ivan Montero](https://arxiv.org/search/cs?searchtype=author&query=Montero%2C+I) (3), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A) (3 and 4), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J) (1) ((1) Idiap Research Institute, (2) EPFL, (3) University of Washington, (4) Allen Institute for Artificial Intelligence)

> Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.

| Comments: | To be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02983](https://arxiv.org/abs/2010.02983) [cs.CL]** |
|           | (or **[arXiv:2010.02983v1](https://arxiv.org/abs/2010.02983v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-2">2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</h2>

Title: [Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](https://arxiv.org/abs/2010.03142)

Authors: [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple low-resource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pre-training corpus. Code, data, and pre-trained models are available at [this https URL](https://github.com/linzehui/mRASP).

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03142](https://arxiv.org/abs/2010.03142) [cs.CL]** |
|           | (or **[arXiv:2010.03142v1](https://arxiv.org/abs/2010.03142v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-3">3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction</h2>

Title: [A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](https://arxiv.org/abs/2010.03155)

Authors: [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found that there is a non-negligible amount of "noise" where errors were inappropriately edited or left uncorrected. To address this, we designed a self-refinement method where the key idea is to denoise these datasets by leveraging the prediction consistency of existing models, and outperformed strong denoising baseline methods. We further applied task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. We then analyzed the effect of the proposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance.

| Comments: | accepted by EMNLP 2020 (Findings)                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03155](https://arxiv.org/abs/2010.03155) [cs.CL]** |
|           | (or **[arXiv:2010.03155v1](https://arxiv.org/abs/2010.03155v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-4">4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages</h2>

Title: [Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](https://arxiv.org/abs/2010.03179)

Authors: [Michael A. Hedderich](https://arxiv.org/search/cs?searchtype=author&query=Hedderich%2C+M+A), [David Adelani](https://arxiv.org/search/cs?searchtype=author&query=Adelani%2C+D), [Dawei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+D), [Jesujoba Alabi](https://arxiv.org/search/cs?searchtype=author&query=Alabi%2C+J), [Udia Markus](https://arxiv.org/search/cs?searchtype=author&query=Markus%2C+U), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and Yorùbá on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.

| Comments: | Accepted at EMNLP'20                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03179](https://arxiv.org/abs/2010.03179) [cs.CL]** |
|           | (or **[arXiv:2010.03179v1](https://arxiv.org/abs/2010.03179v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-5">5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction</h2>

Title: [Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](https://arxiv.org/abs/2010.03260)

Authors: [Mengyun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Xingxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03260](https://arxiv.org/abs/2010.03260) [cs.CL]** |
|           | (or **[arXiv:2010.03260v1](https://arxiv.org/abs/2010.03260v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-6">6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation</h2>

Title: [Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](https://arxiv.org/abs/2010.03412)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.

| Comments: | Accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03412](https://arxiv.org/abs/2010.03412) [cs.CL]** |
|           | (or **[arXiv:2010.03412v1](https://arxiv.org/abs/2010.03412v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-7">7. WER we are and WER we think we are</h2>

Title: [WER we are and WER we think we are](https://arxiv.org/abs/2010.03432)

Authors: [Piotr Szymański](https://arxiv.org/search/cs?searchtype=author&query=Szymański%2C+P), [Piotr Żelasko](https://arxiv.org/search/cs?searchtype=author&query=Żelasko%2C+P), [Mikolaj Morzy](https://arxiv.org/search/cs?searchtype=author&query=Morzy%2C+M), [Adrian Szymczak](https://arxiv.org/search/cs?searchtype=author&query=Szymczak%2C+A), [Marzena Żyła-Hoppe](https://arxiv.org/search/cs?searchtype=author&query=Żyła-Hoppe%2C+M), [Joanna Banaszczak](https://arxiv.org/search/cs?searchtype=author&query=Banaszczak%2C+J), [Lukasz Augustyniak](https://arxiv.org/search/cs?searchtype=author&query=Augustyniak%2C+L), [Jan Mizgajski](https://arxiv.org/search/cs?searchtype=author&query=Mizgajski%2C+J), [Yishay Carmiel](https://arxiv.org/search/cs?searchtype=author&query=Carmiel%2C+Y)

> Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.

| Comments: | Accepted to EMNLP Findings                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.03432](https://arxiv.org/abs/2010.03432) [cs.CL]** |
|           | (or **[arXiv:2010.03432v1](https://arxiv.org/abs/2010.03432v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-8">8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation</h2>

Title: [Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](https://arxiv.org/abs/2010.03486)

Authors: [Valentin Barriere](https://arxiv.org/search/cs?searchtype=author&query=Barriere%2C+V), [Alexandra Balahur](https://arxiv.org/search/cs?searchtype=author&query=Balahur%2C+A)

> Tweets are specific text data when compared to general text. Although sentiment analysis over tweets has become very popular in the last decade for English, it is still difficult to find huge annotated corpora for non-English languages. The recent rise of the transformer models in Natural Language Processing allows to achieve unparalleled performances in many tasks, but these models need a consequent quantity of text to adapt to the tweet domain. We propose the use of a multilingual transformer model, that we pre-train over English tweets and apply data-augmentation using automatic translation to adapt the model to non-English languages. Our experiments in French, Spanish, German and Italian suggest that the proposed technique is an efficient way to improve the results of the transformers over small corpora of tweets in a non-English language.

| Comments: | Accepted to COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03486](https://arxiv.org/abs/2010.03486) [cs.CL]** |
|           | (or **[arXiv:2010.03486v1](https://arxiv.org/abs/2010.03486v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-9">9. TeaForN: Teacher-Forcing with N-grams</h2>

Title: [TeaForN: Teacher-Forcing with N-grams](https://arxiv.org/abs/2010.03494)

Authors: [Sebastian Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.

| Comments: | to be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03494](https://arxiv.org/abs/2010.03494) [cs.CL]** |
|           | (or **[arXiv:2010.03494v1](https://arxiv.org/abs/2010.03494v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-10">10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models</h2>

Title: [Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](https://arxiv.org/abs/2010.03542)

Authors: [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y)

> This paper describes Galileo's performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.

| Comments: | 8 pages, 2 figures, 6 tables. Accepted at Proceedings of 14th International Workshop on Semantic Evaluation (SemEval-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03542](https://arxiv.org/abs/2010.03542) [cs.CL]** |
|           | (or **[arXiv:2010.03542v1](https://arxiv.org/abs/2010.03542v1) [cs.CL]** for this version) |



# 2020-10-07

[Return to Index](#Index)



<h2 id="2020-10-07-1">1. Multi-task Learning for Multilingual Neural Machine Translation</h2>

Title: [Multi-task Learning for Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.02523)

Authors: [Yiren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [ChengXiang Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+C), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H)

> While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02523](https://arxiv.org/abs/2010.02523) [cs.CL]** |
|           | (or **[arXiv:2010.02523v1](https://arxiv.org/abs/2010.02523v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-2">2. Do Explicit Alignments Robustly Improve Multilingual Encoders?</h2>

Title: [Do Explicit Alignments Robustly Improve Multilingual Encoders?](https://arxiv.org/abs/2010.02537)

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M)

> Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02537](https://arxiv.org/abs/2010.02537) [cs.CL]** |
|           | (or **[arXiv:2010.02537v1](https://arxiv.org/abs/2010.02537v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-3">3. The Multilingual Amazon Reviews Corpus</h2>

Title: [The Multilingual Amazon Reviews Corpus](https://arxiv.org/abs/2010.02573)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [György Szarvas](https://arxiv.org/search/cs?searchtype=author&query=Szarvas%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.

| Comments: | To appear in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02573](https://arxiv.org/abs/2010.02573) [cs.CL]** |
|           | (or **[arXiv:2010.02573v1](https://arxiv.org/abs/2010.02573v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-4">4. On the Sparsity of Neural Machine Translation Models</h2>

Title: [On the Sparsity of Neural Machine Translation Models](https://arxiv.org/abs/2010.02646)

Authors: [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02646](https://arxiv.org/abs/2010.02646) [cs.CL]** |
|           | (or **[arXiv:2010.02646v1](https://arxiv.org/abs/2010.02646v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-5">5. On the Sub-Layer Functionalities of Transformer Decoder</h2>

Title: [On the Sub-Layer Functionalities of Transformer Decoder](https://arxiv.org/abs/2010.02648)

Authors: [Yilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Prasad Tadepalli](https://arxiv.org/search/cs?searchtype=author&query=Tadepalli%2C+P), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages -- developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance -- a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.

| Comments: | Findings of the 2020 Conference on Empirical Methods in Natural Language Processing (Long) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02648](https://arxiv.org/abs/2010.02648) [cs.CL]** |
|           | (or **[arXiv:2010.02648v1](https://arxiv.org/abs/2010.02648v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-6">6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</h2>

Title: [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://arxiv.org/abs/2010.02684)

Authors: [Alvin Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+A), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A)

> This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a 'backdoor poisoning' attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier's predictions can be steered to the poison target class with success rates of >80% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.

| Comments: | Accepted in EMNLP-Findings 2020, Camera Ready Version        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2010.02684](https://arxiv.org/abs/2010.02684) [cs.CL]** |
|           | (or **[arXiv:2010.02684v1](https://arxiv.org/abs/2010.02684v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-7">7. Analyzing Individual Neurons in Pre-trained Language Models</h2>

Title: [Analyzing Individual Neurons in Pre-trained Language Models](https://arxiv.org/abs/2010.02695)

Authors: [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

> While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.

| Comments: | Accepted in EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02695](https://arxiv.org/abs/2010.02695) [cs.CL]** |
|           | (or **[arXiv:2010.02695v1](https://arxiv.org/abs/2010.02695v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-8">8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</h2>

Title: [Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](https://arxiv.org/abs/2010.02705)

Authors: [Minki Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+M), [Moonsu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Sung Ju Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J)

> We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.

| Comments: | 19 pages, 9 figures, EMNLP 2020                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02705](https://arxiv.org/abs/2010.02705) [cs.CL]** |
|           | (or **[arXiv:2010.02705v1](https://arxiv.org/abs/2010.02705v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-9">9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs</h2>

Title: [Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](https://arxiv.org/abs/2010.02847)

Authors: [Haiyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Alison Sneyd](https://arxiv.org/search/cs?searchtype=author&query=Sneyd%2C+A), [Mark Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+M)

> It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy "man is to computer-programmer as woman is to homemaker" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.

| Comments: | Accepted at AACL-IJCNLP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02847](https://arxiv.org/abs/2010.02847) [cs.CL]** |
|           | (or **[arXiv:2010.02847v1](https://arxiv.org/abs/2010.02847v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-10">10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</h2>

Title: [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://arxiv.org/abs/2010.02301)

Authors: [Xinyu Hua](https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X), [Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L)

> Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often "rambling" without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.

| Comments: | Accepted at EMNLP 2020 as a long paper                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02301](https://arxiv.org/abs/2010.02301) [cs.CL]** |
|           | (or **[arXiv:2010.02301v1](https://arxiv.org/abs/2010.02301v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-11">11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation</h2>

Title: [We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](https://arxiv.org/abs/2010.02339)

Authors: [Ashiqur R. KhudaBukhsh](https://arxiv.org/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R), [Rupak Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+R), [Mark S. Kamlet](https://arxiv.org/search/cs?searchtype=author&query=Kamlet%2C+M+S), [Tom M. Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell%2C+T+M)

> Polarization among US political parties, media and elites is a widely studied topic. Prominent lines of prior research across multiple disciplines have observed and analyzed growing polarization in social media. In this paper, we present a new methodology that offers a fresh perspective on interpreting polarization through the lens of machine translation. With a novel proposition that two sub-communities are speaking in two different \emph{languages}, we demonstrate that modern machine translation methods can provide a simple yet powerful and interpretable framework to understand the differences between two (or more) large-scale social media discussion data sets at the granularity of words. Via a substantial corpus of 86.6 million comments by 6.5 million users on over 200,000 news videos hosted by YouTube channels of four prominent US news networks, we demonstrate that simple word-level and phrase-level translation pairs can reveal deep insights into the current political divide -- what is \emph{black lives matter} to one can be \emph{all lives matter} to the other.

| Subjects: | **Computation and Language (cs.CL)**; Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02339](https://arxiv.org/abs/2010.02339) [cs.CL]** |
|           | (or **[arXiv:2010.02339v1](https://arxiv.org/abs/2010.02339v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-12">12. Inference Strategies for Machine Translation with Conditional Masking</h2>

Title: [Inference Strategies for Machine Translation with Conditional Masking](https://arxiv.org/abs/2010.02352)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard "mask-predict" algorithm, and provide analyses of its behavior on machine translation tasks.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02352](https://arxiv.org/abs/2010.02352) [cs.CL]** |
|           | (or **[arXiv:2010.02352v1](https://arxiv.org/abs/2010.02352v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-13">13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</h2>

Title: [Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](https://arxiv.org/abs/2010.02394)

Authors: [Lichao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L), [Congying Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+C), [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Tingting Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+T), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S), [Lifang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+L)

> Mixup is the latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT. To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named "mixup-transformer", for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUE benchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.

| Comments: | Accepted by COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02394](https://arxiv.org/abs/2010.02394) [cs.CL]** |
|           | (or **[arXiv:2010.02394v1](https://arxiv.org/abs/2010.02394v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-14">14. Guiding Attention for Self-Supervised Learning with Transformers</h2>

Title: [Guiding Attention for Self-Supervised Learning with Transformers](https://arxiv.org/abs/2010.02399)

Authors: [Ameet Deshpande](https://arxiv.org/search/cs?searchtype=author&query=Deshpande%2C+A), [Karthik Narasimhan](https://arxiv.org/search/cs?searchtype=author&query=Narasimhan%2C+K)

> In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.

| Comments: | Accepted to Findings of EMNLP, 2020                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02399](https://arxiv.org/abs/2010.02399) [cs.CL]** |
|           | (or **[arXiv:2010.02399v1](https://arxiv.org/abs/2010.02399v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-15">15. Adversarial Grammatical Error Correction</h2>

Title: [Adversarial Grammatical Error Correction](https://arxiv.org/abs/2010.02407)

Authors: [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V), [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D)

> Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs) have been successful in generating realistic texts across many different tasks by learning to directly minimize the difference between human-generated and synthetic text. In this work, we present an adversarial learning approach to GEC, using the generator-discriminator framework. The generator is a Transformer model, trained to produce grammatically correct sentences given grammatically incorrect ones. The discriminator is a sentence-pair classification model, trained to judge a given pair of grammatically incorrect-correct sentences on the quality of grammatical correction. We pre-train both the discriminator and the generator on parallel texts and then fine-tune them further using a policy gradient method that assigns high rewards to sentences which could be true corrections of the grammatically incorrect text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that Adversarial-GEC can achieve competitive GEC quality compared to NMT-based baselines.

| Comments: | 13 Pages, EMNLP 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02407](https://arxiv.org/abs/2010.02407) [cs.CL]** |
|           | (or **[arXiv:2010.02407v1](https://arxiv.org/abs/2010.02407v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-16">16. Efficient Inference For Neural Machine Translation</h2>

Title: [Efficient Inference For Neural Machine Translation](https://arxiv.org/abs/2010.02416)

Authors: [Yi-Te Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+Y), [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Yi-Hsiu Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Y), [Ilya Chatsviorkin](https://arxiv.org/search/cs?searchtype=author&query=Chatsviorkin%2C+I)

> Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02416](https://arxiv.org/abs/2010.02416) [cs.CL]** |
|           | (or **[arXiv:2010.02416v1](https://arxiv.org/abs/2010.02416v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-17">17. Iterative Domain-Repaired Back-Translation</h2>

Title: [Iterative Domain-Repaired Back-Translation](https://arxiv.org/abs/2010.02473)

Authors: [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02473](https://arxiv.org/abs/2010.02473) [cs.CL]** |
|           | (or **[arXiv:2010.02473v1](https://arxiv.org/abs/2010.02473v1) [cs.CL]** for this version) |







# 2020-10-06

[Return to Index](#Index)



<h2 id="2020-10-06-1">1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples</h2>

Title: [A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](https://arxiv.org/abs/2010.01345)

Authors: [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.

| Comments: | COLING 2020 Long Paper                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01345](https://arxiv.org/abs/2010.01345) [cs.CL]** |
|           | (or **[arXiv:2010.01345v1](https://arxiv.org/abs/2010.01345v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-2">2. Transformer-Based Neural Text Generation with Syntactic Guidance</h2>

Title: [Transformer-Based Neural Text Generation with Syntactic Guidance](https://arxiv.org/abs/2010.01737)

Authors: [Yinghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y) (Georgia Institute of Technology), [Rui Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+R) (Georgia Institute of Technology), [Isaac Rehg](https://arxiv.org/search/cs?searchtype=author&query=Rehg%2C+I) (Georgia Institute of Technology), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C) (Georgia Institute of Technology)

> We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.

| Comments: | 11 pages, 4 figures and 5 tables                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01737](https://arxiv.org/abs/2010.01737) [cs.CL]** |
|           | (or **[arXiv:2010.01737v1](https://arxiv.org/abs/2010.01737v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-3">3. Second-Order NLP Adversarial Examples</h2>

Title: [Second-Order NLP Adversarial Examples](https://arxiv.org/abs/2010.01770)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X)

> Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at [this https URL](https://github.com/jxmorris12/second-order-adversarial-examples).

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01770](https://arxiv.org/abs/2010.01770) [cs.CL]** |
|           | (or **[arXiv:2010.01770v2](https://arxiv.org/abs/2010.01770v2) [cs.CL]** for this version) |





<h2 id="2020-10-06-4">4. GenAug: Data Augmentation for Finetuning Text Generators</h2>

Title: [GenAug: Data Augmentation for Finetuning Text Generators](https://arxiv.org/abs/2010.01794)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Dongyeop Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+D), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.

| Comments: | EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available at [this https URL](https://github.com/styfeng/GenAug) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.01794](https://arxiv.org/abs/2010.01794) [cs.CL]** |
|           | (or **[arXiv:2010.01794v1](https://arxiv.org/abs/2010.01794v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-5">5. Lifelong Language Knowledge Distillation</h2>

Title: [Lifelong Language Knowledge Distillation](https://arxiv.org/abs/2010.02123)

Authors: [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Shang-Yu Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+S), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02123](https://arxiv.org/abs/2010.02123) [cs.CL]** |
|           | (or **[arXiv:2010.02123v1](https://arxiv.org/abs/2010.02123v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-6">6. A Streaming Approach For Efficient Batched Beam Search</h2>

Title: [A Streaming Approach For Efficient Batched Beam Search](https://arxiv.org/abs/2010.02164)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Violet Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+V), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically ``refills" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF) |
| Cite as:  | **[arXiv:2010.02164](https://arxiv.org/abs/2010.02164) [cs.CL]** |
|           | (or **[arXiv:2010.02164v1](https://arxiv.org/abs/2010.02164v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-7">7. Self-training Improves Pre-training for Natural Language Understanding</h2>

Title: [Self-training Improves Pre-training for Natural Language Understanding](https://arxiv.org/abs/2010.02194)

Authors: [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Beliz Gunel](https://arxiv.org/search/cs?searchtype=author&query=Gunel%2C+B), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02194](https://arxiv.org/abs/2010.02194) [cs.CL]** |
|           | (or **[arXiv:2010.02194v1](https://arxiv.org/abs/2010.02194v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-8">8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation</h2>

Title: [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.01667)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> To improve the performance of Neural Machine Translation~(NMT) for low-resource languages~(LRL), one effective strategy is to leverage parallel data from a related high-resource language~(HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate \emph{into} the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01667](https://arxiv.org/abs/2010.01667) [cs.CL]** |
|           | (or **[arXiv:2010.01667v1](https://arxiv.org/abs/2010.01667v1) [cs.CL]** for this version) |





# 2020-10-05

[Return to Index](#Index)



<h2 id="2020-10-05-1">1. Nearest Neighbor Machine Translation</h2>

Title: [Nearest Neighbor Machine Translation](https://arxiv.org/abs/2010.00710)

Authors: [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We introduce k-nearest-neighbor machine translation (kNN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.00710](https://arxiv.org/abs/2010.00710) [cs.CL]** |
|           | (or **[arXiv:2010.00710v1](https://arxiv.org/abs/2010.00710v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-2">2. A Survey of the State of Explainable AI for Natural Language Processing</h2>

Title: [A Survey of the State of Explainable AI for Natural Language Processing](https://arxiv.org/abs/2010.00711)

Authors: [Marina Danilevsky](https://arxiv.org/search/cs?searchtype=author&query=Danilevsky%2C+M), [Kun Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+K), [Ranit Aharonov](https://arxiv.org/search/cs?searchtype=author&query=Aharonov%2C+R), [Yannis Katsis](https://arxiv.org/search/cs?searchtype=author&query=Katsis%2C+Y), [Ban Kawas](https://arxiv.org/search/cs?searchtype=author&query=Kawas%2C+B), [Prithviraj Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+P)

> Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.

| Comments:    | To appear in AACL-IJCNLP 2020                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.00711](https://arxiv.org/abs/2010.00711) [cs.CL]** |
|              | (or **[arXiv:2010.00711v1](https://arxiv.org/abs/2010.00711v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-3">3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</h2>

Title: [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://arxiv.org/abs/2010.00784)

Authors: [Kristjan Arumae](https://arxiv.org/search/cs?searchtype=author&query=Arumae%2C+K), [Qing Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Parminder Bhatia](https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+P)

> Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.

| Comments: | arXiv admin note: text overlap with [arXiv:2004.03794](https://arxiv.org/abs/2004.03794) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.00784](https://arxiv.org/abs/2010.00784) [cs.CL]** |
|           | (or **[arXiv:2010.00784v1](https://arxiv.org/abs/2010.00784v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-4">4. Which *BERT? A Survey Organizing Contextualized Encoders</h2>

Title: [Which *BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/abs/2010.00854)

Authors: [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B)

> Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.00854](https://arxiv.org/abs/2010.00854) [cs.CL]** |
|           | (or **[arXiv:2010.00854v1](https://arxiv.org/abs/2010.00854v1) [cs.CL]** for this version) |



# 2020-10-02

[Return to Index](#Index)



<h2 id="2020-10-02-1">1. WeChat Neural Machine Translation Systems for WMT20</h2>

Title: [WeChat Neural Machine Translation Systems for WMT20](https://arxiv.org/abs/2010.00247)

Authors: [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Xianfeng Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Qinsong Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Q), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Sifan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H)

> We participate in the WMT 2020 shared news translation task on Chinese to English. Our system is based on the Transformer (Vaswani et al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese to English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.

| Comments: | Accepted at WMT 2020. Our Chinese to English system achieved the highest case-sensitive BLEU score among all submissions |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.00247](https://arxiv.org/abs/2010.00247) [cs.CL]** |
|           | (or **[arXiv:2010.00247v1](https://arxiv.org/abs/2010.00247v1) [cs.CL]** for this version) |



# 2020-10-01

[Return to Index](#Index)



<h2 id="2020-10-01-1">1. Rethinking Attention with Performers</h2>

Title: [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)

Authors: [Krzysztof Choromanski](https://arxiv.org/search/cs?searchtype=author&query=Choromanski%2C+K), [Valerii Likhosherstov](https://arxiv.org/search/cs?searchtype=author&query=Likhosherstov%2C+V), [David Dohan](https://arxiv.org/search/cs?searchtype=author&query=Dohan%2C+D), [Xingyou Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Andreea Gane](https://arxiv.org/search/cs?searchtype=author&query=Gane%2C+A), [Tamas Sarlos](https://arxiv.org/search/cs?searchtype=author&query=Sarlos%2C+T), [Peter Hawkins](https://arxiv.org/search/cs?searchtype=author&query=Hawkins%2C+P), [Jared Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+J), [Afroz Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+A), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [David Belanger](https://arxiv.org/search/cs?searchtype=author&query=Belanger%2C+D), [Lucy Colwell](https://arxiv.org/search/cs?searchtype=author&query=Colwell%2C+L), [Adrian Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+A)

> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.

| Comments: | 36 pages. This is an updated version of a previous submission which can be found at [arXiv:2006.03555](https://arxiv.org/abs/2006.03555). See [this https URL](https://github.com/google-research/google-research/tree/master/protein_lm) for protein language model code, and [this https URL](https://github.com/google-research/google-research/tree/master/performer) for Performer code |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.14794](https://arxiv.org/abs/2009.14794) [cs.LG]** |
|           | (or **[arXiv:2009.14794v1](https://arxiv.org/abs/2009.14794v1) [cs.LG]** for this version) |





<h2 id="2020-10-01-2">2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</h2>

Title: [Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](https://arxiv.org/abs/2009.14304)

Authors: [Saurabh Kulshreshtha](https://arxiv.org/search/cs?searchtype=author&query=Kulshreshtha%2C+S), [José Luis Redondo-García](https://arxiv.org/search/cs?searchtype=author&query=Redondo-García%2C+J+L), [Ching-Yun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C)

> Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work proposes several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs. dictionary-based supervision and rotational vs. fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.

| Comments: | Accepted as a long paper in Findings of EMNLP 2020           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14304](https://arxiv.org/abs/2009.14304) [cs.CL]** |
|           | (or **[arXiv:2009.14304v1](https://arxiv.org/abs/2009.14304v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-3">3. Can Automatic Post-Editing Improve NMT?</h2>

Title: [Can Automatic Post-Editing Improve NMT?](https://arxiv.org/abs/2009.14395)

Authors: [Shamil Chollampatt](https://arxiv.org/search/cs?searchtype=author&query=Chollampatt%2C+S), [Raymond Hendy Susanto](https://arxiv.org/search/cs?searchtype=author&query=Susanto%2C+R+H), [Liling Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L), [Ewa Szymanska](https://arxiv.org/search/cs?searchtype=author&query=Szymanska%2C+E)

> Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at [this https URL](https://github.com/shamilcm/pedra).

| Comments: | In EMNLP 2020                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14395](https://arxiv.org/abs/2009.14395) [cs.CL]** |
|           | (or **[arXiv:2009.14395v1](https://arxiv.org/abs/2009.14395v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-4">4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment</h2>

Title: [Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](https://arxiv.org/abs/2009.14510)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data.

| Comments: | EMNLP-2020 Long Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.14510](https://arxiv.org/abs/2009.14510) [cs.CL]** |
|           | (or **[arXiv:2009.14510v1](https://arxiv.org/abs/2009.14510v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-5">5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation</h2>

Title: [On Romanization for Model Transfer Between Scripts in Neural Machine Translation](https://arxiv.org/abs/2009.14824)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.

| Comments: | accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14824](https://arxiv.org/abs/2009.14824) [cs.CL]** |
|           | (or **[arXiv:2009.14824v1](https://arxiv.org/abs/2009.14824v1) [cs.CL]** for this version) |

