# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-27](#2019-08-27)
  - [1. Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation](#2019-08-27-1)
  - [2. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures](#2019-08-27-2)
  - [3. Multilingual Neural Machine Translation with Language Clustering](#2019-08-27-3)
  - [4. Efficient Bidirectional Neural Machine Translation](#2019-08-27-4)
  - [5. Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation](#2019-08-27-5)
  - [6. An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation](#2019-08-27-6)

- [2019-08-26](#2019-08-26)
  - [1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](#2019-08-26-1)
  - [2. A Lost Croatian Cybernetic Machine Translation Program](#2019-08-26-2)
- [3. Revealing the Dark Secrets of BERT](#2019-08-26-3)
  
- [2019-08-23](#2019-08-23)
  - [1. Denoising based Sequence-to-Sequence Pre-training for Text Generation](#2019-08-23-1)
  - [2. Dual Skew Divergence Loss for Neural Machine Translation](#2019-08-23-2)

- [2019-08-22](#2019-08-22)
  - [1. Improving Neural Machine Translation with Pre-trained Representation](#2019-08-22-1)
  - [2. On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning](#2019-08-22-2)
  - [3. An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing](#2019-08-22-3)
  - [4. A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation](#2019-08-22-4)
  - [5. Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples](#2019-08-22-5)

- [2019-08-21](#2019-08-21)
  - [1. Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior](#2019-08-21-1)
  - [2. ARAML: A Stable Adversarial Training Framework for Text Generation](#2019-08-21-2)
  - [3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers](#2019-08-21-3)

- [2019-08-20](#2019-08-20)
  - [1. UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task](#2019-08-20-1)
  - [2. Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation](#2019-08-20-2)
  - [3. The Transference Architecture for Automatic Post-Editing](#2019-08-20-3)
  - [4. Language Graph Distillation for Low-Resource Machine Translation](#2019-08-20-4)
  - [5. Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation](#2019-08-20-5)
  - [6. Recurrent Graph Syntax Encoder for Neural Machine Translation](#2019-08-20-6)
  - [7. Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces](#2019-08-20-7)

- [2019-08-19](#2019-08-19)
  - [1. Attending to Future Tokens For Bidirectional Sequence Generation](#2019-08-19-1)
  - [2. Towards Making the Most of BERT in Neural Machine Translation](#2019-08-19-2)
  - [3. Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs](#2019-08-19-3)
  - [4. Simple and Effective Noisy Channel Modeling for Neural Machine Translation](#2019-08-19-4)
  - [5. Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring](#2019-08-19-5)
- [2019-08-15](#2019-08-15)
  - [1. On The Evaluation of Machine Translation Systems Trained With Back-Translation](#2019-08-15-1)
- [2019-08-14](#2019-08-14)
  - [1. Neural Text Generation with Unlikelihood Training](#2019-08-14-1)
  - [2. LSTM vs. GRU vs. Bidirectional RNN for script generation](#2019-08-14-2)
  - [3. Attention is not not Explanation](#2019-08-14-3)
  - [4. Neural Machine Translation with Noisy Lexical Constraints](#2019-08-14-4)
- [2019-08-13](#2019-08-13)
  - [1. On the Validity of Self-Attention as Explanation in Transformer Models](#2019-08-13-1)
- [2019-08-12](#2019-08-12)
  - [1. Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling](#2019-08-12-1)
  - [2. UdS Submission for the WMT 19 Automatic Post-Editing Task](#2019-08-12-2)
- [2019-08-09](#2019-08-09)
  - [1. A Test Suite and Manual Evaluation of Document-Level NMT at WMT19](#2019-08-09-1)
- [2019-08-07](#2019-08-07)
  - [1. MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models](#2019-08-07-1)
  - [2. A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data](#2019-08-07-2)
  - [3. Self-Knowledge Distillation in Natural Language Processing](#2019-08-07-3)
- [2019-08-06](#2019-08-06)
  - [1. Invariance-based Adversarial Attack on Neural Machine Translation Systems](#2019-08-06-1)
  - [2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](#2019-08-06-2)
  - [3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](#2019-08-06-3)
  - [4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](#2019-08-06-4)
  - [5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](#2019-08-06-5)
  - [6. Predicting Actions to Help Predict Translations](#2019-08-06-6)
  - [7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](#2019-08-06-7)
- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-27

[Return to Index](#Index)



<h2 id="2019-08-27-1">1. Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation</h2> 

Title: [Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation](https://arxiv.org/abs/1908.08962)

Authors: [Iulia Turc](https://arxiv.org/search/cs?searchtype=author&query=Turc%2C+I), [Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M), [Kenton Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Kristina Toutanova](https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K)

*(Submitted on 23 Aug 2019)*

> Recent developments in NLP have been accompanied by large, expensive models. Knowledge distillation is the standard method to realize these gains in applications with limited resources: a compact student is trained to recover the outputs of a powerful teacher. While most prior work investigates student architectures and transfer techniques, we focus on an often-neglected aspect---student initialization. We argue that a random starting point hinders students from fully leveraging the teacher expertise, even in the presence of a large transfer set. We observe that applying language model pre-training to students unlocks their generalization potential, surprisingly even for very compact networks. We conduct experiments on 4 NLP tasks and 24 sizes of Transformer-based students; for sentiment classification on the Amazon Book Reviews dataset, pre-training boosts size reduction and TPU speed-up from 3.1x/1.25x to 31x/16x. Extensive ablation studies dissect the interaction between pre-training and distillation, revealing a compound effect even when they are applied on the same unlabeled dataset.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.08962 [cs.CL]**                         |
|           | (or **arXiv:1908.08962v1 [cs.CL]** for this version) |





<h2 id="2019-08-27-2">2. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</h2> 

Title: [Neural data-to-text generation: A comparison between pipeline and end-to-end architectures](https://arxiv.org/abs/1908.09022)

Authors: [Thiago Castro Ferreira](https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+T+C), [Chris van der Lee](https://arxiv.org/search/cs?searchtype=author&query=van+der+Lee%2C+C), [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Emiel Krahmer](https://arxiv.org/search/cs?searchtype=author&query=Krahmer%2C+E)

*(Submitted on 23 Aug 2019)*

> Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.

| Comments: | Preprint version of the EMNLP 2019 article           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.09022 [cs.CL]**                         |
|           | (or **arXiv:1908.09022v1 [cs.CL]** for this version) |





<h2 id="2019-08-27-3">3. Multilingual Neural Machine Translation with Language Clustering</h2> 

Title: [Multilingual Neural Machine Translation with Language Clustering](https://arxiv.org/abs/1908.09324)

Authors: [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Jiale Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 25 Aug 2019)*

> Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods

| Comments: | Accepted by EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1908.09324 [cs.CL]**                                 |
|           | (or **arXiv:1908.09324v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-4">4. Efficient Bidirectional Neural Machine Translation</h2> 

Title: [Efficient Bidirectional Neural Machine Translation](https://arxiv.org/abs/1908.09329)

Authors: [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T)

*(Submitted on 25 Aug 2019)*

> The encoder-decoder based neural machine translation usually generates a target sequence token by token from left to right. Due to error propagation, the tokens in the right side of the generated sequence are usually of poorer quality than those in the left side. In this paper, we propose an efficient method to generate a sequence in both left-to-right and right-to-left manners using a single encoder and decoder, combining the advantages of both generation directions. Experiments on three translation tasks show that our method achieves significant improvements over conventional unidirectional approach. Compared with ensemble methods that train and combine two models with different generation directions, our method saves 50% model parameters and about 40% training time, and also improve inference speed.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.09329 [cs.CL]**                                 |
|           | (or **arXiv:1908.09329v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-5">5. Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation</h2> 

Title: [Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation](https://arxiv.org/abs/1908.09532)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Gideon Maillette de Buy Wenniger](https://arxiv.org/search/cs?searchtype=author&query=de+Buy+Wenniger%2C+G+M), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 26 Aug 2019)*

> Machine Translation models are trained to translate a variety of documents from one language into another. However, models specifically trained for a particular characteristics of the documents tend to perform better. Fine-tuning is a technique for adapting an NMT model to some domain. In this work, we want to use this technique to adapt the model to a given test set. In particular, we are using transductive data selection algorithms which take advantage the information of the test set to retrieve sentences from a larger parallel set. 
> In cases where the model is available at translation time (when the test set is provided), it can be adapted with a small subset of data, thereby achieving better performance than a generic model or a domain-adapted model.

| Comments:          | Proceedings of The 8th Workshop on Patent and Scientific Literature Translation, 2019, pages 13--23, Dublin |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of The 8th Workshop on Patent and Scientific Literature Translation, 2019 |
| Cite as:           | **arXiv:1908.09532 [cs.CL]**                                 |
|                    | (or **arXiv:1908.09532v1 [cs.CL]** for this version)         |





<h2 id="2019-08-27-6">6. An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation</h2> 

Title: [An Empirical Study of Domain Adaptation for Unsupervised Neural Machine Translation](https://arxiv.org/abs/1908.09605)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 26 Aug 2019)*

> Domain adaptation methods have been well-studied in supervised neural machine translation (NMT). However, domain adaptation methods for unsupervised neural machine translation (UNMT) have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. Besides the inconsistent domains between training data and test data for supervised NMT, there sometimes exists an inconsistent domain between two monolingual training data for UNMT. In this work, we empirically show different scenarios for unsupervised domain-specific neural machine translation. Based on these scenarios, we propose several potential solutions to improve the performances of domain-specific UNMT systems.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.09605 [cs.CL]**                         |
|           | (or **arXiv:1908.09605v1 [cs.CL]** for this version) |





# 2019-08-26

[Return to Index](#Index)



<h2 id="2019-08-26-1">1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</h2> 
Title: [Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](https://arxiv.org/abs/1908.08597)

Authors:[Danielle Bragg](https://arxiv.org/search/cs?searchtype=author&query=Bragg%2C+D), [Oscar Koller](https://arxiv.org/search/cs?searchtype=author&query=Koller%2C+O), [Mary Bellard](https://arxiv.org/search/cs?searchtype=author&query=Bellard%2C+M), [Larwan Berke](https://arxiv.org/search/cs?searchtype=author&query=Berke%2C+L), [Patrick Boudrealt](https://arxiv.org/search/cs?searchtype=author&query=Boudrealt%2C+P), [Annelies Braffort](https://arxiv.org/search/cs?searchtype=author&query=Braffort%2C+A), [Naomi Caselli](https://arxiv.org/search/cs?searchtype=author&query=Caselli%2C+N), [Matt Huenerfauth](https://arxiv.org/search/cs?searchtype=author&query=Huenerfauth%2C+M), [Hernisa Kacorri](https://arxiv.org/search/cs?searchtype=author&query=Kacorri%2C+H), [Tessa Verhoef](https://arxiv.org/search/cs?searchtype=author&query=Verhoef%2C+T), [Christian Vogler](https://arxiv.org/search/cs?searchtype=author&query=Vogler%2C+C), [Meredith Ringel Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+M+R)

*(Submitted on 22 Aug 2019)*

> Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Computers and Society (cs.CY); Graphics (cs.GR); Human-Computer Interaction (cs.HC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.08597 [cs.CV]**                                 |
|           | (or **arXiv:1908.08597v1 [cs.CV]** for this version)         |





<h2 id="2019-08-26-2">2. A Lost Croatian Cybernetic Machine Translation Program</h2> 
Title: [A Lost Croatian Cybernetic Machine Translation Program](https://arxiv.org/abs/1908.08917)

Authors: [Sandro Skansi](https://arxiv.org/search/cs?searchtype=author&query=Skansi%2C+S), [Leo Mršić](https://arxiv.org/search/cs?searchtype=author&query=Mršić%2C+L), [Ines Skelac](https://arxiv.org/search/cs?searchtype=author&query=Skelac%2C+I)

*(Submitted on 20 Aug 2019)*

> We are exploring the historical significance of research in the field of machine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation in Yugoslavia during the 1950s. We are focused on two important seminal papers written by members of his research group from 1959 and 1962, as well as their legacy in establishing a Croatian machine translation program based around the Faculty of Humanities and Social Sciences of the University of Zagreb in the late 1950s and early 1960s. We are exploring their work in connection with the beginnings of machine translation in the USA and USSR, motivated by the Cold War and the intelligence needs of the period. We also present the approach to machine translation advocated by the Croatian group in Yugoslavia, which is different from the usual logical approaches of the period, and his advocacy of cybernetic methods, which would be adopted as a canon by the mainstream AI community only decades later.

| Comments: | To appear in "A Guide to Deep Learning Basics: Historical, Logical and Philosophical Perspectives" |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| Cite as:  | **arXiv:1908.08917 [cs.CY]**                                 |
|           | (or **arXiv:1908.08917v1 [cs.CY]** for this version)         |



<h2 id="2019-08-26-3">3. Revealing the Dark Secrets of BERT</h2> 

Title: [Revealing the Dark Secrets of BERT](https://arxiv.org/abs/1908.08593)

Authors: [Olga Kovaleva](https://arxiv.org/search/cs?searchtype=author&query=Kovaleva%2C+O), [Alexey Romanov](https://arxiv.org/search/cs?searchtype=author&query=Romanov%2C+A), [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A), [Anna Rumshisky](https://arxiv.org/search/cs?searchtype=author&query=Rumshisky%2C+A)

*(Submitted on 21 Aug 2019)*

> BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.

| Comments: | Accepted to EMNLP 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.08593 [cs.CL]**                                 |
|           | (or **arXiv:1908.08593v1 [cs.CL]** for this version)         |



# 2019-08-23

[Return to Index](#Index)



<h2 id="2019-08-23-1">1. Denoising based Sequence-to-Sequence Pre-training for Text Generation</h2> 
Title: [Denoising based Sequence-to-Sequence Pre-training for Text Generation](https://arxiv.org/abs/1908.08206)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ruoyu Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Sujian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

*(Submitted on 22 Aug 2019)*

> This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.

| Comments: | Accepted to EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08206 [cs.CL]**                         |
|           | (or **arXiv:1908.08206v1 [cs.CL]** for this version) |





<h2 id="2019-08-23-2">2. Dual Skew Divergence Loss for Neural Machine Translation</h2> 
Title: [Dual Skew Divergence Loss for Neural Machine Translation](https://arxiv.org/abs/1908.08399)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Yingting Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S)

*(Submitted on 22 Aug 2019)*

> For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.

| Comments: | 9pages                                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08399 [cs.CL]**                         |
|           | (or **arXiv:1908.08399v1 [cs.CL]** for this version) |