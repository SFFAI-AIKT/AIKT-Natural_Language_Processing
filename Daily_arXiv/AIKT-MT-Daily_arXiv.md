# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-26](#2019-08-26)
  - [1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](#2019-08-26-1)
  - [2. A Lost Croatian Cybernetic Machine Translation Program](#2019-08-26-2)

- [2019-08-23](#2019-08-23)
  - [1. Denoising based Sequence-to-Sequence Pre-training for Text Generation](#2019-08-23-1)
  - [2. Dual Skew Divergence Loss for Neural Machine Translation](#2019-08-23-2)

- [2019-08-22](#2019-08-22)
  - [1. Improving Neural Machine Translation with Pre-trained Representation](#2019-08-22-1)
  - [2. On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning](#2019-08-22-2)
  - [3. An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing](#2019-08-22-3)
  - [4. A novel text representation which enables image classifiers to perform text classification, applied to name disambiguation](#2019-08-22-4)
  - [5. Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples](#2019-08-22-5)

- [2019-08-21](#2019-08-21)
  - [1. Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior](#2019-08-21-1)
  - [2. ARAML: A Stable Adversarial Training Framework for Text Generation](#2019-08-21-2)
  - [3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers](#2019-08-21-3)

- [2019-08-20](#2019-08-20)
  - [1. UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task](#2019-08-20-1)
  - [2. Improving CAT Tools in the Translation Workflow: New Approaches and Evaluation](#2019-08-20-2)
  - [3. The Transference Architecture for Automatic Post-Editing](#2019-08-20-3)
  - [4. Language Graph Distillation for Low-Resource Machine Translation](#2019-08-20-4)
  - [5. Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation](#2019-08-20-5)
  - [6. Recurrent Graph Syntax Encoder for Neural Machine Translation](#2019-08-20-6)
  - [7. Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces](#2019-08-20-7)

- [2019-08-19](#2019-08-19)
  - [1. Attending to Future Tokens For Bidirectional Sequence Generation](#2019-08-19-1)
  - [2. Towards Making the Most of BERT in Neural Machine Translation](#2019-08-19-2)
  - [3. Transformer-based Automatic Post-Editing with a Context-Aware Encoding Approach for Multi-Source Inputs](#2019-08-19-3)
  - [4. Simple and Effective Noisy Channel Modeling for Neural Machine Translation](#2019-08-19-4)
  - [5. Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring](#2019-08-19-5)
- [2019-08-15](#2019-08-15)
  - [1. On The Evaluation of Machine Translation Systems Trained With Back-Translation](#2019-08-15-1)
- [2019-08-14](#2019-08-14)
  - [1. Neural Text Generation with Unlikelihood Training](#2019-08-14-1)
  - [2. LSTM vs. GRU vs. Bidirectional RNN for script generation](#2019-08-14-2)
  - [3. Attention is not not Explanation](#2019-08-14-3)
  - [4. Neural Machine Translation with Noisy Lexical Constraints](#2019-08-14-4)
- [2019-08-13](#2019-08-13)
  - [1. On the Validity of Self-Attention as Explanation in Transformer Models](#2019-08-13-1)
- [2019-08-12](#2019-08-12)
  - [1. Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling](#2019-08-12-1)
  - [2. UdS Submission for the WMT 19 Automatic Post-Editing Task](#2019-08-12-2)
- [2019-08-09](#2019-08-09)
  - [1. A Test Suite and Manual Evaluation of Document-Level NMT at WMT19](#2019-08-09-1)
- [2019-08-07](#2019-08-07)
  - [1. MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models](#2019-08-07-1)
  - [2. A Translate-Edit Model for Natural Language Question to SQL Query Generation on Multi-relational Healthcare Data](#2019-08-07-2)
  - [3. Self-Knowledge Distillation in Natural Language Processing](#2019-08-07-3)
- [2019-08-06](#2019-08-06)
  - [1. Invariance-based Adversarial Attack on Neural Machine Translation Systems](#2019-08-06-1)
  - [2. Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content](#2019-08-06-2)
  - [3. The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation](#2019-08-06-3)
  - [4. JUMT at WMT2019 News Translation Task: A Hybrid approach to Machine Translation for Lithuanian to English](#2019-08-06-4)
  - [5. Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](#2019-08-06-5)
  - [6. Predicting Actions to Help Predict Translations](#2019-08-06-6)
  - [7. Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing](#2019-08-06-7)
- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-26

[Return to Index](#Index)



<h2 id="2019-08-26-1">1. Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</h2> 

Title: [Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective](https://arxiv.org/abs/1908.08597)

Authors:[Danielle Bragg](https://arxiv.org/search/cs?searchtype=author&query=Bragg%2C+D), [Oscar Koller](https://arxiv.org/search/cs?searchtype=author&query=Koller%2C+O), [Mary Bellard](https://arxiv.org/search/cs?searchtype=author&query=Bellard%2C+M), [Larwan Berke](https://arxiv.org/search/cs?searchtype=author&query=Berke%2C+L), [Patrick Boudrealt](https://arxiv.org/search/cs?searchtype=author&query=Boudrealt%2C+P), [Annelies Braffort](https://arxiv.org/search/cs?searchtype=author&query=Braffort%2C+A), [Naomi Caselli](https://arxiv.org/search/cs?searchtype=author&query=Caselli%2C+N), [Matt Huenerfauth](https://arxiv.org/search/cs?searchtype=author&query=Huenerfauth%2C+M), [Hernisa Kacorri](https://arxiv.org/search/cs?searchtype=author&query=Kacorri%2C+H), [Tessa Verhoef](https://arxiv.org/search/cs?searchtype=author&query=Verhoef%2C+T), [Christian Vogler](https://arxiv.org/search/cs?searchtype=author&query=Vogler%2C+C), [Meredith Ringel Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+M+R)

*(Submitted on 22 Aug 2019)*

> Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Computers and Society (cs.CY); Graphics (cs.GR); Human-Computer Interaction (cs.HC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.08597 [cs.CV]**                                 |
|           | (or **arXiv:1908.08597v1 [cs.CV]** for this version)         |





<h2 id="2019-08-26-2">2. A Lost Croatian Cybernetic Machine Translation Program</h2> 

Title: [A Lost Croatian Cybernetic Machine Translation Program](https://arxiv.org/abs/1908.08917)

Authors: [Sandro Skansi](https://arxiv.org/search/cs?searchtype=author&query=Skansi%2C+S), [Leo Mršić](https://arxiv.org/search/cs?searchtype=author&query=Mršić%2C+L), [Ines Skelac](https://arxiv.org/search/cs?searchtype=author&query=Skelac%2C+I)

*(Submitted on 20 Aug 2019)*

> We are exploring the historical significance of research in the field of machine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation in Yugoslavia during the 1950s. We are focused on two important seminal papers written by members of his research group from 1959 and 1962, as well as their legacy in establishing a Croatian machine translation program based around the Faculty of Humanities and Social Sciences of the University of Zagreb in the late 1950s and early 1960s. We are exploring their work in connection with the beginnings of machine translation in the USA and USSR, motivated by the Cold War and the intelligence needs of the period. We also present the approach to machine translation advocated by the Croatian group in Yugoslavia, which is different from the usual logical approaches of the period, and his advocacy of cybernetic methods, which would be adopted as a canon by the mainstream AI community only decades later.

| Comments: | To appear in "A Guide to Deep Learning Basics: Historical, Logical and Philosophical Perspectives" |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| Cite as:  | **arXiv:1908.08917 [cs.CY]**                                 |
|           | (or **arXiv:1908.08917v1 [cs.CY]** for this version)         |



# 2019-08-23

[Return to Index](#Index)



<h2 id="2019-08-23-1">1. Denoising based Sequence-to-Sequence Pre-training for Text Generation</h2> 
Title: [Denoising based Sequence-to-Sequence Pre-training for Text Generation](https://arxiv.org/abs/1908.08206)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ruoyu Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Sujian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

*(Submitted on 22 Aug 2019)*

> This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.

| Comments: | Accepted to EMNLP 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08206 [cs.CL]**                         |
|           | (or **arXiv:1908.08206v1 [cs.CL]** for this version) |





<h2 id="2019-08-23-2">2. Dual Skew Divergence Loss for Neural Machine Translation</h2> 
Title: [Dual Skew Divergence Loss for Neural Machine Translation](https://arxiv.org/abs/1908.08399)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Yingting Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Shu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S)

*(Submitted on 22 Aug 2019)*

> For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.

| Comments: | 9pages                                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1908.08399 [cs.CL]**                         |
|           | (or **arXiv:1908.08399v1 [cs.CL]** for this version) |