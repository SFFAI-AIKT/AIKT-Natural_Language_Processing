# Daily arXiv: Machine Translation - Aug., 2019

### Index

- [2019-08-02](#2019-08-02)
  - [1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](#2019-08-02-1)
  - [2. Learning Joint Acoustic-Phonetic Word Embeddings](#2019-08-02-2)
  - [3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](#2019-08-02-3)

* [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
* [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2019-08-02

[Return to Index](#Index)

<h2 id="2019-08-02-1">1. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data</h2> 
Title: [Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data](https://arxiv.org/abs/1908.00449)

Authors: [Jacob Harer](https://arxiv.org/search/cs?searchtype=author&query=Harer%2C+J), [Chris Reale](https://arxiv.org/search/cs?searchtype=author&query=Reale%2C+C), [Peter Chin](https://arxiv.org/search/cs?searchtype=author&query=Chin%2C+P)

*(Submitted on 1 Aug 2019)*

> Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of 25% F0.5 over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a 10% improvement in recall on the CoNLL 2014 benchmark and the highest to date F0.5 score on the AESW benchmark of 50.43.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1908.00449 [cs.LG]**                                 |
|           | (or **arXiv:1908.00449v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-2">2. Learning Joint Acoustic-Phonetic Word Embeddings</h2> 
Title: [Learning Joint Acoustic-Phonetic Word Embeddings](https://arxiv.org/abs/1908.00493)

Authors: [Mohamed El-Geish](https://arxiv.org/search/cs?searchtype=author&query=El-Geish%2C+M)

*(Submitted on 1 Aug 2019)*

> Most speech recognition tasks pertain to mapping words across two modalities: acoustic and orthographic. In this work, we suggest learning encoders that map variable-length, acoustic or phonetic, sequences that represent words into fixed-dimensional vectors in a shared latent space; such that the distance between two word vectors represents how closely the two words sound. Instead of directly learning the distances between word vectors, we employ weak supervision and model a binary classification task to predict whether two inputs, one of each modality, represent the same word given a distance threshold. We explore various deep-learning models, bimodal contrastive losses, and techniques for mining hard negative examples such as the semi-supervised technique of self-labeling. Our best model achieves an F1 score of 0.95 for the binary classification task.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1908.00493 [cs.LG]**                                 |
|           | (or **arXiv:1908.00493v1 [cs.LG]** for this version)         |



<h2 id="2019-08-02-3">3. JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English</h2> 
Title: [JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English](https://arxiv.org/abs/1908.00323)

Authors: [Sainik Kumar Mahata](https://arxiv.org/search/cs?searchtype=author&query=Mahata%2C+S+K), [Dipankar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Sivaji Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+S)

*(Submitted on 1 Aug 2019)*

> In the current work, we present a description of the system submitted to WMT 2018 News Translation Shared task. The system was created to translate news text from Finnish to English. The system used a Character Based Neural Machine Translation model to accomplish the given task. The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same. Our system garnered a BLEU score of 12.9.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1908.00323 [cs.CL]**                         |
|           | (or **arXiv:1908.00323v1 [cs.CL]** for this version) |