# Daily arXiv: Machine Translation - June, 2020

# Index


- [2020-06-02](#2020-06-02)

  - [1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](#2020-06-02-1)
  - [2. Dynamic Masking for Improved Stability in Spoken Language Translation](#2020-06-02-2)
  - [3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](#2020-06-02-3)
  - [4. Neural Unsupervised Domain Adaptation in NLP---A Survey](#2020-06-02-4)
  - [5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](#2020-06-02-5)
  - [6. Attention Word Embedding](#2020-06-02-6)
  - [7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](#2020-06-02-7)
  - [8. Cascaded Text Generation with Markov Transformers](#2020-06-02-8)
- [2020-06-01](#2020-06-01)

  - [1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP](#2020-06-01-1)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)





# 2020-06-02

[Return to Index](#Index)



<h2 id="2020-06-02-1">1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models</h2>

Title: [A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](https://arxiv.org/abs/2006.00031)

Authors: [Nikolay Arefyev](https://arxiv.org/search/cs?searchtype=author&query=Arefyev%2C+N), [Boris Sheludko](https://arxiv.org/search/cs?searchtype=author&query=Sheludko%2C+B), [Alexander Podolskiy](https://arxiv.org/search/cs?searchtype=author&query=Podolskiy%2C+A), [Alexander Panchenko](https://arxiv.org/search/cs?searchtype=author&query=Panchenko%2C+A)

> Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00031](https://arxiv.org/abs/2006.00031) [cs.CL]** |
|           | (or **[arXiv:2006.00031v1](https://arxiv.org/abs/2006.00031v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-2">2. Dynamic Masking for Improved Stability in Spoken Language Translation</h2>

Title: [Dynamic Masking for Improved Stability in Spoken Language Translation](https://arxiv.org/abs/2006.00249)

Authors: [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> For spoken language translation (SLT) in live scenarios such as conferences, lectures and meetings, it is desirable to show the translation to the user as quickly as possible, avoiding an annoying lag between speaker and translated captions. In other words, we would like low-latency, online SLT. If we assume a pipeline of automatic speech recognition (ASR) and machine translation (MT) then a viable approach to online SLT is to pair an online ASR system, with a a retranslation strategy, where the MT system re-translates every update received from ASR. However this can result in annoying "flicker" as the MT system updates its translation. A possible solution is to add a fixed delay, or "mask" to the the output of the MT system, but a fixed global mask introduces undesirable latency to the output. We show how this mask can be set dynamically, improving the latency-flicker trade-off without sacrificing translation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00249](https://arxiv.org/abs/2006.00249) [cs.CL]** |
|           | (or **[arXiv:2006.00249v1](https://arxiv.org/abs/2006.00249v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-3">3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation</h2>

Title: [Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](https://arxiv.org/abs/2006.00262)

Authors: [Sosuke Nishikawa](https://arxiv.org/search/cs?searchtype=author&query=Nishikawa%2C+S), [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Unsupervised bilingual word embedding (BWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method assumes that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we propose using a pseudo-parallel corpus generated by an unsupervised machine translation model to facilitate structural similarity of the two embedding spaces and improve the quality of BWEs in the mapping method. We show that our approach substantially outperforms baselines and other alternative approaches given the same amount of data, and, through detailed analysis, we argue that data augmentation with the pseudo data from unsupervised machine translation is especially effective for BWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data reflects some nature of the original language that helps learning similar embedding spaces between the source and target languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00262](https://arxiv.org/abs/2006.00262) [cs.CL]** |
|           | (or **[arXiv:2006.00262v1](https://arxiv.org/abs/2006.00262v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-4">4. Neural Unsupervised Domain Adaptation in NLP---A Survey</h2>

Title: [Neural Unsupervised Domain Adaptation in NLP---A Survey](https://arxiv.org/abs/2006.00632)

Authors: [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00632](https://arxiv.org/abs/2006.00632) [cs.CL]** |
|           | (or **[arXiv:2006.00632v1](https://arxiv.org/abs/2006.00632v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-5">5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English</h2>

Title: [Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](https://arxiv.org/abs/2006.00814)

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Michael Ustaszewski](https://arxiv.org/search/cs?searchtype=author&query=Ustaszewski%2C+M), [Emmanuelle Esperança-Rodier](https://arxiv.org/search/cs?searchtype=author&query=Esperança-Rodier%2C+E), [Francis Brunet Manquat](https://arxiv.org/search/cs?searchtype=author&query=Manquat%2C+F+B), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> We conduct in this work an evaluation study comparing offline and online neural machine translation architectures. Two sequence-to-sequence models: convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based Transformer (Vaswani et al. 2017) are considered. We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English-German and German-English language pairs, the latter being particularly sensitive to latency constraints. The evaluation results allow us to identify the strengths and shortcomings of each model when we shift to the online setup.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00814](https://arxiv.org/abs/2006.00814) [cs.CL]** |
|           | (or **[arXiv:2006.00814v1](https://arxiv.org/abs/2006.00814v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-6">6. Attention Word Embedding</h2>

Title: [Attention Word Embedding](https://arxiv.org/abs/2006.00988)

Authors: [Shashank Sonkar](https://arxiv.org/search/cs?searchtype=author&query=Sonkar%2C+S), [Andrew E. Waters](https://arxiv.org/search/cs?searchtype=author&query=Waters%2C+A+E), [Richard G. Baraniuk](https://arxiv.org/search/cs?searchtype=author&query=Baraniuk%2C+R+G)

> Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00988](https://arxiv.org/abs/2006.00988) [cs.CL]** |
|           | (or **[arXiv:2006.00988v1](https://arxiv.org/abs/2006.00988v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-7">7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</h2>

Title: [Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](https://arxiv.org/abs/2006.01080)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.

| Comments: | Accepted at IWSLT 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01080](https://arxiv.org/abs/2006.01080) [cs.CL]** |
|           | (or **[arXiv:2006.01080v1](https://arxiv.org/abs/2006.01080v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-8">8. Cascaded Text Generation with Markov Transformers</h2>

Title: [Cascaded Text Generation with Markov Transformers](https://arxiv.org/abs/2006.01112)

Authors: [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.01112](https://arxiv.org/abs/2006.01112) [cs.CL]** |
|           | (or **[arXiv:2006.01112v1](https://arxiv.org/abs/2006.01112v1) [cs.CL]** for this version) |







# 2020-06-01

[Return to Index](#Index)



<h2 id="2020-06-01-1">1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP</h2>

Title: [Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP]()

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy use of fine-tuning BERT-like models in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of NLP tasks in a uniform toolkit, from text classification to sequence labeling and dependency parsing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.14672](https://arxiv.org/abs/2005.14672) [cs.CL]** |
|           | (or **[arXiv:2005.14672v1](https://arxiv.org/abs/2005.14672v1) [cs.CL]** for this version) |