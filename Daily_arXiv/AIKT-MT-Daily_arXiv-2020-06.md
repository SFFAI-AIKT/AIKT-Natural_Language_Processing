# Daily arXiv: Machine Translation - June, 2020

# Index


- [2020-06-17](#2020-06-17)

  - [1. AVLnet: Learning Audio-Visual Language Representations from Instructional Videos](#2020-06-17-1)
  - [2. Scalable Cross Lingual Pivots to Model Pronoun Gender for Translation](#2020-06-17-2)
  - [3. PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models](#2020-06-17-3)
  - [4. FFR v1.1: Fon-French Neural Machine Translation](#2020-06-17-4)
- [2020-06-16](#2020-06-16)

  - [1. UWSpeech: Speech to Speech Translation for Unwritten Languages](#2020-06-16-1)
  - [2. Regularized Forward-Backward Decoder for Attention Models](#2020-06-16-2)
  - [3. FinEst BERT and CroSloEngual BERT: less is more in multilingual models](#2020-06-16-3)
  - [4. Fine-grained Human Evaluation of Transformer and Recurrent Approaches to Neural Machine Translation for English-to-Chinese](#2020-06-16-4)
  - [5. An Augmented Translation Technique for low Resource language pair: Sanskrit to Hindi translation](#2020-06-16-5)
  - [6. Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers](#2020-06-16-6)
- [2020-06-15](#2020-06-15)

  - [1. NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing](#2020-06-15-1)
  - [2. Sparse and Continuous Attention Mechanisms](#2020-06-15-2)
  - [3. Low-resource Languages: A Review of Past Work and Future Challenges](#2020-06-15-3)
- [2020-06-12](#2020-06-12)

  - [1. Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](#2020-06-12-1)
  - [2. CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP](#2020-06-12-2)
- [2020-06-11](#2020-06-11)

  - [1. Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation](#2020-06-11-1)
  - [2. Universal Vector Neural Machine Translation With Effective Attention](#2020-06-11-2)
  - [3. HausaMT v1.0: Towards English-Hausa Neural Machine Translation](#2020-06-11-3)
  - [4. Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation](#2020-06-11-4)
  - [5. Unsupervised Paraphrase Generation using Pre-trained Language Models](#2020-06-11-5)
  - [6. Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors](#2020-06-11-6)
  - [7. Position Masking for Language Models](#2020-06-11-7)
  - [8. Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus](#2020-06-11-8)
  - [9. Revisiting Few-sample BERT Fine-tuning](#2020-06-11-9)
- [2020-06-09](#2020-06-09)
- [1. Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation](#2020-06-09-1)
  - [2. Modeling Discourse Structure for Document-level Neural Machine Translation](#2020-06-09-2)
  - [3. What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT](#2020-06-09-3)
  - [4. Filtered Inner Product Projection for Multilingual Embedding Alignment](#2020-06-09-4)
  - [5. DeBERTa: Decoding-enhanced BERT with Disentangled Attention](#2020-06-09-5)
- [2020-06-08](#2020-06-08)

  - [1. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](#2020-06-08-1)
  - [2. GMAT: Global Memory Augmentation for Transformers](#2020-06-08-2)
  - [3. ELITR Non-Native Speech Translation at IWSLT 2020](#2020-06-08-3)
  - [4. Unsupervised Translation of Programming Languages](#2020-06-08-4)
- [2020-06-05](#2020-06-05)
- [1. CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning](#2020-06-05-1)
  - [2. Self-Training for End-to-End Speech Translation](#2020-06-05-2)
  - [3. M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training](#2020-06-05-3)
  - [4. Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation](#2020-06-05-4)
  - [5. Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1](#2020-06-05-5)
  - [6. End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020](#2020-06-05-6)
- [2020-06-04](#2020-06-04)

  - [1. The Typology of Polysemy: A Multilingual Distributional Framework](#2020-06-01-1)
  - [2. Norm-Based Curriculum Learning for Neural Machine Translation](#2020-06-01-2)
  - [3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](#2020-06-01-3)
  - [4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer](#2020-06-01-4)
- [2020-06-03](#2020-06-03)

  - [1. WikiBERT models: deep transfer learning for many languages](#2020-06-03-1)
  - [2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](#2020-06-03-2)
- [2020-06-02](#2020-06-02)
- [1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](#2020-06-02-1)
  - [2. Dynamic Masking for Improved Stability in Spoken Language Translation](#2020-06-02-2)
  - [3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](#2020-06-02-3)
  - [4. Neural Unsupervised Domain Adaptation in NLP---A Survey](#2020-06-02-4)
  - [5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](#2020-06-02-5)
  - [6. Attention Word Embedding](#2020-06-02-6)
  - [7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](#2020-06-02-7)
  - [8. Cascaded Text Generation with Markov Transformers](#2020-06-02-8)
- [2020-06-01](#2020-06-01)

  - [1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP](#2020-06-01-1)
- [2020-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-05.md)
- [2020-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-04.md)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-06-17

[Return to Index](#Index)



<h2 id="2020-06-17-1">1. AVLnet: Learning Audio-Visual Language Representations from Instructional Videos</h2>

Title: [AVLnet: Learning Audio-Visual Language Representations from Instructional Videos](https://arxiv.org/abs/2006.09199)

Authors: [Andrew Rouditchenko](https://arxiv.org/search/cs?searchtype=author&query=Rouditchenko%2C+A), [Angie Boggust](https://arxiv.org/search/cs?searchtype=author&query=Boggust%2C+A), [David Harwath](https://arxiv.org/search/cs?searchtype=author&query=Harwath%2C+D), [Dhiraj Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+D), [Samuel Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+S), [Kartik Audhkhasi](https://arxiv.org/search/cs?searchtype=author&query=Audhkhasi%2C+K), [Rogerio Feris](https://arxiv.org/search/cs?searchtype=author&query=Feris%2C+R), [Brian Kingsbury](https://arxiv.org/search/cs?searchtype=author&query=Kingsbury%2C+B), [Michael Picheny](https://arxiv.org/search/cs?searchtype=author&query=Picheny%2C+M), [Antonio Torralba](https://arxiv.org/search/cs?searchtype=author&query=Torralba%2C+A), [James Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J)

> Current methods for learning visually grounded language from videos often rely on time-consuming and expensive data collection, such as human annotated textual summaries or machine generated automatic speech recognition transcripts. In this work, we introduce Audio-Video Language Network (AVLnet), a self-supervised network that learns a shared audio-visual embedding space directly from raw video inputs. We circumvent the need for annotation and instead learn audio-visual language representations directly from randomly segmented video clips and their raw audio waveforms. We train AVLnet on publicly available instructional videos and evaluate our model on video clip and language retrieval tasks on three video datasets. Our proposed model outperforms several state-of-the-art text-video baselines by up to 11.8% in a video clip retrieval task, despite operating on the raw audio instead of manually annotated text captions. Further, we show AVLnet is capable of integrating textual information, increasing its modularity and improving performance by up to 20.3% on the video clip retrieval task. Finally, we perform analysis of AVLnet's learned representations, showing our model has learned to relate visual objects with salient words and natural sounds.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.09199](https://arxiv.org/abs/2006.09199) [cs.CV]** |
|           | (or **[arXiv:2006.09199v1](https://arxiv.org/abs/2006.09199v1) [cs.CV]** for this version) |





<h2 id="2020-06-17-2">2. Scalable Cross Lingual Pivots to Model Pronoun Gender for Translation</h2>

Title: [Scalable Cross Lingual Pivots to Model Pronoun Gender for Translation](https://arxiv.org/abs/2006.08881)

Authors: [Kellie Webster](https://arxiv.org/search/cs?searchtype=author&query=Webster%2C+K), [Emily Pitler](https://arxiv.org/search/cs?searchtype=author&query=Pitler%2C+E)

> Machine translation systems with inadequate document understanding can make errors when translating dropped or neutral pronouns into languages with gendered pronouns (e.g., English). Predicting the underlying gender of these pronouns is difficult since it is not marked textually and must instead be inferred from coreferent mentions in the context. We propose a novel cross-lingual pivoting technique for automatically producing high-quality gender labels, and show that this data can be used to fine-tune a BERT classifier with 92% F1 for Spanish dropped feminine pronouns, compared with 30-51% for neural machine translation models and 54-71% for a non-fine-tuned BERT model. We augment a neural machine translation model with labels from our classifier to improve pronoun translation, while still having parallelizable translation models that translate a sentence at a time.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.08881](https://arxiv.org/abs/2006.08881) [cs.CL]** |
|           | (or **[arXiv:2006.08881v1](https://arxiv.org/abs/2006.08881v1) [cs.CL]** for this version) |





<h2 id="2020-06-17-3">3. PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models</h2>

Title: [PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models](https://arxiv.org/abs/2006.09075)

Authors: [Eyal Ben-David](https://arxiv.org/search/cs?searchtype=author&query=Ben-David%2C+E), [Carmel Rabinovitz](https://arxiv.org/search/cs?searchtype=author&query=Rabinovitz%2C+C), [Roi Reichart](https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R)

> Pivot-based neural representation models have lead to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves in-domain model performance, yields effective reduced-size models and increases model stability.

| Comments: | Accepted to TACL in June 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.09075](https://arxiv.org/abs/2006.09075) [cs.CL]** |
|           | (or **[arXiv:2006.09075v1](https://arxiv.org/abs/2006.09075v1) [cs.CL]** for this version) |





<h2 id="2020-06-17-4">4. FFR v1.1: Fon-French Neural Machine Translation</h2>

Title: [FFR v1.1: Fon-French Neural Machine Translation](https://arxiv.org/abs/2006.09217)

Authors: [Bonaventure F. P. Dossou](https://arxiv.org/search/cs?searchtype=author&query=Dossou%2C+B+F+P), [Chris C. Emezue](https://arxiv.org/search/cs?searchtype=author&query=Emezue%2C+C+C)

> All over the world and especially in Africa, researchers are putting efforts into building Neural Machine Translation (NMT) systems to help tackle the language barriers in Africa, a continent of over 2000 different languages. However, the low-resourceness, diacritical, and tonal complexities of African languages are major issues being faced. The FFR project is a major step towards creating a robust translation model from Fon, a very low-resource and tonal language, to French, for research and public use. In this paper, we introduce FFR Dataset, a corpus of Fon-to-French translations, describe the diacritical encoding process, and introduce our FFR v1.1 model, trained on the dataset. The dataset and model are made publicly available at [this https URL](https://github.com/) bonaventuredossou/ffr-v1, to promote collaboration and reproducibility.

| Comments: | Accepted for publication at the Widening Natural Language Processing (WiNLP) Workshop, The 58th Annual Meeting of the Association for Computational Linguistics, 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.09217](https://arxiv.org/abs/2006.09217) [cs.CL]** |
|           | (or **[arXiv:2006.09217v1](https://arxiv.org/abs/2006.09217v1) [cs.CL]** for this version) |





# 2020-06-16

[Return to Index](#Index)



<h2 id="2020-06-16-1">1. UWSpeech: Speech to Speech Translation for Unwritten Languages</h2>

Title: [UWSpeech: Speech to Speech Translation for Unwritten Languages](https://arxiv.org/abs/2006.07926)

Authors: [Chen Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+C), [Xu Tan](https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+X), [Yi Ren](https://arxiv.org/search/eess?searchtype=author&query=Ren%2C+Y), [Tao Qin](https://arxiv.org/search/eess?searchtype=author&query=Qin%2C+T), [Kejun Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+K), [Tie-Yan Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+T)

> Existing speech to speech translation systems heavily rely on the text of target language: they usually translate source language either to target text and then synthesize target speech from text, or directly to target speech with target text for auxiliary training. However, those methods cannot be applied to unwritten target languages, which have no written text or phoneme available. In this paper, we develop a translation system for unwritten languages, named as UWSpeech, which converts target unwritten speech into discrete tokens with a converter, and then translates source-language speech into target discrete tokens with a translator, and finally synthesizes target speech from target discrete tokens with an inverter. We propose a method called XL-VAE, which enhances vector quantized variational autoencoder (VQ-VAE) with cross-lingual (XL) speech recognition, to train the converter and inverter of UWSpeech jointly. Experiments on Fisher Spanish-English conversation translation dataset show that UWSpeech outperforms direct translation and VQ-VAE baseline by about 16 and 10 BLEU points respectively, which demonstrate the advantages and potentials of UWSpeech.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.07926](https://arxiv.org/abs/2006.07926) [eess.AS]** |
|           | (or **[arXiv:2006.07926v1](https://arxiv.org/abs/2006.07926v1) [eess.AS]** for this version) |





<h2 id="2020-06-16-2">2. Regularized Forward-Backward Decoder for Attention Models</h2>

Title: [Regularized Forward-Backward Decoder for Attention Models](https://arxiv.org/abs/2006.08506)

Authors: [Tobias Watzel](https://arxiv.org/search/eess?searchtype=author&query=Watzel%2C+T), [Ludwig Kürzinger](https://arxiv.org/search/eess?searchtype=author&query=Kürzinger%2C+L), [Lujun Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+L), [Gerhard Rigoll](https://arxiv.org/search/eess?searchtype=author&query=Rigoll%2C+G)

> Nowadays, attention models are one of the popular candidates for speech recognition. So far, many studies mainly focus on the encoder structure or the attention module to enhance the performance of these models. However, mostly ignore the decoder. In this paper, we propose a novel regularization technique incorporating a second decoder during the training phase. This decoder is optimized on time-reversed target labels beforehand and supports the standard decoder during training by adding knowledge from future context. Since it is only added during training, we are not changing the basic structure of the network or adding complexity during decoding. We evaluate our approach on the smaller TEDLIUMv2 and the larger LibriSpeech dataset, achieving consistent improvements on both of them.

| Comments: | Under review for Interspeech 2020                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2006.08506](https://arxiv.org/abs/2006.08506) [eess.AS]** |
|           | (or **[arXiv:2006.08506v1](https://arxiv.org/abs/2006.08506v1) [eess.AS]** for this version) |





<h2 id="2020-06-16-3">3. FinEst BERT and CroSloEngual BERT: less is more in multilingual models</h2>

Title: [FinEst BERT and CroSloEngual BERT: less is more in multilingual models](https://arxiv.org/abs/2006.07890)

Authors: [Matej Ulčar](https://arxiv.org/search/cs?searchtype=author&query=Ulčar%2C+M), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> Large pretrained masked language models have become state-of-the-art solutions for many NLP problems. The research has been mostly focused on English language, though. While massively multilingual models exist, studies have shown that monolingual models produce much better results. We train two trilingual BERT-like models, one for Finnish, Estonian, and English, the other for Croatian, Slovenian, and English. We evaluate their performance on several downstream tasks, NER, POS-tagging, and dependency parsing, using the multilingual BERT and XLM-R as baselines. The newly created FinEst BERT and CroSloEngual BERT improve the results on all tasks in most monolingual and cross-lingual situations

| Comments: | 10 pages, accepted at TSD 2020 conference                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.07890](https://arxiv.org/abs/2006.07890) [cs.CL]** |
|           | (or **[arXiv:2006.07890v1](https://arxiv.org/abs/2006.07890v1) [cs.CL]** for this version) |





<h2 id="2020-06-16-4">4. Fine-grained Human Evaluation of Transformer and Recurrent Approaches to Neural Machine Translation for English-to-Chinese</h2>

Title: [Fine-grained Human Evaluation of Transformer and Recurrent Approaches to Neural Machine Translation for English-to-Chinese](https://arxiv.org/abs/2006.08297)

Authors: [Yuying Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Y), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

> This research presents a fine-grained human evaluation to compare the Transformer and recurrent approaches to neural machine translation (MT), on the translation direction English-to-Chinese. To this end, we develop an error taxonomy compliant with the Multidimensional Quality Metrics (MQM) framework that is customised to the relevant phenomena of this translation direction. We then conduct an error annotation using this customised error taxonomy on the output of state-of-the-art recurrent- and Transformer-based MT systems on a subset of WMT2019's news test set. The resulting annotation shows that, compared to the best recurrent system, the best Transformer system results in a 31% reduction of the total number of errors and it produced significantly less errors in 10 out of 22 error categories. We also note that two of the systems evaluated do not produce any error for a category that was relevant for this translation direction prior to the advent of NMT systems: Chinese classifiers.

| Comments: | Accepted at the 22nd Annual Conference of the European Association for Machine Translation (EAMT 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.08297](https://arxiv.org/abs/2006.08297) [cs.CL]** |
|           | (or **[arXiv:2006.08297v1](https://arxiv.org/abs/2006.08297v1) [cs.CL]** for this version) |





<h2 id="2020-06-16-5">5. 
An Augmented Translation Technique for low Resource language pair: Sanskrit to Hindi translation</h2>

Title: [An Augmented Translation Technique for low Resource language pair: Sanskrit to Hindi translation](https://arxiv.org/abs/2006.08332)

Authors: [Rashi Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+R), [Piyush Jha](https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+P), [Vineet Sahula](https://arxiv.org/search/cs?searchtype=author&query=Sahula%2C+V)

> Neural Machine Translation (NMT) is an ongoing technique for Machine Translation (MT) using enormous artificial neural network. It has exhibited promising outcomes and has shown incredible potential in solving challenging machine translation exercises. One such exercise is the best approach to furnish great MT to language sets with a little preparing information. In this work, Zero Shot Translation (ZST) is inspected for a low resource language pair. By working on high resource language pairs for which benchmarks are available, namely Spanish to Portuguese, and training on data sets (Spanish-English and English-Portuguese) we prepare a state of proof for ZST system that gives appropriate results on the available data. Subsequently the same architecture is tested for Sanskrit to Hindi translation for which data is sparse, by training the model on English-Hindi and Sanskrit-English language pairs. In order to prepare and decipher with ZST system, we broaden the preparation and interpretation pipelines of NMT seq2seq model in tensorflow, incorporating ZST features. Dimensionality reduction of word embedding is performed to reduce the memory usage for data storage and to achieve a faster training and translation cycles. In this work existing helpful technology has been utilized in an imaginative manner to execute our NLP issue of Sanskrit to Hindi translation. A Sanskrit-Hindi parallel corpus of 300 is constructed for testing. The data required for the construction of parallel corpus has been taken from the telecasted news, published on Department of Public Information, state government of Madhya Pradesh, India website.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence, December 2019, pp 377 to 383 |
| DOI:               | [10.1145/3377713.3377774](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3377713.3377774&v=b5e67b3e) |
| Cite as:           | **[arXiv:2006.08332](https://arxiv.org/abs/2006.08332) [cs.CL]** |
|                    | (or **[arXiv:2006.08332v1](https://arxiv.org/abs/2006.08332v1) [cs.CL]** for this version) |





<h2 id="2020-06-16-6">6. Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers</h2>

Title: [Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers](https://arxiv.org/abs/2006.08344)

Authors: [Tim Z. Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T+Z), [Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&query=Gomez%2C+A+N), [Yarin Gal](https://arxiv.org/search/cs?searchtype=author&query=Gal%2C+Y)

> We detect out-of-training-distribution sentences in Neural Machine Translation using the Bayesian Deep Learning equivalent of Transformer models. For this we develop a new measure of uncertainty designed specifically for long sequences of discrete random variables -- i.e. words in the output sentence. Our new measure of uncertainty solves a major intractability in the naive application of existing approaches on long sentences. We use our new measure on a Transformer model trained with dropout approximate inference. On the task of German-English translation using WMT13 and Europarl, we show that with dropout uncertainty our measure is able to identify when Dutch source sentences, sentences which use the same word types as German, are given to the model instead of German.

| Comments: | 19 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2006.08344](https://arxiv.org/abs/2006.08344) [cs.CL]** |
|           | (or **[arXiv:2006.08344v1](https://arxiv.org/abs/2006.08344v1) [cs.CL]** for this version) |





# 2020-06-15

[Return to Index](#Index)



<h2 id="2020-06-15-1">1. NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing</h2>

Title: [NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing](https://arxiv.org/abs/2006.07116)

Authors: [Nikita Klyuchnikov](https://arxiv.org/search/cs?searchtype=author&query=Klyuchnikov%2C+N), [Ilya Trofimov](https://arxiv.org/search/cs?searchtype=author&query=Trofimov%2C+I), [Ekaterina Artemova](https://arxiv.org/search/cs?searchtype=author&query=Artemova%2C+E), [Mikhail Salnikov](https://arxiv.org/search/cs?searchtype=author&query=Salnikov%2C+M), [Maxim Fedorov](https://arxiv.org/search/cs?searchtype=author&query=Fedorov%2C+M), [Evgeny Burnaev](https://arxiv.org/search/cs?searchtype=author&query=Burnaev%2C+E)

> Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure more reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We believe that our results have high potential of usage for both NAS and NLP communities.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.07116](https://arxiv.org/abs/2006.07116) [cs.LG]** |
|           | (or **[arXiv:2006.07116v1](https://arxiv.org/abs/2006.07116v1) [cs.LG]** for this version) |





<h2 id="2020-06-15-2">2. Sparse and Continuous Attention Mechanisms</h2>

Title: [Sparse and Continuous Attention Mechanisms](https://arxiv.org/abs/2006.07214)

Authors: [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T), [Marcos Treviso](https://arxiv.org/search/cs?searchtype=author&query=Treviso%2C+M), [António Farinhas](https://arxiv.org/search/cs?searchtype=author&query=Farinhas%2C+A), [Vlad Niculae](https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V), [Mário A. T. Figueiredo](https://arxiv.org/search/cs?searchtype=author&query=Figueiredo%2C+M+A+T), [Pedro M. Q. Aguiar](https://arxiv.org/search/cs?searchtype=author&query=Aguiar%2C+P+M+Q)

> Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.07214](https://arxiv.org/abs/2006.07214) [cs.LG]** |
|           | (or **[arXiv:2006.07214v1](https://arxiv.org/abs/2006.07214v1) [cs.LG]** for this version) |





<h2 id="2020-06-15-3">3. Low-resource Languages: A Review of Past Work and Future Challenges</h2>

Title: [Low-resource Languages: A Review of Past Work and Future Challenges]()

Authors: [Alexandre Magueresse](https://arxiv.org/search/cs?searchtype=author&query=Magueresse%2C+A), [Vincent Carles](https://arxiv.org/search/cs?searchtype=author&query=Carles%2C+V), [Evan Heetderks](https://arxiv.org/search/cs?searchtype=author&query=Heetderks%2C+E)

> A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2006.07264](https://arxiv.org/abs/2006.07264) [cs.CL]** |
|              | (or **[arXiv:2006.07264v1](https://arxiv.org/abs/2006.07264v1) [cs.CL]** for this version) |







# 2020-06-12

[Return to Index](#Index)



<h2 id="2020-06-12-1">1. Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics</h2>

Title: [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264)

Authors: [Nitika Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+N), [Timothy Baldwin](https://arxiv.org/search/cs?searchtype=author&query=Baldwin%2C+T), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T)

> Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.06264](https://arxiv.org/abs/2006.06264) [cs.CL]** |
|           | (or **[arXiv:2006.06264v2](https://arxiv.org/abs/2006.06264v2) [cs.CL]** for this version) |





<h2 id="2020-06-12-2">2. CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP</h2>

Title: [CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP](https://arxiv.org/abs/2006.06402)

Authors: [Libo Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+L), [Minheng Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+M), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W)

> Multi-lingual contextualized embeddings, such as multilingual-BERT (mBERT), have shown success in a variety of zero-shot cross-lingual tasks. However, these models are limited by having inconsistent contextualized representations of subwords across different languages. Existing work addresses this issue by bilingual projection and fine-tuning technique. We propose a data augmentation framework to generate multi-lingual code-switching data to fine-tune mBERT, which encourages model to align representations from source and multiple target languages once by mixing their context information. Compared with the existing work, our method does not rely on bilingual sentences for training, and requires only one training process for multiple target languages. Experimental results on five tasks with 19 languages show that our method leads to significantly improved performances for all the tasks compared with mBERT.

| Comments: | Accepted at IJCAI2020. SOLE copyright holder is IJCAI (international Joint Conferences on Artificial Intelligence), all rights reserved. [this http URL](http://static.ijcai.org/2020-accepted_papers.html) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.06402](https://arxiv.org/abs/2006.06402) [cs.CL]** |
|           | (or **[arXiv:2006.06402v1](https://arxiv.org/abs/2006.06402v1) [cs.CL]** for this version) |







# 2020-06-11

[Return to Index](#Index)



<h2 id="2020-06-11-1">1. Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation</h2>

Title: [Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation](https://arxiv.org/abs/2006.05474)

Authors: [Changhan Wang](https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+C), [Juan Pino](https://arxiv.org/search/eess?searchtype=author&query=Pino%2C+J), [Jiatao Gu](https://arxiv.org/search/eess?searchtype=author&query=Gu%2C+J)

> Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end automatic speech recognition (ASR) for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling (decoder) for the same language, which is likely to be inefficient for distant target languages. We introduce speech-to-text translation (ST) as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Specifically, we first translate high-resource ASR transcripts into a target low-resource language, with which a ST model is trained. Both ST and target ASR share the same attention-based encoder-decoder architecture and vocabulary. The former task then provides a fully pre-trained model for the latter, bringing up to 24.6% word error rate (WER) reduction to the baseline (direct transfer from high-resource ASR). We show that training ST with human translations is not necessary. ST trained with machine translation (MT) pseudo-labels brings consistent gains. It can even outperform those using human labels when transferred to target ASR by leveraging only 500K MT examples. Even with pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer brings up to 8.9% WER reduction to direct transfer.

| Comments: | Submitted to INTERSPEECH 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2006.05474](https://arxiv.org/abs/2006.05474) [eess.AS]** |
|           | (or **[arXiv:2006.05474v1](https://arxiv.org/abs/2006.05474v1) [eess.AS]** for this version) |





<h2 id="2020-06-11-2">2. Universal Vector Neural Machine Translation With Effective Attention</h2>

Title: [Universal Vector Neural Machine Translation With Effective Attention](https://arxiv.org/abs/2006.05003)

Authors: [Satish Mylapore](https://arxiv.org/search/cs?searchtype=author&query=Mylapore%2C+S), [Ryan Quincy Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+R+Q), [Joshua Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+J), [Robert D. Slater](https://arxiv.org/search/cs?searchtype=author&query=Slater%2C+R+D)

> Neural Machine Translation (NMT) leverages one or more trained neural networks for the translation of phrases. Sutskever introduced a sequence to sequence based encoder-decoder model which became the standard for NMT based systems. Attention mechanisms were later introduced to address the issues with the translation of long sentences and improving overall accuracy. In this paper, we propose a singular model for Neural Machine Translation based on encoder-decoder models. Most translation models are trained as one model for one translation. We introduce a neutral/universal model representation that can be used to predict more than one language depending on the source and a provided target. Secondly, we introduce an attention model by adding an overall learning vector to the multiplicative model. With these two changes, by using the novel universal model the number of models needed for multiple language translation applications are reduced.

| Comments:          | 15pages, 3 figures                                           |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | SMU Data Science Review: Vol. 3 : No. 1 , Article 10. Available at: https://scholar.smu.edu/datasciencereview/vol3/iss1/10 Year March 2020 |
| Cite as:           | **[arXiv:2006.05003](https://arxiv.org/abs/2006.05003) [cs.CL]** |
|                    | (or **[arXiv:2006.05003v1](https://arxiv.org/abs/2006.05003v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-3">3. HausaMT v1.0: Towards English-Hausa Neural Machine Translation</h2>

Title: [HausaMT v1.0: Towards English-Hausa Neural Machine Translation](https://arxiv.org/abs/2006.05014)

Authors: [Adewale Akinfaderin](https://arxiv.org/search/cs?searchtype=author&query=Akinfaderin%2C+A)

> Neural Machine Translation (NMT) for low-resource languages suffers from low performance because of the lack of large amounts of parallel data and language diversity. To contribute to ameliorating this problem, we built a baseline model for English-Hausa machine translation, which is considered a task for low-resource language. The Hausa language is the second largest Afro-Asiatic language in the world after Arabic and it is the third largest language for trading across a larger swath of West Africa countries, after English and French. In this paper, we curated different datasets containing Hausa-English parallel corpus for our translation. We trained baseline models and evaluated the performance of our models using the Recurrent and Transformer encoder-decoder architecture with two tokenization approaches: standard word-level tokenization and Byte Pair Encoding (BPE) subword tokenization.

| Comments: | Accepted at 4th Widening NLP Workshop, Annual Meeting of the Association for Computational Linguistics, ACL 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.05014](https://arxiv.org/abs/2006.05014) [cs.CL]** |
|           | (or **[arXiv:2006.05014v1](https://arxiv.org/abs/2006.05014v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-4">4. Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</h2>

Title: [Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2006.05165)

Authors: [Qiu Ran](https://arxiv.org/search/cs?searchtype=author&query=Ran%2C+Q), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4× speedup while maintaining comparable performance compared with the corresponding autoregressive model.

| Comments: | This work has been accepted for publication at ACL2020       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.05165](https://arxiv.org/abs/2006.05165) [cs.CL]** |
|           | (or **[arXiv:2006.05165v1](https://arxiv.org/abs/2006.05165v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-5">5. Unsupervised Paraphrase Generation using Pre-trained Language Models</h2>

Title: [Unsupervised Paraphrase Generation using Pre-trained Language Models](https://arxiv.org/abs/2006.05477)

Authors: [Chaitra Hegde](https://arxiv.org/search/cs?searchtype=author&query=Hegde%2C+C), [Shrikumar Patil](https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+S)

> Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \cite{radford2019language} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.05477](https://arxiv.org/abs/2006.05477) [cs.CL]** |
|           | (or **[arXiv:2006.05477v1](https://arxiv.org/abs/2006.05477v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-6">6. Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors</h2>

Title: [Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors](https://arxiv.org/abs/2006.05635)

Authors: [Longshaokan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Maryam Fazel-Zarandi](https://arxiv.org/search/cs?searchtype=author&query=Fazel-Zarandi%2C+M), [Aditya Tiwari](https://arxiv.org/search/cs?searchtype=author&query=Tiwari%2C+A), [Spyros Matsoukas](https://arxiv.org/search/cs?searchtype=author&query=Matsoukas%2C+S), [Lazaros Polymenakos](https://arxiv.org/search/cs?searchtype=author&query=Polymenakos%2C+L)

> Speech-based virtual assistants, such as Amazon Alexa, Google assistant, and Apple Siri, typically convert users' audio signals to text data through automatic speech recognition (ASR) and feed the text to downstream dialog models for natural language understanding and response generation. The ASR output is error-prone; however, the downstream dialog models are often trained on error-free text data, making them sensitive to ASR errors during inference time. To bridge the gap and make dialog models more robust to ASR errors, we leverage an ASR error simulator to inject noise into the error-free text data, and subsequently train the dialog models with the augmented data. Compared to other approaches for handling ASR errors, such as using ASR lattice or end-to-end methods, our data augmentation approach does not require any modification to the ASR or downstream dialog models; our approach also does not introduce any additional latency during inference time. We perform extensive experiments on benchmark data and show that our approach improves the performance of downstream dialog models in the presence of ASR errors, and it is particularly effective in the low-resource situations where there are constraints on model size or the training data is scarce.

| Comments: | To be presented at 2nd Workshop on NLP for ConvAI, ACL 2020  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.05635](https://arxiv.org/abs/2006.05635) [cs.CL]** |
|           | (or **[arXiv:2006.05635v1](https://arxiv.org/abs/2006.05635v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-7">7. Position Masking for Language Models</h2>

Title: [Position Masking for Language Models](https://arxiv.org/abs/2006.05676)

Authors: [Andy Wagner](https://arxiv.org/search/cs?searchtype=author&query=Wagner%2C+A), [Tiyasa Mitra](https://arxiv.org/search/cs?searchtype=author&query=Mitra%2C+T), [Mrinal Iyer](https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+M), [Godfrey Da Costa](https://arxiv.org/search/cs?searchtype=author&query=Da+Costa%2C+G), [Marc Tremblay](https://arxiv.org/search/cs?searchtype=author&query=Tremblay%2C+M)

> Masked language modeling (MLM) pre-training models such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. This is an effective technique which has led to good results on all NLP benchmarks. We propose to expand upon this idea by masking the positions of some tokens along with the masked input token ids. We follow the same standard approach as BERT masking a percentage of the tokens positions and then predicting their original values using an additional fully connected classifier stage. This approach has shown good performance gains (.3\% improvement) for the SQUAD additional improvement in convergence times. For the Graphcore IPU the convergence of BERT Base with position masking requires only 50\% of the tokens from the original BERT paper.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.05676](https://arxiv.org/abs/2006.05676) [cs.CL]** |
|           | (or **[arXiv:2006.05676v1](https://arxiv.org/abs/2006.05676v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-8">8. Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus</h2>

Title: [Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus](https://arxiv.org/abs/2006.05754)

Authors: [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Roldano Cattoni](https://arxiv.org/search/cs?searchtype=author&query=Cattoni%2C+R), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).

| Comments: | 9 pages of content, accepted at ACL 2020                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.05754](https://arxiv.org/abs/2006.05754) [cs.CL]** |
|           | (or **[arXiv:2006.05754v1](https://arxiv.org/abs/2006.05754v1) [cs.CL]** for this version) |





<h2 id="2020-06-11-9">9. Revisiting Few-sample BERT Fine-tuning</h2>

Title: [Revisiting Few-sample BERT Fine-tuning](https://arxiv.org/abs/2006.05987)

Authors: [Tianyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Felix Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Arzoo Katiyar](https://arxiv.org/search/cs?searchtype=author&query=Katiyar%2C+A), [Kilian Q. Weinberger](https://arxiv.org/search/cs?searchtype=author&query=Weinberger%2C+K+Q), [Yoav Artzi](https://arxiv.org/search/cs?searchtype=author&query=Artzi%2C+Y)

> We study the problem of few-sample fine-tuning of BERT contextual representations, and identify three sub-optimal choices in current, broadly adopted practices. First, we observe that the omission of the gradient bias correction in the \bertadam optimizer results in fine-tuning instability. We also find that parts of the BERT network provide a detrimental starting point for fine-tuning, and simply re-initializing these layers speeds up learning and improves performance. Finally, we study the effect of training time, and observe that commonly used recipes often do not allocate sufficient time for training. In light of these findings, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe a decrease in their relative impact when modifying the fine-tuning process based on our findings.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.05987](https://arxiv.org/abs/2006.05987) [cs.CL]** |
|           | (or **[arXiv:2006.05987v1](https://arxiv.org/abs/2006.05987v1) [cs.CL]** for this version) |











# 2020-06-09

[Return to Index](#Index)



<h2 id="2020-06-09-1">1. Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation</h2>

Title: [Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation](https://arxiv.org/abs/2006.04050)

Authors: [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Hasan Cavusoglu](https://arxiv.org/search/cs?searchtype=author&query=Cavusoglu%2C+H)

> We describe our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE) (Mayhew et al., 2020). We view MT models at various training stages (i.e., checkpoints) as human learners at different levels. Hence, we employ an ensemble of multi-checkpoints from the same model to generate translation sequences with various levels of fluency. From each checkpoint, for our best model, we sample n-Best sequences (n=10) with a beam width =100. We achieve 37.57 macro F1 with a 6 checkpoint model ensemble on the official English to Portuguese shared task test data, outperforming a baseline Amazon translation system of 21.30 macro F1 and ultimately demonstrating the utility of our intuitive method.

| Comments: | Accepted to the 4th Workshop on Neural Generation and Translation (Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education Mayhew et al., 2020) collocated with ACL 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2006.04050](https://arxiv.org/abs/2006.04050) [cs.CL]** |
|           | (or **[arXiv:2006.04050v1](https://arxiv.org/abs/2006.04050v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-2">2. Modeling Discourse Structure for Document-level Neural Machine Translation</h2>

Title: [Modeling Discourse Structure for Document-level Neural Machine Translation](https://arxiv.org/abs/2006.04721)

Authors: [Junxuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Xiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Jiarui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Jianwei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+J), [Bin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.04721](https://arxiv.org/abs/2006.04721) [cs.CL]** |
|           | (or **[arXiv:2006.04721v1](https://arxiv.org/abs/2006.04721v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-3">3. What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT</h2>

Title: [What's the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific MT](https://arxiv.org/abs/2006.04781)

Authors: [Lukas Fischer](https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+L), [Samuel Läubli](https://arxiv.org/search/cs?searchtype=author&query=Läubli%2C+S)

> Machine translation (MT) has been shown to produce a number of errors that require human post-editing, but the extent to which professional human translation (HT) contains such errors has not yet been compared to MT. We compile pre-translated documents in which MT and HT are interleaved, and ask professional translators to flag errors and post-edit these documents in a blind evaluation. We find that the post-editing effort for MT segments is only higher in two out of three language pairs, and that the number of segments with wrong terminology, omissions, and typographical problems is similar in HT.

| Comments: | EAMT 2020 (Research Track)                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.04781](https://arxiv.org/abs/2006.04781) [cs.CL]** |
|           | (or **[arXiv:2006.04781v1](https://arxiv.org/abs/2006.04781v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-4">4. Filtered Inner Product Projection for Multilingual Embedding Alignment</h2>

Title: [Filtered Inner Product Projection for Multilingual Embedding Alignment](https://arxiv.org/abs/2006.03652)

Authors: [Vin Sachidananda](https://arxiv.org/search/cs?searchtype=author&query=Sachidananda%2C+V), [Ziyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C)

> Due to widespread interest in machine translation and transfer learning, there are numerous algorithms for mapping multiple embeddings to a shared representation space. Recently, these algorithms have been studied in the setting of bilingual dictionary induction where one seeks to align the embeddings of a source and a target language such that translated word pairs lie close to one another in a common representation space. In this paper, we propose a method, Filtered Inner Product Projection (FIPP), for mapping embeddings to a common representation space and evaluate FIPP in the context of bilingual dictionary induction. As semantic shifts are pervasive across languages and domains, FIPP first identifies the common geometric structure in both embeddings and then, only on the common structure, aligns the Gram matrices of these embeddings. Unlike previous approaches, FIPP is applicable even when the source and target embeddings are of differing dimensionalities. We show that our approach outperforms existing methods on the MUSE dataset for various language pairs. Furthermore, FIPP provides computational benefits both in ease of implementation and scalability.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03652](https://arxiv.org/abs/2006.03652) [cs.CL]** |
|           | (or **[arXiv:2006.03652v1](https://arxiv.org/abs/2006.03652v1) [cs.CL]** for this version) |





<h2 id="2020-06-09-5">5. DeBERTa: Decoding-enhanced BERT with Disentangled Attention</h2>

Title: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)

Authors: [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at [this https URL](https://github.com/microsoft/DeBERTa).

| Comments:    | 17 pages,4 figures, 8 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ACM classes: | I.2; I.7                                                     |
| Cite as:     | **[arXiv:2006.03654](https://arxiv.org/abs/2006.03654) [cs.CL]** |
|              | (or **[arXiv:2006.03654v1](https://arxiv.org/abs/2006.03654v1) [cs.CL]** for this version) |













# 2020-06-08

[Return to Index](#Index)



<h2 id="2020-06-08-1">1. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing</h2>

Title: [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)

Authors: [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [Guokun Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+G), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at [this https URL](https://github.com/laiguokun/Funnel-Transformer).

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03236](https://arxiv.org/abs/2006.03236) [cs.LG]** |
|           | (or **[arXiv:2006.03236v1](https://arxiv.org/abs/2006.03236v1) [cs.LG]** for this version) |





<h2 id="2020-06-08-2">2. GMAT: Global Memory Augmentation for Transformers</h2>

Title: [GMAT: Global Memory Augmentation for Transformers](https://arxiv.org/abs/2006.03274)

Authors:[Ankit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the pairwise dot-product attention that has a large Ω(L2) memory requirement for length L sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based global memory of length M (≪L) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable O(M⋅(L+M)) memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03274](https://arxiv.org/abs/2006.03274) [cs.LG]** |
|           | (or **[arXiv:2006.03274v1](https://arxiv.org/abs/2006.03274v1) [cs.LG]** for this version) |





<h2 id="2020-06-08-3">3. ELITR Non-Native Speech Translation at IWSLT 2020</h2>

Title: [ELITR Non-Native Speech Translation at IWSLT 2020](https://arxiv.org/abs/2006.03331)

Authors: [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Jonáš Kratochvíl](https://arxiv.org/search/cs?searchtype=author&query=Kratochvíl%2C+J), [Sangeet Sagar](https://arxiv.org/search/cs?searchtype=author&query=Sagar%2C+S), [Matúš Žilinec](https://arxiv.org/search/cs?searchtype=author&query=Žilinec%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O), [Thai-Son Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T), [Felix Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+F), [Philip Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+P), [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y)

> This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.

| Comments: | IWSLT 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.03331](https://arxiv.org/abs/2006.03331) [cs.CL]** |
|           | (or **[arXiv:2006.03331v1](https://arxiv.org/abs/2006.03331v1) [cs.CL]** for this version) |





<h2 id="2020-06-08-4">4. Unsupervised Translation of Programming Languages</h2>

Title: [Unsupervised Translation of Programming Languages](https://arxiv.org/abs/2006.03511)

Authors: [Marie-Anne Lachaux](https://arxiv.org/search/cs?searchtype=author&query=Lachaux%2C+M), [Baptiste Roziere](https://arxiv.org/search/cs?searchtype=author&query=Roziere%2C+B), [Lowik Chanussot](https://arxiv.org/search/cs?searchtype=author&query=Chanussot%2C+L), [Guillaume Lample](https://arxiv.org/search/cs?searchtype=author&query=Lample%2C+G)

> A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.

| Subjects: | **Computation and Language (cs.CL)**; Programming Languages (cs.PL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.03511](https://arxiv.org/abs/2006.03511) [cs.CL]** |
|           | (or **[arXiv:2006.03511v1](https://arxiv.org/abs/2006.03511v1) [cs.CL]** for this version) |







# 2020-06-05

[Return to Index](#Index)



<h2 id="2020-06-05-1">1. CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning</h2>

Title: [CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning](https://arxiv.org/abs/2006.02814)

Authors: [Sameer Khurana](https://arxiv.org/search/eess?searchtype=author&query=Khurana%2C+S), [Antoine Laurent](https://arxiv.org/search/eess?searchtype=author&query=Laurent%2C+A), [James Glass](https://arxiv.org/search/eess?searchtype=author&query=Glass%2C+J)

> More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.

| Comments: | submitted to INTERSPEECH                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| Cite as:  | **[arXiv:2006.02814](https://arxiv.org/abs/2006.02814) [eess.AS]** |
|           | (or **[arXiv:2006.02814v1](https://arxiv.org/abs/2006.02814v1) [eess.AS]** for this version) |





<h2 id="2020-06-05-2">2. Self-Training for End-to-End Speech Translation</h2>

Title: [Self-Training for End-to-End Speech Translation](https://arxiv.org/abs/2006.02490)

Authors: [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Qiantong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Mohammad Javad Dousti](https://arxiv.org/search/cs?searchtype=author&query=Dousti%2C+M+J), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y)

> One of the main challenges for end-to-end speech translation is data scarcity. We leverage pseudo-labels generated from unlabeled audio by a cascade and an end-to-end speech translation model. This provides 8.3 and 5.7 BLEU gains over a strong semi-supervised baseline on the MuST-C English-French and English-German datasets, reaching state-of-the art performance. The effect of the quality of the pseudo-labels is investigated. Our approach is shown to be more effective than simply pre-training the encoder on the speech recognition task. Finally, we demonstrate the effectiveness of self-training by directly generating pseudo-labels with an end-to-end model instead of a cascade model.

| Comments: | Submitted to INTERSPEECH 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02490](https://arxiv.org/abs/2006.02490) [cs.CL]** |
|           | (or **[arXiv:2006.02490v1](https://arxiv.org/abs/2006.02490v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-3">3. M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training</h2>

Title: [M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training](https://arxiv.org/abs/2006.02635)

Authors: [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Lin Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+L), [Di Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+D), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Bei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Jianlong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Xin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> This paper presents a Multitask Multilingual Multimodal Pre-trained model (M3P) that combines multilingual-monomodal pre-training and monolingual-multimodal pre-training into a unified framework via multitask learning and weight sharing. The model learns universal representations that can map objects that occurred in different modalities or expressed in different languages to vectors in a common semantic space. To verify the generalization capability of M3P, we fine-tune the pre-trained model for different types of downstream tasks: multilingual image-text retrieval, multilingual image captioning, multimodal machine translation, multilingual natural language inference and multilingual text generation. Evaluation shows that M3P can (i) achieve comparable results on multilingual tasks and English multimodal tasks, compared to the state-of-the-art models pre-trained for these two types of tasks separately, and (ii) obtain new state-of-the-art results on non-English multimodal tasks in the zero-shot or few-shot setting. We also build a new Multilingual Image-Language Dataset (MILD) by collecting large amounts of (text-query, image, context) triplets in 8 languages from the logs of a commercial search engine

| Comments: | 10 pages,2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2006.02635](https://arxiv.org/abs/2006.02635) [cs.CL]** |
|           | (or **[arXiv:2006.02635v1](https://arxiv.org/abs/2006.02635v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-4">4. Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation</h2>

Title: [Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation](https://arxiv.org/abs/2006.02876)

Authors: [Idris Abdulmumin](https://arxiv.org/search/cs?searchtype=author&query=Abdulmumin%2C+I), [Bashir Shehu Galadanci](https://arxiv.org/search/cs?searchtype=author&query=Galadanci%2C+B+S), [Abubakar Isa](https://arxiv.org/search/cs?searchtype=author&query=Isa%2C+A)

> Improving neural machine translation (NMT) models using the back-translations of the monolingual target data (synthetic parallel data) is currently the state-of-the-art approach for training improved translation systems. The quality of the backward system - which is trained on the available parallel data and used for the back-translation - has been shown in many studies to affect the performance of the final NMT model. In low resource conditions, the available parallel data is usually not enough to train a backward model that can produce the qualitative synthetic data needed to train a standard translation model. This work proposes a self-training strategy where the output of the backward model is used to improve the model itself through the forward translation technique. The technique was shown to improve baseline low resource IWSLT'14 English-German and IWSLT'15 English-Vietnamese backward translation models by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the improved English-German backward model was used to train a forward model which out-performed another forward model trained using standard back-translation by 2.7 BLEU.

| Comments: | 8 pages, 5 figures, 4 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.02876](https://arxiv.org/abs/2006.02876) [cs.CL]** |
|           | (or **[arXiv:2006.02876v1](https://arxiv.org/abs/2006.02876v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-5">5. Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1</h2>

Title: [Personalizing Grammatical Error Correction: Adaptation to Proficiency Level and L1](https://arxiv.org/abs/2006.02964)

Authors: [Maria Nadejde](https://arxiv.org/search/cs?searchtype=author&query=Nadejde%2C+M), [Joel Tetreault](https://arxiv.org/search/cs?searchtype=author&query=Tetreault%2C+J)

> Grammar error correction (GEC) systems have become ubiquitous in a variety of software applications, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these systems to the user's characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general-purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences. Our study is the broadest of its kind, covering five proficiency levels and twelve different languages, and comparing three different adaptation scenarios: adapting to the proficiency level only, to the first language only, or to both aspects simultaneously. We show that tailoring to both scenarios achieves the largest performance improvement (3.6 F0.5) relative to a strong baseline.

| Comments:          | Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 27-33, Hong Kong, Nov 4, 2019 |
| Cite as:           | **[arXiv:2006.02964](https://arxiv.org/abs/2006.02964) [cs.CL]** |
|                    | (or **[arXiv:2006.02964v1](https://arxiv.org/abs/2006.02964v1) [cs.CL]** for this version) |





<h2 id="2020-06-05-6">6. End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020</h2>

Title: [End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020](https://arxiv.org/abs/2006.02965)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Mattia Antonino Di Gangi](https://arxiv.org/search/cs?searchtype=author&query=Di+Gangi%2C+M+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> This paper describes FBK's participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems' ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whether to work on custom segmentation or not. We used the provided segmentation. Our system is an end-to-end model based on an adaptation of the Transformer for speech data. Its training process is the main focus of this paper and it is based on: i) transfer learning (ASR pretraining and knowledge distillation), ii) data augmentation (SpecAugment, time stretch and synthetic data), iii) combining synthetic and real data marked as different domains, and iv) multi-task learning using the CTC loss. Finally, after the training with word-level knowledge distillation is complete, our ST models are fine-tuned using label smoothed cross entropy. Our best model scored 29 BLEU on the MuST-C En-De test set, which is an excellent result compared to recent papers, and 23.7 BLEU on the same data segmented with VAD, showing the need for researching solutions addressing this specific data condition.

| Comments: | Accepted at IWSLT2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02965](https://arxiv.org/abs/2006.02965) [cs.CL]** |
|           | (or **[arXiv:2006.02965v1](https://arxiv.org/abs/2006.02965v1) [cs.CL]** for this version) |









# 2020-06-04

[Return to Index](#Index)



<h2 id="2020-06-04-1">1. The Typology of Polysemy: A Multilingual Distributional Framework</h2>

Title: [The Typology of Polysemy: A Multilingual Distributional Framework](https://arxiv.org/abs/2006.01966)

Authors: [Ella Rabinovich](https://arxiv.org/search/cs?searchtype=author&query=Rabinovich%2C+E), [Yang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Suzanne Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+S)

> Lexical semantic typology has identified important cross-linguistic generalizations about the variation and commonalities in polysemy patterns---how languages package up meanings into words. Recent computational research has enabled investigation of lexical semantics at a much larger scale, but little work has explored lexical typology across semantic domains, nor the factors that influence cross-linguistic similarities. We present a novel computational framework that quantifies semantic affinity, the cross-linguistic similarity of lexical semantics for a concept. Our approach defines a common multilingual semantic space that enables a direct comparison of the lexical expression of concepts across languages. We validate our framework against empirical findings on lexical semantic typology at both the concept and domain levels. Our results reveal an intricate interaction between semantic domains and extra-linguistic factors, beyond language phylogeny, that co-shape the typology of polysemy across languages.

| Comments: | CogSci 2020 (Annual Meeting of the Cognitive Science Society) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01966](https://arxiv.org/abs/2006.01966) [cs.CL]** |
|           | (or **[arXiv:2006.01966v1](https://arxiv.org/abs/2006.01966v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-2">2. Norm-Based Curriculum Learning for Neural Machine Translation</h2>

Title: [Norm-Based Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2006.02014)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Houtim Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+H), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).

| Comments: | Accepted to ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.02014](https://arxiv.org/abs/2006.02014) [cs.CL]** |
|           | (or **[arXiv:2006.02014v1](https://arxiv.org/abs/2006.02014v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-3">3. Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation</h2>

Title: [Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation](https://arxiv.org/abs/2006.02163)

Authors: [Xuan-Phi Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Wu Kui](https://arxiv.org/search/cs?searchtype=author&query=Kui%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T)

> Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply these principles differently. This work introduces another component to this framework: Multi-Agent Cross-translated Diversification (MACD). The method trains multiple UMT agents and then translates monolingual data back and forth using non-duplicative agents to acquire synthetic parallel data for supervised MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation tasks. Through extensive experimental analyses, we show that MACD is effective because it embraces data diversity while other similar variants do not.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.02163](https://arxiv.org/abs/2006.02163) [cs.CL]** |
|           | (or **[arXiv:2006.02163v1](https://arxiv.org/abs/2006.02163v1) [cs.CL]** for this version) |





<h2 id="2020-06-04-4">4. Improved acoustic word embeddings for zero-resource languages using multilingual transfer</h2>

Title: [Improved acoustic word embeddings for zero-resource languages using multilingual transfer](https://arxiv.org/abs/2006.02295)

Authors: [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Yevgen Matusevych](https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

> Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. Such embeddings can form the basis for speech search, indexing and discovery systems when conventional speech recognition is not possible. In zero-resource settings where unlabelled speech is the only available resource, we need a method that gives robust embeddings on an arbitrary language. Here we explore multilingual transfer: we train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. We consider three multilingual recurrent neural network (RNN) models: a classifier trained on the joint vocabularies of all training languages; a Siamese RNN trained to discriminate between same and different words from multiple languages; and a correspondence autoencoder (CAE) RNN trained to reconstruct word pairs. In a word discrimination task on six target languages, all of these models outperform state-of-the-art unsupervised models trained on the zero-resource languages themselves, giving relative improvements of more than 30% in average precision. When using only a few training languages, the multilingual CAE performs better, but with more training languages the other multilingual models perform similarly. Using more training languages is generally beneficial, but improvements are marginal on some languages. We present probing experiments which show that the CAE encodes more phonetic, word duration, language identity and speaker information than the other multilingual models.

| Comments: | 11 pages, 7 figures, 8 tables. arXiv admin note: text overlap with [arXiv:2002.02109](https://arxiv.org/abs/2002.02109) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2006.02295](https://arxiv.org/abs/2006.02295) [cs.CL]** |
|           | (or **[arXiv:2006.02295v1](https://arxiv.org/abs/2006.02295v1) [cs.CL]** for this version) |







# 2020-06-03

[Return to Index](#Index)



<h2 id="2020-06-03-1">1. WikiBERT models: deep transfer learning for many languages</h2>

Title: [WikiBERT models: deep transfer learning for many languages](https://arxiv.org/abs/2006.01538)

Authors: [Sampo Pyysalo](https://arxiv.org/search/cs?searchtype=author&query=Pyysalo%2C+S), [Jenna Kanerva](https://arxiv.org/search/cs?searchtype=author&query=Kanerva%2C+J), [Antti Virtanen](https://arxiv.org/search/cs?searchtype=author&query=Virtanen%2C+A), [Filip Ginter](https://arxiv.org/search/cs?searchtype=author&query=Ginter%2C+F)

> Deep neural language models such as BERT have enabled substantial recent advances in many natural language processing tasks. Due to the effort and computational cost involved in their pre-training, language-specific models are typically introduced only for a small number of high-resource languages such as English. While multilingual models covering large numbers of languages are available, recent work suggests monolingual training can produce better models, and our understanding of the tradeoffs between mono- and multilingual training is incomplete. In this paper, we introduce a simple, fully automated pipeline for creating language-specific BERT models from Wikipedia data and introduce 42 new such models, most for languages up to now lacking dedicated deep neural language models. We assess the merits of these models using the state-of-the-art UDify parser on Universal Dependencies data, contrasting performance with results using the multilingual BERT model. We find that UDify using WikiBERT models outperforms the parser using mBERT on average, with the language-specific models showing substantially improved performance for some languages, yet limited improvement or a decrease in performance for others. We also present preliminary results as first steps toward an understanding of the conditions under which language-specific models are most beneficial. All of the methods and models introduced in this work are available under open licenses from [this https URL](https://github.com/turkunlp/wikibert).

| Comments: | 7 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2006.01538](https://arxiv.org/abs/2006.01538) [cs.CL]** |
|           | (or **[arXiv:2006.01538v1](https://arxiv.org/abs/2006.01538v1) [cs.CL]** for this version) |





<h2 id="2020-06-03-2">2. Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders</h2>

Title: [Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders](https://arxiv.org/abs/2006.01594)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

> We propose a modular architecture of language-specific encoder-decoders that constitutes a multilingual machine translation system that can be incrementally extended to new languages without the need for retraining the existing system when adding new languages. Differently from previous works, we simultaneously train N languages in all translation directions by alternately freezing encoder or decoder modules, which indirectly forces the system to train in a common intermediate representation for all languages. Experimental results from multilingual machine translation show that we can successfully train this modular architecture improving on the initial languages while falling slightly behind when adding new languages or doing zero-shot translation. Additional comparison of the quality of sentence representation in the task of natural language inference shows that the alternately freezing training is also beneficial in this direction.

| Comments:    | arXiv admin note: text overlap with [arXiv:2004.06575](https://arxiv.org/abs/2004.06575) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2006.01594](https://arxiv.org/abs/2006.01594) [cs.CL]** |
|              | (or **[arXiv:2006.01594v1](https://arxiv.org/abs/2006.01594v1) [cs.CL]** for this version) |





# 2020-06-02

[Return to Index](#Index)



<h2 id="2020-06-02-1">1. A Comparative Study of Lexical Substitution Approaches based on Neural Language Models</h2>

Title: [A Comparative Study of Lexical Substitution Approaches based on Neural Language Models](https://arxiv.org/abs/2006.00031)

Authors: [Nikolay Arefyev](https://arxiv.org/search/cs?searchtype=author&query=Arefyev%2C+N), [Boris Sheludko](https://arxiv.org/search/cs?searchtype=author&query=Sheludko%2C+B), [Alexander Podolskiy](https://arxiv.org/search/cs?searchtype=author&query=Podolskiy%2C+A), [Alexander Panchenko](https://arxiv.org/search/cs?searchtype=author&query=Panchenko%2C+A)

> Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00031](https://arxiv.org/abs/2006.00031) [cs.CL]** |
|           | (or **[arXiv:2006.00031v1](https://arxiv.org/abs/2006.00031v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-2">2. Dynamic Masking for Improved Stability in Spoken Language Translation</h2>

Title: [Dynamic Masking for Improved Stability in Spoken Language Translation](https://arxiv.org/abs/2006.00249)

Authors: [Yuekun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> For spoken language translation (SLT) in live scenarios such as conferences, lectures and meetings, it is desirable to show the translation to the user as quickly as possible, avoiding an annoying lag between speaker and translated captions. In other words, we would like low-latency, online SLT. If we assume a pipeline of automatic speech recognition (ASR) and machine translation (MT) then a viable approach to online SLT is to pair an online ASR system, with a a retranslation strategy, where the MT system re-translates every update received from ASR. However this can result in annoying "flicker" as the MT system updates its translation. A possible solution is to add a fixed delay, or "mask" to the the output of the MT system, but a fixed global mask introduces undesirable latency to the output. We show how this mask can be set dynamically, improving the latency-flicker trade-off without sacrificing translation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00249](https://arxiv.org/abs/2006.00249) [cs.CL]** |
|           | (or **[arXiv:2006.00249v1](https://arxiv.org/abs/2006.00249v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-3">3. Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation</h2>

Title: [Data Augmentation for Learning Bilingual Word Embeddings with Unsupervised Machine Translation](https://arxiv.org/abs/2006.00262)

Authors: [Sosuke Nishikawa](https://arxiv.org/search/cs?searchtype=author&query=Nishikawa%2C+S), [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Unsupervised bilingual word embedding (BWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method assumes that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we propose using a pseudo-parallel corpus generated by an unsupervised machine translation model to facilitate structural similarity of the two embedding spaces and improve the quality of BWEs in the mapping method. We show that our approach substantially outperforms baselines and other alternative approaches given the same amount of data, and, through detailed analysis, we argue that data augmentation with the pseudo data from unsupervised machine translation is especially effective for BWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data reflects some nature of the original language that helps learning similar embedding spaces between the source and target languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00262](https://arxiv.org/abs/2006.00262) [cs.CL]** |
|           | (or **[arXiv:2006.00262v1](https://arxiv.org/abs/2006.00262v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-4">4. Neural Unsupervised Domain Adaptation in NLP---A Survey</h2>

Title: [Neural Unsupervised Domain Adaptation in NLP---A Survey](https://arxiv.org/abs/2006.00632)

Authors: [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00632](https://arxiv.org/abs/2006.00632) [cs.CL]** |
|           | (or **[arXiv:2006.00632v1](https://arxiv.org/abs/2006.00632v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-5">5. Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English</h2>

Title: [Online Versus Offline NMT Quality: An In-depth Analysis on English-German and German-English](https://arxiv.org/abs/2006.00814)

Authors: [Maha Elbayad](https://arxiv.org/search/cs?searchtype=author&query=Elbayad%2C+M), [Michael Ustaszewski](https://arxiv.org/search/cs?searchtype=author&query=Ustaszewski%2C+M), [Emmanuelle Esperança-Rodier](https://arxiv.org/search/cs?searchtype=author&query=Esperança-Rodier%2C+E), [Francis Brunet Manquat](https://arxiv.org/search/cs?searchtype=author&query=Manquat%2C+F+B), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> We conduct in this work an evaluation study comparing offline and online neural machine translation architectures. Two sequence-to-sequence models: convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based Transformer (Vaswani et al. 2017) are considered. We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English-German and German-English language pairs, the latter being particularly sensitive to latency constraints. The evaluation results allow us to identify the strengths and shortcomings of each model when we shift to the online setup.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00814](https://arxiv.org/abs/2006.00814) [cs.CL]** |
|           | (or **[arXiv:2006.00814v1](https://arxiv.org/abs/2006.00814v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-6">6. Attention Word Embedding</h2>

Title: [Attention Word Embedding](https://arxiv.org/abs/2006.00988)

Authors: [Shashank Sonkar](https://arxiv.org/search/cs?searchtype=author&query=Sonkar%2C+S), [Andrew E. Waters](https://arxiv.org/search/cs?searchtype=author&query=Waters%2C+A+E), [Richard G. Baraniuk](https://arxiv.org/search/cs?searchtype=author&query=Baraniuk%2C+R+G)

> Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.00988](https://arxiv.org/abs/2006.00988) [cs.CL]** |
|           | (or **[arXiv:2006.00988v1](https://arxiv.org/abs/2006.00988v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-7">7. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</h2>

Title: [Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?](https://arxiv.org/abs/2006.01080)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.

| Comments: | Accepted at IWSLT 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2006.01080](https://arxiv.org/abs/2006.01080) [cs.CL]** |
|           | (or **[arXiv:2006.01080v1](https://arxiv.org/abs/2006.01080v1) [cs.CL]** for this version) |





<h2 id="2020-06-02-8">8. Cascaded Text Generation with Markov Transformers</h2>

Title: [Cascaded Text Generation with Markov Transformers](https://arxiv.org/abs/2006.01112)

Authors: [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2006.01112](https://arxiv.org/abs/2006.01112) [cs.CL]** |
|           | (or **[arXiv:2006.01112v1](https://arxiv.org/abs/2006.01112v1) [cs.CL]** for this version) |







# 2020-06-01

[Return to Index](#Index)



<h2 id="2020-06-01-1">1. Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP</h2>

Title: [Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP]()

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy use of fine-tuning BERT-like models in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of NLP tasks in a uniform toolkit, from text classification to sequence labeling and dependency parsing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2005.14672](https://arxiv.org/abs/2005.14672) [cs.CL]** |
|           | (or **[arXiv:2005.14672v1](https://arxiv.org/abs/2005.14672v1) [cs.CL]** for this version) |