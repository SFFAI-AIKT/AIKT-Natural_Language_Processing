# Daily arXiv: Machine Translation - January, 2021

# Index


- [2021-01-28](#2021-01-28)
	
  - [1. Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network](#2021-01-28-1)
  - [2. Muppet: Massive Multi-task Representations with Pre-Finetuning](#2021-01-28-2)
  - [3. A Comparison of Approaches to Document-level Machine Translation](#2021-01-28-3)
  - [4. Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT](#2021-01-28-4)
  - [5. First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT](#2021-01-28-5)
  - [6. CLiMP: A Benchmark for Chinese Language Model Evaluation](#2021-01-28-6)
  - [7. VisualMRC: Machine Reading Comprehension on Document Images](#2021-01-28-7)
  - [8. Language Modelling as a Multi-Task Problem](#2021-01-28-8)
  - [9. On the Evolution of Syntactic Information Encoded by BERT's Contextualized Representations](#2021-01-28-9)
- [2021-01-27](#2021-01-27)
	
  - [1. Curriculum Learning: A Survey](#2021-01-27-1)
  - [2. On the Evaluation of Vision-and-Language Navigation Instructions](#2021-01-27-2)
  - [3. Meta-Learning for Effective Multi-task and Multilingual Modelling](#2021-01-27-3)
  - [4. Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings](#2021-01-27-4)
  - [5. Analyzing Zero-shot Cross-lingual Transfer in Supervised NLP Tasks](#2021-01-27-5)
  - [6. Neural machine translation, corpus and frugality](#2021-01-27-6)
  - [7. Spark NLP: Natural Language Understanding at Scale](#2021-01-27-7)
  - [8. Attention Can Reflect Syntactic Structure (If You Let It)](#2021-01-27-8)
- [2021-01-26](#2021-01-26)
	- [1. Analysing the Noise Model Error for Realistic Noisy Label Data](#2021-01-26-1)
  - [2. k-Neighbor Based Curriculum Sampling for Sequence Prediction](#2021-01-26-2)
  - [3. Training Multilingual Pre-trained Language Model with Byte-level Subwords](#2021-01-26-3)
  - [4. Debiasing Pre-trained Contextualised Embeddings](#2021-01-26-4)
  - [5. Dictionary-based Debiasing of Pre-trained Word Embeddings](#2021-01-26-5)
  - [6. RomeBERT: Robust Training of Multi-Exit BERT](#2021-01-26-6)
  - [7. Cross-lingual Visual Pre-training for Multimodal Machine Translation](#2021-01-26-7)
  - [8. PAWLS: PDF Annotation With Labels and Structure](#2021-01-26-8)
- [2021-01-25](#2021-01-25)
	
  - [1. Enriching Non-Autoregressive Transformer with Syntactic and SemanticStructures for Neural Machine Translation](#2021-01-25-1)
  - [2. Enhanced word embeddings using multi-semantic representation through lexical chains](#2021-01-25-2)
  - [3. Streaming Models for Joint Speech Recognition and Translation](#2021-01-25-3)
- [2021-01-22](#2021-01-22)
	
  - [1. Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval](#2021-01-22-1)
  - [2. Adv-OLM: Generating Textual Adversaries via OLM](#2021-01-22-2)
- [2021-01-21](#2021-01-21)
	
  - [1. Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation](#2021-01-21-1)
  - [2. Word Alignment by Fine-tuning Embeddings on Parallel Corpora](#2021-01-21-2)
  - [3. Generating (Formulaic) Text by Splicing Together Nearest Neighbors](#2021-01-21-3)
- [2021-01-20](#2021-01-20)
	
  - [1. ArtEmis: Affective Language for Visual Art](#2021-01-20-1)
- [2021-01-19](#2021-01-19)
	
  - [1. Latent Variable Models for Visual Question Answering](#2021-01-19-1)
  - [2. TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search](#2021-01-19-2)
  - [3. To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph](#2021-01-19-3)
  - [4. GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation](#2021-01-19-4)
  - [5. What Makes Good In-Context Examples for GPT-3?](#2021-01-19-5)
  - [6. Can a Fruit Fly Learn Word Embeddings?](#2021-01-19-6)
- [2021-01-18](#2021-01-18)
	
  - [1. Knowledge Graphs and Natural-Language Processing](#2021-01-18-1)
  - [2. The Impact of Post-editing and Machine Translation on Creativity and Reading Experience](#2021-01-18-2)
  - [3. Empirical Evaluation of Supervision Signals for Style Transfer Models](#2021-01-18-3)
- [2021-01-15](#2021-01-15)
	- [1. Structured Prediction as Translation between Augmented Natural Languages](#2021-01-15-1)
  - [2. Text Augmentation in a Multi-Task View](#2021-01-15-2)
  - [3. Persuasive Natural Language Generation -- A Literature Review](#2021-01-15-3)
- [2021-01-14](#2021-01-14)
	
  - [1. Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding](#2021-01-14-1)
  - [2. Latent Alignment of Procedural Concepts in Multimodal Recipes](#2021-01-14-2)
  - [3. Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration](#2021-01-14-3)
- [2021-01-13](#2021-01-13)
	
  - [1. Quantum Mathematics in Artificial Intelligence](#2021-01-13-1)
  - [2. Explain and Predict, and then Predict again](#2021-01-13-2)
  - [3. Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning](#2021-01-13-3)
  - [4. Transforming Multi-Conditioned Generation from Meaning Representation](#2021-01-13-4)
  - [5. Toward Effective Automated Content Analysis via Crowdsourcing](#2021-01-13-5)
- [2021-01-12](#2021-01-12)
	
  - [1. Misspelling Correction with Pre-trained Contextual Language Model](#2021-01-12-1)
  - [2. SDA: Improving Text Generation with Self Data Augmentation](#2021-01-12-2)
  - [3. Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing](#2021-01-12-3)
  - [4. Learning Better Sentence Representation with Syntax Information](#2021-01-12-4)
  - [5. Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation](#2021-01-12-5)
- [2021-01-11](#2021-01-11)
	
  - [1. MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets](#2021-01-11-1)
- [2021-01-08](#2021-01-08)
	- [1. User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics](#2021-01-08-1)
  - [2. Towards a Smart Data Processing and Storage Model](#2021-01-08-2)
- [2021-01-07](#2021-01-07)
	
  - [1. AutoDropout: Learning Dropout Patterns to Regularize Deep Networks](#2021-01-07-1)
- [2021-01-06](#2021-01-06)
	
  - [1. I-BERT: Integer-only BERT Quantization](#2021-01-06-1)
  - [2. Political Depolarization of News Articles Using Attribute-aware Word Embeddings](#2021-01-06-2)
  - [3. Local Translation Services for Neglected Languages](#2021-01-06-3)
- [2021-01-05](#2021-01-05)
	
  - [1. VinVL: Making Visual Representations Matter in Vision-Language Models](#2021-01-05-1)
  - [2. The Pile: An 800GB Dataset of Diverse Text for Language Modeling](#2021-01-05-2)
  - [3. EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](#2021-01-05-3)
  - [4. Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](#2021-01-05-4)
  - [5. A Graph Total Variation Regularized Softmax for Text Generation](#2021-01-05-5)
  - [6. BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding](#2021-01-05-6)
  - [7. Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](#2021-01-05-7)
  - [8. Understanding Few-Shot Commonsense Knowledge Models](#2021-01-05-8)
  - [9. On-the-Fly Attention Modularization for Neural Generation](#2021-01-05-9)
  - [10. Cross-Document Language Modeling](#2021-01-05-10)
  - [11. Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](#2021-01-05-11)
  - [12. KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](#2021-01-05-12)
  - [13. Decoding Time Lexical Domain Adaptationfor Neural Machine Translation](#2021-01-05-13)
  - [14. Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events](#2021-01-05-14)
  - [15. Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](#2021-01-05-15)
  - [16. How to Train Your Agent to Read and Write](#2021-01-05-16)
- [2021-01-01](#2021-01-01)
	- [1. Understanding and Improving Lexical Choice in Non-Autoregressive Translation](#2021-01-01-1)
  - [2. Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation](#2021-01-01-2)
  - [3. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](#2021-01-01-3)
  - [4. CMV-BERT: Contrastive multi-vocab pretraining of BERT](#2021-01-01-4)
  - [5. Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning](#2021-01-01-5)
  - [6. Transformer Feed-Forward Layers Are Key-Value Memories](#2021-01-01-6)
  - [7. Reservoir Transformer](#2021-01-01-7)
  - [8. Enhancing Pre-trained Language Model with Lexical Simplification](#2021-01-01-8)
  - [9. Accurate Word Representations with Universal Visual Guidance](#2021-01-01-9)
  - [10. Improving Zero-Shot Translation by Disentangling Positional Information](#2021-01-01-10)
  - [11. Improving BERT with Syntax-aware Local Attention](#2021-01-01-11)
  - [12. Synthetic Source Language Augmentation for Colloquial Neural Machine Translation](#2021-01-01-12)
  - [13. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?](#2021-01-01-13)
  - [14. SemGloVe: Semantic Co-occurrences for GloVe from BERT](#2021-01-01-14)
  - [15. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](#2021-01-01-15)
  - [16. Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation](#2021-01-01-16)
  - [17. Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation](#2021-01-01-17)
  - [18. CLEAR: Contrastive Learning for Sentence Representation](#2021-01-01-18)
  - [19. Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings](#2021-01-01-19)
  - [20. Towards Zero-Shot Knowledge Distillation for Natural Language Processing](#2021-01-01-20)
  - [21. Neural Machine Translation: A Review of Methods, Resources, and Tools](#2021-01-01-21)
  - [22. Linear-Time WordPiece Tokenization](#2021-01-01-22)
  - [23. XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders](#2021-01-01-23)
  - [24. How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](#2021-01-01-24)
  - [25. CoCoLM: COmplex COmmonsense Enhanced Language Model](#2021-01-01-25)
  - [26. VOLT: Improving Vocabularization via Optimal Transport for Machine Translation](#2021-01-01-26)
  - [27. ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](#2021-01-01-27)
  - [28. Revisiting Robust Neural Machine Translation: A Transformer Case Study](#2021-01-01-28)
  - [29. FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation](#2021-01-01-29)
  - [30. Making Pre-trained Language Models Better Few-shot Learners](#2021-01-01-30)
  - [31. Shortformer: Better Language Modeling using Shorter Inputs](#2021-01-01-31)
  - [32. Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade](#2021-01-01-32)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-01-28

[Return to Index](#Index)



<h2 id="2021-01-28-1">1. Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network</h2>

Title: [Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network](https://arxiv.org/abs/2101.11562)

Authors: [Yehao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yingwei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Ting Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+T), [Jingwen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Tao Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+T)

> Despite having impressive vision-language (VL) pretraining with BERT-based encoder for VL understanding, the pretraining of a universal encoder-decoder for both VL understanding and generation remains challenging. The difficulty originates from the inherently different peculiarities of the two disciplines, e.g., VL understanding tasks capitalize on the unrestricted message passing across modalities, while generation tasks only employ visual-to-textual message passing. In this paper, we start with a two-stream decoupled design of encoder-decoder structure, in which two decoupled cross-modal encoder and decoder are involved to separately perform each type of proxy tasks, for simultaneous VL understanding and generation pretraining. Moreover, for VL pretraining, the dominant way is to replace some input visual/word tokens with mask tokens and enforce the multi-modal encoder/decoder to reconstruct the original tokens, but no mask token is involved when fine-tuning on downstream tasks. As an alternative, we propose a primary scheduled sampling strategy that elegantly mitigates such discrepancy via pretraining encoder-decoder in a two-pass manner. Extensive experiments demonstrate the compelling generalizability of our pretrained encoder-decoder by fine-tuning on four VL understanding and generation downstream tasks. Source code is available at \url{[this https URL](https://github.com/YehLi/TDEN)}.

| Comments: | AAAI 2021; Code is publicly available at: [this https URL](https://github.com/YehLi/TDEN) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2101.11562](https://arxiv.org/abs/2101.11562) [cs.CV]** |
|           | (or **[arXiv:2101.11562v1](https://arxiv.org/abs/2101.11562v1) [cs.CV]** for this version) |





<h2 id="2021-01-28-2">2. Muppet: Massive Multi-task Representations with Pre-Finetuning</h2>

Title: [Muppet: Massive Multi-task Representations with Pre-Finetuning](https://arxiv.org/abs/2101.11038)

Authors: [Armen Aghajanyan](https://arxiv.org/search/cs?searchtype=author&query=Aghajanyan%2C+A), [Anchit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Akshat Shrivastava](https://arxiv.org/search/cs?searchtype=author&query=Shrivastava%2C+A), [Xilun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Sonal Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+S)

> We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.~RoBERTa) and generation models (e.g.~BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.11038](https://arxiv.org/abs/2101.11038) [cs.CL]** |
|           | (or **[arXiv:2101.11038v1](https://arxiv.org/abs/2101.11038v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-3">3. A Comparison of Approaches to Document-level Machine Translation</h2>

Title: [A Comparison of Approaches to Document-level Machine Translation](https://arxiv.org/abs/2101.11040)

Authors: [Zhiyi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z), [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

> Document-level machine translation conditions on surrounding sentences to produce coherent translations. There has been much recent work in this area with the introduction of custom model architectures and decoding algorithms. This paper presents a systematic comparison of selected approaches from the literature on two benchmarks for which document-level phenomena evaluation suites exist. We find that a simple method based purely on back-translating monolingual document-level data performs as well as much more elaborate alternatives, both in terms of document-level metrics as well as human evaluation.

| Comments: | 10 pages, 5 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.11040](https://arxiv.org/abs/2101.11040) [cs.CL]** |
|           | (or **[arXiv:2101.11040v1](https://arxiv.org/abs/2101.11040v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-4">4. Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT</h2>

Title: [Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT](https://arxiv.org/abs/2101.11043)

Authors: [Isabel Papadimitriou](https://arxiv.org/search/cs?searchtype=author&query=Papadimitriou%2C+I), [Ethan A. Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+E+A), [Richard Futrell](https://arxiv.org/search/cs?searchtype=author&query=Futrell%2C+R), [Kyle Mahowald](https://arxiv.org/search/cs?searchtype=author&query=Mahowald%2C+K)

> We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a "subject") is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.11043](https://arxiv.org/abs/2101.11043) [cs.CL]** |
|           | (or **[arXiv:2101.11043v1](https://arxiv.org/abs/2101.11043v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-5">5. First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT</h2>

Title: [First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT](https://arxiv.org/abs/2101.11109)

Authors: [Benjamin Muller](https://arxiv.org/search/cs?searchtype=author&query=Muller%2C+B), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Benoît Sagot](https://arxiv.org/search/cs?searchtype=author&query=Sagot%2C+B), [Djamé Seddah](https://arxiv.org/search/cs?searchtype=author&query=Seddah%2C+D)

> Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.

| Comments: | Accepted at EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.11109](https://arxiv.org/abs/2101.11109) [cs.CL]** |
|           | (or **[arXiv:2101.11109v1](https://arxiv.org/abs/2101.11109v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-6">6. CLiMP: A Benchmark for Chinese Language Model Evaluation</h2>

Title: [CLiMP: A Benchmark for Chinese Language Model Evaluation](https://arxiv.org/abs/2101.11131)

Authors: [Beilei Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B), [Changbing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C), [Yu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Alex Warstadt](https://arxiv.org/search/cs?searchtype=author&query=Warstadt%2C+A), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of these models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP), which can be used to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs (MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin linguistic phenomena. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8%. We evaluated 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier-noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.11131](https://arxiv.org/abs/2101.11131) [cs.CL]** |
|           | (or **[arXiv:2101.11131v1](https://arxiv.org/abs/2101.11131v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-7">7. VisualMRC: Machine Reading Comprehension on Document Images</h2>

Title: [VisualMRC: Machine Reading Comprehension on Document Images](https://arxiv.org/abs/2101.11272)

Authors: [Ryota Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+R), [Kyosuke Nishida](https://arxiv.org/search/cs?searchtype=author&query=Nishida%2C+K), [Sen Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida%2C+S)

> Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with large-scale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding.

| Comments: | Accepted as a full paper at AAAI 2021. The first two authors have equal contribution |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2101.11272](https://arxiv.org/abs/2101.11272) [cs.CL]** |
|           | (or **[arXiv:2101.11272v1](https://arxiv.org/abs/2101.11272v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-8">8. Language Modelling as a Multi-Task Problem</h2>

Title: [Language Modelling as a Multi-Task Problem](https://arxiv.org/abs/2101.11287)

Authors: [Lucas Weber](https://arxiv.org/search/cs?searchtype=author&query=Weber%2C+L), [Jaap Jumelet](https://arxiv.org/search/cs?searchtype=author&query=Jumelet%2C+J), [Elia Bruni](https://arxiv.org/search/cs?searchtype=author&query=Bruni%2C+E), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D)

> In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multi-task learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling.We argue that this insight is valuable for multi-task learning, linguistics and interpretability research and can lead to exciting new findings in all three domains.

| Comments: | Accepted for publication at EACL 2021                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.11287](https://arxiv.org/abs/2101.11287) [cs.CL]** |
|           | (or **[arXiv:2101.11287v1](https://arxiv.org/abs/2101.11287v1) [cs.CL]** for this version) |





<h2 id="2021-01-28-9">9. On the Evolution of Syntactic Information Encoded by BERT's Contextualized Representations</h2>

Title: [On the Evolution of Syntactic Information Encoded by BERT's Contextualized Representations](https://arxiv.org/abs/2101.11492)

Authors: [Laura Perez-Mayos](https://arxiv.org/search/cs?searchtype=author&query=Perez-Mayos%2C+L), [Roberto Carlini](https://arxiv.org/search/cs?searchtype=author&query=Carlini%2C+R), [Miguel Ballesteros](https://arxiv.org/search/cs?searchtype=author&query=Ballesteros%2C+M), [Leo Wanner](https://arxiv.org/search/cs?searchtype=author&query=Wanner%2C+L)

> The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semantics-related tasks) in different ways along the fine-tuning process depending on the task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.11492](https://arxiv.org/abs/2101.11492) [cs.CL]** |
|           | (or **[arXiv:2101.11492v1](https://arxiv.org/abs/2101.11492v1) [cs.CL]** for this version) |







# 2021-01-27

[Return to Index](#Index)



<h2 id="2021-01-27-1">1. Curriculum Learning: A Survey</h2>

Title: [Curriculum Learning: A Survey](https://arxiv.org/abs/2101.10382)

Authors: [Petru Soviany](https://arxiv.org/search/cs?searchtype=author&query=Soviany%2C+P), [Radu Tudor Ionescu](https://arxiv.org/search/cs?searchtype=author&query=Ionescu%2C+R+T), [Paolo Rota](https://arxiv.org/search/cs?searchtype=author&query=Rota%2C+P), [Nicu Sebe](https://arxiv.org/search/cs?searchtype=author&query=Sebe%2C+N)

> Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.10382](https://arxiv.org/abs/2101.10382) [cs.LG]** |
|           | (or **[arXiv:2101.10382v1](https://arxiv.org/abs/2101.10382v1) [cs.LG]** for this version) |





<h2 id="2021-01-27-2">2. On the Evaluation of Vision-and-Language Navigation Instructions</h2>

Title: [On the Evaluation of Vision-and-Language Navigation Instructions](https://arxiv.org/abs/2101.10504)

Authors: [Ming Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Peter Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+P), [Vihan Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+V), [Su Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Alexander Ku](https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+A), [Jason Baldridge](https://arxiv.org/search/cs?searchtype=author&query=Baldridge%2C+J), [Eugene Ie](https://arxiv.org/search/cs?searchtype=author&query=Ie%2C+E)

> Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2101.10504](https://arxiv.org/abs/2101.10504) [cs.AI]** |
|           | (or **[arXiv:2101.10504v1](https://arxiv.org/abs/2101.10504v1) [cs.AI]** for this version) |





<h2 id="2021-01-27-3">3. Meta-Learning for Effective Multi-task and Multilingual Modelling</h2>

Title: [Meta-Learning for Effective Multi-task and Multilingual Modelling](https://arxiv.org/abs/2101.10368)

Authors: [Ishan Tarunesh](https://arxiv.org/search/cs?searchtype=author&query=Tarunesh%2C+I), [Sushil Khyalia](https://arxiv.org/search/cs?searchtype=author&query=Khyalia%2C+S), [Vishwajeet Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V), [Ganesh Ramakrishnan](https://arxiv.org/search/cs?searchtype=author&query=Ramakrishnan%2C+G), [Preethi Jyothi](https://arxiv.org/search/cs?searchtype=author&query=Jyothi%2C+P)

> Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g. named entity recognition in English) and knowledge of other languages (e.g. question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset. Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multi-task baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model.

| Comments: | 8 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.10368](https://arxiv.org/abs/2101.10368) [cs.CL]** |
|           | (or **[arXiv:2101.10368v1](https://arxiv.org/abs/2101.10368v1) [cs.CL]** for this version) |





<h2 id="2021-01-27-4">4. Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings</h2>

Title: [Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings](https://arxiv.org/abs/2101.10565)

Authors: [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K), [Mauro M. Monsalve-Mercado](https://arxiv.org/search/cs?searchtype=author&query=Monsalve-Mercado%2C+M+M)

> In contrast to their word- or sentence-level counterparts, character embeddings are still poorly understood. We aim at closing this gap with an in-depth study of English character embeddings. For this, we use resources from research on grapheme-color synesthesia -- a neuropsychological phenomenon where letters are associated with colors, which give us insight into which characters are similar for synesthetes and how characters are organized in color space. Comparing 10 different character embeddings, we ask: How similar are character embeddings to a synesthete's perception of characters? And how similar are character embeddings extracted from different models? We find that LSTMs agree with humans more than transformers. Comparing across tasks, grapheme-to-phoneme conversion results in the most human-like character embeddings. Finally, ELMo embeddings differ from both humans and other models.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.10565](https://arxiv.org/abs/2101.10565) [cs.CL]** |
|           | (or **[arXiv:2101.10565v1](https://arxiv.org/abs/2101.10565v1) [cs.CL]** for this version) |





<h2 id="2021-01-27-5">5. Analyzing Zero-shot Cross-lingual Transfer in Supervised NLP Tasks</h2>

Title: [Analyzing Zero-shot Cross-lingual Transfer in Supervised NLP Tasks](https://arxiv.org/abs/2101.10649)

Authors: [Hyunjin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+H), [Judong Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Seongho Joe](https://arxiv.org/search/cs?searchtype=author&query=Joe%2C+S), [Seungjai Min](https://arxiv.org/search/cs?searchtype=author&query=Min%2C+S), [Youngjune Gwon](https://arxiv.org/search/cs?searchtype=author&query=Gwon%2C+Y)

> In zero-shot cross-lingual transfer, a supervised NLP task trained on a corpus in one language is directly applicable to another language without any additional training. A source of cross-lingual transfer can be as straightforward as lexical overlap between languages (e.g., use of the same scripts, shared subwords) that naturally forces text embeddings to occupy a similar representation space. Recently introduced cross-lingual language model (XLM) pretraining brings out neural parameter sharing in Transformer-style networks as the most important factor for the transfer. In this paper, we aim to validate the hypothetically strong cross-lingual transfer properties induced by XLM pretraining. Particularly, we take XLM-RoBERTa (XLMR) in our experiments that extend semantic textual similarity (STS), SQuAD and KorQuAD for machine reading comprehension, sentiment analysis, and alignment of sentence embeddings under various cross-lingual settings. Our results indicate that the presence of cross-lingual transfer is most pronounced in STS, sentiment analysis the next, and MRC the last. That is, the complexity of a downstream task softens the degree of crosslingual transfer. All of our results are empirically observed and measured, and we make our code and data publicly available.

| Comments: | 6 pages, 4 figures, to be published in 25th International Conference on Pattern Recognition, ICPR 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.10649](https://arxiv.org/abs/2101.10649) [cs.CL]** |
|           | (or **[arXiv:2101.10649v1](https://arxiv.org/abs/2101.10649v1) [cs.CL]** for this version) |





<h2 id="2021-01-27-6">6. Neural machine translation, corpus and frugality</h2>

Title: [Neural machine translation, corpus and frugality](https://arxiv.org/abs/2101.10650)

Authors: [Raoul Blin](https://arxiv.org/search/cs?searchtype=author&query=Blin%2C+R)

> In machine translation field, in both academia and industry, there is a growing interest in increasingly powerful systems, using corpora of several hundred million to several billion examples. These systems represent the state-of-the-art. Here we defend the idea of developing in parallel <<frugal>> bilingual translation systems, trained with relatively small corpora. Based on the observation of a standard human professional translator, we estimate that the corpora should be composed at maximum of a monolingual sub-corpus of 75 million examples for the source language, a second monolingual sub-corpus of 6 million examples for the target language, and an aligned bilingual sub-corpus of 6 million bi-examples. A less desirable alternative would be an aligned bilingual corpus of 47.5 million bi-examples.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.10650](https://arxiv.org/abs/2101.10650) [cs.CL]** |
|           | (or **[arXiv:2101.10650v1](https://arxiv.org/abs/2101.10650v1) [cs.CL]** for this version) |





<h2 id="2021-01-27-7">7. Spark NLP: Natural Language Understanding at Scale</h2>

Title: [Spark NLP: Natural Language Understanding at Scale](https://arxiv.org/abs/2101.10848)

Authors: [Veysel Kocaman](https://arxiv.org/search/cs?searchtype=author&query=Kocaman%2C+V), [David Talby](https://arxiv.org/search/cs?searchtype=author&query=Talby%2C+D)

> Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant and accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100 pre trained pipelines and models in more than 192 languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing nine times growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the worlds most widely used NLP library in the enterprise.

| Comments: | =Accepted as a publication in Elsevier, Software Impacts Journal. arXiv admin note: substantial text overlap with [arXiv:2012.04005](https://arxiv.org/abs/2012.04005) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.10848](https://arxiv.org/abs/2101.10848) [cs.CL]** |
|           | (or **[arXiv:2101.10848v1](https://arxiv.org/abs/2101.10848v1) [cs.CL]** for this version) |





<h2 id="2021-01-27-8">8. Attention Can Reflect Syntactic Structure (If You Let It)</h2>

Title: [Attention Can Reflect Syntactic Structure (If You Let It)](https://arxiv.org/abs/2101.10927)

Authors: [Vinit Ravishankar](https://arxiv.org/search/cs?searchtype=author&query=Ravishankar%2C+V), [Artur Kulmizev](https://arxiv.org/search/cs?searchtype=author&query=Kulmizev%2C+A), [Mostafa Abdou](https://arxiv.org/search/cs?searchtype=author&query=Abdou%2C+M), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A), [Joakim Nivre](https://arxiv.org/search/cs?searchtype=author&query=Nivre%2C+J)

> Since the popularization of the Transformer as a general-purpose feature encoder for NLP, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English -- a language with rigid word order and a lack of inflectional morphology. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of attention as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EACL 2021                                                    |
| Cite as:           | **[arXiv:2101.10927](https://arxiv.org/abs/2101.10927) [cs.CL]** |
|                    | (or **[arXiv:2101.10927v1](https://arxiv.org/abs/2101.10927v1) [cs.CL]** for this version) |







# 2021-01-26

[Return to Index](#Index)



<h2 id="2021-01-26-1">1. Analysing the Noise Model Error for Realistic Noisy Label Data</h2>

Title: [Analysing the Noise Model Error for Realistic Noisy Label Data](https://arxiv.org/abs/2101.09763)

Authors: [Michael A. Hedderich](https://arxiv.org/search/cs?searchtype=author&query=Hedderich%2C+M+A), [Dawei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+D), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Distant and weak supervision allow to obtain large amounts of labeled training data quickly and cheaply, but these automatic annotations tend to contain a high amount of errors. A popular technique to overcome the negative effects of these noisy labels is noise modelling where the underlying noise process is modelled. In this work, we study the quality of these estimated noise models from the theoretical side by deriving the expected error of the noise model. Apart from evaluating the theoretical results on commonly used synthetic noise, we also publish NoisyNER, a new noisy label dataset from the NLP domain that was obtained through a realistic distant supervision technique. It provides seven sets of labels with differing noise patterns to evaluate different noise levels on the same instances. Parallel, clean labels are available making it possible to study scenarios where a small amount of gold-standard data can be leveraged. Our theoretical results and the corresponding experiments give insights into the factors that influence the noise model estimation like the noise distribution and the sampling technique.

| Comments: | Accepted at AAAI 2021, additional material at [this https URL](https://github.com/uds-lsv/noise-estimation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2101.09763](https://arxiv.org/abs/2101.09763) [cs.LG]** |
|           | (or **[arXiv:2101.09763v1](https://arxiv.org/abs/2101.09763v1) [cs.LG]** for this version) |





<h2 id="2021-01-26-2">2. k-Neighbor Based Curriculum Sampling for Sequence Prediction</h2>

Title: [k-Neighbor Based Curriculum Sampling for Sequence Prediction](https://arxiv.org/abs/2101.09313)

Authors: [James O' Neill](https://arxiv.org/search/cs?searchtype=author&query=Neill%2C+J+O), [Danushka Bollegala](https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D)

> Multi-step ahead prediction in language models is challenging due to the discrepancy between training and test time processes. At test time, a sequence predictor is required to make predictions given past predictions as the input, instead of the past targets that are provided during training. This difference, known as exposure bias, can lead to the compounding of errors along a generated sequence at test time. To improve generalization in neural language models and address compounding errors, we propose \textit{Nearest-Neighbor Replacement Sampling} -- a curriculum learning-based method that gradually changes an initially deterministic teacher policy to a stochastic policy. A token at a given time-step is replaced with a sampled nearest neighbor of the past target with a truncated probability proportional to the cosine similarity between the original word and its top k most similar words. This allows the learner to explore alternatives when the current policy provided by the teacher is sub-optimal or difficult to learn from. The proposed method is straightforward, online and requires little additional memory requirements. We report our findings on two language modelling benchmarks and find that the proposed method further improves performance when used in conjunction with scheduled sampling.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1809.05916](https://arxiv.org/abs/1809.05916) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.09313](https://arxiv.org/abs/2101.09313) [cs.CL]** |
|           | (or **[arXiv:2101.09313v1](https://arxiv.org/abs/2101.09313v1) [cs.CL]** for this version) |





<h2 id="2021-01-26-3">3. Training Multilingual Pre-trained Language Model with Byte-level Subwords</h2>

Title: [Training Multilingual Pre-trained Language Model with Byte-level Subwords](https://arxiv.org/abs/2101.09469)

Authors: [Junqiu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Yinpeng Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X)

> The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. One of the fundamental components in pre-trained language models is the vocabulary, especially for training multilingual models on many different languages. In the technical report, we present our practices on training multilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte Pair Encoding). In the experiment, we adopted the architecture of NEZHA as the underlying pre-trained language model and the results show that NEZHA trained with byte-level subwords consistently outperforms Google multilingual BERT and vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release the source code of our byte-level vocabulary building tools and the multilingual pre-trained language models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.09469](https://arxiv.org/abs/2101.09469) [cs.CL]** |
|           | (or **[arXiv:2101.09469v1](https://arxiv.org/abs/2101.09469v1) [cs.CL]** for this version) |





<h2 id="2021-01-26-4">4. Debiasing Pre-trained Contextualised Embeddings</h2>

Title: [Debiasing Pre-trained Contextualised Embeddings](https://arxiv.org/abs/2101.09523)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Danushka Bollegala](https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D)

> In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.09523](https://arxiv.org/abs/2101.09523) [cs.CL]** |
|           | (or **[arXiv:2101.09523v1](https://arxiv.org/abs/2101.09523v1) [cs.CL]** for this version) |



<h2 id="2021-01-26-5">5. Dictionary-based Debiasing of Pre-trained Word Embeddings</h2>

Title: [Dictionary-based Debiasing of Pre-trained Word Embeddings](https://arxiv.org/abs/2101.09525)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Danushka Bollegala](https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D)

> Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases.
> In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner.
> We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used.
> Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words.
> Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it
> (a) retains the semantics of the pre-trained word embeddings,
> (b) agrees with the unbiased definition of the word according to the dictionary, and
> (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space.
> Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.09525](https://arxiv.org/abs/2101.09525) [cs.CL]** |
|           | (or **[arXiv:2101.09525v1](https://arxiv.org/abs/2101.09525v1) [cs.CL]** for this version) |





<h2 id="2021-01-26-6">6. RomeBERT: Robust Training of Multi-Exit BERT</h2>

Title: [RomeBERT: Robust Training of Multi-Exit BERT](https://arxiv.org/abs/2101.09755)

Authors: [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Zuohui Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z), [Yongfeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> BERT has achieved superior performances on Natural Language Understanding (NLU) tasks. However, BERT possesses a large number of parameters and demands certain resources to deploy. For acceleration, Dynamic Early Exiting for BERT (DeeBERT) has been proposed recently, which incorporates multiple exits and adopts a dynamic early-exit mechanism to ensure efficient inference. While obtaining an efficiency-performance tradeoff, the performances of early exits in multi-exit BERT are significantly worse than late exits. In this paper, we leverage gradient regularized self-distillation for RObust training of Multi-Exit BERT (RomeBERT), which can effectively solve the performance imbalance problem between early and late exits. Moreover, the proposed RomeBERT adopts a one-stage joint training strategy for multi-exits and the BERT backbone while DeeBERT needs two stages that require more training time. Extensive experiments on GLUE datasets are performed to demonstrate the superiority of our approach. Our code is available at [this https URL](https://github.com/romebert/RomeBERT).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.09755](https://arxiv.org/abs/2101.09755) [cs.CL]** |
|           | (or **[arXiv:2101.09755v1](https://arxiv.org/abs/2101.09755v1) [cs.CL]** for this version) |





<h2 id="2021-01-26-7">7. Cross-lingual Visual Pre-training for Multimodal Machine Translation</h2>

Title: [Cross-lingual Visual Pre-training for Multimodal Machine Translation](https://arxiv.org/abs/2101.10044)

Authors: [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Menekse Kuyu](https://arxiv.org/search/cs?searchtype=author&query=Kuyu%2C+M), [Mustafa Sercan Amac](https://arxiv.org/search/cs?searchtype=author&query=Amac%2C+M+S), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Erkut Erdem](https://arxiv.org/search/cs?searchtype=author&query=Erdem%2C+E), [Aykut Erdem](https://arxiv.org/search/cs?searchtype=author&query=Erdem%2C+A), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.

| Comments: | Accepted to EACL 2021 (Camera-ready version)                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2101.10044](https://arxiv.org/abs/2101.10044) [cs.CL]** |
|           | (or **[arXiv:2101.10044v1](https://arxiv.org/abs/2101.10044v1) [cs.CL]** for this version) |





<h2 id="2021-01-26-8">8. PAWLS: PDF Annotation With Labels and Structure</h2>

Title: [PAWLS: PDF Annotation With Labels and Structure](https://arxiv.org/abs/2101.10281)

Authors: [Mark Neumann](https://arxiv.org/search/cs?searchtype=author&query=Neumann%2C+M), [Zejiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Sam Skjonsberg](https://arxiv.org/search/cs?searchtype=author&query=Skjonsberg%2C+S)

> Adobe's Portable Document Format (PDF) is a popular way of distributing view-only documents with a rich visual markup. This presents a challenge to NLP practitioners who wish to use the information contained within PDF documents for training models or data analysis, because annotating these documents is difficult. In this paper, we present PDF Annotation with Labels and Structure (PAWLS), a new annotation tool designed specifically for the PDF document format. PAWLS is particularly suited for mixed-mode annotation and scenarios in which annotators require extended context to annotate accurately. PAWLS supports span-based textual annotation, N-ary relations and freeform, non-textual bounding boxes, all of which can be exported in convenient formats for training multi-modal machine learning models. A read-only PAWLS server is available at [this https URL](https://pawls.apps.allenai.org/) and the source code is available at [this https URL](https://github.com/allenai/pawls).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.10281](https://arxiv.org/abs/2101.10281) [cs.CL]** |
|           | (or **[arXiv:2101.10281v1](https://arxiv.org/abs/2101.10281v1) [cs.CL]** for this version) |













# 2021-01-25

[Return to Index](#Index)



<h2 id="2021-01-25-1">1. Enriching Non-Autoregressive Transformer with Syntactic and SemanticStructures for Neural Machine Translation</h2>

Title: [Enriching Non-Autoregressive Transformer with Syntactic and SemanticStructures for Neural Machine Translation](https://arxiv.org/abs/2101.08942)

Authors: [Ye Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yao Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Jian-Guo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Wenting Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S)

> The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in the existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structures of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several state-of-the-art non-autoregressive models.

| Comments: | 10 pages, Appear in EACL 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.08942](https://arxiv.org/abs/2101.08942) [cs.CL]** |
|           | (or **[arXiv:2101.08942v1](https://arxiv.org/abs/2101.08942v1) [cs.CL]** for this version) |





<h2 id="2021-01-25-2">2. Enhanced word embeddings using multi-semantic representation through lexical chains</h2>

Title: [Enhanced word embeddings using multi-semantic representation through lexical chains](https://arxiv.org/abs/2101.09023)

Authors: [Terry Ruas](https://arxiv.org/search/cs?searchtype=author&query=Ruas%2C+T), [Charles Henrique Porto Ferreira](https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+C+H+P), [William Grosky](https://arxiv.org/search/cs?searchtype=author&query=Grosky%2C+W), [Fabrício Olivetti de França](https://arxiv.org/search/cs?searchtype=author&query=de+França%2C+F+O), [Débora Maria Rossi Medeiros](https://arxiv.org/search/cs?searchtype=author&query=Medeiros%2C+D+M+R)

> The relationship between words in a sentence often tells us more about the underlying semantic content of a document than its actual words, individually. In this work, we propose two novel algorithms, called Flexible Lexical Chain II and Fixed Lexical Chain II. These algorithms combine the semantic relations derived from lexical chains, prior knowledge from lexical databases, and the robustness of the distributional hypothesis in word embeddings as building blocks forming a single system. In short, our approach has three main contributions: (i) a set of techniques that fully integrate word embeddings and lexical chains; (ii) a more robust semantic representation that considers the latent relation between words in a document; and (iii) lightweight word embeddings models that can be extended to any natural language task. We intend to assess the knowledge of pre-trained models to evaluate their robustness in the document classification task. The proposed techniques are tested against seven word embeddings algorithms using five different machine learning classifiers over six scenarios in the document classification task. Our results show the integration between lexical chains and word embeddings representations sustain state-of-the-art results, even against more complex systems.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Information Sciences. Volume 532, September 2020, Pages 16-32 |
| DOI:               | [10.1016/j.ins.2020.04.048](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1016%2Fj.ins.2020.04.048&v=2666f305) |
| Cite as:           | **[arXiv:2101.09023](https://arxiv.org/abs/2101.09023) [cs.CL]** |
|                    | (or **[arXiv:2101.09023v1](https://arxiv.org/abs/2101.09023v1) [cs.CL]** for this version) |





<h2 id="2021-01-25-3">3. Streaming Models for Joint Speech Recognition and Translation</h2>

Title: [Streaming Models for Joint Speech Recognition and Translation](https://arxiv.org/abs/2101.09149)

Authors: [Orion Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+O), [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Christian Gollan](https://arxiv.org/search/cs?searchtype=author&query=Gollan%2C+C), [Joris Kluivers](https://arxiv.org/search/cs?searchtype=author&query=Kluivers%2C+J)

> Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99% of consecutive quality at a lag of just under a second.

| Comments: | Camera Ready for EACL 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.09149](https://arxiv.org/abs/2101.09149) [cs.CL]** |
|           | (or **[arXiv:2101.09149v1](https://arxiv.org/abs/2101.09149v1) [cs.CL]** for this version) |





# 2021-01-22

[Return to Index](#Index)



<h2 id="2021-01-22-1">1. Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval</h2>

Title: [Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval](https://arxiv.org/abs/2101.08370)

Authors: [Robert Litschko](https://arxiv.org/search/cs?searchtype=author&query=Litschko%2C+R), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Simone Paolo Ponzetto](https://arxiv.org/search/cs?searchtype=author&query=Ponzetto%2C+S+P), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G)

> Pretrained multilingual text encoders based on neural Transformer architectures, such as multilingual BERT (mBERT) and XLM, have achieved strong performance on a myriad of language understanding tasks. Consequently, they have been adopted as a go-to paradigm for multilingual and cross-lingual representation learning and transfer, rendering cross-lingual word embeddings (CLWEs) effectively obsolete. However, questions remain to which extent this finding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual IR (CLIR) tasks. Therefore, in this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a large number of language pairs. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR -- a setup with no relevance judgments for IR-specific fine-tuning -- pretrained encoders fail to significantly outperform models based on CLWEs. For sentence-level CLIR, we demonstrate that state-of-the-art performance can be achieved. However, the peak performance is not met using the general-purpose multilingual text encoders `off-the-shelf', but rather relying on their variants that have been further specialized for sentence understanding tasks.

| Comments:    | accepted at ECIR'21 (preprint)                               |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| ACM classes: | H.3.3; I.2.7                                                 |
| Cite as:     | **[arXiv:2101.08370](https://arxiv.org/abs/2101.08370) [cs.CL]** |
|              | (or **[arXiv:2101.08370v1](https://arxiv.org/abs/2101.08370v1) [cs.CL]** for this version) |





<h2 id="2021-01-22-2">2. Adv-OLM: Generating Textual Adversaries via OLM</h2>

Title: [Adv-OLM: Generating Textual Adversaries via OLM](https://arxiv.org/abs/2101.08523)

Authors: [Vijit Malik](https://arxiv.org/search/cs?searchtype=author&query=Malik%2C+V), [Ashwani Bhat](https://arxiv.org/search/cs?searchtype=author&query=Bhat%2C+A), [Ashutosh Modi](https://arxiv.org/search/cs?searchtype=author&query=Modi%2C+A)

> Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.

| Comments: | 5 Pages + 1 Page references + 3 Pages Appendix, Accepted at EACL 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.08523](https://arxiv.org/abs/2101.08523) [cs.CL]** |
|           | (or **[arXiv:2101.08523v1](https://arxiv.org/abs/2101.08523v1) [cs.CL]** for this version) |



# 2021-01-21

[Return to Index](#Index)



<h2 id="2021-01-21-1">1. Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation</h2>

Title: [Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation](https://arxiv.org/abs/2101.08106)

Authors:[Lingyun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+L), [Minghui Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M), [Yaliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Ying Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y)

> Despite pre-trained language models such as BERT have achieved appealing performance in a wide range of natural language processing tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment for data-scarce domain BERT knowledge distillation, by learning a cross-domain manipulation scheme that automatically augments the target with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced selector to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on four different tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ∼13.3%) when only a few labeled examples available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.08106](https://arxiv.org/abs/2101.08106) [cs.CL]** |
|           | (or **[arXiv:2101.08106v1](https://arxiv.org/abs/2101.08106v1) [cs.CL]** for this version) |





<h2 id="2021-01-21-2">2. Word Alignment by Fine-tuning Embeddings on Parallel Corpora</h2>

Title: [Word Alignment by Fine-tuning Embeddings on Parallel Corpora](https://arxiv.org/abs/2101.08231)

Authors:[Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel texts. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWESOME (Aligning Word Embedding Spaces of Multilingual Encoders), with pre-trained models is available at [this https URL](https://github.com/neulab/awesome-align)

| Comments: | EACL 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.08231](https://arxiv.org/abs/2101.08231) [cs.CL]** |
|           | (or **[arXiv:2101.08231v1](https://arxiv.org/abs/2101.08231v1) [cs.CL]** for this version) |





<h2 id="2021-01-21-3">3. Generating (Formulaic) Text by Splicing Together Nearest Neighbors</h2>

Title: [Generating (Formulaic) Text by Splicing Together Nearest Neighbors](https://arxiv.org/abs/2101.08248)

Authors:[Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Arturs Backurs](https://arxiv.org/search/cs?searchtype=author&query=Backurs%2C+A), [Karl Stratos](https://arxiv.org/search/cs?searchtype=author&query=Stratos%2C+K)

> We propose to tackle conditional text generation tasks, especially those which require generating formulaic text, by splicing together segments of text from retrieved "neighbor" source-target pairs. Unlike recent work that conditions on retrieved neighbors in an encoder-decoder setting but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text (i.e., by inserting or replacing them) to form an output. Standard techniques for training such a policy require an oracle derivation for each generation, and we prove that finding the shortest such derivation can be reduced to parsing under a particular weighted context-free grammar. We find that policies learned in this way allow for interpretable table-to-text and headline generation that is competitive with or better than state-of-the-art autoregressive token-level policies in terms of automatic metrics, and moreover allows for faster decoding.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.08248](https://arxiv.org/abs/2101.08248) [cs.CL]** |
|           | (or **[arXiv:2101.08248v1](https://arxiv.org/abs/2101.08248v1) [cs.CL]** for this version) |





# 2021-01-20

[Return to Index](#Index)



<h2 id="2021-01-20-1">1. ArtEmis: Affective Language for Visual Art</h2>

Title: [ArtEmis: Affective Language for Visual Art](https://arxiv.org/abs/2101.07396)

Authors: [Panos Achlioptas](https://arxiv.org/search/cs?searchtype=author&query=Achlioptas%2C+P), [Maks Ovsjanikov](https://arxiv.org/search/cs?searchtype=author&query=Ovsjanikov%2C+M), [Kilichbek Haydarov](https://arxiv.org/search/cs?searchtype=author&query=Haydarov%2C+K), [Mohamed Elhoseiny](https://arxiv.org/search/cs?searchtype=author&query=Elhoseiny%2C+M), [Leonidas Guibas](https://arxiv.org/search/cs?searchtype=author&query=Guibas%2C+L)

> We present a novel large-scale dataset and accompanying machine learning models aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, we focus on the affective experience triggered by visual artworks and ask the annotators to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. As we demonstrate below, this leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., "freedom" or "love"), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. We focus on visual art (e.g., paintings, artistic photographs) as it is a prime example of imagery created to elicit emotional responses from its viewers. Our dataset, termed ArtEmis, contains 439K emotion attributions and explanations from humans, on 81K artworks from WikiArt. Building on this data, we train and demonstrate a series of captioning systems capable of expressing and explaining emotions from visual stimuli. Remarkably, the captions produced by these systems often succeed in reflecting the semantic and abstract content of the image, going well beyond systems trained on existing datasets. The collected dataset and developed methods are available at [this https URL](https://artemisdataset.org/).

| Comments: | [this https URL](https://artemisdataset.org/)                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2101.07396](https://arxiv.org/abs/2101.07396) [cs.CV]** |
|           | (or **[arXiv:2101.07396v1](https://arxiv.org/abs/2101.07396v1) [cs.CV]** for this version) |









# 2021-01-19

[Return to Index](#Index)



<h2 id="2021-01-19-1">1. Latent Variable Models for Visual Question Answering</h2>

Title: [Latent Variable Models for Visual Question Answering](https://arxiv.org/abs/2101.06399)

Authors: [Zixu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yishu Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Conventional models for Visual Question Answering (VQA) explore deterministic approaches with various types of image features, question features, and attention mechanisms. However, there exist other modalities that can be explored in addition to image and question pairs to bring extra information to the models. In this work, we propose latent variable models for VQA where extra information (e.g. captions and answer categories) are incorporated as latent variables to improve inference, which in turn benefits question-answering performance. Experiments on the VQA v2.0 benchmarking dataset demonstrate the effectiveness of our proposed models in that they improve over strong baselines, especially those that do not rely on extensive language-vision pre-training.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.06399](https://arxiv.org/abs/2101.06399) [cs.CV]** |
|           | (or **[arXiv:2101.06399v1](https://arxiv.org/abs/2101.06399v1) [cs.CV]** for this version) |





<h2 id="2021-01-19-2">2. TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search</h2>

Title: [TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search](https://arxiv.org/abs/2101.06323)

Authors: [Jason Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Yanling Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Yuming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Hao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Xue Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Markus Pelger](https://arxiv.org/search/cs?searchtype=author&query=Pelger%2C+M), [Liangjie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Tianqi Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+T), [Ruofei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Huasha Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Text encoders based on C-DSSM or transformers have demonstrated strong performance in many Natural Language Processing (NLP) tasks. Low latency variants of these models have also been developed in recent years in order to apply them in the field of sponsored search which has strict computational constraints. However these models are not the panacea to solve all the Natural Language Understanding (NLU) challenges as the pure semantic information in the data is not sufficient to fully identify the user intents. We propose the TextGNN model that naturally extends the strong twin tower structured encoders with the complementary graph information from user historical behaviors, which serves as a natural guide to help us better understand the intents and hence generate better language representations. The model inherits all the benefits of twin tower models such as C-DSSM and TwinBERT so that it can still be used in the low latency environment while achieving a significant performance gain than the strong encoder-only counterpart baseline models in both offline evaluations and online production system. In offline experiments, the model achieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for long-tail low-frequency Ads, and in the online A/B testing, the model shows a 2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.

| Comments: | To appear in the The Web Conference 2021 Conference          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.06323](https://arxiv.org/abs/2101.06323) [cs.CL]** |
|           | (or **[arXiv:2101.06323v1](https://arxiv.org/abs/2101.06323v1) [cs.CL]** for this version) |





<h2 id="2021-01-19-3">3. To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph</h2>

Title: [To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph](https://arxiv.org/abs/2101.06397)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R)

> In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called n-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.

| Comments: | arXiv admin note: text overlap with [arXiv:2009.07489](https://arxiv.org/abs/2009.07489) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.06397](https://arxiv.org/abs/2101.06397) [cs.CL]** |
|           | (or **[arXiv:2101.06397v1](https://arxiv.org/abs/2101.06397v1) [cs.CL]** for this version) |





<h2 id="2021-01-19-4">4. GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation</h2>

Title: [GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation](https://arxiv.org/abs/2101.06561)

Authors: [Daniel Khashabi](https://arxiv.org/search/cs?searchtype=author&query=Khashabi%2C+D), [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G), [Jonathan Bragg](https://arxiv.org/search/cs?searchtype=author&query=Bragg%2C+J), [Nicholas Lourie](https://arxiv.org/search/cs?searchtype=author&query=Lourie%2C+N), [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S)

> Leaderboards have eased model development for many NLP datasets by standardizing their evaluation and delegating it to an independent external repository. Their adoption, however, is so far limited to tasks that can be reliably evaluated in an automatic manner. This work introduces GENIE, an extensible human evaluation leaderboard, which brings the ease of leaderboards to text generation tasks. GENIE automatically posts leaderboard submissions to crowdsourcing platforms asking human annotators to evaluate them on various axes (e.g., correctness, conciseness, fluency) and compares their answers to various automatic metrics. We introduce several datasets in English to GENIE, representing four core challenges in text generation: machine translation, summarization, commonsense reasoning, and machine comprehension. We provide formal granular evaluation metrics and identify areas for future research. We make GENIE publicly available and hope that it will spur progress in language generation models as well as their automatic and manual evaluation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.06561](https://arxiv.org/abs/2101.06561) [cs.CL]** |
|           | (or **[arXiv:2101.06561v1](https://arxiv.org/abs/2101.06561v1) [cs.CL]** for this version) |





<h2 id="2021-01-19-5">5. What Makes Good In-Context Examples for GPT-3?</h2>

Title: [What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/abs/2101.06804)

Authors: [Jiachang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Dinghan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Bill Dolan](https://arxiv.org/search/cs?searchtype=author&query=Dolan%2C+B), [Lawrence Carin](https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its powerful and versatile in-context few-shot learning ability. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's few-shot capabilities. Inspired by the recent success of leveraging a retrieval module to augment large-scale neural network models, we propose to retrieve examples that are semantically-similar to a test sample to formulate its corresponding prompt. Intuitively, the in-context examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's extensive knowledge. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (41.9% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset). We hope our investigation could help understand the behaviors of GPT-3 and large-scale pre-trained LMs in general and enhance their few-shot capabilities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.06804](https://arxiv.org/abs/2101.06804) [cs.CL]** |
|           | (or **[arXiv:2101.06804v1](https://arxiv.org/abs/2101.06804v1) [cs.CL]** for this version) |





<h2 id="2021-01-19-6">6. Can a Fruit Fly Learn Word Embeddings?</h2>

Title: [Can a Fruit Fly Learn Word Embeddings?](https://arxiv.org/abs/2101.06887)

Authors: [Yuchen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Chaitanya K. Ryali](https://arxiv.org/search/cs?searchtype=author&query=Ryali%2C+C+K), [Benjamin Hoover](https://arxiv.org/search/cs?searchtype=author&query=Hoover%2C+B), [Leopold Grinberg](https://arxiv.org/search/cs?searchtype=author&query=Grinberg%2C+L), [Saket Navlakha](https://arxiv.org/search/cs?searchtype=author&query=Navlakha%2C+S), [Mohammed J. Zaki](https://arxiv.org/search/cs?searchtype=author&query=Zaki%2C+M+J), [Dmitry Krotov](https://arxiv.org/search/cs?searchtype=author&query=Krotov%2C+D)

> The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory modalities. These cells are inhibited by the anterior paired lateral neuron, thus creating a sparse high dimensional representation of the inputs. In this work we study a mathematical formalization of this network motif and apply it to learning the correlational structure between words and their context in a corpus of unstructured text, a common natural language processing (NLP) task. We show that this network can learn semantic representations of words and can generate both static and context-dependent word embeddings. Unlike conventional methods (e.g., BERT, GloVe) that use dense representations for word embedding, our algorithm encodes semantic meaning of words and their context in the form of sparse binary hash codes. The quality of the learned representations is evaluated on word similarity analysis, word-sense disambiguation, and document classification. It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint).

| Comments: | Accepted for publication at ICLR 2021                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.06887](https://arxiv.org/abs/2101.06887) [cs.CL]** |
|           | (or **[arXiv:2101.06887v1](https://arxiv.org/abs/2101.06887v1) [cs.CL]** for this version) |









# 2021-01-18

[Return to Index](#Index)



<h2 id="2021-01-18-1">1. Knowledge Graphs and Natural-Language Processing</h2>

Title: [Knowledge Graphs and Natural-Language Processing](https://arxiv.org/abs/2101.06111)

Authors: [Andreas L Opdahl](https://arxiv.org/search/cs?searchtype=author&query=Opdahl%2C+A+L)

> Emergency-relevant data comes in many varieties. It can be high volume and high velocity, and reaction times are critical, calling for efficient and powerful techniques for data analysis and management. Knowledge graphs represent data in a rich, flexible, and uniform way that is well matched with the needs of emergency management. They build on existing standards, resources, techniques, and tools for semantic data and computing. This chapter explains the most important semantic technologies and how they support knowledge graphs. We proceed to discuss their benefits and challenges and give examples of relevant semantic data sources and vocabularies. Natural-language texts -- in particular those collected from social media such as Twitter -- is a type of data source that poses particular analysis challenges. We therefore include an overview of techniques for processing natural-language texts.

| Comments: | In Big Data in Emergency Management: Exploitation Techniques for Social and Mobile Data (pp. 75-91). Springer, Cham |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Computation and Language (cs.CL) |
| DOI:      | [10.1007/978-3-030-48099-8](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-3-030-48099-8&v=de8d3b97) |
| Cite as:  | **[arXiv:2101.06111](https://arxiv.org/abs/2101.06111) [cs.CY]** |
|           | (or **[arXiv:2101.06111v1](https://arxiv.org/abs/2101.06111v1) [cs.CY]** for this version) |





<h2 id="2021-01-18-2">2. The Impact of Post-editing and Machine Translation on Creativity and Reading Experience</h2>

Title: [The Impact of Post-editing and Machine Translation on Creativity and Reading Experience](https://arxiv.org/abs/2101.06125)

Authors: [Ana Guerberof Arenas](https://arxiv.org/search/cs?searchtype=author&query=Arenas%2C+A+G), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

> This article presents the results of a study involving the translation of a fictional story from English into Catalan in three modalities: machine-translated (MT), post-edited (MTPE) and translated without aid (HT). Each translation was analysed to evaluate its creativity. Subsequently, a cohort of 88 Catalan participants read the story in a randomly assigned modality and completed a survey. The results show that HT presented a higher creativity score if compared to MTPE and MT. HT also ranked higher in narrative engagement, and translation reception, while MTPE ranked marginally higher in enjoyment. HT and MTPE show no statistically significant differences in any category, whereas MT does in all variables tested. We conclude that creativity is highest when professional translators intervene in the process, especially when working without any aid. We hypothesize that creativity in translation could be the factor that enhances reading engagement and the reception of translated literary texts.

| Comments: | 28 pages, 10 tables, 4 figures. Translation Spaces (2020)    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1075/ts.20035.gue](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1075%2Fts.20035.gue&v=1b7ea813) |
| Cite as:  | **[arXiv:2101.06125](https://arxiv.org/abs/2101.06125) [cs.CL]** |
|           | (or **[arXiv:2101.06125v1](https://arxiv.org/abs/2101.06125v1) [cs.CL]** for this version) |





<h2 id="2021-01-18-3">3. Empirical Evaluation of Supervision Signals for Style Transfer Models</h2>

Title: [Empirical Evaluation of Supervision Signals for Style Transfer Models](https://arxiv.org/abs/2101.06172)

Authors: [Yevgeniy Puzikov](https://arxiv.org/search/cs?searchtype=author&query=Puzikov%2C+Y), [Simoes Stanley](https://arxiv.org/search/cs?searchtype=author&query=Stanley%2C+S), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I), [Immanuel Schweizer](https://arxiv.org/search/cs?searchtype=author&query=Schweizer%2C+I)

> Text style transfer has gained increasing attention from the research community over the recent years. However, the proposed approaches vary in many ways, which makes it hard to assess the individual contribution of the model components. In style transfer, the most important component is the optimization technique used to guide the learning in the absence of parallel training data. In this work we empirically compare the dominant optimization paradigms which provide supervision signals during training: backtranslation, adversarial training and reinforcement learning. We find that backtranslation has model-specific limitations, which inhibits training style transfer models. Reinforcement learning shows the best performance gains, while adversarial training, despite its popularity, does not offer an advantage over the latter alternative. In this work we also experiment with Minimum Risk Training, a popular technique in the machine translation community, which, to our knowledge, has not been empirically evaluated in the task of style transfer. We fill this research gap and empirically show its efficacy.

| Comments:    | 13 pages, 6 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2101.06172](https://arxiv.org/abs/2101.06172) [cs.CL]** |
|              | (or **[arXiv:2101.06172v1](https://arxiv.org/abs/2101.06172v1) [cs.CL]** for this version) |









# 2021-01-15

[Return to Index](#Index)



<h2 id="2021-01-15-1">1. Structured Prediction as Translation between Augmented Natural Languages</h2>

Title: [Structured Prediction as Translation between Augmented Natural Languages](https://arxiv.org/abs/2101.05779)

Authors:[Giovanni Paolini](https://arxiv.org/search/cs?searchtype=author&query=Paolini%2C+G), [Ben Athiwaratkun](https://arxiv.org/search/cs?searchtype=author&query=Athiwaratkun%2C+B), [Jason Krone](https://arxiv.org/search/cs?searchtype=author&query=Krone%2C+J), [Jie Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J), [Alessandro Achille](https://arxiv.org/search/cs?searchtype=author&query=Achille%2C+A), [Rishita Anubhai](https://arxiv.org/search/cs?searchtype=author&query=Anubhai%2C+R), [Cicero Nogueira dos Santos](https://arxiv.org/search/cs?searchtype=author&query=Santos%2C+C+N+d), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B), [Stefano Soatto](https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S)

> We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.05779](https://arxiv.org/abs/2101.05779) [cs.LG]** |
|           | (or **[arXiv:2101.05779v1](https://arxiv.org/abs/2101.05779v1) [cs.LG]** for this version) |





<h2 id="2021-01-15-2">2. Text Augmentation in a Multi-Task View</h2>

Title: [Text Augmentation in a Multi-Task View](https://arxiv.org/abs/2101.05469)

Authors:[Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Chengyu Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Shiqi Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training. In this paper, we propose an alternative perspective -- a multi-task view (MTV) of data augmentation -- in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger levels of augmentation. In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that the MTV leads to higher and more robust performance improvements than traditional augmentation.

| Comments: | Accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.05469](https://arxiv.org/abs/2101.05469) [cs.CL]** |
|           | (or **[arXiv:2101.05469v1](https://arxiv.org/abs/2101.05469v1) [cs.CL]** for this version) |





<h2 id="2021-01-15-3">3. Persuasive Natural Language Generation -- A Literature Review</h2>

Title: [Persuasive Natural Language Generation -- A Literature Review](https://arxiv.org/abs/2101.05786)

Authors:[Sebastian Duerr](https://arxiv.org/search/cs?searchtype=author&query=Duerr%2C+S), [Peter A. Gloor](https://arxiv.org/search/cs?searchtype=author&query=Gloor%2C+P+A)

> This literature review focuses on the use of Natural Language Generation (NLG) to automatically detect and generate persuasive texts. Extending previous research on automatic identification of persuasion in text, we concentrate on generative aspects through conceptualizing determinants of persuasion in five business-focused categories: benevolence, linguistic appropriacy, logical argumentation, trustworthiness, tools and datasets. These allow NLG to increase an existing message's persuasiveness. Previous research illustrates key aspects in each of the above mentioned five categories. A research agenda to further study persuasive NLG is developed. The review includes analysis of seventy-seven articles, outlining the existing body of knowledge and showing the steady progress in this research field.

| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7; J.4                                                   |
| Cite as:     | **[arXiv:2101.05786](https://arxiv.org/abs/2101.05786) [cs.CL]** |
|              | (or **[arXiv:2101.05786v1](https://arxiv.org/abs/2101.05786v1) [cs.CL]** for this version) |





# 2021-01-14

[Return to Index](#Index)



<h2 id="2021-01-14-1">1. Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding</h2>

Title: [Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding](https://arxiv.org/abs/2101.05208)

Authors: [Dexin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

> Visual context provides grounding information for multimodal machine translation (MMT). However, previous MMT models and probing studies on visual features suggest that visual information is less explored in MMT as it is often redundant to textual information. In this paper, we propose an object-level visual context modeling framework (OVC) to efficiently capture and explore visual information for multimodal machine translation. With detected objects, the proposed OVC encourages MMT to ground translation on desirable visual objects by masking irrelevant objects in the visual modality. We equip the proposed with an additional object-masking loss to achieve this goal. The object-masking loss is estimated according to the similarity between masked objects and the source texts so as to encourage masking source-irrelevant objects. Additionally, in order to generate vision-consistent target words, we further propose a vision-weighted translation loss for OVC. Experiments on MMT datasets demonstrate that the proposed OVC model outperforms state-of-the-art MMT models and analyses show that masking irrelevant objects helps grounding in MMT.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.05208](https://arxiv.org/abs/2101.05208) [cs.CV]** |
|           | (or **[arXiv:2101.05208v1](https://arxiv.org/abs/2101.05208v1) [cs.CV]** for this version) |





<h2 id="2021-01-14-2">2. Latent Alignment of Procedural Concepts in Multimodal Recipes</h2>

Title: [Latent Alignment of Procedural Concepts in Multimodal Recipes](https://arxiv.org/abs/2101.04727)

Authors: [Hossein Rajaby Faghihi](https://arxiv.org/search/cs?searchtype=author&query=Faghihi%2C+H+R), [Roshanak Mirzaee](https://arxiv.org/search/cs?searchtype=author&query=Mirzaee%2C+R), [Sudarshan Paliwal](https://arxiv.org/search/cs?searchtype=author&query=Paliwal%2C+S), [Parisa Kordjamshidi](https://arxiv.org/search/cs?searchtype=author&query=Kordjamshidi%2C+P)

> We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a reading comprehension on a recipe containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max-pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19\% improvement over the baselines.

| Comments:          | Published in ALVR 2020, a workshop in ACL 2020               |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Proceedings of the First Workshop on Advances in Language and Vision Research 2020 (26-31) |
| DOI:               | [10.18653/v1/2020.alvr-1.5](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2020.alvr-1.5&v=43bac969) |
| Cite as:           | **[arXiv:2101.04727](https://arxiv.org/abs/2101.04727) [cs.CL]** |
|                    | (or **[arXiv:2101.04727v1](https://arxiv.org/abs/2101.04727v1) [cs.CL]** for this version) |







<h2 id="2021-01-14-3">3. Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration</h2>

Title: [Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration](https://arxiv.org/abs/2101.05162)

Authors: [B. Mansurov](https://arxiv.org/search/cs?searchtype=author&query=Mansurov%2C+B), [A. Mansurov](https://arxiv.org/search/cs?searchtype=author&query=Mansurov%2C+A)

> In this paper, we introduce a data-driven approach to transliterating Uzbek dictionary words from the Cyrillic script into the Latin script, and vice versa. We heuristically align characters of words in the source script with sub-strings of the corresponding words in the target script and train a decision tree classifier that learns these alignments. On the test set, our Cyrillic to Latin model achieves a character level micro-averaged F1 score of 0.9992, and our Latin to Cyrillic model achieves the score of 0.9959. Our contribution is a novel method of producing machine transliterated texts for the low-resource Uzbek language.

| Comments: | 9 pages, 11 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.05162](https://arxiv.org/abs/2101.05162) [cs.CL]** |
|           | (or **[arXiv:2101.05162v1](https://arxiv.org/abs/2101.05162v1) [cs.CL]** for this version) |









# 2021-01-13

[Return to Index](#Index)



<h2 id="2021-01-13-1">1. Quantum Mathematics in Artificial Intelligence</h2>

Title: [Quantum Mathematics in Artificial Intelligence](https://arxiv.org/abs/2101.04255)

Authors:[Dominic Widdows](https://arxiv.org/search/cs?searchtype=author&query=Widdows%2C+D), [Kirsty Kitto](https://arxiv.org/search/cs?searchtype=author&query=Kitto%2C+K), [Trevor Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+T)

> In the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. At the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news.
> The mathematical techniques underlying both these areas have more in common than is sometimes realized. Vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. Quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks.
> This paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (AI), particularly in automated reasoning and natural language processing (NLP). Techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. Application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition.
> Some of these approaches can potentially be implemented on quantum hardware. Many of the practical steps in this implementation are in early stages, and some are already realized. Explaining some of the common mathematical tools can help researchers in both AI and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.04255](https://arxiv.org/abs/2101.04255) [cs.AI]** |
|           | (or **[arXiv:2101.04255v1](https://arxiv.org/abs/2101.04255v1) [cs.AI]** for this version) |





<h2 id="2021-01-13-2">2. Explain and Predict, and then Predict again</h2>

Title: [Explain and Predict, and then Predict again](https://arxiv.org/abs/2101.04109)

Authors:[Zijian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Koustav Rudra](https://arxiv.org/search/cs?searchtype=author&query=Rudra%2C+K), [Avishek Anand](https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+A)

> A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, that uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. And then we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- fact verification, sentiment classification, and QA -- and find that we substantially outperform existing approaches.

| Comments:    | Accepted in the WSDM 2021 and the camera-ready version will be there soon |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.m; I.2.7                                                 |
| DOI:         | [10.1145/3437963.3441758](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3437963.3441758&v=6e4daadf) |
| Cite as:     | **[arXiv:2101.04109](https://arxiv.org/abs/2101.04109) [cs.CL]** |
|              | (or **[arXiv:2101.04109v1](https://arxiv.org/abs/2101.04109v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-3">3. Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning</h2>

Title: [Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning](https://arxiv.org/abs/2101.04229)

Authors:[Evgeny Lagutin](https://arxiv.org/search/cs?searchtype=author&query=Lagutin%2C+E), [Daniil Gavrilov](https://arxiv.org/search/cs?searchtype=author&query=Gavrilov%2C+D), [Pavel Kalaidin](https://arxiv.org/search/cs?searchtype=author&query=Kalaidin%2C+P)

> Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019). Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li at al., 2020). In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.

| Comments: | accepted to EACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.04229](https://arxiv.org/abs/2101.04229) [cs.CL]** |
|           | (or **[arXiv:2101.04229v1](https://arxiv.org/abs/2101.04229v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-4">4. Transforming Multi-Conditioned Generation from Meaning Representation</h2>

Title: [Transforming Multi-Conditioned Generation from Meaning Representation](https://arxiv.org/abs/2101.04257)

Authors:[Joosung Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J)

> In task-oriented conversation systems, natural language generation systems that generate sentences with specific information related to conversation flow are useful. Our study focuses on language generation by considering various information representing the meaning of utterances as multiple conditions of generation. NLG from meaning representations, the conditions for sentence meaning, generally goes through two steps: sentence planning and surface realization. However, we propose a simple one-stage framework to generate utterances directly from MR (Meaning Representation). Our model is based on GPT2 and generates utterances with flat conditions on slot and value pairs, which does not need to determine the structure of the sentence. We evaluate several systems in the E2E dataset with 6 automatic metrics. Our system is a simple method, but it demonstrates comparable performance to previous systems in automated metrics. In addition, using only 10\% of the data set without any other techniques, our model achieves comparable performance, and shows the possibility of performing zero-shot generation and expanding to other datasets.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.04257](https://arxiv.org/abs/2101.04257) [cs.CL]** |
|           | (or **[arXiv:2101.04257v1](https://arxiv.org/abs/2101.04257v1) [cs.CL]** for this version) |





<h2 id="2021-01-13-5">5. Toward Effective Automated Content Analysis via Crowdsourcing</h2>

Title: [Toward Effective Automated Content Analysis via Crowdsourcing](https://arxiv.org/abs/2101.04615)

Authors:[Jiele Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Chau-Wai Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+C), [Xinyan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X), [Xianpeng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X)

> Many computer scientists use the aggregated answers of online workers to represent ground truth. Prior work has shown that aggregation methods such as majority voting are effective for measuring relatively objective features. For subjective features such as semantic connotation, online workers, known for optimizing their hourly earnings, tend to deteriorate in the quality of their responses as they work longer. In this paper, we aim to address this issue by proposing a quality-aware semantic data annotation system. We observe that with timely feedback on workers' performance quantified by quality scores, better informed online workers can maintain the quality of their labeling throughout an extended period of time. We validate the effectiveness of the proposed annotation system through i) evaluating performance based on an expert-labeled dataset, and ii) demonstrating machine learning tasks that can lead to consistent learning behavior with 70%-80% accuracy. Our results suggest that with our system, researchers can collect high-quality answers of subjective semantic features at a large scale.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.04615](https://arxiv.org/abs/2101.04615) [cs.CL]** |
|           | (or **[arXiv:2101.04615v1](https://arxiv.org/abs/2101.04615v1) [cs.CL]** for this version) |





# 2021-01-12

[Return to Index](#Index)



<h2 id="2021-01-12-1">1. Misspelling Correction with Pre-trained Contextual Language Model</h2>

Title: [Misspelling Correction with Pre-trained Contextual Language Model](https://arxiv.org/abs/2101.03204)

Authors: [Yifei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Xiaonan Jing](https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+X), [Youlim Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+Y), [Julia Taylor Rayz](https://arxiv.org/search/cs?searchtype=author&query=Rayz%2C+J+T)

> Spelling irregularities, known now as spelling mistakes, have been found for several centuries. As humans, we are able to understand most of the misspelled words based on their location in the sentence, perceived pronunciation, and context. Unlike humans, computer systems do not possess the convenient auto complete functionality of which human brains are capable. While many programs provide spelling correction functionality, many systems do not take context into account. Moreover, Artificial Intelligence systems function in the way they are trained on. With many current Natural Language Processing (NLP) systems trained on grammatically correct text data, many are vulnerable against adversarial examples, yet correctly spelled text processing is crucial for learning. In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model BERT. We present two experiments, based on BERT and the edit distance algorithm, for ranking and selecting candidate corrections. The results of our experiments demonstrated that when combined properly, contextual word embeddings of BERT and edit distance are capable of effectively correcting spelling errors.

| Comments: | Accepted by 2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC). IEEE |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.03204](https://arxiv.org/abs/2101.03204) [cs.CL]** |
|           | (or **[arXiv:2101.03204v1](https://arxiv.org/abs/2101.03204v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-2">2. SDA: Improving Text Generation with Self Data Augmentation</h2>

Title: [SDA: Improving Text Generation with Self Data Augmentation](https://arxiv.org/abs/2101.03236)

Authors: [Ping Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P), [Ruiyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yizhe Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Chunyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> Data augmentation has been widely used to improve deep neural networks in many research fields, such as computer vision. However, less work has been done in the context of text, partially due to its discrete nature and the complexity of natural languages. In this paper, we propose to improve the standard maximum likelihood estimation (MLE) paradigm by incorporating a self-imitation-learning phase for automatic data augmentation. Unlike most existing sentence-level augmentation strategies, which are only applied to specific models, our method is more general and could be easily adapted to any MLE-based training procedure. In addition, our framework allows task-specific evaluation metrics to be designed to flexibly control the generated sentences, for example, in terms of controlling vocabulary usage and avoiding nontrivial repetitions. Extensive experimental results demonstrate the superiority of our method on two synthetic and several standard real datasets, significantly improving related baselines.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03236](https://arxiv.org/abs/2101.03236) [cs.CL]** |
|           | (or **[arXiv:2101.03236v1](https://arxiv.org/abs/2101.03236v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-3">3. Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing</h2>

Title: [Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing](https://arxiv.org/abs/2101.03289)

Authors: [Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M), [Viet Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V), [Amir Pouran Ben Veyseh](https://arxiv.org/search/cs?searchtype=author&query=Veyseh%2C+A+P+B), [Thien Huu Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H)

> We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: [this https URL](https://github.com/nlp-uoregon/trankit). A demo website for our toolkit is also available at: [this http URL](http://nlp.uoregon.edu/trankit).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03289](https://arxiv.org/abs/2101.03289) [cs.CL]** |
|           | (or **[arXiv:2101.03289v1](https://arxiv.org/abs/2101.03289v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-4">4. Learning Better Sentence Representation with Syntax Information</h2>

Title: [Learning Better Sentence Representation with Syntax Information](https://arxiv.org/abs/2101.03343)

Authors: [Chen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C) (University of Science and Technology of China)

> Sentence semantic understanding is a key topic in the field of natural language processing. Recently, contextualized word representations derived from pre-trained language models such as ELMO and BERT have shown significant improvements for a wide range of semantic tasks, e.g. question answering, text classification and sentiment analysis. However, how to add external knowledge to further improve the semantic modeling capability of model is worth probing. In this paper, we propose a novel approach to combining syntax information with a pre-trained language model. In order to evaluate the effect of the pre-training model, first, we introduce RNN-based and Transformer-based pre-trained language models; secondly, to better integrate external knowledge, such as syntactic information integrate with the pre-training model, we propose a dependency syntax expansion (DSE) model. For evaluation, we have selected two subtasks: sentence completion task and biological relation extraction task. The experimental results show that our model achieves 91.2\% accuracy, outperforming the baseline model by 37.8\% on sentence completion task. And it also gets competitive performance by 75.1\% F1 score on relation extraction task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03343](https://arxiv.org/abs/2101.03343) [cs.CL]** |
|           | (or **[arXiv:2101.03343v1](https://arxiv.org/abs/2101.03343v1) [cs.CL]** for this version) |





<h2 id="2021-01-12-5">5. Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation</h2>

Title: [Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation](https://arxiv.org/abs/2101.04030)

Authors: [Ritam Mallick](https://arxiv.org/search/cs?searchtype=author&query=Mallick%2C+R), [Seba Susan](https://arxiv.org/search/cs?searchtype=author&query=Susan%2C+S), [Vaibhaw Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+V), [Rizul Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+R), [Prateek Rawal](https://arxiv.org/search/cs?searchtype=author&query=Rawal%2C+P)

> Neural Machine Translation model is a sequence-to-sequence converter based on neural networks. Existing models use recurrent neural networks to construct both the encoder and decoder modules. In alternative research, the recurrent networks were substituted by convolutional neural networks for capturing the syntactic structure in the input sentence and decreasing the processing time. We incorporate the goodness of both approaches by proposing a convolutional-recurrent encoder for capturing the context information as well as the sequential information from the source sentence. Word embedding and position embedding of the source sentence is performed prior to the convolutional encoding layer which is basically a n-gram feature extractor capturing phrase-level context information. The rectified output of the convolutional encoding layer is added to the original embedding vector, and the sum is normalized by layer normalization. The normalized output is given as a sequential input to the recurrent encoding layer that captures the temporal information in the sequence. For the decoder, we use the attention-based recurrent neural network. Translation task on the German-English dataset verifies the efficacy of the proposed approach from the higher BLEU scores achieved as compared to the state of the art.

| Comments: | Accepted in 36th ACM/SIGAPP Symposium On Applied Computing 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.04030](https://arxiv.org/abs/2101.04030) [cs.CL]** |
|           | (or **[arXiv:2101.04030v1](https://arxiv.org/abs/2101.04030v1) [cs.CL]** for this version) |





# 2021-01-11

[Return to Index](#Index)



<h2 id="2021-01-11-1">1. MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets</h2>

Title: [MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets](https://arxiv.org/abs/2101.03028)

Authors:[Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Peng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Chenghao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C)

> Natural language processing (NLP) has been applied to various fields including text classification and sentiment analysis. In the shared task of sentiment analysis of code-mixed tweets, which is a part of the SemEval-2020 competition~\cite{patwa2020sentimix}, we preprocess datasets by replacing emoji and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.03028](https://arxiv.org/abs/2101.03028) [cs.CL]** |
|           | (or **[arXiv:2101.03028v1](https://arxiv.org/abs/2101.03028v1) [cs.CL]** for this version) |









# 2021-01-08

[Return to Index](#Index)



<h2 id="2021-01-08-1">1. User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics</h2>

Title: [User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics](https://arxiv.org/abs/2101.02244)

Authors: [Anamaria Crisan](https://arxiv.org/search/cs?searchtype=author&query=Crisan%2C+A), [Michael Correll](https://arxiv.org/search/cs?searchtype=author&query=Correll%2C+M)

> Topic models are widely used analysis techniques for clustering documents and surfacing thematic elements of text corpora. These models remain challenging to optimize and often require a "human-in-the-loop" approach where domain experts use their knowledge to steer and adjust. However, the fragility, incompleteness, and opacity of these models means even minor changes could induce large and potentially undesirable changes in resulting model. In this paper we conduct a simulation-based analysis of human-centered interactions with topic models, with the objective of measuring the sensitivity of topic models to common classes of user actions. We find that user interactions have impacts that differ in magnitude but often negatively affect the quality of the resulting modelling in a way that can be difficult for the user to evaluate. We suggest the incorporation of sensitivity and "multiverse" analyses to topic model interfaces to surface and overcome these deficiencies.

| Comments:    | 16 Pages, 9 Figures, CHI 2021 Conference                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| MSC classes: | 68U15                                                        |
| ACM classes: | H.5.0                                                        |
| Cite as:     | **[arXiv:2101.02244](https://arxiv.org/abs/2101.02244) [cs.HC]** |
|              | (or **[arXiv:2101.02244v1](https://arxiv.org/abs/2101.02244v1) [cs.HC]** for this version) |





<h2 id="2021-01-08-2">2. Towards a Smart Data Processing and Storage Model</h2>

Title: [Towards a Smart Data Processing and Storage Model](https://arxiv.org/abs/2101.02522)

Authors: [Ronie Salgado](https://arxiv.org/search/cs?searchtype=author&query=Salgado%2C+R), [Marcus Denker](https://arxiv.org/search/cs?searchtype=author&query=Denker%2C+M) (RMOD), [Stéphane Ducasse](https://arxiv.org/search/cs?searchtype=author&query=Ducasse%2C+S) (RMOD), [Anne Etien](https://arxiv.org/search/cs?searchtype=author&query=Etien%2C+A) (RMOD), [Vincent Aranega](https://arxiv.org/search/cs?searchtype=author&query=Aranega%2C+V) (RMOD)

> In several domains it is crucial to store and manipulate data whose origin needs to be completely traceable to guarantee the consistency, trustworthiness and reliability on the data itself typically for ethical and legal reasons. It is also important to guarantee that such properties are also carried further when such data is composed and processed into new data. In this article we present the main requirements and theorethical problems that arise by the design of a system supporting data with such capabilities. We present an architecture for implementing a system as well as a prototype developed in Pharo.

| Subjects:          | **Computation and Language (cs.CL)**; Programming Languages (cs.PL); Software Engineering (cs.SE) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | IWST20: International Workshop on Smalltalk Technologies, Sep 2020, Novi Sad, Serbia |
| Cite as:           | **[arXiv:2101.02522](https://arxiv.org/abs/2101.02522) [cs.CL]** |
|                    | (or **[arXiv:2101.02522v1](https://arxiv.org/abs/2101.02522v1) [cs.CL]** for this version) |







# 2021-01-07

[Return to Index](#Index)



<h2 id="2021-01-07-1">1. AutoDropout: Learning Dropout Patterns to Regularize Deep Networks</h2>

Title: [AutoDropout: Learning Dropout Patterns to Regularize Deep Networks](https://arxiv.org/abs/2101.01761)

Authors: [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> Neural networks are often over-parameterized and hence benefit from aggressive regularization. Conventional regularization methods, such as Dropout or weight decay, do not leverage the structures of the network's inputs and hidden states. As a result, these conventional methods are less effective than methods that leverage the structures, such as SpatialDropout and DropBlock, which randomly drop the values at certain contiguous areas in the hidden states and setting them to zero. Although the locations of dropout areas random, the patterns of SpatialDropout and DropBlock are manually designed and fixed. Here we propose to learn the dropout patterns. In our method, a controller learns to generate a dropout pattern at every channel and layer of a target network, such as a ConvNet or a Transformer. The target network is then trained with the dropout pattern, and its resulting validation performance is used as a signal for the controller to learn from. We show that this method works well for both image recognition on CIFAR-10 and ImageNet, as well as language modeling on Penn Treebank and WikiText-2. The learned dropout patterns also transfers to different tasks and datasets, such as from language model on Penn Treebank to Engligh-French translation on WMT 2014. Our code will be available.

| Comments: | Accepted to AAAI 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2101.01761](https://arxiv.org/abs/2101.01761) [cs.LG]** |
|           | (or **[arXiv:2101.01761v1](https://arxiv.org/abs/2101.01761v1) [cs.LG]** for this version) |







# 2021-01-06

[Return to Index](#Index)



<h2 id="2021-01-06-1">1. I-BERT: Integer-only BERT Quantization</h2>

Title: [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)

Authors: [Sehoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Amir Gholami](https://arxiv.org/search/cs?searchtype=author&query=Gholami%2C+A), [Zhewei Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Michael W. Mahoney](https://arxiv.org/search/cs?searchtype=author&query=Mahoney%2C+M+W), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K)

> Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for many edge processors, and it has been a challenge to deploy these models for edge applications and devices that have resource constraints. While quantization can be a viable solution to this, previous work on quantizing Transformer based models uses floating-point arithmetic during inference, thus limiting model deployment on many edge processors. In this work, we propose a novel integer-only quantization scheme for Transformer based models that quantizes the entire inference process. In particular, we demonstrate how to approximate nonlinear operations in Transformer architectures, e.g., GELU, Softmax, and Layer Normalization, with lightweight integer computations. We use those approximations in our method, I-BERT, with an end-to-end integer-only inference, and without any floating point calculation. We test our approach on GLUE downstream tasks using RoBERTa-Base and RoBERTa-Large. For both cases, with an 8-bit integer-only quantization scheme, I-BERT achieves similar accuracy as compared to the full-precision baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.01321](https://arxiv.org/abs/2101.01321) [cs.CL]** |
|           | (or **[arXiv:2101.01321v1](https://arxiv.org/abs/2101.01321v1) [cs.CL]** for this version) |





<h2 id="2021-01-06-2">2. Political Depolarization of News Articles Using Attribute-aware Word Embeddings</h2>

Title: [Political Depolarization of News Articles Using Attribute-aware Word Embeddings](https://arxiv.org/abs/2101.01391)

Authors: [Ruibo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+R), [Lili Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Chenyan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+C), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> Political polarization in the US is on the rise. This polarization negatively affects the public sphere by contributing to the creation of ideological echo chambers. In this paper, we focus on addressing one of the factors that contributes to this polarity, polarized media. We introduce a framework for depolarizing news articles. Given an article on a certain topic with a particular ideological slant (eg., liberal or conservative), the framework first detects polar language in the article and then generates a new article with the polar language replaced with neutral expressions. To detect polar words, we train a multi-attribute-aware word embedding model that is aware of ideology and topics on 360k full-length media articles. Then, for text generation, we propose a new algorithm called Text Annealing Depolarization Algorithm (TADA). TADA retrieves neutral expressions from the word embedding model that not only decrease ideological polarity but also preserve the original argument of the text, while maintaining grammatical correctness. We evaluate our framework by comparing the depolarized output of our model in two modes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics. Based on feedback from 161 human testers, our framework successfully depolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs in fully-automatic mode. Furthermore, 81.2% of the testers agree that the non-polar content information is well-preserved and 79% agree that depolarization does not harm semantic correctness when they compare the original text and the depolarized text. Our work shows that data-driven methods can help to locate political polarity and aid in the depolarization of articles.

| Comments: | In Proceedings of the 15th International AAAI Conference on Weblogs and Social Media (ICWSM 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2101.01391](https://arxiv.org/abs/2101.01391) [cs.CL]** |
|           | (or **[arXiv:2101.01391v1](https://arxiv.org/abs/2101.01391v1) [cs.CL]** for this version) |





<h2 id="2021-01-06-3">3. Local Translation Services for Neglected Languages</h2>

Title: [Local Translation Services for Neglected Languages](https://arxiv.org/abs/2101.01628)

Authors: [David Noever](https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D), [Josh Kalin](https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J), [Matt Ciolino](https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M), [Dom Hambrick](https://arxiv.org/search/cs?searchtype=author&query=Hambrick%2C+D), [Gerry Dozier](https://arxiv.org/search/cs?searchtype=author&query=Dozier%2C+G)

> Taking advantage of computationally lightweight, but high-quality translators prompt consideration of new applications that address neglected languages. Locally run translators for less popular languages may assist data projects with protected or personal data that may require specific compliance checks before posting to a public translation API, but which could render reasonable, cost-effective solutions if done with an army of local, small-scale pair translators. Like handling a specialist's dialect, this research illustrates translating two historically interesting, but obfuscated languages: 1) hacker-speak ("l33t") and 2) reverse (or "mirror") writing as practiced by Leonardo da Vinci. The work generalizes a deep learning architecture to translatable variants of hacker-speak with lite, medium, and hard vocabularies. The original contribution highlights a fluent translator of hacker-speak in under 50 megabytes and demonstrates a generator for augmenting future datasets with greater than a million bilingual sentence pairs. The long short-term memory, recurrent neural network (LSTM-RNN) extends previous work demonstrating an English-to-foreign translation service built from as little as 10,000 bilingual sentence pairs. This work further solves the equivalent translation problem in twenty-six additional (non-obfuscated) languages and rank orders those models and their proficiency quantitatively with Italian as the most successful and Mandarin Chinese as the most challenging. For neglected languages, the method prototypes novel services for smaller niche translations such as Kabyle (Algerian dialect) which covers between 5-7 million speakers but one which for most enterprise translators, has not yet reached development. One anticipates the extension of this approach to other important dialects, such as translating technical (medical or legal) jargon and processing health records.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.01628](https://arxiv.org/abs/2101.01628) [cs.CL]** |
|           | (or **[arXiv:2101.01628v1](https://arxiv.org/abs/2101.01628v1) [cs.CL]** for this version) |







# 2021-01-05

[Return to Index](#Index)



<h2 id="2021-01-05-1">1. VinVL: Making Visual Representations Matter in Vision-Language Models</h2>

Title: [VinVL: Making Visual Representations Matter in Vision-Language Models](https://arxiv.org/abs/2101.00529)

Authors: [Pengchuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Xiujun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaowei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X), [Jianwei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used \emph{bottom-up and top-down} model \cite{anderson2018bottom}, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \oscar \cite{li2020oscar}, and utilize an improved approach \short\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00529](https://arxiv.org/abs/2101.00529) [cs.CV]** |
|           | (or **[arXiv:2101.00529v1](https://arxiv.org/abs/2101.00529v1) [cs.CV]** for this version) |





<h2 id="2021-01-05-2">2. The Pile: An 800GB Dataset of Diverse Text for Language Modeling</h2>

Title: [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)

Authors: [Leo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Stella Biderman](https://arxiv.org/search/cs?searchtype=author&query=Biderman%2C+S), [Sid Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+S), [Laurence Golding](https://arxiv.org/search/cs?searchtype=author&query=Golding%2C+L), [Travis Hoppe](https://arxiv.org/search/cs?searchtype=author&query=Hoppe%2C+T), [Charles Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+C), [Jason Phang](https://arxiv.org/search/cs?searchtype=author&query=Phang%2C+J), [Horace He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H), [Anish Thite](https://arxiv.org/search/cs?searchtype=author&query=Thite%2C+A), [Noa Nabeshima](https://arxiv.org/search/cs?searchtype=author&query=Nabeshima%2C+N), [Shawn Presser](https://arxiv.org/search/cs?searchtype=author&query=Presser%2C+S), [Connor Leahy](https://arxiv.org/search/cs?searchtype=author&query=Leahy%2C+C)

> Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00027](https://arxiv.org/abs/2101.00027) [cs.CL]** |
|           | (or **[arXiv:2101.00027v1](https://arxiv.org/abs/2101.00027v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-3">3. EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets</h2>

Title: [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://arxiv.org/abs/2101.00063)

Authors: [Xiaohan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Zhangyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Deep, heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focus on reducing inference cost/time, while still requiring expensive training process. Other works use extremely large batch sizes to shorten the pre-training time at the expense of high demand for computation resources. In this paper, inspired by the Early-Bird Lottery Tickets studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. We are the first to identify structured winning tickets in the early stage of BERT training, and use them for efficient training. Comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks show that EarlyBERT easily achieves comparable performance to standard BERT with 35~45% less training time.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00063](https://arxiv.org/abs/2101.00063) [cs.CL]** |
|           | (or **[arXiv:2101.00063v1](https://arxiv.org/abs/2101.00063v1) [cs.CL]** for this version) |



<h2 id="2021-01-05-4">4. Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment</h2>

Title: [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://arxiv.org/abs/2101.00148)

Authors: [Haoyue Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+H), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Sida I. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S+I)

> Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00148](https://arxiv.org/abs/2101.00148) [cs.CL]** |
|           | (or **[arXiv:2101.00148v1](https://arxiv.org/abs/2101.00148v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-5">5. A Graph Total Variation Regularized Softmax for Text Generation</h2>

Title: [A Graph Total Variation Regularized Softmax for Text Generation](https://arxiv.org/abs/2101.00153)

Authors: [Liu Bin](https://arxiv.org/search/cs?searchtype=author&query=Bin%2C+L), [Wang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+W), [Yin Guosheng](https://arxiv.org/search/cs?searchtype=author&query=Guosheng%2C+Y)

> The softmax operator is one of the most important functions in machine learning models. When applying neural networks to multi-category classification, the correlations among different categories are often ignored. For example, in text generation, a language model makes a choice of each new word based only on the former selection of its context. In this scenario, the link statistics information of concurrent words based on a corpus (an analogy of the natural way of expression) is also valuable in choosing the next word, which can help to improve the sentence's fluency and smoothness. To fully explore such important information, we propose a graph softmax function for text generation. It is expected that the final classification result would be dominated by both the language model and graphical text relationships among words. We use a graph total variation term to regularize softmax so as to incorporate the concurrent relationship into the language model. The total variation of the generated words should be small locally. We apply the proposed graph softmax to GPT2 for the text generation task. Experimental results demonstrate that the proposed graph softmax achieves better BLEU and perplexity than softmax. Human testers can also easily distinguish the text generated by the graph softmax or softmax.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00153](https://arxiv.org/abs/2101.00153) [cs.CL]** |
|           | (or **[arXiv:2101.00153v1](https://arxiv.org/abs/2101.00153v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-6">6. BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding</h2>

Title: [BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding](https://arxiv.org/abs/2101.00204)

Authors: [Abhik Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+A), [Tahmid Hasan](https://arxiv.org/search/cs?searchtype=author&query=Hasan%2C+T), [Kazi Samin](https://arxiv.org/search/cs?searchtype=author&query=Samin%2C+K), [M. Sohel Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+M+S), [Anindya Iqbal](https://arxiv.org/search/cs?searchtype=author&query=Iqbal%2C+A), [Rifat Shahriyar](https://arxiv.org/search/cs?searchtype=author&query=Shahriyar%2C+R)

> Pre-training language models on large volume of data with self-supervised objectives has become a standard practice in natural language processing. However, most such state-of-the-art models are available in only English and other resource-rich languages. Even in multilingual models, which are trained on hundreds of languages, low-resource ones still remain underrepresented. Bangla, the seventh most widely spoken language in the world, is still low in terms of resources. Few downstream task datasets for language understanding in Bangla are publicly available, and there is a clear shortage of good quality data for pre-training. In this work, we build a Bangla natural language understanding model pre-trained on 18.6 GB data we crawled from top Bangla sites on the internet. We introduce a new downstream task dataset and benchmark on four tasks on sentence classification, document classification, natural language understanding, and sequence tagging. Our model outperforms multilingual baselines and previous state-of-the-art results by 1-6%. In the process, we identify a major shortcoming of multilingual models that hurt performance for low-resource languages that don't share writing scripts with any high resource one, which we name the `Embedding Barrier'. We perform extensive experiments to study this barrier. We release all our datasets and pre-trained models to aid future NLP research on Bangla and other low-resource languages. Our code and data are available at [this https URL](https://github.com/csebuetnlp/banglabert).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00204](https://arxiv.org/abs/2101.00204) [cs.CL]** |
|           | (or **[arXiv:2101.00204v1](https://arxiv.org/abs/2101.00204v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-7">7. Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers</h2>

Title: [Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](https://arxiv.org/abs/2101.00234)

Authors: [Machel Reid](https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M), [Edison Marrese-Taylor](https://arxiv.org/search/cs?searchtype=author&query=Marrese-Taylor%2C+E), [Yutaka Matsuo](https://arxiv.org/search/cs?searchtype=author&query=Matsuo%2C+Y)

> The advent of the Transformer can arguably be described as a driving force behind many of the recent advances in natural language processing. However, despite their sizeable performance improvements, as recently shown, the model is severely over-parameterized, being parameter inefficient and computationally expensive to train. Inspired by the success of parameter-sharing in pretrained deep contextualized word representation encoders, we explore parameter-sharing methods in Transformers, with a specific focus on encoder-decoder models for sequence-to-sequence tasks such as neural machine translation. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer, a parameter efficient Transformer-based model which combines the newly proposed Sandwich-style parameter sharing technique - designed to overcome the deficiencies in naive cross-layer parameter sharing for generative models - and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization, and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2101.00234](https://arxiv.org/abs/2101.00234) [cs.CL]** |
|           | (or **[arXiv:2101.00234v1](https://arxiv.org/abs/2101.00234v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-8">8. Understanding Few-Shot Commonsense Knowledge Models</h2>

Title: [Understanding Few-Shot Commonsense Knowledge Models](https://arxiv.org/abs/2101.00297)

Authors: [Jeff Da](https://arxiv.org/search/cs?searchtype=author&query=Da%2C+J), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A)

> Providing natural language processing systems with commonsense knowledge is a critical challenge for achieving language understanding. Recently, commonsense knowledge models have emerged as a suitable approach for hypothesizing situation-relevant commonsense knowledge on-demand in natural language applications. However, these systems are limited by the fixed set of relations captured by schemas of the knowledge bases on which they're trained.
> To address this limitation, we investigate training commonsense knowledge models in a few-shot setting with limited tuples per commonsense relation in the graph. We perform five separate studies on different dimensions of few-shot commonsense knowledge learning, providing a roadmap on best practices for training these systems efficiently. Importantly, we find that human quality ratings for knowledge produced from a few-shot trained system can achieve performance within 6% of knowledge produced from fully supervised systems. This few-shot performance enables coverage of a wide breadth of relations in future commonsense systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00297](https://arxiv.org/abs/2101.00297) [cs.CL]** |
|           | (or **[arXiv:2101.00297v1](https://arxiv.org/abs/2101.00297v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-9">9. On-the-Fly Attention Modularization for Neural Generation</h2>

Title: [On-the-Fly Attention Modularization for Neural Generation](https://arxiv.org/abs/2101.00371)

Authors: [Yue Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y), [Chandra Bhagavatula](https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Jena D. Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+J+D), [Antoine Bosselut](https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A), [Jackie Chi Kit Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+J+C+K), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: generated text is repetitive, generic, self-inconsistent, and lacking commonsense. The empirical analyses on sentence-level attention patterns reveal that neural text degeneration may be associated with insufficient learning of inductive biases by the attention mechanism. Our findings motivate on-the-fly attention modularization, a simple but effective method for injecting inductive biases into attention computation during inference. The resulting text produced by the language model with attention modularization can yield enhanced diversity and commonsense reasoning while maintaining fluency and coherence.

| Comments: | 10 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.00371](https://arxiv.org/abs/2101.00371) [cs.CL]** |
|           | (or **[arXiv:2101.00371v1](https://arxiv.org/abs/2101.00371v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-10">10. Cross-Document Language Modeling</h2>

Title: [Cross-Document Language Modeling](https://arxiv.org/abs/2101.00406)

Authors: [Avi Caciularu](https://arxiv.org/search/cs?searchtype=author&query=Caciularu%2C+A), [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E), [Arie Cattan](https://arxiv.org/search/cs?searchtype=author&query=Cattan%2C+A), [Ido Dagan](https://arxiv.org/search/cs?searchtype=author&query=Dagan%2C+I)

> We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our cross-document language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including cross-document event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00406](https://arxiv.org/abs/2101.00406) [cs.CL]** |
|           | (or **[arXiv:2101.00406v1](https://arxiv.org/abs/2101.00406v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-11">11. Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</h2>

Title: [Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](https://arxiv.org/abs/2101.00416)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we generalize text infilling (e.g., masked language models) by proposing Sequence Span Rewriting (SSR) as a self-supervised sequence-to-sequence (seq2seq) pre-training objective. SSR provides more fine-grained learning signals for text representations by supervising the model to rewrite imperfect spans to ground truth, and it is more consistent than text infilling with many downstream seq2seq tasks that rewrite a source sentences into a target sentence. Our experiments with T5 models on various seq2seq tasks show that SSR can substantially improve seq2seq pre-training. Moreover, we observe SSR is especially helpful to improve pre-training a small-size seq2seq model with a powerful imperfect span generator, which indicates a new perspective of transferring knowledge from a large model to a smaller model for seq2seq pre-training.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00416](https://arxiv.org/abs/2101.00416) [cs.CL]** |
|           | (or **[arXiv:2101.00416v1](https://arxiv.org/abs/2101.00416v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-12">12. KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation</h2>

Title: [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://arxiv.org/abs/2101.00419)

Authors: [Yiran Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+Y), [Zai Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Z), [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Yunpu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We extend the popular BART architecture to a multi-modal model. We design a new pretraining task to improve the model performance on Visual Commonsense Generation task. Our pretraining task improves the Visual Commonsense Generation performance by leveraging knowledge from a large language model pretrained on an external knowledge graph. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on Visual Commonsense Generation. Experimental results show that by pretraining, our model reaches state-of-the-art performance on the Visual Commonsense Generation task.

| Comments: | Work in progress. The first three authors contribute equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2101.00419](https://arxiv.org/abs/2101.00419) [cs.CL]** |
|           | (or **[arXiv:2101.00419v1](https://arxiv.org/abs/2101.00419v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-13">13. Decoding Time Lexical Domain Adaptationfor Neural Machine Translation</h2>

Title: [Decoding Time Lexical Domain Adaptationfor Neural Machine Translation](https://arxiv.org/abs/2101.00421)

Authors: [Nikolay Bogoychev](https://arxiv.org/search/cs?searchtype=author&query=Bogoychev%2C+N), [Pinzhen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P)

> Machine translation systems are vulnerable to domain mismatch, especially when the task is low-resource. In this setting, out of domain translations are often of poor quality and prone to hallucinations, due to the translation model preferring to predict common words it has seen during training, as opposed to the more uncommon ones from a different domain. We present two simple methods for improving translation quality in this particular setting: First, we use lexical shortlisting in order to restrict the neural network predictions by IBM model computed alignments. Second, we perform n-best list reordering by reranking all translations based on the amount they overlap with each other. Our methods are computationally simpler and faster than alternative approaches, and show a moderate success on low-resource settings with explicit out of domain test sets. However, our methods lose their effectiveness when the domain mismatch is too great, or in high resource setting.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00421](https://arxiv.org/abs/2101.00421) [cs.CL]** |
|           | (or **[arXiv:2101.00421v1](https://arxiv.org/abs/2101.00421v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-14">14. Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events</h2>

Title: [Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events](https://arxiv.org/abs/2101.00822)

Authors: [Le Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L), [Tao Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+T), [Chaochun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Liefeng Bo](https://arxiv.org/search/cs?searchtype=author&query=Bo%2C+L), [Wen Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+W), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> Large-scale pretrained language models have shown thrilling generation capabilities, especially when they generate consistent long text in thousands of words with ease. However, users of these models can only control the prefix of sentences or certain global aspects of generated text. It is challenging to simultaneously achieve fine-grained controllability and preserve the state-of-the-art unconditional text generation capability. In this paper, we first propose a new task named "Outline to Story" (O2S) as a test bed for fine-grained controllable generation of long text, which generates a multi-paragraph story from cascaded events, i.e. a sequence of outline events that guide subsequent paragraph generation. We then create dedicate datasets for future benchmarks, built by state-of-the-art keyword extraction techniques. Finally, we propose an extremely simple yet strong baseline method for the O2S task, which fine tunes pre-trained language models on augmented sequences of outline-story pairs with simple language modeling objective. Our method does not introduce any new parameters or perform any architecture modification, except several special tokens as delimiters to build augmented sequences. Extensive experiments on various datasets demonstrate state-of-the-art conditional story generation performance with our model, achieving better fine-grained controllability and user flexibility. Our paper is among the first ones by our knowledge to propose a model and to create datasets for the task of "outline to story". Our work also instantiates research interest of fine-grained controllable generation of open-domain long text, where controlling inputs are represented by short text.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00822](https://arxiv.org/abs/2101.00822) [cs.CL]** |
|           | (or **[arXiv:2101.00822v1](https://arxiv.org/abs/2101.00822v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-15">15. Transformer-based Conditional Variational Autoencoder for Controllable Story Generation</h2>

Title: [Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](https://arxiv.org/abs/2101.00828)

Authors: [Le Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L), [Tao Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+T), [Chaochun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Liefeng Bo](https://arxiv.org/search/cs?searchtype=author&query=Bo%2C+L), [Wen Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+W), [Changyou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C)

> We investigate large-scale latent variable models (LVMs) for neural story generation -- an under-explored application for open-domain long text -- with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models -- GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00828](https://arxiv.org/abs/2101.00828) [cs.CL]** |
|           | (or **[arXiv:2101.00828v1](https://arxiv.org/abs/2101.00828v1) [cs.CL]** for this version) |





<h2 id="2021-01-05-16">16. How to Train Your Agent to Read and Write</h2>

Title: [How to Train Your Agent to Read and Write](https://arxiv.org/abs/2101.00916)

Authors: [Li Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Mengge He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+M), [Guanghui Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G), [Mingkui Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q)

> Reading and writing research papers is one of the most privileged abilities that a qualified researcher should master. However, it is difficult for new researchers (\eg{students}) to fully {grasp} this ability. It would be fascinating if we could train an intelligent agent to help people read and summarize papers, and perhaps even discover and exploit the potential knowledge clues to write novel papers. Although there have been existing works focusing on summarizing (\emph{i.e.}, reading) the knowledge in a given text or generating (\emph{i.e.}, writing) a text based on the given knowledge, the ability of simultaneously reading and writing is still under development. Typically, this requires an agent to fully understand the knowledge from the given text materials and generate correct and fluent novel paragraphs, which is very challenging in practice. In this paper, we propose a Deep ReAder-Writer (DRAW) network, which consists of a \textit{Reader} that can extract knowledge graphs (KGs) from input paragraphs and discover potential knowledge, a graph-to-text \textit{Writer} that generates a novel paragraph, and a \textit{Reviewer} that reviews the generated paragraph from three different aspects. Extensive experiments show that our DRAW network outperforms considered baselines and several state-of-the-art methods on AGENDA and M-AGENDA datasets. Our code and supplementary are released at [this https URL](https://github.com/menggehe/DRAW).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2101.00916](https://arxiv.org/abs/2101.00916) [cs.CL]** |
|           | (or **[arXiv:2101.00916v1](https://arxiv.org/abs/2101.00916v1) [cs.CL]** for this version) |



# 2021-01-01

[Return to Index](#Index)



<h2 id="2021-01-01-1">1. Understanding and Improving Lexical Choice in Non-Autoregressive Translation</h2>

Title: [Understanding and Improving Lexical Choice in Non-Autoregressive Translation](https://arxiv.org/abs/2012.14583)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14583](https://arxiv.org/abs/2012.14583) [cs.CL]** |
|           | (or **[arXiv:2012.14583v1](https://arxiv.org/abs/2012.14583v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-2">2. Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation</h2>

Title: [Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation](https://arxiv.org/abs/2012.14681)

Authors: [Hyojung Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+H), [Sathish Indurthi](https://arxiv.org/search/cs?searchtype=author&query=Indurthi%2C+S), [Mohd Abbas Zaidi](https://arxiv.org/search/cs?searchtype=author&query=Zaidi%2C+M+A), [Nikhil Kumar Lakumarapu](https://arxiv.org/search/cs?searchtype=author&query=Lakumarapu%2C+N+K), [Beomseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+B), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Chanwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C), [Inchul Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+I)

> Recently, simultaneous translation has gathered a lot of attention since it enables compelling applications such as subtitle translation for a live event or real-time video-call translation. Some of these translation applications allow editing of partial translation giving rise to re-translation approaches. The current re-translation approaches are based on autoregressive sequence generation models (ReTA), which generate tar-get tokens in the (partial) translation sequentially. The multiple re-translations with sequential generation inReTAmodelslead to an increased inference time gap between the incoming source input and the corresponding target output as the source input grows. Besides, due to the large number of inference operations involved, the ReTA models are not favorable for resource-constrained devices. In this work, we propose a faster re-translation system based on a non-autoregressive sequence generation model (FReTNA) to overcome the aforementioned limitations. We evaluate the proposed model on multiple translation tasks and our model reduces the inference times by several orders and achieves a competitive BLEUscore compared to the ReTA and streaming (Wait-k) models.The proposed model reduces the average computation time by a factor of 20 when compared to the ReTA model by incurring a small drop in the translation quality. It also outperforms the streaming-based Wait-k model both in terms of computation time (1.5 times lower) and translation quality.

| Comments: | work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.14681](https://arxiv.org/abs/2012.14681) [cs.CL]** |
|           | (or **[arXiv:2012.14681v1](https://arxiv.org/abs/2012.14681v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-3">3. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</h2>

Title: [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)

Authors: [Yang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Tengchao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+T), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Guoxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Yijuan Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Dinei Florencio](https://arxiv.org/search/cs?searchtype=author&query=Florencio%2C+D), [Cha Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Lidong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L)

> Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this paper, we present \textbf{LayoutLMv2} by pre-training text, layout and image in a multi-modal framework, where new model architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852), RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.14740](https://arxiv.org/abs/2012.14740) [cs.CL]** |
|           | (or **[arXiv:2012.14740v1](https://arxiv.org/abs/2012.14740v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-4">4. CMV-BERT: Contrastive multi-vocab pretraining of BERT</h2>

Title: [CMV-BERT: Contrastive multi-vocab pretraining of BERT](https://arxiv.org/abs/2012.14763)

Authors: [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Daniel Cheung](https://arxiv.org/search/cs?searchtype=author&query=Cheung%2C+D)

> In this work, we represent CMV-BERT, which improves the pretraining of a language model via two ingredients: (a) contrastive learning, which is well studied in the area of computer vision; (b) multiple vocabularies, one of which is fine-grained and the other is coarse-grained. The two methods both provide different views of an original sentence, and both are shown to be beneficial. Downstream tasks demonstrate our proposed CMV-BERT are effective in improving the pretrained language models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14763](https://arxiv.org/abs/2012.14763) [cs.CL]** |
|           | (or **[arXiv:2012.14763v1](https://arxiv.org/abs/2012.14763v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-5">5. Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning</h2>

Title: [Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning](https://arxiv.org/abs/2012.14768)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder layers (instead of the uppermost layer) for sequence-to-sequence (Seq2Seq) models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers. In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction. It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. The source code will be released.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14768](https://arxiv.org/abs/2012.14768) [cs.CL]** |
|           | (or **[arXiv:2012.14768v1](https://arxiv.org/abs/2012.14768v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-6">6. Transformer Feed-Forward Layers Are Key-Value Memories</h2>

Title: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)

Authors: [Mor Geva](https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M), [Roei Schuster](https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+R), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.14913](https://arxiv.org/abs/2012.14913) [cs.CL]** |
|           | (or **[arXiv:2012.14913v1](https://arxiv.org/abs/2012.14913v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-7">7. Reservoir Transformer</h2>

Title: [Reservoir Transformer](https://arxiv.org/abs/2012.15045)

Authors: [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Ari S. Morcos](https://arxiv.org/search/cs?searchtype=author&query=Morcos%2C+A+S), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear "reservoir" layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15045](https://arxiv.org/abs/2012.15045) [cs.CL]** |
|           | (or **[arXiv:2012.15045v1](https://arxiv.org/abs/2012.15045v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-8">8. Enhancing Pre-trained Language Model with Lexical Simplification</h2>

Title: [Enhancing Pre-trained Language Model with Lexical Simplification](https://arxiv.org/abs/2012.15070)

Authors: [Rongzhou Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+R), [Jiayi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> For both human readers and pre-trained language models (PrLMs), lexical diversity may lead to confusion and inaccuracy when understanding the underlying semantic meanings of given sentences. By substituting complex words with simple alternatives, lexical simplification (LS) is a recognized method to reduce such lexical diversity, and therefore to improve the understandability of sentences. In this paper, we leverage LS and propose a novel approach which can effectively improve the performance of PrLMs in text classification. A rule-based simplification process is applied to a given sentence. PrLMs are encouraged to predict the real label of the given sentence with auxiliary inputs from the simplified version. Using strong PrLMs (BERT and ELECTRA) as baselines, our approach can still further improve the performance in various text classification tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15070](https://arxiv.org/abs/2012.15070) [cs.CL]** |
|           | (or **[arXiv:2012.15070v1](https://arxiv.org/abs/2012.15070v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-9">9. Accurate Word Representations with Universal Visual Guidance</h2>

Title: [Accurate Word Representations with Universal Visual Guidance](https://arxiv.org/abs/2012.15086)

Authors: [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Haojie Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M)

> Word representation is a fundamental component in neural language understanding models. Recently, pre-trained language models (PrLMs) offer a new performant method of contextualized word representations by leveraging the sequence-level context for modeling. Although the PrLMs generally give more accurate contextualized word representations than non-contextualized models do, they are still subject to a sequence of text contexts without diverse hints for word representation from multimodality. This paper thus proposes a visual representation method to explicitly enhance conventional word embedding with multiple-aspect senses from visual guidance. In detail, we build a small-scale word-image dictionary from a multimodal seed dataset where each word corresponds to diverse related images. The texts and paired images are encoded in parallel, followed by an attention layer to integrate the multimodal representations. We show that the method substantially improves the accuracy of disambiguation. Experiments on 12 natural language understanding and machine translation tasks further verify the effectiveness and the generalization capability of the proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15086](https://arxiv.org/abs/2012.15086) [cs.CL]** |
|           | (or **[arXiv:2012.15086v1](https://arxiv.org/abs/2012.15086v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-1">10. Improving Zero-Shot Translation by Disentangling Positional Information</h2>

Title: [Improving Zero-Shot Translation by Disentangling Positional Information](https://arxiv.org/abs/2012.15127)

Authors: [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15127](https://arxiv.org/abs/2012.15127) [cs.CL]** |
|           | (or **[arXiv:2012.15127v1](https://arxiv.org/abs/2012.15127v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-11">11. Improving BERT with Syntax-aware Local Attention</h2>

Title: [Improving BERT with Syntax-aware Local Attention](https://arxiv.org/abs/2012.15150)

Authors: [Zhongli Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Qingyu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Q), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Yunbo Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y)

> Pre-trained Transformer-based neural language models, such as BERT, have achieved remarkable results on varieties of NLP tasks. Recent works have shown that attention-based models can benefit from more focused attention over local regions. Most of them restrict the attention scope within a linear span, or confine to certain tasks such as machine translation and question answering. In this paper, we propose a syntax-aware local attention, where the attention scopes are restrained based on the distances in the syntactic structure. The proposed syntax-aware local attention can be integrated with pretrained language models, such as BERT, to render the model to focus on syntactically relevant words. We conduct experiments on various single-sentence benchmarks, including sentence classification and sequence labeling tasks. Experimental results show consistent gains over BERT on all benchmark datasets. The extensive studies verify that our model achieves better performance owing to more focused attention over syntactically relevant words.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15150](https://arxiv.org/abs/2012.15150) [cs.CL]** |
|           | (or **[arXiv:2012.15150v1](https://arxiv.org/abs/2012.15150v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-12">12. Synthetic Source Language Augmentation for Colloquial Neural Machine Translation</h2>

Title: [Synthetic Source Language Augmentation for Colloquial Neural Machine Translation](https://arxiv.org/abs/2012.15178)

Authors: [Asrul Sani Ariesandy](https://arxiv.org/search/cs?searchtype=author&query=Ariesandy%2C+A+S), [Mukhlis Amien](https://arxiv.org/search/cs?searchtype=author&query=Amien%2C+M), [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Radityo Eko Prasojo](https://arxiv.org/search/cs?searchtype=author&query=Prasojo%2C+R+E)

> Neural machine translation (NMT) is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source of formal Indonesian language and show that it improves the baseline Id-En models (in BLEU) over the new test data.

| Comments:    | 5 pages                                                      |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2012.15178](https://arxiv.org/abs/2012.15178) [cs.CL]** |
|              | (or **[arXiv:2012.15178v1](https://arxiv.org/abs/2012.15178v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-13">13. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?</h2>

Title: [Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?](https://arxiv.org/abs/2012.15180)

Authors: [Thang M. Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+T+M), [Trung Bui](https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T), [Long Mai](https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+L), [Anh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A)

> Do state-of-the-art natural language understanding models care about word order - one of the most important characteristics of a sequence? Not always! We found 75% to 90% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Despite BERT embeddings are famously contextual, the contribution of each individual word to downstream tasks is almost unchanged even after the word's context is shuffled. BERT-based models are able to exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequence-pair inputs in natural language inference) to make correct decisions when tokens are arranged in random orders. Encouraging classifiers to capture word order information improves the performance on most GLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.

| Comments: | 23 pages, 13 figures. Preprint. Work in progress             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15180](https://arxiv.org/abs/2012.15180) [cs.CL]** |
|           | (or **[arXiv:2012.15180v1](https://arxiv.org/abs/2012.15180v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-14">14. SemGloVe: Semantic Co-occurrences for GloVe from BERT</h2>

Title: [SemGloVe: Semantic Co-occurrences for GloVe from BERT](https://arxiv.org/abs/2012.15197)

Authors: [Leilei Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+L), [Zhiyang Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Linchao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+L), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> GloVe learns word embeddings by leveraging statistical information from word co-occurrence matrices. However, word pairs in the matrices are extracted from a predefined local context window, which might lead to limited word pairs and potentially semantic irrelevant word pairs. In this paper, we propose SemGloVe, which distills semantic co-occurrences from BERT into static GloVe word embeddings. Particularly, we propose two models to extract co-occurrence statistics based on either the masked language model or the multi-head attention weights of BERT. Our methods can extract word pairs without limiting by the local window assumption and can define the co-occurrence weights by directly considering the semantic distance between word pairs. Experiments on several word similarity datasets and four external tasks show that SemGloVe can outperform GloVe.

| Comments: | 10 pages, 3 figures, 5 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2012.15197](https://arxiv.org/abs/2012.15197) [cs.CL]** |
|           | (or **[arXiv:2012.15197v1](https://arxiv.org/abs/2012.15197v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-15">15. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</h2>

Title: [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://arxiv.org/abs/2012.15409)

Authors: [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Can Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Guocheng Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+G), [Xinyan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X), [Hao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Jiachen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e. text or image) or limited multi-modal data (i.e. image-text pairs). In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15409](https://arxiv.org/abs/2012.15409) [cs.CL]** |
|           | (or **[arXiv:2012.15409v1](https://arxiv.org/abs/2012.15409v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-16">16. Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation</h2>

Title: [Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation](https://arxiv.org/abs/2012.15416)

Authors: [Damian Pascual](https://arxiv.org/search/cs?searchtype=author&query=Pascual%2C+D), [Beni Egressy](https://arxiv.org/search/cs?searchtype=author&query=Egressy%2C+B), [Florian Bolli](https://arxiv.org/search/cs?searchtype=author&query=Bolli%2C+F), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Large pre-trained language models are capable of generating realistic text. However, controlling these models so that the generated text satisfies lexical constraints, i.e., contains specific words, is a challenging problem. Given that state-of-the-art language models are too large to be trained from scratch in a manageable time, it is desirable to control these models without re-training them. Methods capable of doing this are called plug-and-play. Recent plug-and-play methods have been successful in constraining small bidirectional language models as well as forward models in tasks with a restricted search space, e.g., machine translation. However, controlling large transformer-based models to meet lexical constraints without re-training them remains a challenge. In this work, we propose Directed Beam Search (DBS), a plug-and-play method for lexically constrained language generation. Our method can be applied to any language model, is easy to implement and can be used for general language generation. In our experiments we use DBS to control GPT-2. We demonstrate its performance on keyword-to-phrase generation and we obtain comparable results as a state-of-the-art non-plug-and-play model for lexically constrained story generation.

| Comments: | Preprint. Work in progress                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15416](https://arxiv.org/abs/2012.15416) [cs.CL]** |
|           | (or **[arXiv:2012.15416v1](https://arxiv.org/abs/2012.15416v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-17">17. Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation</h2>

Title: [Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/2012.15455)

Authors: [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

> We explore two types of monolingual data that can be included in knowledge distillation training for neural machine translation (NMT). The first is the source-side monolingual data. Second, is the target-side monolingual data that is used as back-translation data. Both datasets are (forward-)translated by a teacher model from source-language to target-language, which are then combined into a dataset for smaller student models. We find that source-side monolingual data improves model performance when evaluated by test-set originated from source-side. Likewise, target-side data has a positive effect on the test-set in the opposite direction. We also show that it is not required to train the student model with the same data used by the teacher, as long as the domains are the same. Finally, we find that combining source-side and target-side yields in better performance than relying on just one side of the monolingual data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15455](https://arxiv.org/abs/2012.15455) [cs.CL]** |
|           | (or **[arXiv:2012.15455v1](https://arxiv.org/abs/2012.15455v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-18">18. CLEAR: Contrastive Learning for Sentence Representation</h2>

Title: [CLEAR: Contrastive Learning for Sentence Representation](https://arxiv.org/abs/2012.15466)

Authors: [Zhuofeng Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Sinong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Madian Khabsa](https://arxiv.org/search/cs?searchtype=author&query=Khabsa%2C+M), [Fei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Hao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+H)

> Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks. Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks.

| Comments: | 10 pages, 2 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15466](https://arxiv.org/abs/2012.15466) [cs.CL]** |
|           | (or **[arXiv:2012.15466v1](https://arxiv.org/abs/2012.15466v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-19">19. Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings</h2>

Title: [Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings](https://arxiv.org/abs/2012.15484)

Authors: [Kiran Ramnath](https://arxiv.org/search/cs?searchtype=author&query=Ramnath%2C+K), [Mark Hasegawa-Johnson](https://arxiv.org/search/cs?searchtype=author&query=Hasegawa-Johnson%2C+M)

> Fact-based Visual Question Answering (FVQA), a challenging variant of VQA, requires a QA-system to include facts from a diverse knowledge graph (KG) in its reasoning process to produce an answer. Large KGs, especially common-sense KGs, are known to be incomplete, i.e. not all non-existent facts are always incorrect. Therefore, being able to reason over incomplete KGs for QA is a critical requirement in real-world applications that has not been addressed extensively in the literature. We develop a novel QA architecture that allows us to reason over incomplete KGs, something current FVQA state-of-the-art (SOTA) approaches lack.We use KG Embeddings, a technique widely used for KG completion, for the downstream task of FVQA. We also employ a new image representation technique we call "Image-as-Knowledge" to enable this capability, alongside a simple one-step co-Attention mechanism to attend to text and image during QA. Our FVQA architecture is faster during inference time, being O(m), as opposed to existing FVQA SOTA methods which are O(N logN), where m is number of vertices, N is number of edges (which is O(m^2)). We observe that our architecture performs comparably in the standard answer-retrieval baseline with existing methods; while for missing-edge reasoning, our KG representation outperforms the SOTA representation by 25%, and image representation outperforms the SOTA representation by 2.6%.

| Comments: | 9 pages, 10 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15484](https://arxiv.org/abs/2012.15484) [cs.CL]** |
|           | (or **[arXiv:2012.15484v1](https://arxiv.org/abs/2012.15484v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-20">20. Towards Zero-Shot Knowledge Distillation for Natural Language Processing</h2>

Title: [Towards Zero-Shot Knowledge Distillation for Natural Language Processing](https://arxiv.org/abs/2012.15495)

Authors: [Ahmad Rashid](https://arxiv.org/search/cs?searchtype=author&query=Rashid%2C+A), [Vasileios Lioutas](https://arxiv.org/search/cs?searchtype=author&query=Lioutas%2C+V), [Abbas Ghaddar](https://arxiv.org/search/cs?searchtype=author&query=Ghaddar%2C+A), [Mehdi Rezagholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Rezagholizadeh%2C+M)

> Knowledge Distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher's training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-Shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out of domain data and adversarial training to learn the teacher's output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher's classification score (accuracy or F1) while compressing the model 30 times.

| Comments: | 13 pages, 8 tables, 2 algorithms and 1 figure                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2012.15495](https://arxiv.org/abs/2012.15495) [cs.CL]** |
|           | (or **[arXiv:2012.15495v1](https://arxiv.org/abs/2012.15495v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-21">21. Neural Machine Translation: A Review of Methods, Resources, and Tools</h2>

Title: [Neural Machine Translation: A Review of Methods, Resources, and Tools](https://arxiv.org/abs/2012.15515)

Authors: [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zonghan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Gang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions.

| Comments: | Accepted by AI Open                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15515](https://arxiv.org/abs/2012.15515) [cs.CL]** |
|           | (or **[arXiv:2012.15515v1](https://arxiv.org/abs/2012.15515v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-22">22. Linear-Time WordPiece Tokenization</h2>

Title: [Linear-Time WordPiece Tokenization](https://arxiv.org/abs/2012.15524)

Authors: [Xinying Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Alex Salcianu](https://arxiv.org/search/cs?searchtype=author&query=Salcianu%2C+A), [Yang Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Dave Dopson](https://arxiv.org/search/cs?searchtype=author&query=Dopson%2C+D), [Denny Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+D)

> WordPiece tokenization is a subword-based tokenization schema adopted by BERT: it segments the input text via a longest-match-first tokenization strategy, known as Maximum Matching or MaxMatch. To the best of our knowledge, all published MaxMatch algorithms are quadratic (or higher). In this paper, we propose LinMaxMatch, a novel linear-time algorithm for MaxMatch and WordPiece tokenization. Inspired by the Aho-Corasick algorithm, we introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. Experimental results show that our algorithm is 3x faster on average than two production systems by HuggingFace and TensorFlow Text. Regarding long-tail inputs, our algorithm is 4.5x faster at the 95 percentile. This work has immediate practical value (reducing inference latency, saving compute resources, etc.) and is of theoretical interest by providing an optimal complexity solution to the decades-old MaxMatch problem.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15524](https://arxiv.org/abs/2012.15524) [cs.CL]** |
|           | (or **[arXiv:2012.15524v1](https://arxiv.org/abs/2012.15524v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-23">23. XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders</h2>

Title: [XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders](https://arxiv.org/abs/2012.15547)

Authors: [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Akiko Eriguchi](https://arxiv.org/search/cs?searchtype=author&query=Eriguchi%2C+A), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Arul Menezes](https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+A), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Multilingual machine translation enables a single model to translate between different languages. Most existing multilingual machine translation systems adopt a randomly initialized Transformer backbone. In this work, inspired by the recent success of language model pre-training, we present XLM-T, which initializes the model with an off-the-shelf pretrained cross-lingual Transformer encoder and fine-tunes it with multilingual parallel data. This simple method achieves significant improvements on a WMT dataset with 10 language pairs and the OPUS-100 corpus with 94 pairs. Surprisingly, the method is also effective even upon the strong baseline with back-translation. Moreover, extensive analysis of XLM-T on unsupervised syntactic parsing, word alignment, and multilingual classification explains its effectiveness for machine translation. The code will be at [this https URL](https://aka.ms/xlm-t).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15547](https://arxiv.org/abs/2012.15547) [cs.CL]** |
|           | (or **[arXiv:2012.15547v1](https://arxiv.org/abs/2012.15547v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-24">24. How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</h2>

Title: [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://arxiv.org/abs/2012.15613)

Authors: [Phillip Rust](https://arxiv.org/search/cs?searchtype=author&query=Rust%2C+P), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> In this work we provide a \textit{systematic empirical comparison} of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first establish if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for a performance difference. To disentangle the impacting variables, we train new monolingual models on the same data, but with different tokenizers, both the monolingual and the multilingual version. We find that while the pretraining data size is an important factor, the designated tokenizer of the monolingual model plays an equally important role in the downstream performance. Our results show that languages which are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15613](https://arxiv.org/abs/2012.15613) [cs.CL]** |
|           | (or **[arXiv:2012.15613v1](https://arxiv.org/abs/2012.15613v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-25">25. CoCoLM: COmplex COmmonsense Enhanced Language Model</h2>

Title: [CoCoLM: COmplex COmmonsense Enhanced Language Model](https://arxiv.org/abs/2012.15643)

Authors: [Changlong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C), [Hongming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yangqiu Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y), [Wilfred Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+W)

> Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contains rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with the complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address this problem, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Different from existing fine-tuning approaches, we do not focus on a specific task and propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graphs ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense knowledge among eventualities. Experiments on multiple downstream commonsense tasks that requires the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15643](https://arxiv.org/abs/2012.15643) [cs.CL]** |
|           | (or **[arXiv:2012.15643v1](https://arxiv.org/abs/2012.15643v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-26">26. VOLT: Improving Vocabularization via Optimal Transport for Machine Translation</h2>

Title: [VOLT: Improving Vocabularization via Optimal Transport for Machine Translation](https://arxiv.org/abs/2012.15671)

Authors: [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Chun Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+C), [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> It is well accepted that the choice of token vocabulary largely affects the performance of machine translation. However, due to expensive trial costs, most studies only conduct simple trials with dominant approaches (e.g BPE) and commonly used vocabulary sizes. In this paper, we find an exciting relation between an information-theoretic feature and BLEU scores. With this observation, we formulate the quest of vocabularization -- finding the best token dictionary with a proper size -- as an optimal transport problem. We then propose VOLT, a simple and efficient vocabularization solution without the full and costly trial training. We evaluate our approach on multiple machine translation tasks, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. Empirical results show that VOLT beats widely-used vocabularies on diverse scenarios. For example, VOLT achieves 70% vocabulary size reduction and 0.6 BLEU gain on English-German translation. Also, one advantage of VOLT lies in its low resource consumption. Compared to naive BPE-search, VOLT reduces the search time from 288 GPU hours to 0.5 CPU hours.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15671](https://arxiv.org/abs/2012.15671) [cs.CL]** |
|           | (or **[arXiv:2012.15671v1](https://arxiv.org/abs/2012.15671v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-27">27. ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</h2>

Title: [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674)

Authors: [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance on downstream cross-lingual tasks. This improvement stems from the learning of a large amount of monolingual and parallel corpora. While it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for the low-resource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to break the constraint of parallel corpus size on the model performance. Our key insight is to integrate the idea of back translation in the pre-training process. We generate pseudo-parallel sentences pairs on a monolingual corpus to enable the learning of semantic alignment between different languages, which enhances the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results on various cross-lingual downstream tasks. The codes and pre-trained models will be made publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15674](https://arxiv.org/abs/2012.15674) [cs.CL]** |
|           | (or **[arXiv:2012.15674v1](https://arxiv.org/abs/2012.15674v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-28">28. Revisiting Robust Neural Machine Translation: A Transformer Case Study</h2>

Title: [Revisiting Robust Neural Machine Translation: A Transformer Case Study](https://arxiv.org/abs/2012.15710)

Authors: [Peyman Passban](https://arxiv.org/search/cs?searchtype=author&query=Passban%2C+P), [Puneeth S.M. Saladi](https://arxiv.org/search/cs?searchtype=author&query=Saladi%2C+P+S), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Transformers (Vaswani et al., 2017) have brought a remarkable improvement in the performance of neural machine translation (NMT) systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.
> Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15710](https://arxiv.org/abs/2012.15710) [cs.CL]** |
|           | (or **[arXiv:2012.15710v1](https://arxiv.org/abs/2012.15710v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-29">29. FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation</h2>

Title: [FDMT: A Benchmark Dataset for Fine-grained Domain Adaptation in Machine Translation](https://arxiv.org/abs/2012.15717)

Authors: [Wenhao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Tong Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+T), [Xu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Jian Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Wei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Yanfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Previous domain adaptation research usually neglect the diversity in translation within a same domain, which is a core problem for adapting a general neural machine translation (NMT) model into a specific domain in real-world scenarios. One representative of such challenging scenarios is to deploy a translation system for a conference with a specific topic, e.g. computer networks or natural language processing, where there is usually extremely less resources due to the limited time schedule. To motivate a wide investigation in such settings, we present a real-world fine-grained domain adaptation task in machine translation (FDMT). The FDMT dataset (Zh-En) consists of four sub-domains of information technology: autonomous vehicles, AI education, real-time networks and smart phone. To be closer to reality, FDMT does not employ any in-domain bilingual training data. Instead, each sub-domain is equipped with monolingual data, bilingual dictionary and knowledge base, to encourage in-depth exploration of these available resources. Corresponding development set and test set are provided for evaluation purpose. We make quantitative experiments and deep analyses in this new setting, which benchmarks the fine-grained domain adaptation task and reveals several challenging problems that need to be addressed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15717](https://arxiv.org/abs/2012.15717) [cs.CL]** |
|           | (or **[arXiv:2012.15717v1](https://arxiv.org/abs/2012.15717v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-30">30. Making Pre-trained Language Models Better Few-shot Learners</h2>

Title: [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723)

Authors: [Tianyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Adam Fisch](https://arxiv.org/search/cs?searchtype=author&query=Fisch%2C+A), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15723](https://arxiv.org/abs/2012.15723) [cs.CL]** |
|           | (or **[arXiv:2012.15723v1](https://arxiv.org/abs/2012.15723v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-31">31. Shortformer: Better Language Modeling using Shorter Inputs</h2>

Title: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)

Authors: [Ofir Press](https://arxiv.org/search/cs?searchtype=author&query=Press%2C+O), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We explore the benefits of decreasing the input length of transformers. First, we show that initially training the model on short subsequences, before moving on to longer ones, both reduces overall training time and, surprisingly, gives a large improvement in perplexity. We then show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens (when generating sequences that are larger than the maximal length that the transformer can handle at once). Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. By combining these techniques, we increase training speed by 65%, make generation nine times faster, and substantially improve perplexity on WikiText-103, without adding any parameters.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2012.15832](https://arxiv.org/abs/2012.15832) [cs.CL]** |
|           | (or **[arXiv:2012.15832v1](https://arxiv.org/abs/2012.15832v1) [cs.CL]** for this version) |





<h2 id="2021-01-01-32">32. Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade</h2>

Title: [Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade](https://arxiv.org/abs/2012.15833)

Authors: [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Xiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X)

> Fully non-autoregressive neural machine translation (NAT) is proposed to simultaneously predict tokens with single forward of neural networks, which significantly reduces the inference latency at the expense of quality drop compared to the Transformer baseline. In this work, we target on closing the performance gap while maintaining the latency advantage. We first inspect the fundamental issues of fully NAT models, and adopt dependency reduction in the learning space of output tokens as the basic guidance. Then, we revisit methods in four different aspects that have been proven effective for improving NAT models, and carefully combine these techniques with necessary modifications. Our extensive experiments on three translation benchmarks show that the proposed system achieves the new state-of-the-art results for fully NAT models, and obtains comparable performance with the autoregressive and iterative NAT systems. For instance, one of the proposed models achieves 27.49 BLEU points on WMT14 En-De with approximately 16.5X speed up at inference time.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2012.15833](https://arxiv.org/abs/2012.15833) [cs.CL]** |
|           | (or **[arXiv:2012.15833v1](https://arxiv.org/abs/2012.15833v1) [cs.CL]** for this version) |

