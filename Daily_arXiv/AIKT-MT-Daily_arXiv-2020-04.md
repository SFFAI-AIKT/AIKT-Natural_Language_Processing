# Daily arXiv: Machine Translation - Apr., 2020

# Index

- [2020-04-30](#2020-04-30)
  - [1. Neural Machine Translation for Low-Resourced Indian Languages](#2020-04-30-1)
  - [2. Synonymy = Translational Equivalence](#2020-04-30-2)
  - [3. Revisiting Pre-Trained Models for Chinese Natural Language Processing](#2020-04-30-3)
  - [4. Revisiting Round-Trip Translation for Quality Estimation](#2020-04-30-4)
  - [5. Multiscale Collaborative Deep Models for Neural Machine Translation](#2020-04-30-5)
  - [6. Automatically Identifying Gender Issues in Machine Translation using Perturbations](#2020-04-30-6)
  - [7. Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning](#2020-04-30-7)
  - [8. Adversarial Subword Regularization for Robust Neural Machine Translation](#2020-04-30-8)
  - [9. Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings](#2020-04-30-9)
  - [10. SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](#2020-04-30-10)
  - [11. Reevaluating Adversarial Examples in Natural Language](#2020-04-30-11)
  - [12. Syntax-aware Data Augmentation for Neural Machine Translation](#2020-04-30-12)
  - [13. Towards Character-Level Transformer NMT by Finetuning Subword Systems](#2020-04-30-13)
- [2020-04-29](#2020-04-29)
  - [1. Cross-lingual Information Retrieval with BERT](#2020-04-29-1)
  - [2. Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets](#2020-04-29-2)
  - [3. Simultaneous Translation Policies: From Fixed to Adaptive](#2020-04-29-3)
  - [4. MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP](#2020-04-29-4)
  - [5. Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models](#2020-04-29-5)
  - [6. Self-Attention with Cross-Lingual Position Representation](#2020-04-29-6)
  - [7. Scheduled DropHead: A Regularization Method for Transformer Models](#2020-04-29-7)
  - [8. Extending Multilingual BERT to Low-Resource Languages](#2020-04-29-8)
  - [9. Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data](#2020-04-29-9)
- [2020-04-28](#2020-4-28)
  - [1. Jointly Trained Transformers models for Spoken Language Translation](#2020-04-28-1)
  - [2. A Batch Normalized Inference Network Keeps the KL Vanishing Away](#2020-04-28-2)
  - [3. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](#2020-04-28-3)
  - [4. All Word Embeddings from One Embedding](#2020-04-28-4)
  - [5. Neural Machine Translation with Monte-Carlo Tree Search](#2020-04-28-5)
  - [6. Lexically Constrained Neural Machine Translation with Levenshtein Transformer](#2020-04-28-6)
  - [7. LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning](#2020-04-28-7)
  - [8. Intelligent Translation Memory Matching and Retrieval with Sentence Encoders](#2020-04-28-8)
  - [9. DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](#2020-04-28-9)
- [2020-04-27](#2020-4-27)
  - [1. Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent Neural Networks](#2020-04-27-1)
  - [2. A Tool for Facilitating OCR Postediting in Historical Documents](#2020-04-27-2)
  - [3. Multiple Segmentations of Thai Sentences for Neural Machine Translation](#2020-04-27-3)
  - [4. On Sparsifying Encoder Outputs in Sequence-to-Sequence Models](#2020-04-27-4)
  - [5. Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation](#2020-04-27-5)
  - [6. Lite Transformer with Long-Short Range Attention](#2020-04-27-6)
- [2020-04-24](#2020-04-24)
  - [1. Revisiting the Context Window for Cross-lingual Word Embeddings](#2020-04-24-1)
  - [2. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](#2020-04-24-2)
  - [3. Self-Attention Attribution: Interpreting Information Interactions Inside Transformer](#2020-04-24-3)
  - [4. Correct Me If You Can: Learning from Error Corrections and Markings](#2020-04-24-4)
- [2020-04-23](#2020-04-23)
  - [1. CORD-19: The Covid-19 Open Research Dataset](#2020-04-23-1)
  - [2. ESPnet-ST: All-in-One Speech Translation Toolkit](#2020-04-23-2)
  - [3. Testing Machine Translation via Referential Transparency](#2020-04-23-3)
  - [4. A Study of Non-autoregressive Model for Sequence Generation](#2020-04-23-4)
  - [5. When and Why is Unsupervised Neural Machine Translation Useless?](#2020-04-23-5)
- [2020-04-22](#2020-04-22)
  - [1. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](#2020-04-22-1)
  - [2. Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns](#2020-04-22-2)
  - [3. Curriculum Pre-training for End-to-End Speech Translation](#2020-04-22-3)
  - [4. Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms](#2020-04-22-4)
  - [5. Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation](#2020-04-22-5)
  - [6. Energy-Based Models for Text](#2020-04-22-6)
- [2020-04-21](#2020-04-21)
  - [1. The Cost of Training NLP Models: A Concise Overview](#2020-04-21-1)
  - [2. Adversarial Training for Large Neural Language Models](#2020-04-21-2)
  - [3. A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT](#2020-04-21-3)
  - [4. MPNet: Masked and Permuted Pre-training for Language Understanding](#2020-04-21-4)
  - [5. PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation](#2020-04-21-5)
- [2020-04-20](#2020-04-20)
  - [1. Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings](#2020-04-20-1)
  - [2. Understanding the Difficulty of Training Transformers](#2020-04-20-2)
  - [3. Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation](#2020-04-20-3)
  - [4. Batch Clustering for Multilingual News Streaming](#2020-04-20-4)
- [2020-04-17](#2020-04-17)
  - [1. Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation](#2020-04-17-1)
  - [2. Non-Autoregressive Machine Translation with Latent Alignments](#2020-04-17-2)
  - [3. Do sequence-to-sequence VAEs learn global features of sentences?](#2020-04-17-3)
  - [4. Cross-lingual Contextualized Topic Models with Zero-shot Learning](#2020-04-17-4)
- [2020-04-16](#2020-04-16)
  - [1. A hybrid classical-quantum workflow for natural language processing](#2020-04-16-1)
  - [2. lamBERT: Language and Action Learning Using Multimodal BERT](#2020-04-16-2)
  - [3. Balancing Training for Multilingual Neural Machine Translation](#2020-04-16-3)
  - [4. PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](#2020-04-16-4)
  - [5. Document-level Representation Learning using Citation-informed Transformers](#2020-04-16-5)
- [2020-04-15](#2020-04-15)
  - [1.Code Completion using Neural Attention and Byte Pair Encoding ](#2020-04-15-1)
  - [2. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](#2020-04-15-2)
  - [3. What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models](#2020-04-15-3)
  - [4. Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders](#2020-04-15-4)
- [2020-04-14](#2020-04-14)
  - [1. On the Language Neutrality of Pre-trained Multilingual Representations](#2020-04-14-1)
  - [2. Joint translation and unit conversion for end-to-end localization](#2020-04-14-2)
  - [3. When Does Unsupervised Machine Translation Work?](#2020-04-14-3)
- [2020-04-13](#2020-04-13)
  - [1. An In-depth Walkthrough on Evolution of Neural Machine Translation](#2020-04-13-1)
  - [2. Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data](#2020-04-13-2)
  - [3. Automated Spelling Correction for Clinical Text Mining in Russian](#2020-04-13-3)
  - [4. Longformer: The Long-Document Transformer](#2020-04-13-4)
- [2020-04-10](#2020-04-10-1)
  - [1. Learning to Scale Multilingual Representations for Vision-Language Tasks](#2020-04-10-1)
  - [2. On optimal transformer depth for low-resource language translation](#2020-04-10-2)
  - [3. Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](#2020-04-10-3)
  - [4. Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios](#2020-04-10-4)
  - [5. Translation Artifacts in Cross-lingual Transfer Learning](#2020-04-10-5)
- [2020-04-09](#2020-04-09)
  - [1. Re-translation versus Streaming for Simultaneous Translation](#2020-04-09-1)
  - [2. Dynamic Data Selection and Weighting for Iterative Back-Translation](#2020-04-09-2)
  - [3. Byte Pair Encoding is Suboptimal for Language Model Pretraining](#2020-04-09-3)
  - [4. Improving BERT with Self-Supervised Attention](#2020-04-09-4)
  - [5. Explicit Reordering for Neural Machine Translation](#2020-04-09-5)
  - [6. Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation](#2020-04-09-6)
- [2020-04-08](#2020-04-08)
  - [1. Multilingual enrichment of disease biomedical ontologies](#2020-04-08-1)
  - [2. Unsupervised Neural Machine Translation with Indirect Supervision](#2020-04-08-2)
  - [3. Self-Induced Curriculum Learning in Neural Machine Translation](#2020-04-08-3)
  - [4. Machine Translation with Unsupervised Length-Constraints](#2020-04-08-4)
  - [5. Towards Multimodal Simultaneous Neural Machine Translation](#2020-04-08-5)
  - [6. Improving Fluency of Non-Autoregressive Machine Translation](#2020-04-08-6)
- [2020-04-07](#2020-04-07)
  - [1. Neural Machine Translation with Imbalanced Classes](#2020-04-07-1)
  - [2. Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation](#2020-04-07-2)
  - [3. Meta-Learning for Few-Shot NMT Adaptation](#2020-04-07-3)
  - [4. Applying Cyclical Learning Rate to Neural Machine Translation](#2020-04-07-4)
  - [5. Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation](#2020-04-07-5)
  - [6. Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech](#2020-04-07-6)
  - [7. Reference Language based Unsupervised Neural Machine Translation](#2020-04-07-7)
  - [8. Detecting and Understanding Generalization Barriers for Neural Machine Translation](#2020-04-07-8)
  - [9. AR: Auto-Repair the Synthetic Data for Neural Machine Translation](#2020-04-07-9)
  - [10. Understanding Learning Dynamics for Neural Machine Translation](#2020-04-07-10)
- [2020-04-06](#2020-04-06)
  - [1. XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation](#2020-04-06-1)
  - [2. Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation](#2020-04-06-2)
  - [3. Aligned Cross Entropy for Non-Autoregressive Machine Translation](#2020-04-06-3)
  - [4. A Set of Recommendations for Assessing Human-Machine Parity in Language Translation](#2020-04-06-4)
- [2020-04-03](#2020-04-03)
  - [1. Igbo-English Machine Translation: An Evaluation Benchmark](#2020-04-03-1)
  - [2. Mapping Languages: The Corpus of Global Language Use](#2020-04-03-2)
- [2020-04-02](#2020-04-02)
  - [1. Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study](#2020-04-02-1)
  - [2. Sign Language Translation with Transformers](#2020-04-02-2)
- [2020-04-01](#2020-04-01)
  - [1. The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe](#2020-04-01-1)
  - [2. MULTEXT-East](#2020-04-01-2)
  - [3. Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks](#2020-04-01-3)
  - [4. On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation](#2020-04-01-4)
  - [5. Evaluating Amharic Machine Translation](#2020-04-01-5)
  - [6. Low Resource Neural Machine Translation: A Benchmark for Five African Languages](#2020-04-01-6)
- [2020-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-03.md)
- [2020-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-02.md)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-04-30

[Return to Index](#Index)



<h2 id="2020-04-30-1">1. Neural Machine Translation for Low-Resourced Indian Languages</h2>

Title: [Neural Machine Translation for Low-Resourced Indian Languages](https://arxiv.org/abs/2004.13819)

Authors: [Himanshu Choudhary](https://arxiv.org/search/cs?searchtype=author&query=Choudhary%2C+H), [Shivansh Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+S), [Rajesh Rohilla](https://arxiv.org/search/cs?searchtype=author&query=Rohilla%2C+R)

> A large number of significant assets are available online in English, which is frequently translated into native languages to ease the information sharing among local people who are not much familiar with English. However, manual translation is a very tedious, costly, and time-taking process. To this end, machine translation is an effective approach to convert text to a different language without any human involvement. Neural machine translation (NMT) is one of the most proficient translation techniques amongst all existing machine translation systems. In this paper, we have applied NMT on two of the most morphological rich Indian languages, i.e. English-Tamil and English-Malayalam. We proposed a novel NMT model using Multihead self-attention along with pre-trained Byte-Pair-Encoded (BPE) and MultiBPE embeddings to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for low resourced morphological rich Indian languages which do not have much translation available online. We also collected corpus from different sources, addressed the issues with these publicly available data and refined them for further uses. We used the BLEU score for evaluating our system performance. Experimental results and survey confirmed that our proposed translator (24.34 and 9.78 BLEU score) outperforms Google translator (9.40 and 5.94 BLEU score) respectively.

| Comments: | Conference paper accepted to LREC-2020                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.13819](https://arxiv.org/abs/2004.13819) [cs.CL]** |
|           | (or **[arXiv:2004.13819v1](https://arxiv.org/abs/2004.13819v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-2">2. Synonymy = Translational Equivalence</h2>

Title: [Synonymy = Translational Equivalence](https://arxiv.org/abs/2004.13886)

Authors: [Bradley Hauer](https://arxiv.org/search/cs?searchtype=author&query=Hauer%2C+B), [Grzegorz Kondrak](https://arxiv.org/search/cs?searchtype=author&query=Kondrak%2C+G)

> Synonymy and translational equivalence are the relations of sameness of meaning within and across languages. As the principal relations in wordnets and multi-wordnets, they are vital to computational lexical semantics, yet the field suffers from the absence of a common formal framework to define their properties and mutual relationship. This paper proposes a unifying treatment of these two relations, which is validated by experiments on existing resources. The theory establishes a solid foundation for critically re-evaluating prior work in cross-lingual semantics, and facilitating the creation, verification, and amelioration of lexical resources.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13886](https://arxiv.org/abs/2004.13886) [cs.CL]** |
|           | (or **[arXiv:2004.13886v1](https://arxiv.org/abs/2004.13886v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-3">3. Revisiting Pre-Trained Models for Chinese Natural Language Processing</h2>

Title: [Revisiting Pre-Trained Models for Chinese Natural Language Processing](https://arxiv.org/abs/2004.13922)

Authors: [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Guoping Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+G)

> Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and various variants have been proposed to further improve the performance of the pre-trained models. In this paper, we target on revisiting Chinese pre-trained models to examine their effectiveness in a non-English language and release the Chinese pre-trained model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy. We carried out extensive experiments on various Chinese NLP tasks, covering sentence-level to document-level, to revisit the existing pre-trained models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.

| Comments: | 11 pages, as an extension to [arXiv:1906.08101](https://arxiv.org/abs/1906.08101) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.13922](https://arxiv.org/abs/2004.13922) [cs.CL]** |
|           | (or **[arXiv:2004.13922v1](https://arxiv.org/abs/2004.13922v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-4">4. Revisiting Round-Trip Translation for Quality Estimation</h2>

Title: [Revisiting Round-Trip Translation for Quality Estimation](https://arxiv.org/abs/2004.13937)

Authors: [Jihyung Moon](https://arxiv.org/search/cs?searchtype=author&query=Moon%2C+J), [Hyunchang Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+H), [Eunjeong L. Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+E+L)

> Quality estimation (QE) is the task of automatically evaluating the quality of translations without human-translated references. Calculating BLEU between the input sentence and round-trip translation (RTT) was once considered as a metric for QE, however, it was found to be a poor predictor of translation quality. Recently, various pre-trained language models have made breakthroughs in NLP tasks by providing semantically meaningful word and sentence embeddings. In this paper, we employ semantic embeddings to RTT-based QE. Our method achieves the highest correlations with human judgments, compared to previous WMT 2019 quality estimation metric task submissions. While backward translation models can be a drawback when using RTT, we observe that with semantic-level metrics, RTT-based QE is robust to the choice of the backward translation system. Additionally, the proposed method shows consistent performance for both SMT and NMT forward translation systems, implying the method does not penalize a certain type of model.

| Comments: | To be published in EAMT 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.13937](https://arxiv.org/abs/2004.13937) [cs.CL]** |
|           | (or **[arXiv:2004.13937v1](https://arxiv.org/abs/2004.13937v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-5">5. Multiscale Collaborative Deep Models for Neural Machine Translation</h2>

Title: [Multiscale Collaborative Deep Models for Neural Machine Translation](https://arxiv.org/abs/2004.14021)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-German task that significantly outperforms state-of-the-art deep NMT models.

| Comments: | Accpeted by ACL-2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14021](https://arxiv.org/abs/2004.14021) [cs.CL]** |
|           | (or **[arXiv:2004.14021v1](https://arxiv.org/abs/2004.14021v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-6">6. Automatically Identifying Gender Issues in Machine Translation using Perturbations</h2>

Title: [Automatically Identifying Gender Issues in Machine Translation using Perturbations](https://arxiv.org/abs/2004.14065)

Authors: [Hila Gonen](https://arxiv.org/search/cs?searchtype=author&query=Gonen%2C+H), [Kellie Webster](https://arxiv.org/search/cs?searchtype=author&query=Webster%2C+K)

> The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Where previous studies have identified concerns using manually-curated synthetic examples, we develop a novel technique to leverage real world data to explore challenges for deployed systems. We use our new method to compile an evaluation benchmark spanning examples relating to four languages from three language families, which we will publicly release to facilitate research. The examples in our benchmark expose the ways in which gender is represented in a model and the unintended consequences these gendered representations can have in downstream applications.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14065](https://arxiv.org/abs/2004.14065) [cs.CL]** |
|           | (or **[arXiv:2004.14065v1](https://arxiv.org/abs/2004.14065v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-7">7. Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning</h2>

Title: [Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning](https://arxiv.org/abs/2004.14074)

Authors: [Alexandre Tamborrino](https://arxiv.org/search/cs?searchtype=author&query=Tamborrino%2C+A), [Nicola Pellicano](https://arxiv.org/search/cs?searchtype=author&query=Pellicano%2C+N), [Baptiste Pannier](https://arxiv.org/search/cs?searchtype=author&query=Pannier%2C+B), [Pascal Voitot](https://arxiv.org/search/cs?searchtype=author&query=Voitot%2C+P), [Louise Naudin](https://arxiv.org/search/cs?searchtype=author&query=Naudin%2C+L)

> Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g ×10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.14074](https://arxiv.org/abs/2004.14074) [cs.CL]** |
|           | (or **[arXiv:2004.14074v1](https://arxiv.org/abs/2004.14074v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-8">8. Adversarial Subword Regularization for Robust Neural Machine Translation</h2>

Title: [Adversarial Subword Regularization for Robust Neural Machine Translation](https://arxiv.org/abs/2004.14109)

Authors: [Jungsoo Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J), [Mujeen Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+M), [Jinhyuk Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Jaewoo Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J)

> Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of machine translation. As NMT models experience various subword candidates, they become more robust to segmentation errors. However, the distribution of subword segmentations heavily relies on the subword language models from which erroneous segmentations of unseen words are less likely to be sampled. In this paper, we present adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for choosing segmentation among candidates. We experimentally show that our model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the robustness of NMT models in low-resource datasets.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14109](https://arxiv.org/abs/2004.14109) [cs.CL]** |
|           | (or **[arXiv:2004.14109v1](https://arxiv.org/abs/2004.14109v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-9">9. Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings</h2>

Title: [Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings](https://arxiv.org/abs/2004.14120)

Authors: [António Góis](https://arxiv.org/search/cs?searchtype=author&query=Góis%2C+A), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [André Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A)

> Recent research in neural machine translation has explored flexible generation orders, as an alternative to left-to-right generation. However, training non-monotonic models brings a new complication: how to search for a good ordering when there is a combinatorial explosion of orderings arriving at the same final result? Also, how do these automatic orderings compare with the actual behaviour of human translators? Current models rely on manually built biases or are left to explore all possibilities on their own. In this paper, we analyze the orderings produced by human post-editors and use them to train an automatic post-editing system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs.

| Comments: | Accepted at EAMT 2020; dataset available here: [this https URL](https://github.com/antoniogois/keystrokes_ape) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.14120](https://arxiv.org/abs/2004.14120) [cs.CL]** |
|           | (or **[arXiv:2004.14120v1](https://arxiv.org/abs/2004.14120v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-10">10. SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check</h2>

Title: [SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://arxiv.org/abs/2004.14166)

Authors: [Xingyi Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+X), [Weidi Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Kunlong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Shaohua Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S), [Feng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Taifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Wei Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+W), [Yuan Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y)

> Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments~\footnote{The dataset and all code for this paper are available at [this https URL](https://github.com/ACL2020SpellGCN/SpellGCN) } are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14166](https://arxiv.org/abs/2004.14166) [cs.CL]** |
|           | (or **[arXiv:2004.14166v1](https://arxiv.org/abs/2004.14166v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-11">11. Reevaluating Adversarial Examples in Natural Language</h2>

Title: [Reevaluating Adversarial Examples in Natural Language](https://arxiv.org/abs/2004.14174)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X), [Eli Lifland](https://arxiv.org/search/cs?searchtype=author&query=Lifland%2C+E), [Jack Lanchantin](https://arxiv.org/search/cs?searchtype=author&query=Lanchantin%2C+J), [Yangfeng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+Y), [Yanjun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y)

> State-of-the-art attacks on NLP models have different definitions of what constitutes a successful attack. These differences make the attacks difficult to compare. We propose to standardize definitions of natural language adversarial examples based on a set of linguistic constraints: semantics, grammaticality, edit distance, and non-suspicion. We categorize previous attacks based on these constraints. For each constraint, we suggest options for human and automatic evaluation methods. We use these methods to evaluate two state-of-the-art synonym substitution attacks. We find that perturbations often do not preserve semantics, and 45\% introduce grammatical errors. Next, we conduct human studies to find a threshold for each evaluation method that aligns with human judgment. Human surveys reveal that to truly preserve semantics, we need to significantly increase the minimum cosine similarity between the embeddings of swapped words and sentence encodings of original and perturbed inputs. After tightening these constraints to agree with the judgment of our human annotators, the attacks produce valid, successful adversarial examples. But quality comes at a cost: attack success rate drops by over 70 percentage points. Finally, we introduce TextAttack, a library for adversarial attacks in NLP.

| Comments: | 14 pages; 10 Tables; 4 Figures                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.14174](https://arxiv.org/abs/2004.14174) [cs.CL]** |
|           | (or **[arXiv:2004.14174v1](https://arxiv.org/abs/2004.14174v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-12">12. Syntax-aware Data Augmentation for Neural Machine Translation</h2>

Title: [Syntax-aware Data Augmentation for Neural Machine Translation](https://arxiv.org/abs/2004.14200)

Authors: [Sufeng Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+S), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R)

> Data augmentation is an effective performance enhancement in neural machine translation (NMT) by generating additional bilingual data. In this paper, we propose a novel data augmentation enhancement strategy for neural machine translation. Different from existing data augmentation methods which simply choose words with the same probability across different sentences for modification, we set sentence-specific probability for word selection by considering their roles in sentence. We use dependency parse tree of input sentence as an effective clue to determine selecting probability for every words in each sentence. Our proposed method is evaluated on WMT14 English-to-German dataset and IWSLT14 German-to-English dataset. The result of extensive experiments show our proposed syntax-aware data augmentation method may effectively boost existing sentence-independent methods for significant translation performance improvement.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.14200](https://arxiv.org/abs/2004.14200) [cs.CL]** |
|           | (or **[arXiv:2004.14200v1](https://arxiv.org/abs/2004.14200v1) [cs.CL]** for this version) |





<h2 id="2020-04-30-13">13. Towards Character-Level Transformer NMT by Finetuning Subword Systems</h2>

Title: [Towards Character-Level Transformer NMT by Finetuning Subword Systems](https://arxiv.org/abs/2004.14280)

Authors: [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. A few approaches have been proposed that partially overcome this problem by using explicit segmentation into tokens. We show that by initially training a subword model based on this segmentation and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring segmentation. Without changing the vanilla 6-layer Transformer Base architecture, we train purely character-level models. Our character-level models better capture morphological phenomena and show much higher robustness towards source-side noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance character-based models that are not extremely large.

| Comments: | 6 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.14280](https://arxiv.org/abs/2004.14280) [cs.CL]** |
|           | (or **[arXiv:2004.14280v1](https://arxiv.org/abs/2004.14280v1) [cs.CL]** for this version) |



# 2020-04-29

[Return to Index](#Index)



<h2 id="2020-04-29-1">1. Cross-lingual Information Retrieval with BERT</h2>

Title: [Cross-lingual Information Retrieval with BERT](https://arxiv.org/abs/2004.13005)

Authors: [Zhuolin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Amro El-Jaroudi](https://arxiv.org/search/cs?searchtype=author&query=El-Jaroudi%2C+A), [William Hartmann](https://arxiv.org/search/cs?searchtype=author&query=Hartmann%2C+W), [Damianos Karakos](https://arxiv.org/search/cs?searchtype=author&query=Karakos%2C+D), [Lingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L)

> Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.

| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13005](https://arxiv.org/abs/2004.13005) [cs.IR]** |
|           | (or **[arXiv:2004.13005v1](https://arxiv.org/abs/2004.13005v1) [cs.IR]** for this version) |





<h2 id="2020-04-29-2">2. Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets</h2>

Title: [Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets](https://arxiv.org/abs/2004.13138)

Authors: [Jinghui Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Brian MacNamee](https://arxiv.org/search/cs?searchtype=author&query=MacNamee%2C+B)

> Active learning has been shown to be an effective way to alleviate some of the effort required in utilising large collections of unlabelled data for machine learning tasks without needing to fully label them. The representation mechanism used to represent text documents when performing active learning, however, has a significant influence on how effective the process will be. While simple vector representations such as bag-of-words and embedding-based representations based on techniques such as word2vec have been shown to be an effective way to represent documents during active learning, the emergence of representation mechanisms based on the pre-trained transformer-based neural network models popular in natural language processing research (e.g. BERT) offer a promising, and as yet not fully explored, alternative. This paper describes a comprehensive evaluation of the effectiveness of representations based on pre-trained transformer-based language models for active learning. This evaluation shows that transformer-based models, especially BERT-like models, that have not yet been widely used in active learning, achieve a significant improvement over more commonly used vector representations like bag-of-words or other classical word embeddings like word2vec. This paper also investigates the effectiveness of representations based on variants of BERT such as Roberta, Albert as well as comparing the effectiveness of the [CLS] token representation and the aggregated representation that can be generated using BERT-like models. Finally, we propose an approach Adaptive Tuning Active Learning. Our experiments show that the limited label information acquired in active learning can not only be used for training a classifier but can also adaptively improve the embeddings generated by the BERT-like language models as well.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:1910.03505](https://arxiv.org/abs/1910.03505) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.13138](https://arxiv.org/abs/2004.13138) [cs.IR]** |
|           | (or **[arXiv:2004.13138v1](https://arxiv.org/abs/2004.13138v1) [cs.IR]** for this version) |





<h2 id="2020-04-29-3">3. Simultaneous Translation Policies: From Fixed to Adaptive</h2>

Title: [Simultaneous Translation Policies: From Fixed to Adaptive](https://arxiv.org/abs/2004.13169)

Authors: [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Kaibo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Hairong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform the simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese->English and German->English show that our adaptive policies can outperform the fixed policies by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses greedy full-sentence translation in BLEU scores, but with much lower latency.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13169](https://arxiv.org/abs/2004.13169) [cs.CL]** |
|           | (or **[arXiv:2004.13169v1](https://arxiv.org/abs/2004.13169v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-4">4. MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP</h2>

Title: [MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP](https://arxiv.org/abs/2004.13240)

Authors: [M Saiful Bari](https://arxiv.org/search/cs?searchtype=author&query=Bari%2C+M+S), [Muhammad Tasnim Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+M+T), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13240](https://arxiv.org/abs/2004.13240) [cs.CL]** |
|           | (or **[arXiv:2004.13240v1](https://arxiv.org/abs/2004.13240v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-5">5. Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models</h2>

Title: [Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models](https://arxiv.org/abs/2004.13270)

Authors: [Shilin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+S), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Machine translation (MT) systems translate text between different languages by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples. Although neural machine translation (NMT) has led the field of MT, we have a poor understanding on how and why it works. In this paper, we bridge the gap by assessing the bilingual knowledge learned by NMT models with phrase table -- an interpretable table of bilingual lexicons. We extract the phrase table from the training examples that an NMT model correctly predicts. Extensive experiments on widely-used datasets show that the phrase table is reasonable and consistent against language pairs and random seeds. Equipped with the interpretable phrase table, we find that NMT models learn patterns from simple to complex and distill essential bilingual knowledge from the training examples. We also revisit some advances that potentially affect the learning of bilingual knowledge (e.g., back-translation), and report some interesting findings. We believe this work opens a new angle to interpret NMT with statistic models, and provides empirical supports for recent advances in improving NMT models.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.13270](https://arxiv.org/abs/2004.13270) [cs.CL]** |
|           | (or **[arXiv:2004.13270v1](https://arxiv.org/abs/2004.13270v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-6">6. Self-Attention with Cross-Lingual Position Representation</h2>

Title: [Self-Attention with Cross-Lingual Position Representation](https://arxiv.org/abs/2004.13310)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, \eg machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with \emph{cross-lingual position representations} to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT'14 English$\Rightarrow$German, WAT'17 Japanese$\Rightarrow$English, and WMT'17 Chinese$\Leftrightarrow$English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.

| Comments: | To appear in ACL 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.13310](https://arxiv.org/abs/2004.13310) [cs.CL]** |
|           | (or **[arXiv:2004.13310v1](https://arxiv.org/abs/2004.13310v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-7">7. Scheduled DropHead: A Regularization Method for Transformer Models</h2>

Title: [Scheduled DropHead: A Regularization Method for Transformer Models](https://arxiv.org/abs/2004.13342)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> In this paper, we introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism, which is a key component of transformer, a state-of-the-art model for various NLP tasks. In contrast to the conventional dropout mechanisms which randomly drop units or connections, the proposed DropHead is a structured dropout method. It drops entire attention-heads during training and It prevents the multi-head attention model from being dominated by a small portion of attention heads while also reduces the risk of overfitting the training data, thus making use of the multi-head attention mechanism more efficiently. Motivated by recent studies about the learning dynamic of the multi-head attention mechanism, we propose a specific dropout rate schedule to adaptively adjust the dropout rate of DropHead and achieve better regularization effect. Experimental results on both machine translation and text classification benchmark datasets demonstrate the effectiveness of the proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13342](https://arxiv.org/abs/2004.13342) [cs.CL]** |
|           | (or **[arXiv:2004.13342v1](https://arxiv.org/abs/2004.13342v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-8">8. Extending Multilingual BERT to Low-Resource Languages</h2>

Title: [Extending Multilingual BERT to Low-Resource Languages](https://arxiv.org/abs/2004.13640)

Authors: [Zihan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Karthikeyan K](https://arxiv.org/search/cs?searchtype=author&query=K%2C+K), [Stephen Mayhew](https://arxiv.org/search/cs?searchtype=author&query=Mayhew%2C+S), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success has focused only on the top 104 languages in Wikipedia that it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT (E-BERT) so that it can benefit any new language, and show that our approach benefits languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6% F1 on languages that are already in M-BERT and 23% F1 increase on new languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13640](https://arxiv.org/abs/2004.13640) [cs.CL]** |
|           | (or **[arXiv:2004.13640v1](https://arxiv.org/abs/2004.13640v1) [cs.CL]** for this version) |





<h2 id="2020-04-29-9">9. Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data</h2>

Title: [Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data](https://arxiv.org/abs/2004.13645)

Authors: [Alana Marzoev](https://arxiv.org/search/cs?searchtype=author&query=Marzoev%2C+A), [Samuel Madden](https://arxiv.org/search/cs?searchtype=author&query=Madden%2C+S), [M. Frans Kaashoek](https://arxiv.org/search/cs?searchtype=author&query=Kaashoek%2C+M+F), [Michael Cafarella](https://arxiv.org/search/cs?searchtype=author&query=Cafarella%2C+M), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> Large, human-annotated datasets are central to the development of natural language processing models. Collecting these datasets can be the most challenging part of the development process. We address this problem by introducing a general purpose technique for ``simulation-to-real'' transfer in language understanding problems with a delimited set of target behaviors, making it possible to develop models that can interpret natural utterances without natural training data. We begin with a synthetic data generation procedure, and train a model that can accurately interpret utterances produced by the data generator. To generalize to natural utterances, we automatically find projections of natural language utterances onto the support of the synthetic language, using learned sentence embeddings to define a distance metric. With only synthetic training data, our approach matches or outperforms state-of-the-art models trained on natural language data in several domains. These results suggest that simulation-to-real transfer is a practical framework for developing NLP applications, and that improved models for transfer might provide wide-ranging improvements in downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.13645](https://arxiv.org/abs/2004.13645) [cs.CL]** |
|           | (or **[arXiv:2004.13645v1](https://arxiv.org/abs/2004.13645v1) [cs.CL]** for this version) |









# 2020-04-28

[Return to Index](#Index)



<h2 id="2020-04-28-1">1. Jointly Trained Transformers models for Spoken Language Translation</h2>

Title: [Jointly Trained Transformers models for Spoken Language Translation](https://arxiv.org/abs/2004.12111)

Authors:[Hari Krishna Vydana](https://arxiv.org/search/cs?searchtype=author&query=Vydana%2C+H+K), [Martin Karafi'at](https://arxiv.org/search/cs?searchtype=author&query=Karafi'at%2C+M), [Katerina Zmolikova](https://arxiv.org/search/cs?searchtype=author&query=Zmolikova%2C+K), [Luk'as Burget](https://arxiv.org/search/cs?searchtype=author&query=Burget%2C+L), [Honza Cernocky](https://arxiv.org/search/cs?searchtype=author&query=Cernocky%2C+H)

> Conventional spoken language translation (SLT) systems are pipeline based systems, where we have an Automatic Speech Recognition (ASR) system to convert the modality of source from speech to text and a Machine Translation (MT) systems to translate source text to text in target language. Recent progress in the sequence-sequence architectures have reduced the performance gap between the pipeline based SLT systems (cascaded ASR-MT) and End-to-End approaches. Though End-to-End and cascaded ASR-MT systems are reaching to the comparable levels of performances, we can see a large performance gap using the ASR hypothesis and oracle text w.r.t MT models. This performance gap indicates that the MT systems are prone to large performance degradation due to noisy ASR hypothesis as opposed to oracle text transcript. In this work this degradation in the performance is reduced by creating an end to-end differentiable pipeline between the ASR and MT systems. In this work, we train SLT systems with ASR objective as an auxiliary loss and both the networks are connected through the neural hidden representations. This train ing would have an End-to-End differentiable path w.r.t to the final objective function as well as utilize the ASR objective for better performance of the SLT systems. This architecture has improved from BLEU from 36.8 to 44.5. Due to the Multi-task training the model also generates the ASR hypothesis which are used by a pre-trained MT model. Combining the proposed systems with the MT model has increased the BLEU score by 1. All the experiments are reported on English-Portuguese speech translation task using How2 corpus. The final BLEU score is on-par with the best speech translation system on How2 dataset with no additional training data and language model and much less parameters.

| Comments:    | 7-pages,3 figures                                            |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2004.12111](https://arxiv.org/abs/2004.12111) [cs.SD]** |
|              | (or **[arXiv:2004.12111v1](https://arxiv.org/abs/2004.12111v1) [cs.SD]** for this version) |





<h2 id="2020-04-28-2">2. A Batch Normalized Inference Network Keeps the KL Vanishing Away</h2>

Title: [A Batch Normalized Inference Network Keeps the KL Vanishing Away](https://arxiv.org/abs/2004.12585)

Authors:[Qile Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q), [Wei Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+W), [Xiaojiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xiyao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Xiaolin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Dapeng Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D)

> Variational Autoencoder (VAE) is widely used as a generative model to approximate a model's posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as "posterior collapse". Previous approaches consider the Kullback Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior's parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.

| Comments: | camera-ready for ACL 2020                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2004.12585](https://arxiv.org/abs/2004.12585) [cs.LG]** |
|           | (or **[arXiv:2004.12585v1](https://arxiv.org/abs/2004.12585v1) [cs.LG]** for this version) |





<h2 id="2020-04-28-3">3. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</h2>

Title: [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832)

Authors:[Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab%2C+O), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia%2C+M)

> Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.

| Comments: | Accepted at SIGIR 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2004.12832](https://arxiv.org/abs/2004.12832) [cs.IR]** |
|           | (or **[arXiv:2004.12832v1](https://arxiv.org/abs/2004.12832v1) [cs.IR]** for this version) |





<h2 id="2020-04-28-4">4. All Word Embeddings from One Embedding</h2>

Title: [All Word Embeddings from One Embedding](https://arxiv.org/abs/2004.12073)

Authors:[Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Sosuke Kobayashi](https://arxiv.org/search/cs?searchtype=author&query=Kobayashi%2C+S)

> In neural network-based models for natural language processing (NLP), the largest part of the parameters often consists of word embeddings. Conventional models prepare a large embedding matrix whose size depends on the vocabulary size. Therefore, storing these models in memory and disk storage is costly. In this study, to reduce the total number of parameters, the embeddings for all words are represented by transforming a shared embedding. The proposed method, ALONE (all word embeddings from one), constructs the embedding of a word by modifying the shared embedding with a filter vector, which is word-specific but non-trainable. Then, we input the constructed embedding into a feed-forward neural network to increase its expressiveness. Naively, the filter vectors occupy the same memory size as the conventional embedding matrix, which depends on the vocabulary size. To solve this issue, we also introduce a memory-efficient filter construction approach. We indicate our ALONE can be used as word representation sufficiently through an experiment on the reconstruction of pre-trained word embeddings. In addition, we also conduct experiments on NLP application tasks: machine translation and summarization. We combined ALONE with the current state-of-the-art encoder-decoder model, the Transformer, and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with less parameters.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.12073](https://arxiv.org/abs/2004.12073) [cs.CL]** |
|           | (or **[arXiv:2004.12073v1](https://arxiv.org/abs/2004.12073v1) [cs.CL]** for this version) |





<h2 id="2020-04-28-5">5. Neural Machine Translation with Monte-Carlo Tree Search</h2>

Title: [Neural Machine Translation with Monte-Carlo Tree Search](https://arxiv.org/abs/2004.12527)

Authors:[Jerrod Parker](https://arxiv.org/search/cs?searchtype=author&query=Parker%2C+J), [Jerry Zikun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J+Z)

> Recent algorithms in machine translation have included a value network to assist the policy network when deciding which word to output at each step of the translation. The addition of a value network helps the algorithm perform better on evaluation metrics like the BLEU score. After training the policy and value networks in a supervised setting, the policy and value networks can be jointly improved through common actor-critic methods. The main idea of our project is to instead leverage Monte-Carlo Tree Search (MCTS) to search for good output words with guidance from a combined policy and value network architecture in a similar fashion as AlphaZero. This network serves both as a local and a global look-ahead reference that uses the result of the search to improve itself. Experiments using the IWLST14 German to English translation dataset show that our method outperforms the actor-critic methods used in recent machine translation papers.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.12527](https://arxiv.org/abs/2004.12527) [cs.CL]** |
|           | (or **[arXiv:2004.12527v1](https://arxiv.org/abs/2004.12527v1) [cs.CL]** for this version) |





<h2 id="2020-04-28-6">6. Lexically Constrained Neural Machine Translation with Levenshtein Transformer</h2>

Title: [Lexically Constrained Neural Machine Translation with Levenshtein Transformer](https://arxiv.org/abs/2004.12681)

Authors:[Raymond Hendy Susanto](https://arxiv.org/search/cs?searchtype=author&query=Susanto%2C+R+H), [Shamil Chollampatt](https://arxiv.org/search/cs?searchtype=author&query=Chollampatt%2C+S), [Liling Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L)

> This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.

| Comments: | 8 pages, In Proceedings of ACL 2020                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.12681](https://arxiv.org/abs/2004.12681) [cs.CL]** |
|           | (or **[arXiv:2004.12681v1](https://arxiv.org/abs/2004.12681v1) [cs.CL]** for this version) |





<h2 id="2020-04-28-7">7. LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning</h2>

Title: [LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning](https://arxiv.org/abs/2004.12817)

Authors:[Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Hao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Jianfeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Hongzhi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> While pre-training and fine-tuning, e.g., BERT~\citep{devlin2018bert}, GPT-2~\citep{radford2019language}, have achieved great success in language understanding and generation tasks, the pre-trained models are usually too big for online deployment in terms of both memory cost and inference speed, which hinders them from practical online usage. In this paper, we propose LightPAFF, a Lightweight Pre-training And Fine-tuning Framework that leverages two-stage knowledge distillation to transfer knowledge from a big teacher model to a lightweight student model in both pre-training and fine-tuning stages. In this way the lightweight model can achieve similar accuracy as the big teacher model, but with much fewer parameters and thus faster online inference speed. LightPAFF can support different pre-training methods (such as BERT, GPT-2 and MASS~\citep{song2019mass}) and be applied to many downstream tasks. Experiments on three language understanding tasks, three language modeling tasks and three sequence to sequence generation tasks demonstrate that while achieving similar accuracy with the big BERT, GPT-2 and MASS models, LightPAFF reduces the model size by nearly 5x and improves online inference speed by 5x-7x.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.12817](https://arxiv.org/abs/2004.12817) [cs.CL]** |
|           | (or **[arXiv:2004.12817v1](https://arxiv.org/abs/2004.12817v1) [cs.CL]** for this version) |





<h2 id="2020-04-28-8">8. Intelligent Translation Memory Matching and Retrieval with Sentence Encoders</h2>

Title: [Intelligent Translation Memory Matching and Retrieval with Sentence Encoders](https://arxiv.org/abs/2004.12894)

Authors:[Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Ruslan Mitkov](https://arxiv.org/search/cs?searchtype=author&query=Mitkov%2C+R)

> Matching and retrieving previously translated segments from a Translation Memory is the key functionality in Translation Memories systems. However this matching and retrieving process is still limited to algorithms based on edit distance which we have identified as a major drawback in Translation Memories systems. In this paper we introduce sentence encoders to improve the matching and retrieving process in Translation Memories systems - an effective and efficient solution to replace edit distance based algorithms.

| Comments: | Accepted to EAMT 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.12894](https://arxiv.org/abs/2004.12894) [cs.CL]** |
|           | (or **[arXiv:2004.12894v1](https://arxiv.org/abs/2004.12894v1) [cs.CL]** for this version) |





<h2 id="2020-04-28-9">9. DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</h2>

Title: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993)

Authors:[Ji Xin](https://arxiv.org/search/cs?searchtype=author&query=Xin%2C+J), [Raphael Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+R), [Jaejun Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Yaoliang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Jimmy Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J)

> Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at [this https URL](https://github.com/castorini/DeeBERT).

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.12993](https://arxiv.org/abs/2004.12993) [cs.CL]** |
|           | (or **[arXiv:2004.12993v1](https://arxiv.org/abs/2004.12993v1) [cs.CL]** for this version) |







# 2020-04-27

[Return to Index](#Index)



<h2 id="2020-04-27-1">1. Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent Neural Networks</h2>

Title: [Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent Neural Networks](https://arxiv.org/abs/2004.11405)

Authors: [Nachum Dershowitz](https://arxiv.org/search/cs?searchtype=author&query=Dershowitz%2C+N), [Ori Terner](https://arxiv.org/search/cs?searchtype=author&query=Terner%2C+O)

> Many of the great Jewish works of the Middle Ages were written in Judeo-Arabic, a Jewish branch of the Arabic language family that incorporates the Hebrew script as its writing system. In this work we are trying to train a model that will automatically transliterate Judeo-Arabic into Arabic script; thus we aspire to enable Arabic readers to access those writings. We adopt a recurrent neural network (RNN) approach to the problem, applying connectionist temporal classification loss to deal with unequal input/output lengths. This choice obligates adjustments, termed doubling, in the training data to avoid input sequences that are shorter than their corresponding outputs. We also utilize a pretraining stage with a different loss function to help the network converge. Furthermore, since only a single source of parallel text was available for training, we examine the possibility of generating data synthetically from other Arabic original text from the time in question, leveraging the fact that, though the convention for mapping applied by the Judeo-Arabic author has a one-to-many relation from Judeo-Arabic to Arabic, its reverse (from Arabic to Judeo-Arabic) is a proper function. By this we attempt to train a model that has the capability to memorize words in the output language, and that also utilizes the context for distinguishing ambiguities in the transliteration. We examine this ability by testing on shuffled data that lacks context. We obtain an improvement over the baseline results (9.5% error), achieving 2% error with our system. On the shuffled test data, the error rises to 2.5%.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.11405](https://arxiv.org/abs/2004.11405) [cs.CL]** |
|           | (or **[arXiv:2004.11405v1](https://arxiv.org/abs/2004.11405v1) [cs.CL]** for this version) |



<h2 id="2020-04-27-2">2. A Tool for Facilitating OCR Postediting in Historical Documents</h2>

Title: [A Tool for Facilitating OCR Postediting in Historical Documents](https://arxiv.org/abs/2004.11471)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Mohammad Aboomar](https://arxiv.org/search/cs?searchtype=author&query=Aboomar%2C+M), [Jan Buts](https://arxiv.org/search/cs?searchtype=author&query=Buts%2C+J), [James Hadley](https://arxiv.org/search/cs?searchtype=author&query=Hadley%2C+J), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

> Optical character recognition (OCR) for historical documents is a complex procedure subject to a unique set of material issues, including inconsistencies in typefaces and low quality scanning. Consequently, even the most sophisticated OCR engines produce errors. This paper reports on a tool built for postediting the output of Tesseract, more specifically for correcting common errors in digitized historical documents. The proposed tool suggests alternatives for word forms not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The tool is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary ,1719). As demonstrated below, the tool is successful in correcting a number of common errors. If sometimes unreliable, it is also transparent and subject to human intervention.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Workshop on Language Technologies for Historical and Ancient Languages, LT4HALA (2020) |
| Cite as:           | **[arXiv:2004.11471](https://arxiv.org/abs/2004.11471) [cs.CL]** |
|                    | (or **[arXiv:2004.11471v1](https://arxiv.org/abs/2004.11471v1) [cs.CL]** for this version) |



<h2 id="2020-04-27-3">3. Multiple Segmentations of Thai Sentences for Neural Machine Translation</h2>

Title: [Multiple Segmentations of Thai Sentences for Neural Machine Translation](https://arxiv.org/abs/2004.11472)

Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Wichaya Pidchamook](https://arxiv.org/search/cs?searchtype=author&query=Pidchamook%2C+W), [Chao-Hong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [James Hadley](https://arxiv.org/search/cs?searchtype=author&query=Hadley%2C+J), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

> Thai is a low-resource language, so it is often the case that data is not available in sufficient quantities to train an Neural Machine Translation (NMT) model which perform to a high level of quality. In addition, the Thai script does not use white spaces to delimit the boundaries between words, which adds more complexity when building sequence to sequence models. In this work, we explore how to augment a set of English--Thai parallel data by replicating sentence-pairs with different word segmentation methods on Thai, as training data for NMT model training. Using different merge operations of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Spoken Language Technologies for Under-resourced languages and CCURL Collaboration and Computing for Under-Resourced Languages Workshop, SLTU-CCURL (2020) |
| Cite as:           | **[arXiv:2004.11472](https://arxiv.org/abs/2004.11472) [cs.CL]** |
|                    | (or **[arXiv:2004.11472v1](https://arxiv.org/abs/2004.11472v1) [cs.CL]** for this version) |



<h2 id="2020-04-27-4">4. On Sparsifying Encoder Outputs in Sequence-to-Sequence Models</h2>

Title: [On Sparsifying Encoder Outputs in Sequence-to-Sequence Models](https://arxiv.org/abs/2004.11854)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Sequence-to-sequence models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypothesize that these encoder outputs can be compressed to shorten the sequence delivered for decoding. We take Transformer as the testbed and introduce a layer of stochastic gates in-between the encoder and the decoder. The gates are regularized using the expected value of the sparsity-inducing L0penalty, resulting in completely masking-out a subset of encoder outputs. In other words, via joint training, the L0DROP layer forces Transformer to route information through a subset of its encoder states. We investigate the effects of this sparsification on two machine translation and two summarization tasks. Experiments show that, depending on the task, around 40-70% of source encodings can be pruned without significantly compromising quality. The decrease of the output length endows L0DROP with the potential of improving decoding efficiency, where it yields a speedup of up to 1.65x on document summarization tasks against the standard Transformer. We analyze the L0DROP behaviour and observe that it exhibits systematic preferences for pruning certain word types, e.g., function words and punctuation get pruned most. Inspired by these observations, we explore the feasibility of specifying rule-based patterns that mask out encoder outputs based on information such as part-of-speech tags, word frequency and word position.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.11854](https://arxiv.org/abs/2004.11854) [cs.CL]** |
|           | (or **[arXiv:2004.11854v1](https://arxiv.org/abs/2004.11854v1) [cs.CL]** for this version) |



<h2 id="2020-04-27-5">5. Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</h2>

Title: [Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation](https://arxiv.org/abs/2004.11867)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Philip Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+P), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.

| Comments: | ACL2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.11867](https://arxiv.org/abs/2004.11867) [cs.CL]** |
|           | (or **[arXiv:2004.11867v1](https://arxiv.org/abs/2004.11867v1) [cs.CL]** for this version) |



<h2 id="2020-04-27-6">6. Lite Transformer with Long-Short Range Attention</h2>

Title: [Lite Transformer with Long-Short Range Attention](https://arxiv.org/abs/2004.11886)

Authors: [Zhanghao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Zhijian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Ji Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Yujun Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Song Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S)

> Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at [this https URL](https://github.com/mit-han-lab/lite-transformer).

| Comments: | ICLR 2020. The first two authors contributed equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.11886](https://arxiv.org/abs/2004.11886) [cs.CL]** |
|           | (or **[arXiv:2004.11886v1](https://arxiv.org/abs/2004.11886v1) [cs.CL]** for this version) |





# 2020-04-24

[Return to Index](#Index)



<h2 id="2020-04-24-1">1. Revisiting the Context Window for Cross-lingual Word Embeddings</h2>

Title: [Revisiting the Context Window for Cross-lingual Word Embeddings](https://arxiv.org/abs/2004.10813)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows. The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.

| Comments: | ACL2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.10813](https://arxiv.org/abs/2004.10813) [cs.CL]** |
|           | (or **[arXiv:2004.10813v1](https://arxiv.org/abs/2004.10813v1) [cs.CL]** for this version) |





<h2 id="2020-04-24-2">2. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</h2>

Title: [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/abs/2004.10964)

Authors: [Suchin Gururangan](https://arxiv.org/search/cs?searchtype=author&query=Gururangan%2C+S), [Ana Marasović](https://arxiv.org/search/cs?searchtype=author&query=Marasović%2C+A), [Swabha Swayamdipta](https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.10964](https://arxiv.org/abs/2004.10964) [cs.CL]** |
|           | (or **[arXiv:2004.10964v1](https://arxiv.org/abs/2004.10964v1) [cs.CL]** for this version) |





<h2 id="2020-04-24-3">3. Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</h2>

Title: [Self-Attention Attribution: Interpreting Information Interactions Inside Transformer](https://arxiv.org/abs/2004.11207)

Authors: [Yaru Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K)

> The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution algorithm to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we extract the most salient dependencies in each layer to construct an attribution graph, which reveals the hierarchical interactions inside Transformer. Furthermore, we apply self-attention attribution to identify the important attention heads, while others can be pruned with only marginal performance degradation. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.11207](https://arxiv.org/abs/2004.11207) [cs.CL]** |
|           | (or **[arXiv:2004.11207v1](https://arxiv.org/abs/2004.11207v1) [cs.CL]** for this version) |





<h2 id="2020-04-24-4">4. Correct Me If You Can: Learning from Error Corrections and Markings</h2>

Title: [Correct Me If You Can: Learning from Error Corrections and Markings](https://arxiv.org/abs/2004.11222)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [Nathaniel Berger](https://arxiv.org/search/cs?searchtype=author&query=Berger%2C+N), [Stefan Riezler](https://arxiv.org/search/cs?searchtype=author&query=Riezler%2C+S)

> Sequence-to-sequence learning involves a trade-off between signal strength and annotation cost of training data. For example, machine translation data range from costly expert-generated translations that enable supervised learning, to weak quality-judgment feedback that facilitate reinforcement learning. We present the first user study on annotation cost and machine learnability for the less popular annotation mode of error markings. We show that error markings for translations of TED talks from English to German allow precise credit assignment while requiring significantly less human effort than correcting/post-editing, and that error-marked data can be used successfully to fine-tune neural machine translation models.

| Comments: | To appear at EAMT 2020 (Research Track)                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.11222](https://arxiv.org/abs/2004.11222) [cs.CL]** |
|           | (or **[arXiv:2004.11222v1](https://arxiv.org/abs/2004.11222v1) [cs.CL]** for this version) |





# 2020-04-23

[Return to Index](#Index)



<h2 id="2020-04-23-1">1. CORD-19: The Covid-19 Open Research Dataset</h2>

Title: [CORD-19: The Covid-19 Open Research Dataset](https://arxiv.org/abs/2004.10706)

Authors: [Lucy Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L+L), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Yoganand Chandrasekhar](https://arxiv.org/search/cs?searchtype=author&query=Chandrasekhar%2C+Y), [Russell Reas](https://arxiv.org/search/cs?searchtype=author&query=Reas%2C+R), [Jiangjiang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Darrin Eide](https://arxiv.org/search/cs?searchtype=author&query=Eide%2C+D), [Kathryn Funk](https://arxiv.org/search/cs?searchtype=author&query=Funk%2C+K), [Rodney Kinney](https://arxiv.org/search/cs?searchtype=author&query=Kinney%2C+R), [Ziyang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [William Merrill](https://arxiv.org/search/cs?searchtype=author&query=Merrill%2C+W), [Paul Mooney](https://arxiv.org/search/cs?searchtype=author&query=Mooney%2C+P), [Dewey Murdick](https://arxiv.org/search/cs?searchtype=author&query=Murdick%2C+D), [Devvret Rishi](https://arxiv.org/search/cs?searchtype=author&query=Rishi%2C+D), [Jerry Sheehan](https://arxiv.org/search/cs?searchtype=author&query=Sheehan%2C+J), [Zhihong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Brandon Stilson](https://arxiv.org/search/cs?searchtype=author&query=Stilson%2C+B), [Alex D. Wade](https://arxiv.org/search/cs?searchtype=author&query=Wade%2C+A+D), [Kuansan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Chris Wilhelm](https://arxiv.org/search/cs?searchtype=author&query=Wilhelm%2C+C), [Boya Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+B), [Douglas Raymond](https://arxiv.org/search/cs?searchtype=author&query=Raymond%2C+D), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S), [Oren Etzioni](https://arxiv.org/search/cs?searchtype=author&query=Etzioni%2C+O), [Sebastian Kohlmeier](https://arxiv.org/search/cs?searchtype=author&query=Kohlmeier%2C+S)

> The Covid-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on Covid-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 75K times and has served as the basis of many Covid-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and preview tools and upcoming shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for Covid-19.

| Comments: | 10 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Digital Libraries (cs.DL)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2004.10706](https://arxiv.org/abs/2004.10706) [cs.DL]** |
|           | (or **[arXiv:2004.10706v1](https://arxiv.org/abs/2004.10706v1) [cs.DL]** for this version) |





<h2 id="2020-04-23-2">2. ESPnet-ST: All-in-One Speech Translation Toolkit</h2>

Title: [ESPnet-ST: All-in-One Speech Translation Toolkit](https://arxiv.org/abs/2004.10234)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Shigeki Karita](https://arxiv.org/search/cs?searchtype=author&query=Karita%2C+S), [Nelson Enrique Yalta Soplin](https://arxiv.org/search/cs?searchtype=author&query=Soplin%2C+N+E+Y), [Tomoki Hayashi](https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at [this https URL](https://github.com/espnet/espnet).

| Comments: | Accepted at ACL 2020 System Demonstration                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2004.10234](https://arxiv.org/abs/2004.10234) [cs.CL]** |
|           | (or **[arXiv:2004.10234v1](https://arxiv.org/abs/2004.10234v1) [cs.CL]** for this version) |





<h2 id="2020-04-23-3">3. Testing Machine Translation via Referential Transparency</h2>

Title: [Testing Machine Translation via Referential Transparency](https://arxiv.org/abs/2004.10361)

Authors: [Pinjia He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Zhendong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Z)

> Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives, such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have invariant translation when used in different contexts. Our practical implementation, Purity, detects when this invariance property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which led to 123 and 142 erroneous translations with high precision (79.3\% and 78.3\%). The translation errors are diverse, including under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic. These translation errors could lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts.

| Subjects: | **Computation and Language (cs.CL)**; Software Engineering (cs.SE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.10361](https://arxiv.org/abs/2004.10361) [cs.CL]** |
|           | (or **[arXiv:2004.10361v1](https://arxiv.org/abs/2004.10361v1) [cs.CL]** for this version) |





<h2 id="2020-04-23-4">4. A Study of Non-autoregressive Model for Sequence Generation</h2>

Title: [A Study of Non-autoregressive Model for Sequence Generation](https://arxiv.org/abs/2004.10454)

Authors: [Yi Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y), [Jinglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Sheng Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.

| Comments: | Accepted by ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2004.10454](https://arxiv.org/abs/2004.10454) [cs.CL]** |
|           | (or **[arXiv:2004.10454v1](https://arxiv.org/abs/2004.10454v1) [cs.CL]** for this version) |





<h2 id="2020-04-23-5">5. When and Why is Unsupervised Neural Machine Translation Useless?</h2>

Title: [When and Why is Unsupervised Neural Machine Translation Useless?](https://arxiv.org/abs/2004.10581)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Miguel Graça](https://arxiv.org/search/cs?searchtype=author&query=Graça%2C+M), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> This paper studies the practicality of the current state-of-the-art unsupervised methods in neural machine translation (NMT). In ten translation tasks with various data settings, we analyze the conditions under which the unsupervised methods fail to produce reasonable translations. We show that their performance is severely affected by linguistic dissimilarity and domain mismatch between source and target monolingual data. Such conditions are common for low-resource language pairs, where unsupervised learning works poorly. In all of our experiments, supervised and semi-supervised baselines with 50k-sentence bilingual data outperform the best unsupervised results. Our analyses pinpoint the limits of the current unsupervised NMT and also suggest immediate research directions.

| Comments: | Will appear at EAMT 2020; Extended version of EAMT camera-ready (including appendix) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.10581](https://arxiv.org/abs/2004.10581) [cs.CL]** |
|           | (or **[arXiv:2004.10581v1](https://arxiv.org/abs/2004.10581v1) [cs.CL]** for this version) |







# 2020-04-22

[Return to Index](#Index)



<h2 id="2020-04-22-1">1. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</h2>

Title: [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813)

Authors: [Nils Reimers](https://arxiv.org/search/cs?searchtype=author&query=Reimers%2C+N), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09813](https://arxiv.org/abs/2004.09813) [cs.CL]** |
|           | (or **[arXiv:2004.09813v1](https://arxiv.org/abs/2004.09813v1) [cs.CL]** for this version) |





<h2 id="2020-04-22-2">2. Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns</h2>

Title: [Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns](https://arxiv.org/abs/2004.09894)

Authors: [KayYen Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+K), [Sameen Maruf](https://arxiv.org/search/cs?searchtype=author&query=Maruf%2C+S), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

> The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.

| Comments: | Accepted to ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.09894](https://arxiv.org/abs/2004.09894) [cs.CL]** |
|           | (or **[arXiv:2004.09894v1](https://arxiv.org/abs/2004.09894v1) [cs.CL]** for this version) |





<h2 id="2020-04-22-3">3. Curriculum Pre-training for End-to-End Speech Translation</h2>

Title: [Curriculum Pre-training for End-to-End Speech Translation](https://arxiv.org/abs/2004.10093)

Authors: [Chengyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Zhenglu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> End-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.

| Comments: | accepted by ACL2020                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2004.10093](https://arxiv.org/abs/2004.10093) [cs.CL]** |
|           | (or **[arXiv:2004.10093v1](https://arxiv.org/abs/2004.10093v1) [cs.CL]** for this version) |





<h2 id="2020-04-22-4">4. Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms</h2>

Title: [Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms](https://arxiv.org/abs/2004.10102)

Authors: [Goro Kobayashi](https://arxiv.org/search/cs?searchtype=author&query=Kobayashi%2C+G), [Tatsuki Kuribayashi](https://arxiv.org/search/cs?searchtype=author&query=Kuribayashi%2C+T), [Sho Yokoi](https://arxiv.org/search/cs?searchtype=author&query=Yokoi%2C+S), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> Because attention modules are core components of Transformer-based models that have recently achieved considerable success in natural language processing, the community has a great deal of interest in why attention modules are successful and what kind of linguistic information they capture. In particular, previous studies have mainly analyzed attention weights to see how much information the attention modules gather from each input to produce an output. In this study, we point out that attention weights alone are only one of the two factors determining the output of self-attention modules, and we propose to incorporate the other factor as well, namely, the transformed input vectors into the analysis. That is, we measure the norm of the weighted vectors as the contribution of each input to an output. Our analysis of self-attention modules in BERT and the Transformer-based neural machine translation system shows that the attention modules behave very intuitively, contrary to previous findings. That is, our analysis reveals that (1) BERT's attention modules do not pay so much attention to special tokens, and (2) Transformer's attention modules capture word alignment quite well.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.10102](https://arxiv.org/abs/2004.10102) [cs.CL]** |
|           | (or **[arXiv:2004.10102v1](https://arxiv.org/abs/2004.10102v1) [cs.CL]** for this version) |





<h2 id="2020-04-22-5">5. Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation</h2>

Title: [Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation](https://arxiv.org/abs/2004.10171)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

> Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.

| Comments: | Accepted to ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.10171](https://arxiv.org/abs/2004.10171) [cs.CL]** |
|           | (or **[arXiv:2004.10171v1](https://arxiv.org/abs/2004.10171v1) [cs.CL]** for this version) |





<h2 id="2020-04-22-6">6. Energy-Based Models for Text</h2>

Title: [Energy-Based Models for Text](https://arxiv.org/abs/2004.10188)

Authors: [Anton Bakhtin](https://arxiv.org/search/cs?searchtype=author&query=Bakhtin%2C+A), [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Sam Gross](https://arxiv.org/search/cs?searchtype=author&query=Gross%2C+S), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M), [Arthur Szlam](https://arxiv.org/search/cs?searchtype=author&query=Szlam%2C+A)

> Current large-scale auto-regressive language models display impressive fluency and can generate convincing text. In this work we start by asking the question: Can the generations of these models be reliably distinguished from real text by statistical discriminators? We find experimentally that the answer is affirmative when we have access to the training data for the model, and guardedly affirmative even if we do not. This suggests that the auto-regressive models can be improved by incorporating the (globally normalized) discriminators into the generative process. We give a formalism for this using the Energy-Based Model framework, and show that it indeed improves the results of the generative models, measured both in terms of perplexity and in terms of human evaluation.

| Comments: | long journal version                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.10188](https://arxiv.org/abs/2004.10188) [cs.CL]** |
|           | (or **[arXiv:2004.10188v1](https://arxiv.org/abs/2004.10188v1) [cs.CL]** for this version) |







# 2020-04-21

[Return to Index](#Index)



<h2 id="2020-04-21-1">1. The Cost of Training NLP Models: A Concise Overview</h2>

Title: [The Cost of Training NLP Models: A Concise Overview](https://arxiv.org/abs/2004.08900)

Authors: [Or Sharir](https://arxiv.org/search/cs?searchtype=author&query=Sharir%2C+O), [Barak Peleg](https://arxiv.org/search/cs?searchtype=author&query=Peleg%2C+B), [Yoav Shoham](https://arxiv.org/search/cs?searchtype=author&query=Shoham%2C+Y)

> We review the cost of training large-scale language models, and the drivers of these costs. The intended audience includes engineers and scientists budgeting their model-training experiments, as well as non-practitioners trying to make sense of the economics of modern-day Natural Language Processing (NLP).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.08900](https://arxiv.org/abs/2004.08900) [cs.CL]** |
|           | (or **[arXiv:2004.08900v1](https://arxiv.org/abs/2004.08900v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-2">2. Adversarial Training for Large Neural Language Models</h2>

Title: [Adversarial Training for Large Neural Language Models](https://arxiv.org/abs/2004.08994)

Authors: [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Yu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Hoifung Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code and pre-trained models will be made publicly available at [this https URL](https://github.com/namisan/mt-dnn).

| Comments: | 13 pages, 9 tables, 2 figures                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.08994](https://arxiv.org/abs/2004.08994) [cs.CL]** |
|           | (or **[arXiv:2004.08994v1](https://arxiv.org/abs/2004.08994v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-3">3. A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT</h2>

Title: [A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT](https://arxiv.org/abs/2004.09205)

Authors: [Chi-Liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Tsung-Yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Hung-Yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> Recently, multilingual BERT works remarkably well on cross-lingual transfer tasks, superior to static non-contextualized word embeddings. In this work, we provide an in-depth experimental study to supplement the existing literature of cross-lingual ability. We compare the cross-lingual ability of non-contextualized and contextualized representation model with the same data. We found that datasize and context window size are crucial factors to the transferability. We also observe the language-specific information in multilingual BERT. By manipulating the latent representations, we can control the output languages of multilingual BERT, and achieve unsupervised token translation. We further show that based on the observation, there is a computationally cheap but effective approach to improve the cross-lingual ability of multilingual BERT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09205](https://arxiv.org/abs/2004.09205) [cs.CL]** |
|           | (or **[arXiv:2004.09205v1](https://arxiv.org/abs/2004.09205v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-4">4. MPNet: Masked and Permuted Pre-training for Language Understanding</h2>

Title: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)

Authors: [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Jianfeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\footnote{\url{[this https URL](https://github.com/microsoft/MPNet)}}.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09297](https://arxiv.org/abs/2004.09297) [cs.CL]** |
|           | (or **[arXiv:2004.09297v1](https://arxiv.org/abs/2004.09297v1) [cs.CL]** for this version) |





<h2 id="2020-04-21-5">5. PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation</h2>

Title: [PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation](https://arxiv.org/abs/2004.09447)

Authors: [Vivek Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+V), [Mayank Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M)

> Code-mixing is the phenomenon of using more than one language in a sentence. It is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, it adds to the challenge of processing and understanding natural language to a much larger extent. This paper presents a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators. We are releasing the parallel corpus to facilitate future research opportunities in code-mixed machine translation. The annotated corpus is available at [this https URL](https://doi.org/10.5281/zenodo.3605597).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.09447](https://arxiv.org/abs/2004.09447) [cs.CL]** |
|           | (or **[arXiv:2004.09447v1](https://arxiv.org/abs/2004.09447v1) [cs.CL]** for this version) |







# 2020-04-20

[Return to Index](#Index)



<h2 id="2020-04-20-1">1. Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings</h2>

Title: [Geometry-aware Domain Adaptation for Unsupervised Alignment of Word Embeddings](https://arxiv.org/abs/2004.08243)

Authors: [Pratik Jawanpuria](https://arxiv.org/search/cs?searchtype=author&query=Jawanpuria%2C+P), [Mayank Meghwanshi](https://arxiv.org/search/cs?searchtype=author&query=Meghwanshi%2C+M), [Bamdev Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+B)

> We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.

| Comments: | Accepted as a short paper in ACL 2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.08243](https://arxiv.org/abs/2004.08243) [cs.LG]** |
|           | (or **[arXiv:2004.08243v1](https://arxiv.org/abs/2004.08243v1) [cs.LG]** for this version) |





<h2 id="2020-04-20-2">2. Understanding the Difficulty of Training Transformers</h2>

Title: [Understanding the Difficulty of Training Transformers](https://arxiv.org/abs/2004.08249)

Authors: [Liyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> Transformers have been proved effective for many deep learning tasks. Training transformers, however, requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (the standard SGD fails to train Transformers effectively). In this paper, we study Transformer training from both theoretical and empirical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that substantially influences training. Specifically, we observe that for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable since it amplifies small parameter perturbations (e.g., parameter updates) and result in significant disturbances in the model output, yet a light dependency limits the potential of model training and can lead to an inferior trained model. Inspired by our analysis, we propose Admin (**A****d**aptive **m**odel **i****n**itialization) to stabilize the training in the early stage and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.08249](https://arxiv.org/abs/2004.08249) [cs.LG]** |
|           | (or **[arXiv:2004.08249v1](https://arxiv.org/abs/2004.08249v1) [cs.LG]** for this version) |





<h2 id="2020-04-20-3">3. Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation</h2>

Title: [Enriching the Transformer with Linguistic and Semantic Factors for Low-Resource Machine Translation](https://arxiv.org/abs/2004.08053)

Authors: [Jordi Armengol-Estapé](https://arxiv.org/search/cs?searchtype=author&query=Armengol-Estapé%2C+J), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C)

> Introducing factors, that is to say, word features such as linguistic information referring to the source tokens, is known to improve the results of neural machine translation systems in certain settings, typically in recurrent architectures. This study proposes enhancing the current state-of-the-art neural machine translation architecture, the Transformer, so that it allows to introduce external knowledge. In particular, our proposed modification, the Factored Transformer, uses factors, either linguistic or semantic, that insert additional knowledge into the machine translation system. Apart from using different kinds of features, we study the effect of different architectural configurations. Specifically, we analyze the performance of combining words and features at the embedding level or at the encoder level, and we experiment with two different combination strategies. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both extremely low-resourced and very distant languages, and obtain an improvement of 1.2 BLEU. These improvements are achieved with linguistic and not with semantic information.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2004.08053](https://arxiv.org/abs/2004.08053) [cs.CL]** |
|              | (or **[arXiv:2004.08053v1](https://arxiv.org/abs/2004.08053v1) [cs.CL]** for this version) |





<h2 id="2020-04-20-4">4. Batch Clustering for Multilingual News Streaming</h2>

Title: [Batch Clustering for Multilingual News Streaming](https://arxiv.org/abs/2004.08123)

Authors: [Mathis Linger](https://arxiv.org/search/cs?searchtype=author&query=Linger%2C+M), [Mhamed Hajaiej](https://arxiv.org/search/cs?searchtype=author&query=Hajaiej%2C+M)

> Nowadays, digital news articles are widely available, published by various editors and often written in different languages. This large volume of diverse and unorganized information makes human reading very difficult or almost impossible. This leads to a need for algorithms able to arrange high amount of multilingual news into stories. To this purpose, we extend previous works on Topic Detection and Tracking, and propose a new system inspired from newsLens. We process articles per batch, looking for monolingual local topics which are then linked across time and languages. Here, we introduce a novel "replaying" strategy to link monolingual local topics into stories. Besides, we propose new fine tuned multilingual embedding using SBERT to create crosslingual stories. Our system gives monolingual state-of-the-art results on dataset of Spanish and German news and crosslingual state-of-the-art results on English, Spanish and German news.

| Comments:          | 7 pages, 2 figures                                           |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Journal reference: | Proceedings of Text2Story - Third Workshop on Narrative Extraction From Texts co-located with 42nd European Conference on Information Retrieval (ECIR 2020) Lisbon, Portugal, April 14th, 2020 |
| Cite as:           | **[arXiv:2004.08123](https://arxiv.org/abs/2004.08123) [cs.CL]** |
|                    | (or **[arXiv:2004.08123v1](https://arxiv.org/abs/2004.08123v1) [cs.CL]** for this version) |







# 2020-04-17

[Return to Index](#Index)



<h2 id="2020-04-17-1">1. Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation</h2>

Title: [Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation](https://arxiv.org/abs/2004.07324)

Authors: [Idriss Mghabbar](https://arxiv.org/search/cs?searchtype=author&query=Mghabbar%2C+I), [Pirashanth Ratnamogan](https://arxiv.org/search/cs?searchtype=author&query=Ratnamogan%2C+P)

> Lack of specialized data makes building a multi-domain neural machine translation tool challenging. Although emerging literature dealing with low resource languages starts to show promising results, most state-of-the-art models used millions of sentences. Today, the majority of multi-domain adaptation techniques are based on complex and sophisticated architectures that are not adapted for real-world applications. So far, no scalable method is performing better than the simple yet effective mixed-finetuning, i.e finetuning a generic model with a mix of all specialized data and generic data. In this paper, we propose a new training pipeline where knowledge distillation and multiple specialized teachers allow us to efficiently finetune a model without adding new costs at inference time. Our experiments demonstrated that our training pipeline allows improving the performance of multi-domain translation over finetuning in configurations with 2, 3, and 4 domains by up to 2 points in BLEU.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 24th European Conference on Artificial Intelligence (ECAI), 2020 |
| Cite as:           | **[arXiv:2004.07324](https://arxiv.org/abs/2004.07324) [cs.CL]** |
|                    | (or **[arXiv:2004.07324v1](https://arxiv.org/abs/2004.07324v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-2">2. Non-Autoregressive Machine Translation with Latent Alignments</h2>

Title: [Non-Autoregressive Machine Translation with Latent Alignments](https://arxiv.org/abs/2004.07437)

Authors: [Chitwan Saharia](https://arxiv.org/search/cs?searchtype=author&query=Saharia%2C+C), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W), [Saurabh Saxena](https://arxiv.org/search/cs?searchtype=author&query=Saxena%2C+S), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M)

> This paper investigates two latent alignment models for non-autoregressive machine translation, namely CTC and Imputer. CTC generates outputs in a single step, makes strong conditional independence assumptions about output variables, and marginalizes out latent alignments using dynamic programming. Imputer generates outputs in a constant number of steps, and approximately marginalizes out possible generation orders and latent alignments for training. These models are simpler than existing non-autoregressive methods, since they do not require output length prediction as a pre-process. In addition, our architecture is simpler than typical encoder-decoder architectures, since input-output cross attention is not used. On the competitive WMT'14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the baseline autoregressive Transformer with 27.8 BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07437](https://arxiv.org/abs/2004.07437) [cs.CL]** |
|           | (or **[arXiv:2004.07437v1](https://arxiv.org/abs/2004.07437v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-3">3. Do sequence-to-sequence VAEs learn global features of sentences?</h2>

Title: [Do sequence-to-sequence VAEs learn global features of sentences?](https://arxiv.org/abs/2004.07683)

Authors: [Tom Bosc](https://arxiv.org/search/cs?searchtype=author&query=Bosc%2C+T), [Pascal Vincent](https://arxiv.org/search/cs?searchtype=author&query=Vincent%2C+P)

> A longstanding goal in NLP is to compute global sentence representations. Such representations would be useful for sample-efficient semi-supervised learning and controllable text generation. To learn to represent global and local information separately, Bowman & al. (2016) proposed to train a sequence-to-sequence model with the variational auto-encoder (VAE) objective. What precisely is encoded in these latent variables expected to capture global features? We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we see that VAEs are prone to memorizing the first words and the sentence length, drastically limiting their usefulness. To alleviate this, we propose variants based on bag-of-words assumptions and language model pretraining. These variants learn latents that are more global: they are more predictive of topic or sentiment labels, and their reconstructions are more faithful to the labels of the original documents.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07683](https://arxiv.org/abs/2004.07683) [cs.CL]** |
|           | (or **[arXiv:2004.07683v1](https://arxiv.org/abs/2004.07683v1) [cs.CL]** for this version) |





<h2 id="2020-04-17-4">4. Cross-lingual Contextualized Topic Models with Zero-shot Learning</h2>

Title: [Cross-lingual Contextualized Topic Models with Zero-shot Learning](https://arxiv.org/abs/2004.07737)

Authors: [Federico Bianchi](https://arxiv.org/search/cs?searchtype=author&query=Bianchi%2C+F), [Silvia Terragni](https://arxiv.org/search/cs?searchtype=author&query=Terragni%2C+S), [Dirk Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+D), [Debora Nozza](https://arxiv.org/search/cs?searchtype=author&query=Nozza%2C+D), [Elisabetta Fersini](https://arxiv.org/search/cs?searchtype=author&query=Fersini%2C+E)

> Many data sets in a domain (reviews, forums, news, etc.) exist in parallel languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model, i.e., our model learns topics on one language (here, English), and predicts them for documents in other languages. By using the text of the same document in different languages, we can evaluate the quality of the predictions. Our results show that topics are coherent and stable across languages, which suggests exciting future research directions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2004.07737](https://arxiv.org/abs/2004.07737) [cs.CL]** |
|           | (or **[arXiv:2004.07737v1](https://arxiv.org/abs/2004.07737v1) [cs.CL]** for this version) |







# 2020-04-16

[Return to Index](#Index)



<h2 id="2020-04-16-1">1. A hybrid classical-quantum workflow for natural language processing</h2>

Title: [A hybrid classical-quantum workflow for natural language processing](https://arxiv.org/abs/2004.06800)

Authors: [Lee J. O'Riordan](https://arxiv.org/search/quant-ph?searchtype=author&query=O'Riordan%2C+L+J), [Myles Doyle](https://arxiv.org/search/quant-ph?searchtype=author&query=Doyle%2C+M), [Fabio Baruffa](https://arxiv.org/search/quant-ph?searchtype=author&query=Baruffa%2C+F), [Venkatesh Kannan](https://arxiv.org/search/quant-ph?searchtype=author&query=Kannan%2C+V)

> Natural language processing (NLP) problems are ubiquitous in classical computing, where they often require significant computational resources to infer sentence meanings. With the appearance of quantum computing hardware and simulators, it is worth developing methods to examine such problems on these platforms. In this manuscript we demonstrate the use of quantum computing models to perform NLP tasks, where we represent corpus meanings, and perform comparisons between sentences of a given structure. We develop a hybrid workflow for representing small and large scale corpus data sets to be encoded, processed, and decoded using a quantum circuit model. In addition, we provide our results showing the efficacy of the method, and release our developed toolkit as an open software suite.

| Comments: | For associated code, see [this https URL](https://github.com/ICHEC/QNLP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Quantum Physics (quant-ph)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2004.06800](https://arxiv.org/abs/2004.06800) [quant-ph]** |
|           | (or **[arXiv:2004.06800v1](https://arxiv.org/abs/2004.06800v1) [quant-ph]** for this version) |





<h2 id="2020-04-16-2">2. lamBERT: Language and Action Learning Using Multimodal BERT</h2>

Title: [lamBERT: Language and Action Learning Using Multimodal BERT](https://arxiv.org/abs/2004.07093)

Authors: [Kazuki Miyazawa](https://arxiv.org/search/cs?searchtype=author&query=Miyazawa%2C+K), [Tatsuya Aoki](https://arxiv.org/search/cs?searchtype=author&query=Aoki%2C+T), [Takato Horii](https://arxiv.org/search/cs?searchtype=author&query=Horii%2C+T), [Takayuki Nagai](https://arxiv.org/search/cs?searchtype=author&query=Nagai%2C+T)

> Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other models, such as the convolutional neural network-based model and the lamBERT model without pre-training.

| Comments: | 8 pages, 9 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2004.07093](https://arxiv.org/abs/2004.07093) [cs.LG]** |
|           | (or **[arXiv:2004.07093v1](https://arxiv.org/abs/2004.07093v1) [cs.LG]** for this version) |





<h2 id="2020-04-16-3">3. Balancing Training for Multilingual Neural Machine Translation</h2>

Title: [Balancing Training for Multilingual Neural Machine Translation](https://arxiv.org/abs/2004.06748)

Authors: [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.

| Comments: | Accepted at ACL 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.06748](https://arxiv.org/abs/2004.06748) [cs.CL]** |
|           | (or **[arXiv:2004.06748v2](https://arxiv.org/abs/2004.06748v2) [cs.CL]** for this version) |





<h2 id="2020-04-16-4">4. PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation</h2>

Title: [PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](https://arxiv.org/abs/2004.07159)

Authors: [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Chen Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W)

> Self-supervised pre-training has emerged as a powerful technique for natural language understanding and generation, such as BERT, MASS and BART. The existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. In this work, we present PALM which pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus especially for downstream generation conditioned on context, such as generative question answering and conversational response generation. PALM minimizes the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. With a novel pre-training scheme, PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on Gigaword and conversational response generation on Cornell Movie Dialogues.

| Comments: | arXiv admin note: text overlap with [arXiv:1910.10683](https://arxiv.org/abs/1910.10683), [arXiv:1910.13461](https://arxiv.org/abs/1910.13461) by other authors |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.07159](https://arxiv.org/abs/2004.07159) [cs.CL]** |
|           | (or **[arXiv:2004.07159v1](https://arxiv.org/abs/2004.07159v1) [cs.CL]** for this version) |





<h2 id="2020-04-16-5">5. Document-level Representation Learning using Citation-informed Transformers</h2>

Title: [Document-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2004.07180)

Authors: [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A), [Sergey Feldman](https://arxiv.org/search/cs?searchtype=author&query=Feldman%2C+S), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S)

> Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2004.07180](https://arxiv.org/abs/2004.07180) [cs.CL]** |
|           | (or **[arXiv:2004.07180v1](https://arxiv.org/abs/2004.07180v1) [cs.CL]** for this version) |









# 2020-04-15

[Return to Index](#Index)



<h2 id="2020-04-15-1">1. Code Completion using Neural Attention and Byte Pair Encoding</h2>

Title: [Code Completion using Neural Attention and Byte Pair Encoding](https://arxiv.org/abs/2004.06343)

Authors: [Youri Arkesteijn](https://arxiv.org/search/cs?searchtype=author&query=Arkesteijn%2C+Y), [Nikhil Saldanha](https://arxiv.org/search/cs?searchtype=author&query=Saldanha%2C+N), [Bastijn Kostense](https://arxiv.org/search/cs?searchtype=author&query=Kostense%2C+B)

*(Submitted on 14 Apr 2020)*

> In this paper, we aim to do code completion based on implementing a Neural Network from Li et. al.. Our contribution is that we use an encoding that is in-between character and word encoding called Byte Pair Encoding (BPE). We use this on the source code files treating them as natural text without first going through the abstract syntax tree (AST). We have implemented two models: an attention-enhanced LSTM and a pointer network, where the pointer network was originally introduced to solve out of vocabulary problems. We are interested to see if BPE can replace the need for the pointer network for code completion.

| Comments: | 4 pages, 4 figures, 1 table                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Software Engineering (cs.SE) |
| Cite as:  | [arXiv:2004.06343](https://arxiv.org/abs/2004.06343) [cs.CL] |
|           | (or [arXiv:2004.06343v1](https://arxiv.org/abs/2004.06343v1) [cs.CL] for this version) |





<h2 id="2020-04-15-2">2. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are</h2>

Title: [Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](https://arxiv.org/abs/2004.06358)

Authors: [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Matthias Paulik](https://arxiv.org/search/cs?searchtype=author&query=Paulik%2C+M)

*(Submitted on 14 Apr 2020)*

> Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.
> Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.

| Comments: | ACL 2020 theme track                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.06358](https://arxiv.org/abs/2004.06358) [cs.CL] |
|           | (or [arXiv:2004.06358v1](https://arxiv.org/abs/2004.06358v1) [cs.CL] for this version) |





<h2 id="2020-04-15-3">3. What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models</h2>

Title: [What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models](https://arxiv.org/abs/2004.06499)

Authors: [Wietse de Vries](https://arxiv.org/search/cs?searchtype=author&query=de+Vries%2C+W), [Andreas van Cranenburgh](https://arxiv.org/search/cs?searchtype=author&query=van+Cranenburgh%2C+A), [Malvina Nissim](https://arxiv.org/search/cs?searchtype=author&query=Nissim%2C+M)

*(Submitted on 14 Apr 2020)*

> Experiments with transfer learning on pre-trained language models such as BERT have shown that the layers of these models resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers of the network. We investigate to what extent these results also hold for a language other than English. For this we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, by considering the task of part-of-speech tagging in more detail, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations and it is therefore useful to combine information from different layers for best results, instead of selecting a single layer based on the best overall performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.06499](https://arxiv.org/abs/2004.06499) [cs.CL] |
|           | (or [arXiv:2004.06499v1](https://arxiv.org/abs/2004.06499v1) [cs.CL] for this version) |





<h2 id="2020-04-15-4">4. Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders</h2>

Title: [Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders](https://arxiv.org/abs/2004.06575)

Authors: [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

*(Submitted on 14 Apr 2020)*

> State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, and when adding new languages, without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:2004.06575](https://arxiv.org/abs/2004.06575) [cs.CL] |
|              | (or [arXiv:2004.06575v1](https://arxiv.org/abs/2004.06575v1) [cs.CL] for this version) |







# 2020-04-14

[Return to Index](#Index)



<h2 id="2020-04-14-1">1. On the Language Neutrality of Pre-trained Multilingual Representations</h2>

Title: [On the Language Neutrality of Pre-trained Multilingual Representations](https://arxiv.org/abs/2004.05160)

Authors: [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Rudolf Rosa](https://arxiv.org/search/cs?searchtype=author&query=Rosa%2C+R), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

*(Submitted on 9 Apr 2020)*

> Multilingual contextual embeddings, such as multilingual BERT (mBERT) and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the language-neutrality of mBERT with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. Contextual embeddings are still by default only moderately language-neutral, however, we show two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for languages, and second by fitting an explicit projection on small parallel data. In addition, we show how to reach state-of-the-art accuracy on language identification and word alignment in parallel sentences.

| Comments: | 11 pages, 3 figures. arXiv admin note: text overlap with [arXiv:1911.03310](https://arxiv.org/abs/1911.03310) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.05160](https://arxiv.org/abs/2004.05160) [cs.CL] |
|           | (or [arXiv:2004.05160v1](https://arxiv.org/abs/2004.05160v1) [cs.CL] for this version) |







<h2 id="2020-04-14-2">2. Joint translation and unit conversion for end-to-end localization</h2>

Title: [Joint translation and unit conversion for end-to-end localization](https://arxiv.org/abs/2004.05219)

Authors: [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Stanislas Lauly](https://arxiv.org/search/cs?searchtype=author&query=Lauly%2C+S), [Yaser Al-Onaizan](https://arxiv.org/search/cs?searchtype=author&query=Al-Onaizan%2C+Y)

*(Submitted on 10 Apr 2020)*

> A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which leads to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05219](https://arxiv.org/abs/2004.05219) [cs.CL] |
|           | (or [arXiv:2004.05219v1](https://arxiv.org/abs/2004.05219v1) [cs.CL] for this version) |







<h2 id="2020-04-14-3">3. When Does Unsupervised Machine Translation Work?</h2>

Title: [When Does Unsupervised Machine Translation Work?](https://arxiv.org/abs/2004.05516)

Authors: [Kelly Marchisio](https://arxiv.org/search/cs?searchtype=author&query=Marchisio%2C+K), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

*(Submitted on 12 Apr 2020)*

> Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which these methods succeed, and where they fail. We conduct an extensive empirical evaluation of unsupervised MT using dissimilar language pairs, dissimilar domains, diverse datasets, and authentic low-resource languages. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that random word embedding initialization can dramatically affect downstream translation performance. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05516](https://arxiv.org/abs/2004.05516) [cs.CL] |
|           | (or [arXiv:2004.05516v1](https://arxiv.org/abs/2004.05516v1) [cs.CL] for this version) |







# 2020-04-13

[Return to Index](#Index)



<h2 id="2020-04-13-1">1. An In-depth Walkthrough on Evolution of Neural Machine Translation</h2>

Title: [An In-depth Walkthrough on Evolution of Neural Machine Translation](https://arxiv.org/abs/2004.04902)

Authors: [Rohan Jagtap](https://arxiv.org/search/cs?searchtype=author&query=Jagtap%2C+R), [Dr. Sudhir N. Dhage](https://arxiv.org/search/cs?searchtype=author&query=Dhage%2C+D+S+N)

*(Submitted on 10 Apr 2020)*

> Neural Machine Translation (NMT) methodologies have burgeoned from using simple feed-forward architectures to the state of the art; viz. BERT model. The use cases of NMT models have been broadened from just language translations to conversational agents (chatbots), abstractive text summarization, image captioning, etc. which have proved to be a gem in their respective applications. This paper aims to study the major trends in Neural Machine Translation, the state of the art models in the domain and a high level comparison between them.

| Comments: | 10 pages, 10 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | [arXiv:2004.04902](https://arxiv.org/abs/2004.04902) [cs.CL] |
|           | (or [arXiv:2004.04902v1](https://arxiv.org/abs/2004.04902v1) [cs.CL] for this version) |





<h2 id="2020-04-13-2">2. Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data</h2>

Title: [Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data](https://arxiv.org/abs/2004.04972)

Authors: [Soumi Maiti](https://arxiv.org/search/cs?searchtype=author&query=Maiti%2C+S), [Erik Marchi](https://arxiv.org/search/cs?searchtype=author&query=Marchi%2C+E), [Alistair Conkie](https://arxiv.org/search/cs?searchtype=author&query=Conkie%2C+A)

*(Submitted on 10 Apr 2020)*

> We present progress towards bilingual Text-to-Speech which is able to transform a monolingual voice to speak a second language while preserving speaker voice quality. We demonstrate that a bilingual speaker embedding space contains a separate distribution for each language and that a simple transform in speaker space generated by the speaker embedding can be used to control the degree of accent of a synthetic voice in a language. The same transform can be applied even to monolingual speakers.
> In our experiments speaker data from an English-Spanish (Mexican) bilingual speaker was used, and the goal was to enable English speakers to speak Spanish and Spanish speakers to speak English. We found that the simple transform was sufficient to convert a voice from one language to the other with a high degree of naturalness. In one case the transformed voice outperformed a native language voice in listening tests. Experiments further indicated that the transform preserved many of the characteristics of the original voice. The degree of accent present can be controlled and naturalness is relatively consistent across a range of accent values.

| Comments: | Accepted to IEEE ICASSP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2004.04972](https://arxiv.org/abs/2004.04972) [cs.CL] |
|           | (or [arXiv:2004.04972v1](https://arxiv.org/abs/2004.04972v1) [cs.CL] for this version) |





<h2 id="2020-04-13-3">3. Automated Spelling Correction for Clinical Text Mining in Russian</h2>

Title: [Automated Spelling Correction for Clinical Text Mining in Russian](https://arxiv.org/abs/2004.04987)

Authors: [Ksenia Balabaeva](https://arxiv.org/search/cs?searchtype=author&query=Balabaeva%2C+K), [Anastasia Funkner](https://arxiv.org/search/cs?searchtype=author&query=Funkner%2C+A), [Sergey Kovalchuk](https://arxiv.org/search/cs?searchtype=author&query=Kovalchuk%2C+S)

*(Submitted on 10 Apr 2020)*

> The main goal of this paper is to develop a spell checker module for clinical text in Russian. The described approach combines string distance measure algorithms with technics of machine learning embedding methods. Our overall precision is 0.86, lexical precision - 0.975 and error precision is 0.74. We develop spell checker as a part of medical text mining tool regarding the problems of misspelling, negation, experiencer and temporality detection.

| Comments: | This paper is accepted for publication to MIE 2020 Conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04987](https://arxiv.org/abs/2004.04987) [cs.CL] |
|           | (or [arXiv:2004.04987v1](https://arxiv.org/abs/2004.04987v1) [cs.CL] for this version) |





<h2 id="2020-04-13-4">4. Longformer: The Long-Document Transformer</h2>

Title: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

Authors: [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E), [Arman Cohan](https://arxiv.org/search/cs?searchtype=author&query=Cohan%2C+A)

*(Submitted on 10 Apr 2020)*

> Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.05150](https://arxiv.org/abs/2004.05150) [cs.CL] |
|           | (or [arXiv:2004.05150v1](https://arxiv.org/abs/2004.05150v1) [cs.CL] for this version) |





# 2020-04-10

[Return to Index](#Index)



<h2 id="2020-04-10-1">1. Learning to Scale Multilingual Representations for Vision-Language Tasks</h2>

Title: [Learning to Scale Multilingual Representations for Vision-Language Tasks](https://arxiv.org/abs/2004.04312)

Authors: [Andrea Burns](https://arxiv.org/search/cs?searchtype=author&query=Burns%2C+A), [Donghyun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Derry Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D), [Kate Saenko](https://arxiv.org/search/cs?searchtype=author&query=Saenko%2C+K), [Bryan A. Plummer](https://arxiv.org/search/cs?searchtype=author&query=Plummer%2C+B+A)

*(Submitted on 9 Apr 2020)*

> Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that represents many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for few. We use a novel masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04312](https://arxiv.org/abs/2004.04312) [cs.CV] |
|           | (or [arXiv:2004.04312v1](https://arxiv.org/abs/2004.04312v1) [cs.CV] for this version) |





<h2 id="2020-04-10-2">2. On optimal transformer depth for low-resource language translation</h2>

Title: [On optimal transformer depth for low-resource language translation](https://arxiv.org/abs/2004.04418)

Authors: [Elan van Biljon](https://arxiv.org/search/cs?searchtype=author&query=van+Biljon%2C+E), [Arnu Pretorius](https://arxiv.org/search/cs?searchtype=author&query=Pretorius%2C+A), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J)

*(Submitted on 9 Apr 2020)*

> Transformers have shown great promise as an approach to Neural Machine Translation (NMT) for low-resource languages. However, at the same time, transformer models remain difficult to optimize and require careful tuning of hyper-parameters to be useful in this setting. Many NMT toolkits come with a set of default hyper-parameters, which researchers and practitioners often adopt for the sake of convenience and avoiding tuning. These configurations, however, have been optimized for large-scale machine translation data sets with several millions of parallel sentences for European languages like English and French. In this work, we find that the current trend in the field to use very large models is detrimental for low-resource languages, since it makes training more difficult and hurts overall performance, confirming previous observations. We see our work as complementary to the Masakhane project ("Masakhane" means "We Build Together" in isiZulu.) In this spirit, low-resource NMT systems are now being built by the community who needs them the most. However, many in the community still have very limited access to the type of computational resources required for building extremely large models promoted by industrial research. Therefore, by showing that transformer models perform well (and often best) at low-to-moderate depth, we hope to convince fellow researchers to devote less computational resources, as well as time, to exploring overly large models during the development of these systems.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04418](https://arxiv.org/abs/2004.04418) [cs.CL] |
|           | (or [arXiv:2004.04418v1](https://arxiv.org/abs/2004.04418v1) [cs.CL] for this version) |





<h2 id="2020-04-10-3">3. Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</h2>

Title: [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://arxiv.org/abs/2004.04498)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 9 Apr 2020)*

> Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019).
> Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a `balanced' dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch.
> A known pitfall of transfer learning on new domains is `catastrophic forgetting', which we address both in adaptation and in inference. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. During inference we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of `black box` online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.

| Comments: | ACL 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04498](https://arxiv.org/abs/2004.04498) [cs.CL] |
|           | (or [arXiv:2004.04498v1](https://arxiv.org/abs/2004.04498v1) [cs.CL] for this version) |





<h2 id="2020-04-10-4">4. Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios</h2>

Title: [Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios](https://arxiv.org/abs/2004.04507)

Authors: [Haipeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

*(Submitted on 9 Apr 2020)*

> Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not an adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04507](https://arxiv.org/abs/2004.04507) [cs.CL] |
|           | (or [arXiv:2004.04507v1](https://arxiv.org/abs/2004.04507v1) [cs.CL] for this version) |





<h2 id="2020-04-10-5">5. Translation Artifacts in Cross-lingual Transfer Learning</h2>

Title: [Translation Artifacts in Cross-lingual Transfer Learning](https://arxiv.org/abs/2004.04721)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

*(Submitted on 9 Apr 2020)*

> Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.04721](https://arxiv.org/abs/2004.04721) [cs.CL] |
|           | (or [arXiv:2004.04721v1](https://arxiv.org/abs/2004.04721v1) [cs.CL] for this version) |



# 2020-04-09

[Return to Index](#Index)



<h2 id="2020-04-09-1">1. Re-translation versus Streaming for Simultaneous Translation</h2>

Title: [Re-translation versus Streaming for Simultaneous Translation](https://arxiv.org/abs/2004.03643)

Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G)

*(Submitted on 7 Apr 2020)*

> There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation's ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03643](https://arxiv.org/abs/2004.03643) [cs.CL] |
|           | (or [arXiv:2004.03643v1](https://arxiv.org/abs/2004.03643v1) [cs.CL] for this version) |





<h2 id="2020-04-09-2">2. Dynamic Data Selection and Weighting for Iterative Back-Translation</h2>

Title: [Dynamic Data Selection and Weighting for Iterative Back-Translation](https://arxiv.org/abs/2004.03672)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 7 Apr 2020)*

> Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality \textit{and} reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03672](https://arxiv.org/abs/2004.03672) [cs.CL] |
|           | (or [arXiv:2004.03672v1](https://arxiv.org/abs/2004.03672v1) [cs.CL] for this version) |





<h2 id="2020-04-09-3">3. Byte Pair Encoding is Suboptimal for Language Model Pretraining</h2>

Title: [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/abs/2004.03720)

Authors: [Kaj Bostrom](https://arxiv.org/search/cs?searchtype=author&query=Bostrom%2C+K), [Greg Durrett](https://arxiv.org/search/cs?searchtype=author&query=Durrett%2C+G)

*(Submitted on 7 Apr 2020)*

> The success of pretrained transformer language models in natural language processing has led to a wide range of different pretraining setups. These models employ a variety of subword tokenization methods, most notably byte pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE. We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.

| Comments:    | 4 pages, 2 figures                                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:2004.03720](https://arxiv.org/abs/2004.03720) [cs.CL] |
|              | (or [arXiv:2004.03720v1](https://arxiv.org/abs/2004.03720v1) [cs.CL] for this version) |





<h2 id="2020-04-09-4">4. Improving BERT with Self-Supervised Attention</h2>

Title: [Improving BERT with Self-Supervised Attention](https://arxiv.org/abs/2004.03808)

Authors: [Xiaoyu Kou](https://arxiv.org/search/cs?searchtype=author&query=Kou%2C+X), [Yaming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Yujing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Ce Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Yiren Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yunhai Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+Y), [Yan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jing Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+J)

*(Submitted on 8 Apr 2020)*

> One of the most popular paradigms of applying large, pre-trained NLP models such as BERT is to fine-tune it on a smaller dataset. However, one challenge remains as the fine-tuned model often overfits on smaller datasets. A symptom of this phenomenon is that irrelevant words in the sentences, even when they are obvious to humans, can substantially degrade the performance of these fine-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Specifically, SSA automatically generates weak, token-level attention labels iteratively by "probing" the fine-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their benefits. Empirically, on a variety of public datasets, we illustrate significant performance improvement using our SSA-enhanced BERT model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03808](https://arxiv.org/abs/2004.03808) [cs.CL] |
|           | (or [arXiv:2004.03808v1](https://arxiv.org/abs/2004.03808v1) [cs.CL] for this version) |





<h2 id="2020-04-09-5">5. Explicit Reordering for Neural Machine Translation</h2>

Title: [Explicit Reordering for Neural Machine Translation](https://arxiv.org/abs/2004.03818)

Authors: [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 8 Apr 2020)*

> In Transformer-based neural machine translation (NMT), the positional encoding mechanism helps the self-attention networks to learn the source representation with order dependency, which makes the Transformer-based NMT achieve state-of-the-art results for various translation tasks. However, Transformer-based NMT only adds representations of positions sequentially to word vectors in the input sentence and does not explicitly consider reordering information in this sentence. In this paper, we first empirically investigate the relationship between source reordering information and translation performance. The empirical findings show that the source input with the target order learned from the bilingual parallel dataset can substantially improve translation performance. Thus, we propose a novel reordering method to explicitly model this reordering information for the Transformer-based NMT. The empirical results on the WMT14 English-to-German, WAT ASPEC Japanese-to-English, and WMT17 Chinese-to-English translation tasks show the effectiveness of the proposed approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03818](https://arxiv.org/abs/2004.03818) [cs.CL] |
|           | (or [arXiv:2004.03818v1](https://arxiv.org/abs/2004.03818v1) [cs.CL] for this version) |





<h2 id="2020-04-09-6">6. Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation</h2>

Title: [Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation](https://arxiv.org/abs/2004.04002)

Authors: [Stig-Arne Grönroos](https://arxiv.org/search/cs?searchtype=author&query=Grönroos%2C+S), [Sami Virpioja](https://arxiv.org/search/cs?searchtype=author&query=Virpioja%2C+S), [Mikko Kurimo](https://arxiv.org/search/cs?searchtype=author&query=Kurimo%2C+M)

*(Submitted on 8 Apr 2020)*

> There are several approaches for improving neural machine translation for low-resource languages: Monolingual data can be exploited via pretraining or data augmentation; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models; Subword segmentation and regularization techniques can be applied to ensure high coverage of the vocabulary. We review these approaches in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are related, with one being a very low-resource and the other a higher-resource language. We test various methods on three artificially restricted translation tasks---English to Estonian (low-resource) and Finnish (high-resource), English to Slovak and Czech, English to Danish and Swedish---and one real-world task, Norwegian to North Sámi and Finnish. The experiments show positive effects especially for scheduled multi-task learning, denoising autoencoder, and subword sampling.

| Comments: | 26 pages, 12 tables, 7 figures. Submitted (Mar 2020) to the Machine Translation journal Special Issue on Machine Translation for Low-Resource Languages (Springer) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.04002](https://arxiv.org/abs/2004.04002) [cs.CL] |
|           | (or [arXiv:2004.04002v1](https://arxiv.org/abs/2004.04002v1) [cs.CL] for this version) |





# 2020-04-08

[Return to Index](#Index)



<h2 id="2020-04-08-1">1. Multilingual enrichment of disease biomedical ontologies</h2>

Title: [Multilingual enrichment of disease biomedical ontologies](https://arxiv.org/abs/2004.03181)

Authors: [Léo Bouscarrat](https://arxiv.org/search/q-bio?searchtype=author&query=Bouscarrat%2C+L) (QARMA, TALEP), [Antoine Bonnefoy](https://arxiv.org/search/q-bio?searchtype=author&query=Bonnefoy%2C+A), [Cécile Capponi](https://arxiv.org/search/q-bio?searchtype=author&query=Capponi%2C+C) (LIF, QARMA), [Carlos Ramisch](https://arxiv.org/search/q-bio?searchtype=author&query=Ramisch%2C+C) (TALEP)

*(Submitted on 7 Apr 2020)*

> Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects: coverage and quality. We look at the coverage of two biomedical ontologies focusing on diseases with respect to Wikidata for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both ontologies, plus Arabic, Chinese and Russian for the second one. We first use direct links between Wikidata and the studied ontologies and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to Wikidata with a commercial machine translation tool, here Google Cloud Translation.

| Subjects:          | **Quantitative Methods (q-bio.QM)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2nd workshop on MultilingualBIO: Multilingual Biomedical Text Processing, May 2020, Marseille, France |
| Cite as:           | [arXiv:2004.03181](https://arxiv.org/abs/2004.03181) [q-bio.QM] |
|                    | (or [arXiv:2004.03181v1](https://arxiv.org/abs/2004.03181v1) [q-bio.QM] for this version) |





<h2 id="2020-04-08-2">2. Unsupervised Neural Machine Translation with Indirect Supervision</h2>

Title: [Unsupervised Neural Machine Translation with Indirect Supervision](https://arxiv.org/abs/2004.03137)

Authors: [Hongxiao Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+H), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

*(Submitted on 7 Apr 2020)*

> Neural machine translation~(NMT) is ineffective for zero-resource languages. Recent works exploring the possibility of unsupervised neural machine translation (UNMT) with only monolingual data can achieve promising results. However, there are still big gaps between UNMT and NMT with parallel supervision. In this work, we introduce a multilingual unsupervised NMT (\method) framework to leverage weakly supervised signals from high-resource language pairs to zero-resource translation directions. More specifically, for unsupervised language pairs \texttt{En-De}, we can make full use of the information from parallel dataset \texttt{En-Fr} to jointly train the unsupervised translation directions all in one model. \method is based on multilingual models which require no changes to the standard unsupervised NMT. Empirical results demonstrate that \method significantly improves the translation quality by more than 3 BLEU score on six benchmark unsupervised translation directions.

| Subjects:      | **Computation and Language (cs.CL)**                         |
| -------------- | ------------------------------------------------------------ |
| Report number: | 10                                                           |
| Cite as:       | [arXiv:2004.03137](https://arxiv.org/abs/2004.03137) [cs.CL] |
|                | (or [arXiv:2004.03137v1](https://arxiv.org/abs/2004.03137v1) [cs.CL] for this version) |





<h2 id="2020-04-08-3">3. Self-Induced Curriculum Learning in Neural Machine Translation</h2>

Title: [Self-Induced Curriculum Learning in Neural Machine Translation](https://arxiv.org/abs/2004.03151)

Authors: [Dana Ruiter](https://arxiv.org/search/cs?searchtype=author&query=Ruiter%2C+D), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J)

*(Submitted on 7 Apr 2020)*

> Self-supervised neural machine translation (SS-NMT) learns how to extract/select suitable training data from comparable -- rather than parallel -- corpora and how to translate, in a way that the two tasks support each other in a virtuous circle. SS-NMT has been shown to be competitive with state-of-the-art unsupervised NMT. In this study we provide an in-depth analysis of the sampling choices the SS-NMT model takes during training. We show that, without it having been told to do so, the model selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) a denoising curriculum. We observe that the dynamics of the mutual-supervision of both system internal representation types is vital for the extraction and hence translation performance. We show that in terms of the human Gunning-Fog Readability index (GF), SS-NMT starts by extracting and learning from Wikipedia data suitable for high school (GF=10--11) and quickly moves towards content suitable for first year undergraduate students (GF=13).

| Comments: | 13 pages, 7 images                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.03151](https://arxiv.org/abs/2004.03151) [cs.CL] |
|           | (or [arXiv:2004.03151v1](https://arxiv.org/abs/2004.03151v1) [cs.CL] for this version) |





<h2 id="2020-04-08-4">4. Machine Translation with Unsupervised Length-Constraints</h2>

Title: [Machine Translation with Unsupervised Length-Constraints](https://arxiv.org/abs/2004.03176)

Authors: [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

*(Submitted on 7 Apr 2020)*

> We have seen significant improvements in machine translation due to the usage of deep learning. While the improvements in translation quality are impressive, the encoder-decoder architecture enables many more possibilities. In this paper, we explore one of these, the generation of constraint translation. We focus on length constraints, which are essential if the translation should be displayed in a given format. In this work, we propose an end-to-end approach for this task. Compared to a traditional method that first translates and then performs sentence compression, the text compression is learned completely unsupervised. By combining the idea with zero-shot multilingual machine translation, we are also able to perform unsupervised monolingual sentence compression. In order to fulfill the length constraints, we investigated several methods to integrate the constraints into the model. Using the presented technique, we are able to significantly improve the translation quality under constraints. Furthermore, we are able to perform unsupervised monolingual sentence compression.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.03176](https://arxiv.org/abs/2004.03176) [cs.CL] |
|           | (or [arXiv:2004.03176v1](https://arxiv.org/abs/2004.03176v1) [cs.CL] for this version) |





<h2 id="2020-04-08-5">5. Towards Multimodal Simultaneous Neural Machine Translation</h2>

Title: [Towards Multimodal Simultaneous Neural Machine Translation](https://arxiv.org/abs/2004.03180)

Authors: [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Tosho Hirasawa](https://arxiv.org/search/cs?searchtype=author&query=Hirasawa%2C+T), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

*(Submitted on 7 Apr 2020)*

> Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly harder than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT) which leverages visual information as an additional modality. Although the usefulness of images as an additional modality is moderate for full sentence translation, we verified, for the first time, its importance for simultaneous translation. Our experiments with the Multi30k dataset showed that MSNMT in a simultaneous setting significantly outperforms its text-only counterpart in situations where 5 or fewer input tokens are needed to begin translation. We then verified the importance of visual information during decoding by (a) performing an adversarial evaluation of MSNMT where we studied how models behave with incongruent input modality and (b) analyzing the image attention.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03180](https://arxiv.org/abs/2004.03180) [cs.CL] |
|           | (or [arXiv:2004.03180v1](https://arxiv.org/abs/2004.03180v1) [cs.CL] for this version) |





<h2 id="2020-04-08-6">6. Improving Fluency of Non-Autoregressive Machine Translation</h2>

Title: [Improving Fluency of Non-Autoregressive Machine Translation](https://arxiv.org/abs/2004.03227)

Authors: [Zdeněk Kasner](https://arxiv.org/search/cs?searchtype=author&query=Kasner%2C+Z), [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Jindřich Helcl](https://arxiv.org/search/cs?searchtype=author&query=Helcl%2C+J)

*(Submitted on 7 Apr 2020)*

> Non-autoregressive (nAR) models for machine translation (MT) manifest superior decoding speed when compared to autoregressive (AR) models, at the expense of impaired fluency of their outputs. We improve the fluency of a nAR model with connectionist temporal classification (CTC) by employing additional features in the scoring model used during beam search decoding. Since the beam search decoding in our model only requires to run the network in a single forward pass, the decoding speed is still notably higher than in standard AR models. We train models for three language pairs: German, Czech, and Romanian from and into English. The results show that our proposed models can be more efficient in terms of decoding speed and still achieve a competitive BLEU score relative to AR models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.03227](https://arxiv.org/abs/2004.03227) [cs.CL] |
|           | (or [arXiv:2004.03227v1](https://arxiv.org/abs/2004.03227v1) [cs.CL] for this version) |





# 2020-04-07

[Return to Index](#Index)



<h2 id="2020-04-07-1">1. Neural Machine Translation with Imbalanced Classes</h2>

Title: [Neural Machine Translation with Imbalanced Classes](https://arxiv.org/abs/2004.02334)

Authors: [Thamme Gowda](https://arxiv.org/search/cs?searchtype=author&query=Gowda%2C+T), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

*(Submitted on 5 Apr 2020)*

> We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore the effect of class imbalance on NMT. We analyze the effect of vocabulary sizes on NMT performance and reveal an explanation for 'why' certain vocabulary sizes are better than others.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02334](https://arxiv.org/abs/2004.02334) [cs.CL] |
|           | (or [arXiv:2004.02334v1](https://arxiv.org/abs/2004.02334v1) [cs.CL] for this version) |





<h2 id="2020-04-07-2">2. Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation</h2>

Title: [Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation](https://arxiv.org/abs/2004.02577)

Authors: [Wei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W), [Chongxuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Tianhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

*(Submitted on 6 Apr 2020)*

> Existing data augmentation approaches for neural machine translation (NMT) have predominantly relied on back-translating in-domain (IND) monolingual corpora. These methods suffer from issues associated with a domain information gap, which leads to translation errors for low frequency and out-of-vocabulary terminology. This paper proposes a dictionary-based data augmentation (DDA) method for cross-domain NMT. DDA synthesizes a domain-specific dictionary with general domain corpora to automatically generate a large-scale pseudo-IND parallel corpus. The generated pseudo-IND data can be used to enhance a general domain trained baseline. The experiments show that the DDA-enhanced NMT models demonstrate consistent significant improvements, outperforming the baseline models by 3.75-11.53 BLEU. The proposed method is also able to further improve the performance of the back-translation based and IND-finetuned NMT models. The improvement is associated with the enhanced domain coverage produced by DDA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02577](https://arxiv.org/abs/2004.02577) [cs.CL] |
|           | (or [arXiv:2004.02577v1](https://arxiv.org/abs/2004.02577v1) [cs.CL] for this version) |





<h2 id="2020-04-07-3">3. Meta-Learning for Few-Shot NMT Adaptation</h2>

Title: [Meta-Learning for Few-Shot NMT Adaptation](https://arxiv.org/abs/2004.02745)

Authors: [Amr Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+A), [Hany Hassan](https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+H), [Hal Daumé III](https://arxiv.org/search/cs?searchtype=author&query=III%2C+H+D)

*(Submitted on 6 Apr 2020)*

> We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target domains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in-domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02745](https://arxiv.org/abs/2004.02745) [cs.CL] |
|           | (or [arXiv:2004.02745v1](https://arxiv.org/abs/2004.02745v1) [cs.CL] for this version) |



<h2 id="2020-04-07-4">4. Applying Cyclical Learning Rate to Neural Machine Translation</h2>

Title: [Applying Cyclical Learning Rate to Neural Machine Translation](https://arxiv.org/abs/2004.02401)

Authors: [Choon Meng Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+C+M), [Jianfeng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Wei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W)

*(Submitted on 6 Apr 2020)*

> In training deep learning networks, the optimizer and related learning rate are often used without much thought or with minimal tuning, even though it is crucial in ensuring a fast convergence to a good quality minimum of the loss function that can also generalize well on the test dataset. Drawing inspiration from the successful application of cyclical learning rate policy for computer vision related convolutional networks and datasets, we explore how cyclical learning rate can be applied to train transformer-based neural networks for neural machine translation. From our carefully designed experiments, we show that the choice of optimizers and the associated cyclical learning rate policy can have a significant impact on the performance. In addition, we establish guidelines when applying cyclical learning rates to neural machine translation tasks. Thus with our work, we hope to raise awareness of the importance of selecting the right optimizers and the accompanying learning rate policy, at the same time, encourage further research into easy-to-use learning rate policies.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02401](https://arxiv.org/abs/2004.02401) [cs.LG] |
|           | (or [arXiv:2004.02401v1](https://arxiv.org/abs/2004.02401v1) [cs.LG] for this version) |





<h2 id="2020-04-07-5">5. Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation</h2>

Title: [Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation](https://arxiv.org/abs/2004.02071)

Authors: [Sreyashi Nag](https://arxiv.org/search/cs?searchtype=author&query=Nag%2C+S), [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Varun Lakshminarasimhan](https://arxiv.org/search/cs?searchtype=author&query=Lakshminarasimhan%2C+V), [Swapnil Singhavi](https://arxiv.org/search/cs?searchtype=author&query=Singhavi%2C+S)

*(Submitted on 5 Apr 2020)*

> We explore ways of incorporating bilingual dictionaries to enable semi-supervised neural machine translation. Conventional back-translation methods have shown success in leveraging target side monolingual data. However, since the quality of back-translation models is tied to the size of the available parallel corpora, this could adversely impact the synthetically generated sentences in a low resource setting. We propose a simple data augmentation technique to address both this shortcoming. We incorporate widely available bilingual dictionaries that yield word-by-word translations to generate synthetic sentences. This automatically expands the vocabulary of the model while maintaining high quality content. Our method shows an appreciable improvement in performance over strong baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02071](https://arxiv.org/abs/2004.02071) [cs.CL] |
|           | (or [arXiv:2004.02071v1](https://arxiv.org/abs/2004.02071v1) [cs.CL] for this version) |







<h2 id="2020-04-07-6">6. Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech</h2>

Title: [Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech](https://arxiv.org/abs/2004.02077)

Authors: [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Scott Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+S)

*(Submitted on 5 Apr 2020)*

> While there is a large body of research studying deep learning methods for text generation from structured data, almost all of it focuses purely on English. In this paper, we study the effectiveness of machine translation based pre-training for data-to-text generation in non-English languages. Since the structured data is generally expressed in English, text generation into other languages involves elements of translation, transliteration and copying - elements already encoded in neural machine translation systems. Moreover, since data-to-text corpora are typically small, this task can benefit greatly from pre-training. Based on our experiments on Czech, a morphologically complex language, we find that pre-training lets us train end-to-end models with significantly improved performance, as judged by automatic metrics and human evaluation. We also show that this approach enjoys several desirable properties, including improved performance in low data scenarios and robustness to unseen slot values.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02077](https://arxiv.org/abs/2004.02077) [cs.CL] |
|           | (or [arXiv:2004.02077v1](https://arxiv.org/abs/2004.02077v1) [cs.CL] for this version) |







<h2 id="2020-04-07-7">7. Reference Language based Unsupervised Neural Machine Translation</h2>

Title: [Reference Language based Unsupervised Neural Machine Translation](https://arxiv.org/abs/2004.02127)

Authors: [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

*(Submitted on 5 Apr 2020)*

> Exploiting common language as an auxiliary for better translation has a long tradition in machine translation, which lets supervised learning based machine translation enjoy the enhancement delivered by the well-used pivot language, in case that the prerequisite of parallel corpus from source language to target language cannot be fully satisfied. The rising of unsupervised neural machine translation (UNMT) seems completely relieving the parallel corpus curse, though still subject to unsatisfactory performance so far due to vague clues available used for its core back-translation training. Further enriching the idea of pivot translation by freeing the use of parallel corpus other than its specified source and target, we propose a new reference language based UNMT framework, in which the reference language only shares parallel corpus with the source, indicating clear enough signal to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline in terms of only one auxiliary language, demonstrating the usefulness of the proposed reference language based UNMT with a good start.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02127](https://arxiv.org/abs/2004.02127) [cs.CL] |
|           | (or [arXiv:2004.02127v1](https://arxiv.org/abs/2004.02127v1) [cs.CL] for this version) |







<h2 id="2020-04-07-8">8. Detecting and Understanding Generalization Barriers for Neural Machine Translation</h2>

Title: [Detecting and Understanding Generalization Barriers for Neural Machine Translation](https://arxiv.org/abs/2004.02181)

Authors: [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Conghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 5 Apr 2020)*

> Generalization to unseen instances is our eternal pursuit for all data-driven models. However, for realistic task like machine translation, the traditional approach measuring generalization in an average sense provides poor understanding for the fine-grained generalization ability. As a remedy, this paper attempts to identify and understand generalization barrier words within an unseen input sentence that \textit{cause} the degradation of fine-grained generalization. We propose a principled definition of generalization barrier words and a modified version which is tractable in computation. Based on the modified one, we propose three simple methods for barrier detection by the search-aware risk estimation through counterfactual generation. We then conduct extensive analyses on those detected generalization barrier words on both Zh⇔En NIST benchmarks from various perspectives. Potential usage of the detected barrier words is also discussed.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2004.02181](https://arxiv.org/abs/2004.02181) [cs.CL] |
|           | (or [arXiv:2004.02181v1](https://arxiv.org/abs/2004.02181v1) [cs.CL] for this version) |







<h2 id="2020-04-07-9">9. AR: Auto-Repair the Synthetic Data for Neural Machine Translation</h2>

Title: [AR: Auto-Repair the Synthetic Data for Neural Machine Translation](https://arxiv.org/abs/2004.02196)

Authors: [Shanbo Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Shaohui Kuang](https://arxiv.org/search/cs?searchtype=author&query=Kuang%2C+S), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Changfeng Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

*(Submitted on 5 Apr 2020)*

> Compared with only using limited authentic parallel data as training corpus, many studies have proved that incorporating synthetic parallel data, which generated by back translation (BT) or forward translation (FT, or selftraining), into the NMT training process can significantly improve translation quality. However, as a well-known shortcoming, synthetic parallel data is noisy because they are generated by an imperfect NMT system. As a result, the improvements in translation quality bring by the synthetic parallel data are greatly diminished. In this paper, we propose a novel Auto- Repair (AR) framework to improve the quality of synthetic data. Our proposed AR model can learn the transformation from low quality (noisy) input sentence to high quality sentence based on large scale monolingual data with BT and FT techniques. The noise in synthetic parallel data will be sufficiently eliminated by the proposed AR model and then the repaired synthetic parallel data can help the NMT models to achieve larger improvements. Experimental results show that our approach can effective improve the quality of synthetic parallel data and the NMT model with the repaired synthetic data achieves consistent improvements on both WMT14 EN!DE and IWSLT14 DE!EN translation tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.02196](https://arxiv.org/abs/2004.02196) [cs.CL] |
|           | (or [arXiv:2004.02196v1](https://arxiv.org/abs/2004.02196v1) [cs.CL] for this version) |







<h2 id="2020-04-07-10">10. Understanding Learning Dynamics for Neural Machine Translation</h2>

Title: [Understanding Learning Dynamics for Neural Machine Translation](https://arxiv.org/abs/2004.02199)

Authors: [Conghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Guanlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 5 Apr 2020)*

> Despite the great success of NMT, there still remains a severe challenge: it is hard to interpret the internal dynamics during its training process. In this paper we propose to understand learning dynamics of NMT by using a recent proposed technique named Loss Change Allocation (LCA)~\citep{lan-2019-loss-change-allocation}. As LCA requires calculating the gradient on an entire dataset for each update, we instead present an approximate to put it into practice in NMT scenario. %motivated by the lesson from sgd. Our simulated experiment shows that such approximate calculation is efficient and is empirically proved to deliver consistent results to the brute-force implementation. In particular, extensive experiments on two standard translation benchmark datasets reveal some valuable findings.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.02199](https://arxiv.org/abs/2004.02199) [cs.CL] |
|           | (or [arXiv:2004.02199v1](https://arxiv.org/abs/2004.02199v1) [cs.CL] for this version) |







# 2020-04-06

[Return to Index](#Index)



<h2 id="2020-04-04-1">1. XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation</h2>

Title: [XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation](https://arxiv.org/abs/2004.01401)

Authors: [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Ning Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+N), [Fenfei Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+F), [Weizhen Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Guihong Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+G), [Xiaodong Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X), [Bruce Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Rahul Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+R), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Sining Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+S), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Jiun-Hung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Winnie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Shuguang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Fan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 3 Apr 2020)*

> In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01401](https://arxiv.org/abs/2004.01401) [cs.CL] |
|           | (or [arXiv:2004.01401v1](https://arxiv.org/abs/2004.01401v1) [cs.CL] for this version) |





<h2 id="2020-04-04-2">2. Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation</h2>

Title: [Learning synchronous context-free grammars with multiple specialised non-terminals for hierarchical phrase-based translation](https://arxiv.org/abs/2004.01422)

Authors: [Felipe Sánchez-Martínez](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Martínez%2C+F), [Juan Antonio Pérez-Ortiz](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Ortiz%2C+J+A), [Rafael C. Carrasco](https://arxiv.org/search/cs?searchtype=author&query=Carrasco%2C+R+C)

*(Submitted on 3 Apr 2020)*

> Translation models based on hierarchical phrase-based statistical machine translation (HSMT) have shown better performances than the non-hierarchical phrase-based counterparts for some language pairs. The standard approach to HSMT learns and apply a synchronous context-free grammar with a single non-terminal. The hypothesis behind the grammar refinement algorithm presented in this work is that this single non-terminal is overloaded, and insufficiently discriminative, and therefore, an adequate split of it into more specialised symbols could lead to improved models. This paper presents a method to learn synchronous context-free grammars with a huge number of initial non-terminals, which are then grouped via a clustering algorithm. Our experiments show that the resulting smaller set of non-terminals correctly capture the contextual information that makes it possible to statistically significantly improve the BLEU score of the standard HSMT approach.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01422](https://arxiv.org/abs/2004.01422) [cs.CL] |
|           | (or [arXiv:2004.01422v1](https://arxiv.org/abs/2004.01422v1) [cs.CL] for this version) |





<h2 id="2020-04-04-3">3. Aligned Cross Entropy for Non-Autoregressive Machine Translation</h2>

Title: [Aligned Cross Entropy for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2004.01655)

Authors: [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Vladimir Karpukhin](https://arxiv.org/search/cs?searchtype=author&query=Karpukhin%2C+V), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

*(Submitted on 3 Apr 2020)*

> Non-autoregressive machine translation models significantly speed up decoding by allowing for parallel prediction of the entire target sequence. However, modeling word order is more challenging due to the lack of autoregressive factors in the model. This difficultly is compounded during training with cross entropy loss, which can highly penalize small shifts in word order. In this paper, we propose aligned cross entropy (AXE) as an alternative loss function for training of non-autoregressive models. AXE uses a differentiable dynamic program to assign loss based on the best possible monotonic alignment between target tokens and model predictions. AXE-based training of conditional masked language models (CMLMs) substantially improves performance on major WMT benchmarks, while setting a new state of the art for non-autoregressive models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.01655](https://arxiv.org/abs/2004.01655) [cs.CL] |
|           | (or [arXiv:2004.01655v1](https://arxiv.org/abs/2004.01655v1) [cs.CL] for this version) |





<h2 id="2020-04-04-4">4. A Set of Recommendations for Assessing Human-Machine Parity in Language Translation</h2>

Title: [A Set of Recommendations for Assessing Human-Machine Parity in Language Translation](https://arxiv.org/abs/2004.01694)

Authors: [Samuel Läubli](https://arxiv.org/search/cs?searchtype=author&query=Läubli%2C+S), [Sheila Castilho](https://arxiv.org/search/cs?searchtype=author&query=Castilho%2C+S), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Qinlan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Q), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

*(Submitted on 3 Apr 2020)*

> The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a number of empirical investigations. We reassess Hassan et al.'s 2018 investigation into Chinese to English news translation, showing that the finding of human-machine parity was owed to weaknesses in the evaluation design - which is currently considered best practice in the field. We show that the professional human translations contained significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human-machine parity in particular, for which we offer a set of recommendations based on our empirical findings.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Journal of Artificial Intelligence Research 67 (2020) 653-672 |
| DOI:               | [10.1613/jair.1.11371](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1613%2Fjair.1.11371&v=de784ce7) |
| Cite as:           | [arXiv:2004.01694](https://arxiv.org/abs/2004.01694) [cs.CL] |
|                    | (or [arXiv:2004.01694v1](https://arxiv.org/abs/2004.01694v1) [cs.CL] for this version) |







# 2020-04-03

[Return to Index](#Index)



<h2 id="2020-04-03-1">1. Igbo-English Machine Translation: An Evaluation Benchmark</h2>

Title: [Igbo-English Machine Translation: An Evaluation Benchmark](https://arxiv.org/abs/2004.00648)

Authors: [Ignatius Ezeani](https://arxiv.org/search/cs?searchtype=author&query=Ezeani%2C+I), [Paul Rayson](https://arxiv.org/search/cs?searchtype=author&query=Rayson%2C+P), [Ikechukwu Onyenwe](https://arxiv.org/search/cs?searchtype=author&query=Onyenwe%2C+I), [Chinedu Uchechukwu](https://arxiv.org/search/cs?searchtype=author&query=Uchechukwu%2C+C), [Mark Hepple](https://arxiv.org/search/cs?searchtype=author&query=Hepple%2C+M)

*(Submitted on 1 Apr 2020)*

> Although researchers and practitioners are pushing the boundaries and enhancing the capacities of NLP tools and methods, works on African languages are lagging. A lot of focus on well resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese etc. Over 97% of the world's 7000 languages, including African languages, are low resourced for NLP i.e. they have little or no data, tools, and techniques for NLP research. For instance, only 5 out of 2965, 0.19% authors of full text papers in the ACL Anthology extracted from the 5 major conferences in 2018 ACL, NAACL, EMNLP, COLING and CoNLL, are affiliated to African institutions. In this work, we discuss our effort toward building a standard machine translation benchmark dataset for Igbo, one of the 3 major Nigerian languages. Igbo is spoken by more than 50 million people globally with over 50% of the speakers are in southeastern Nigeria. Igbo is low resourced although there have been some efforts toward developing IgboNLP such as part of speech tagging and diacritic restoration

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.00648](https://arxiv.org/abs/2004.00648) [cs.CL] |
|           | (or [arXiv:2004.00648v1](https://arxiv.org/abs/2004.00648v1) [cs.CL] for this version) |





<h2 id="2020-04-03-2">2. Mapping Languages: The Corpus of Global Language Use</h2>

Title: [Mapping Languages: The Corpus of Global Language Use](https://arxiv.org/abs/2004.00798)

Authors: [Jonathan Dunn](https://arxiv.org/search/cs?searchtype=author&query=Dunn%2C+J)

*(Submitted on 2 Apr 2020)*

> This paper describes a web-based corpus of global language use with a focus on how this corpus can be used for data-driven language mapping. First, the corpus provides a representation of where national varieties of major languages are used (e.g., English, Arabic, Russian) together with consistently collected data for each variety. Second, the paper evaluates a language identification model that supports more local languages with smaller sample sizes than alternative off-the-shelf models. Improved language identification is essential for moving beyond majority languages. Given the focus on language mapping, the paper analyzes how well this digital language data represents actual populations by (i) systematically comparing the corpus with demographic ground-truth data and (ii) triangulating the corpus with an alternate Twitter-based dataset. In total, the corpus contains 423 billion words representing 148 languages (with over 1 million words from each language) and 158 countries (again with over 1 million words from each country), all distilled from Common Crawl web data. The main contribution of this paper, in addition to describing this publicly-available corpus, is to provide a comprehensive analysis of the relationship between two sources of digital data (the web and Twitter) as well as their connection to underlying populations.

| Comments: | This is a pre-print of an article published in Language Resources and Evaluation. The final authenticated version is available online at: [this https URL](https://doi.org/10.1007/s10579-020-09489-2) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| DOI:      | [10.1007/s10579-020-09489-2](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2Fs10579-020-09489-2&v=6886dc37) |
| Cite as:  | [arXiv:2004.00798](https://arxiv.org/abs/2004.00798) [cs.CL] |
|           | (or [arXiv:2004.00798v1](https://arxiv.org/abs/2004.00798v1) [cs.CL] for this version) |





# 2020-04-02

[Return to Index](#Index)



<h2 id="2020-04-02-1">1. Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study</h2>

Title: [Assessing Human Translations from French to Bambara for Machine Learning: a Pilot Study](https://arxiv.org/abs/2004.00068)

Authors: [Michael Leventhal](https://arxiv.org/search/cs?searchtype=author&query=Leventhal%2C+M), [Allahsera Tapo](https://arxiv.org/search/cs?searchtype=author&query=Tapo%2C+A), [Sarah Luger](https://arxiv.org/search/cs?searchtype=author&query=Luger%2C+S), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M), [Christopher M. Homan](https://arxiv.org/search/cs?searchtype=author&query=Homan%2C+C+M)

*(Submitted on 31 Mar 2020)*

> We present novel methods for assessing the quality of human-translated aligned texts for learning machine translation models of under-resourced languages. Malian university students translated French texts, producing either written or oral translations to Bambara. Our results suggest that similar quality can be obtained from either written or spoken translations for certain kinds of texts. They also suggest specific instructions that human translators should be given in order to improve the quality of their work.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2004.00068](https://arxiv.org/abs/2004.00068) [cs.CL] |
|           | (or [arXiv:2004.00068v1](https://arxiv.org/abs/2004.00068v1) [cs.CL] for this version) |







<h2 id="2020-04-02-2">2. Sign Language Translation with Transformers</h2>

Title: [Sign Language Translation with Transformers](https://arxiv.org/abs/2004.00588)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K)

*(Submitted on 1 Apr 2020)*

> Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. Though SLT has gathered interest recently, little study has been performed on the translation system. This paper focuses on the translation system and improves performance by utilizing Transformer networks. We report a wide range of experimental results for various Transformer setups and introduce the use of Spatial-Temporal Multi-Cue (STMC) networks in an end-to-end SLT system with Transformer.
> We perform experiments on RWTH-PHOENIX-Weather 2014T, a challenging SLT benchmark dataset of German sign language, and ASLG-PC12, a dataset involving American Sign Language (ASL) recently used in gloss-to-text translation. Our methodology improves on the current state-of-the-art by over 5 and 7 points respectively in BLEU-4 score on ground truth glosses and by using an STMC network to predict glosses of the RWTH-PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an improvement of over 16 points in BLEU-4. Our findings also demonstrate that end-to-end translation on predicted glosses provides even better performance than translation on ground truth glosses. This shows potential for further improvement in SLT by either jointly training the SLR and translation systems or by revising the gloss annotation system.

| Comments: | 14 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2004.00588](https://arxiv.org/abs/2004.00588) [cs.CL] |
|           | (or [arXiv:2004.00588v1](https://arxiv.org/abs/2004.00588v1) [cs.CL] for this version) |









# 2020-04-01

[Return to Index](#Index)



<h2 id="2020-04-01-1">1. The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe</h2>

Title: [The European Language Technology Landscape in 2020: Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe](https://arxiv.org/abs/2003.13833)

Authors: [Georg Rehm](https://arxiv.org/search/cs?searchtype=author&query=Rehm%2C+G), [Katrin Marheinecke](https://arxiv.org/search/cs?searchtype=author&query=Marheinecke%2C+K), [Stefanie Hegele](https://arxiv.org/search/cs?searchtype=author&query=Hegele%2C+S), [Stelios Piperidis](https://arxiv.org/search/cs?searchtype=author&query=Piperidis%2C+S), [Kalina Bontcheva](https://arxiv.org/search/cs?searchtype=author&query=Bontcheva%2C+K), [Jan Hajič](https://arxiv.org/search/cs?searchtype=author&query=Hajič%2C+J), [Khalid Choukri](https://arxiv.org/search/cs?searchtype=author&query=Choukri%2C+K), [Andrejs Vasiļjevs](https://arxiv.org/search/cs?searchtype=author&query=Vasiļjevs%2C+A), [Gerhard Backfried](https://arxiv.org/search/cs?searchtype=author&query=Backfried%2C+G), [Christoph Prinz](https://arxiv.org/search/cs?searchtype=author&query=Prinz%2C+C), [José Manuel Gómez Pérez](https://arxiv.org/search/cs?searchtype=author&query=Pérez%2C+J+M+G), [Luc Meertens](https://arxiv.org/search/cs?searchtype=author&query=Meertens%2C+L), [Paul Lukowicz](https://arxiv.org/search/cs?searchtype=author&query=Lukowicz%2C+P), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Andrea Lösch](https://arxiv.org/search/cs?searchtype=author&query=Lösch%2C+A), [Philipp Slusallek](https://arxiv.org/search/cs?searchtype=author&query=Slusallek%2C+P), [Morten Irgens](https://arxiv.org/search/cs?searchtype=author&query=Irgens%2C+M), [Patrick Gatellier](https://arxiv.org/search/cs?searchtype=author&query=Gatellier%2C+P), [Joachim Köhler](https://arxiv.org/search/cs?searchtype=author&query=Köhler%2C+J), [Laure Le Bars](https://arxiv.org/search/cs?searchtype=author&query=Bars%2C+L+L), [Dimitra Anastasiou](https://arxiv.org/search/cs?searchtype=author&query=Anastasiou%2C+D), [Albina Auksoriūtė](https://arxiv.org/search/cs?searchtype=author&query=Auksoriūtė%2C+A), [Núria Bel](https://arxiv.org/search/cs?searchtype=author&query=Bel%2C+N), [António Branco](https://arxiv.org/search/cs?searchtype=author&query=Branco%2C+A), [Gerhard Budin](https://arxiv.org/search/cs?searchtype=author&query=Budin%2C+G), [Walter Daelemans](https://arxiv.org/search/cs?searchtype=author&query=Daelemans%2C+W), [Koenraad De Smedt](https://arxiv.org/search/cs?searchtype=author&query=De+Smedt%2C+K), [Radovan Garabík](https://arxiv.org/search/cs?searchtype=author&query=Garabík%2C+R), [Maria Gavriilidou](https://arxiv.org/search/cs?searchtype=author&query=Gavriilidou%2C+M), [Dagmar Gromann](https://arxiv.org/search/cs?searchtype=author&query=Gromann%2C+D), [Svetla Koeva](https://arxiv.org/search/cs?searchtype=author&query=Koeva%2C+S), [Simon Krek](https://arxiv.org/search/cs?searchtype=author&query=Krek%2C+S), [Cvetana Krstev](https://arxiv.org/search/cs?searchtype=author&query=Krstev%2C+C), [Krister Lindén](https://arxiv.org/search/cs?searchtype=author&query=Lindén%2C+K), [Bernardo Magnini](https://arxiv.org/search/cs?searchtype=author&query=Magnini%2C+B), [Jan Odijk](https://arxiv.org/search/cs?searchtype=author&query=Odijk%2C+J), [Maciej Ogrodniczuk](https://arxiv.org/search/cs?searchtype=author&query=Ogrodniczuk%2C+M), [Eiríkur Rögnvaldsson](https://arxiv.org/search/cs?searchtype=author&query=Rögnvaldsson%2C+E), [Mike Rosner](https://arxiv.org/search/cs?searchtype=author&query=Rosner%2C+M), [Bolette Sandford Pedersen](https://arxiv.org/search/cs?searchtype=author&query=Pedersen%2C+B+S), [Inguna Skadiņa](https://arxiv.org/search/cs?searchtype=author&query=Skadiņa%2C+I), [Marko Tadić](https://arxiv.org/search/cs?searchtype=author&query=Tadić%2C+M), [Dan Tufiş](https://arxiv.org/search/cs?searchtype=author&query=Tufiş%2C+D), [Tamás Váradi](https://arxiv.org/search/cs?searchtype=author&query=Váradi%2C+T), [Kadri Vider](https://arxiv.org/search/cs?searchtype=author&query=Vider%2C+K), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

*(Submitted on 30 Mar 2020)*

> Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe's specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI, including many opportunities, synergies but also misconceptions, has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.

| Comments: | Proceedings of the 12th Language Resources and Evaluation Conference (LREC 2020). To appear |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | [arXiv:2003.13833](https://arxiv.org/abs/2003.13833) [cs.CL] |
|           | (or [arXiv:2003.13833v1](https://arxiv.org/abs/2003.13833v1) [cs.CL] for this version) |





<h2 id="2020-04-01-2">2. MULTEXT-East</h2>

Title: [MULTEXT-East](https://arxiv.org/abs/2003.14026)

Authors: [Tomaž Erjavec](https://arxiv.org/search/cs?searchtype=author&query=Erjavec%2C+T)

*(Submitted on 31 Mar 2020)*

> MULTEXT-East language resources, a multilingual dataset for language engineering research, focused on the morphosyntactic level of linguistic description. The MULTEXT-East dataset includes the EAGLES-based morphosyntactic specifications, morphosyntactic lexicons, and an annotated multilingual corpora. The parallel corpus, the novel "1984" by George Orwell, is sentence aligned and contains hand-validated morphosyntactic descriptions and lemmas. The resources are uniformly encoded in XML, using the Text Encoding Initiative Guidelines, TEI P5, and cover 16 languages: Bulgarian, Croatian, Czech, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbian, Slovak, Slovene, and Ukrainian. This dataset is extensively documented, and freely available for research purposes. This case study gives a history of the development of the MULTEXT-East resources, presents their encoding and components, discusses related work and gives some conclusions.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | Published in: Nancy Ide, James Pustejovsky, eds. 2007. Handbook of linguistic annotation. pp. 441-462. Springer |
| DOI:               | [10.1007/978-94-024-0881-2_17](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-94-024-0881-2_17&v=6fb4b8b1) |
| Cite as:           | [arXiv:2003.14026](https://arxiv.org/abs/2003.14026) [cs.CL] |
|                    | (or [arXiv:2003.14026v1](https://arxiv.org/abs/2003.14026v1) [cs.CL] for this version) |





<h2 id="2020-04-01-3">3. Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks</h2>

Title: [Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks](https://arxiv.org/abs/2003.14056)

Authors: [Prajit Dhar](https://arxiv.org/search/cs?searchtype=author&query=Dhar%2C+P), [Arianna Bisazza](https://arxiv.org/search/cs?searchtype=author&query=Bisazza%2C+A)

*(Submitted on 31 Mar 2020)*

> It is now established that modern neural language models can be successfully trained on multiple languages simultaneously without changes to the underlying architecture, providing an easy way to adapt a variety of NLP models to low-resource languages. But what kind of knowledge is really shared among languages within these models? Does multilingual training mostly lead to an alignment of the lexical representation spaces or does it also enable the sharing of purely grammatical knowledge? In this paper we dissect different forms of cross-lingual transfer and look for its most determining factors, using a variety of models and probing tasks. We find that exposing our language models to a related language does not always increase grammatical knowledge in the target language, and that optimal conditions for lexical-semantic transfer may not be optimal for syntactic transfer.

| Comments: | 9 pages single column with 6 figures                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2003.14056](https://arxiv.org/abs/2003.14056) [cs.CL] |
|           | (or [arXiv:2003.14056v1](https://arxiv.org/abs/2003.14056v1) [cs.CL] for this version) |





<h2 id="2020-04-01-4">4. On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation</h2>

Title: [On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation](https://arxiv.org/abs/2003.14324)

Authors: [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E)

*(Submitted on 31 Mar 2020)*

> New machine translations (MT) technologies are emerging rapidly and with them, bold claims of achieving human parity such as: (i) the results produced approach "accuracy achieved by average bilingual human translators" (Wu et al., 2017b) or (ii) the "translation quality is at human parity when compared to professional human translators" (Hassan et al., 2018) have seen the light of day (Laubli et al., 2018). Aside from the fact that many of these papers craft their own definition of human parity, these sensational claims are often not supported by a complete analysis of all aspects involved in translation. Establishing the discrepancies between the strengths of statistical approaches to MT and the way humans translate has been the starting point of our research. By looking at MT output and linguistic theory, we were able to identify some remaining issues. The problems range from simple number and gender agreement errors to more complex phenomena such as the correct translation of aspectual values and tenses. Our experiments confirm, along with other studies (Bentivogli et al., 2016), that neural MT has surpassed statistical MT in many aspects. However, some problems remain and others have emerged. We cover a series of problems related to the integration of specific linguistic features into statistical and neural MT, aiming to analyse and provide a solution to some of them. Our work focuses on addressing three main research questions that revolve around the complex relationship between linguistics and MT in general. We identify linguistic information that is lacking in order for automatic translation systems to produce more accurate translations and integrate additional features into the existing pipelines. We identify overgeneralization or 'algorithmic bias' as a potential drawback of neural MT and link it to many of the remaining linguistic issues.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2003.14324](https://arxiv.org/abs/2003.14324) [cs.CL] |
|           | (or [arXiv:2003.14324v1](https://arxiv.org/abs/2003.14324v1) [cs.CL] for this version) |





<h2 id="2020-04-01-5">5. Evaluating Amharic Machine Translation</h2>

Title: [Evaluating Amharic Machine Translation](https://arxiv.org/abs/2003.14386)

Authors: [Asmelash Teka Hadgu](https://arxiv.org/search/cs?searchtype=author&query=Hadgu%2C+A+T), [Adam Beaudoin](https://arxiv.org/search/cs?searchtype=author&query=Beaudoin%2C+A), [Abel Aregawi](https://arxiv.org/search/cs?searchtype=author&query=Aregawi%2C+A)

*(Submitted on 31 Mar 2020)*

> Machine translation (MT) systems are now able to provide very accurate results for high resource language pairs. However, for many low resource languages, MT is still under active research. In this paper, we develop and share a dataset to automatically evaluate the quality of MT systems for Amharic. We compare two commercially available MT systems that support translation of Amharic to and from English to assess the current state of MT for Amharic. The BLEU score results show that the results for Amharic translation are promising but still low. We hope that this dataset will be useful to the research community both in academia and industry as a benchmark to evaluate Amharic MT systems.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.14386](https://arxiv.org/abs/2003.14386) [cs.CL] |
|           | (or [arXiv:2003.14386v1](https://arxiv.org/abs/2003.14386v1) [cs.CL] for this version) |







<h2 id="2020-04-01-6">6. Low Resource Neural Machine Translation: A Benchmark for Five African Languages</h2>

Title: [Low Resource Neural Machine Translation: A Benchmark for Five African Languages](https://arxiv.org/abs/2003.14402)

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 31 Mar 2020)*

> Recent advents in Neural Machine Translation (NMT) have shown improvements in low-resource language (LRL) translation tasks. In this work, we benchmark NMT between English and five African LRL pairs (Swahili, Amharic, Tigrigna, Oromo, Somali [SATOS]). We collected the available resources on the SATOS languages to evaluate the current state of NMT for LRLs. Our evaluation, comparing a baseline single language pair NMT model against semi-supervised learning, transfer learning, and multilingual modeling, shows significant performance improvements both in the En-LRL and LRL-En directions. In terms of averaged BLEU score, the multilingual approach shows the largest gains, up to +5 points, in six out of ten translation directions. To demonstrate the generalization capability of each model, we also report results on multi-domain test sets. We release the standardized experimental data and the test sets for future works addressing the challenges of NMT in under-resourced settings, in particular for the SATOS languages.

| Comments: | Accepted for AfricaNLP workshop at ICLR 2020                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2003.14402](https://arxiv.org/abs/2003.14402) [cs.CL] |
|           | (or [arXiv:2003.14402v1](https://arxiv.org/abs/2003.14402v1) [cs.CL] for this version) |