# Daily arXiv: Machine Translation - October, 2020

# Index


- [2020-10-30](#2020-10-30)

  - [1. Fusion Models for Improved Visual Captioning](#2020-10-30-1)
  - [2. CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models](#2020-10-30-2)
  - [3. Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection](#2020-10-30-3)
  - [4. Tilde at WMT 2020: News Task Systems](#2020-10-30-4)
  - [5. Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model](#2020-10-30-5)
  - [6. Unbabel's Participation in the WMT20 Metrics Shared Task](#2020-10-30-6)
  - [7. Contextual BERT: Conditioning the Language Model Using a Global State](#2020-10-30-7)
- [2020-10-29](#2020-10-29)

  - [1. The Volctrans Machine Translation System for WMT20](#2020-10-29-1)
  - [2. Bridging the Modality Gap for Speech-to-Text Translation](#2020-10-29-2)
- [2020-10-28](#2020-10-28)

  - [1. VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning](#2020-10-28-1)
  - [2. Data Troubles in Sentence Level Confidence Estimation for Machine Translation](#2020-10-28-2)
  - [3. Volctrans Parallel Corpus Filtering System for WMT 2020](#2020-10-28-3)
  - [4. Multitask Training with Text Data for End-to-End Speech Recognition](#2020-10-28-4)
  - [5. Evaluating Gender Bias in Speech Translation](#2020-10-28-5)
- [2020-10-27](#2020-10-27)
- [1. Anchor-based Bilingual Word Embeddings for Low-Resource Languages](#2020-10-27-1)
  - [2. Rapid Domain Adaptation for Machine Translation with Monolingual Data](#2020-10-27-2)
  - [3. Dynamic Contextualized Word Embeddings](#2020-10-27-3)
  - [4. Improving Multilingual Models with Language-Clustered Vocabularies](#2020-10-27-4)
  - [5. Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model](#2020-10-27-5)
  - [6. Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation](#2020-10-27-6)
  - [7. Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions](#2020-10-27-7)
  - [8. Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation](#2020-10-27-8)
  - [9. Orthros: Non-autoregressive End-to-end Speech Translation with Dual-decoder](#2020-10-27-9)
  - [10. Autoencoding Improves Pre-trained Word Embeddings](#2020-10-27-10)
  - [11. Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding](#2020-10-27-11)
  - [12. The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task](#2020-10-27-12)
  - [13. Constraint Translation Candidates: A Bridge between Neural Query Translation and Cross-lingual Information Retrieval](#2020-10-27-13)
  - [14. Exploiting Neural Query Translation into Cross Lingual Information Retrieval](#2020-10-27-14)
- [2020-10-26](#2020-10-26)

  - [1. Multilingual BERT Post-Pretraining Alignment](#2020-10-26-1)
  - [2. On the Transformer Growth for Progressive BERT Training](#2020-10-26-2)
  - [3. DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries](#2020-10-26-3)
  - [4. Language-Conditioned Imitation Learning for Robot Manipulation Tasks](#2020-10-26-4)
  - [5. How Phonotactics Affect Multilingual and Zero-shot ASR Performance](#2020-10-26-5)
  - [6. A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios](#2020-10-26-6)
- [2020-10-23](#2020-10-23)

  - [1. Similarity Analysis of Self-Supervised Speech Representations](#2020-10-23-1)
  - [2. Autoregressive Modeling is Misspecified for Some Sequence Distributions](#2020-10-23-2)
  - [3. Improving Simultaneous Translation with Pseudo References](#2020-10-23-3)
  - [4. A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks](#2020-10-23-4)
  - [5. MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation](#2020-10-23-5)
  - [6. A Technical Report: BUT Speech Translation Systems](#2020-10-23-6)
  - [7. CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20](#2020-10-23-7)
  - [8. Not all parameters are born equal: Attention is mostly what you need](#2020-10-23-8)
  - [9. mT5: A massively multilingual pre-trained text-to-text transformer](#2020-10-23-9)
  - [10. UniCase -- Rethinking Casing in Language Models](#2020-10-23-10)
- [2020-10-22](#2020-10-22)	

  - [1. Towards End-to-End In-Image Neural Machine Translation](#2020-10-22-1)
  - [2. Multi-Unit Transformer for Neural Machine Translation](#2020-10-22-2)
  - [3. Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](#2020-10-22-3)
  - [4. What makes multilingual BERT multilingual?](#2020-10-22-4)
  - [5. Token Drop mechanism for Neural Machine Translation](#2020-10-22-5)
  - [6. NeuSpell: A Neural Spelling Correction Toolkit](#2020-10-22-6)
  - [7. Beyond English-Centric Multilingual Machine Translation](#2020-10-22-7)
  - [8. Sentence Boundary Augmentation For Neural Machine Translation Robustness](#2020-10-22-8)
  - [9. Cascaded Models With Cyclic Feedback For Direct Speech Translation](#2020-10-22-9)
- [2020-10-21](#2020-10-21)

  - [1. Word Shape Matters: Robust Machine Translation with Visual Embedding](#2020-10-21-1)
  - [2. Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization](#2020-10-21-2)
  - [3. Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training](#2020-10-21-3)
  - [4. Complete Multilingual Neural Machine Translation](#2020-10-21-4)
  - [5. Human-Paraphrased References Improve Neural Machine Translation](#2020-10-21-5)
  - [6. CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](#2020-10-21-6)
  - [7. Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages](#2020-10-21-7)
  - [8. Optimal Subarchitecture Extraction For BERT](#2020-10-21-8)
- [2020-10-20](#2020-10-20)

  - [1. Emerging Trends of Multimodal Research in Vision and Language](#2020-10-20-1)
  - [2. A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences](#2020-10-20-2)
  - [3. Incorporate Semantic Structures into Machine Translation Evaluation via UCCA](#2020-10-20-3)
  - [4. Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach](#2020-10-20-4)
  - [5. Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation](#2020-10-20-5)
  - [6. Revisiting Modularized Multilingual NMT to Meet Industrial Demands](#2020-10-20-6)
  - [7. Diving Deep into Context-Aware Neural Machine Translation](#2020-10-20-7)
  - [8. Cold-start Active Learning through Self-supervised Language Modeling](#2020-10-20-8)
  - [9. Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines](#2020-10-20-9)
- [2020-10-19](#2020-10-19)

  - [1. DiDi's Machine Translation System for WMT2020](#2020-10-19-1)
  - [2. Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation](#2020-10-19-2)
  - [3. It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT](#2020-10-19-3)
  - [4. Multi-Adversarial Learning for Cross-Lingual Word Embeddings](#2020-10-19-4)
  - [5. Adaptive Feature Selection for End-to-End Speech Translation](#2020-10-19-5)
  - [6. Mischief: A Simple Black-Box Attack Against Transformer Architectures](#2020-10-19-6)
  - [7. Explicit Alignment Objectives for Multilingual Bidirectional Encoders](#2020-10-19-7)
- [2020-10-16](#2020-10-16)

  - [1. Decoding Methods for Neural Narrative Generation](#2020-10-16-1)
  - [2. Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses](#2020-10-16-2)
  - [3. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses](#2020-10-16-3)
  - [4. Does Chinese BERT Encode Word Structure?](#2020-10-16-4)
  - [5. Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings](#2020-10-16-5)
  - [6. Tokenization Repair in the Presence of Spelling Errors](#2020-10-16-6)
- [2020-10-15](#2020-10-15)

  - [1. The EOS Decision and Length Extrapolation](#2020-10-15-1)
  - [2. Dissecting the components and factors of Neural Text Generation](#2020-10-15-2)
  - [3. Random Network Distillation as a Diversity Metric for Both Image and Text Generation](#2020-10-15-3)
  - [4. MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings](#2020-10-15-4)
  - [5. Memformer: The Memory-Augmented Transformer](#2020-10-15-5)
  - [6. DA-Transformer: Distance-aware Transformer](#2020-10-15-6)
  - [7. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](#2020-10-15-7)
- [2020-10-14](#2020-10-14)

  - [1. Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation](#2020-10-14-1)
  - [2. Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model](#2020-10-14-2)
  - [3. Towards Machine Translation for the Kurdish Language](#2020-10-14-3)
  - [4. Incorporating BERT into Parallel Sequence Decoding with Adapters](#2020-10-14-4)
  - [5. Mitigating Gender Bias in Machine Translation with Target Gender Annotations](#2020-10-14-5)
  - [6. CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations](#2020-10-14-6)
  - [7. The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT](#2020-10-14-7)
  - [8. Fine-grained linguistic evaluation for state-of-the-art Machine Translation](#2020-10-14-8)
  - [9. Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language](#2020-10-14-9)
  - [10. Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!](#2020-10-14-10)
- [2020-10-13](#2020-10-13)

  - [1. Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation](#2020-10-13-1)
  - [2. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?](#2020-10-13-2)
  - [3. Load What You Need: Smaller Versions of Multilingual BERT](#2020-10-13-3)
  - [4. Controllable Paraphrasing and Translation with a Syntactic Exemplar](#2020-10-13-4)
  - [5. Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models](#2020-10-13-5)
  - [6. Do Language Embeddings Capture Scales?](#2020-10-13-6)
  - [7. Gradient-based Analysis of NLP Models is Manipulable](#2020-10-13-7)
  - [8. It's not a Non-Issue: Negation as a Source of Error in Machine Translation](#2020-10-13-8)
  - [9. Structural Knowledge Distillation](#2020-10-13-9)
  - [10. SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task](#2020-10-13-10)
  - [11. fairseq S2T: Fast Speech-to-Text Modeling with fairseq](#2020-10-13-11)
  - [12. Machine Translation of Mathematical Text](#2020-10-13-12)
  - [13. Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It](#2020-10-13-13)
  - [14. Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task](#2020-10-13-14)
  - [15. ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization](#2020-10-13-15)
  - [16. What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](#2020-10-13-16)
  - [17. On Long-Tailed Phenomena in Neural Machine Translation](#2020-10-13-17)
  - [18. Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns](#2020-10-13-18)
- [2020-10-12](#2020-10-12)

  - [1. Query-Key Normalization for Transformers](#2020-10-12-1)
  - [2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](#2020-10-12-2)
  - [3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](#2020-10-12-3)
  - [4. Token-level Adaptive Training for Neural Machine Translation](#2020-10-12-4)
  - [5. A Survey of Knowledge-Enhanced Text Generation](#2020-10-12-5)
  - [6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](#2020-10-12-6)
  - [7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](#2020-10-12-7)
  - [8. Self-Paced Learning for Neural Machine Translation](#2020-10-12-8)
  - [9. Recursive Top-Down Production for Sentence Generation with Latent Trees](#2020-10-12-9)
- [2020-10-09](#2020-10-09)

  - [1. Shallow-to-Deep Training for Neural Machine Translation](#2020-10-09-1)
  - [2. Improving Attention Mechanism with Query-Value Interaction](#2020-10-09-2)
  - [3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](#2020-10-09-3)
  - [4. What Can We Do to Improve Peer Review in NLP?](#2020-10-09-4)
  - [5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks](#2020-10-09-5)
  - [6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](#2020-10-09-6)
  - [7. Cross-Thought for Sentence Encoder Pre-training](#2020-10-09-7)
  - [8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation](#2020-10-09-8)
  - [9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](#2020-10-09-9)
- [2020-10-08](#2020-10-08)

  - [1. Plug and Play Autoencoders for Conditional Text Generation](#2020-10-08-1)
  - [2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](#2020-10-08-2)
  - [3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](#2020-10-08-3)
  - [4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](#2020-10-08-4)
  - [5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](#2020-10-08-5)
  - [6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](#2020-10-08-6)
  - [7. WER we are and WER we think we are](#2020-10-08-7)
  - [8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](#2020-10-08-8)
  - [9. TeaForN: Teacher-Forcing with N-grams](#2020-10-08-9)
  - [10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](#2020-10-08-10)
- [2020-10-07](#2020-10-07)

  - [1. Multi-task Learning for Multilingual Neural Machine Translation](#2020-10-07-1)
  - [2. Do Explicit Alignments Robustly Improve Multilingual Encoders?](#2020-10-07-2)
  - [3. The Multilingual Amazon Reviews Corpus](#2020-10-07-3)
  - [4. On the Sparsity of Neural Machine Translation Models](#2020-10-07-4)
  - [5. On the Sub-Layer Functionalities of Transformer Decoder](#2020-10-07-5)
  - [6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](#2020-10-07-6)
  - [7. Analyzing Individual Neurons in Pre-trained Language Models](#2020-10-07-7)
  - [8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](#2020-10-07-8)
  - [9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](#2020-10-07-9)
  - [10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](#2020-10-07-10)
  - [11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](#2020-10-07-11)
  - [12. Inference Strategies for Machine Translation with Conditional Masking](#2020-10-07-12)
  - [13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](#2020-10-07-13)
  - [14. Guiding Attention for Self-Supervised Learning with Transformers](#2020-10-07-14)
  - [15. Adversarial Grammatical Error Correction](#2020-10-07-15)
  - [16. Efficient Inference For Neural Machine Translation](#2020-10-07-16)
  - [17. Iterative Domain-Repaired Back-Translation](#2020-10-07-17)
- [2020-10-06](#2020-10-06)

  - [1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](#2020-10-06-1)
  - [2. Transformer-Based Neural Text Generation with Syntactic Guidance](#2020-10-06-2)
  - [3. Second-Order NLP Adversarial Examples](#2020-10-06-3)
  - [4. GenAug: Data Augmentation for Finetuning Text Generators](#2020-10-06-4)
  - [5. Lifelong Language Knowledge Distillation](#2020-10-06-5)
  - [6. A Streaming Approach For Efficient Batched Beam Search](#2020-10-06-6)
  - [7. Self-training Improves Pre-training for Natural Language Understanding](#2020-10-06-7)
  - [8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](#2020-10-06-8)
- [2020-10-05](#2020-10-05)

  - [1. Nearest Neighbor Machine Translation](#2020-10-05-1)
  - [2. A Survey of the State of Explainable AI for Natural Language Processing](#2020-10-05-2)
  - [3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](#2020-10-05-3)
  - [4. Which *BERT? A Survey Organizing Contextualized Encoders](#2020-10-05-4)


- [2020-10-02](#2020-10-2)

  - [1. WeChat Neural Machine Translation Systems for WMT20](#2020-10-2-1)
- [2020-10-01](#2020-10-01)

  - [1. Rethinking Attention with Performers](#2020-10-01-1)
  - [2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](#2020-10-01-2)
  - [3. Can Automatic Post-Editing Improve NMT?](#2020-10-01-3)
  - [4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](#2020-10-01-4)
  - [5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation](#2020-10-01-5)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)





# 2020-10-30

[Return to Index](#Index)



<h2 id="2020-10-30-1">1. Fusion Models for Improved Visual Captioning
</h2>

Title: [Fusion Models for Improved Visual Captioning](https://arxiv.org/abs/2010.15251)

Authors: [Marimuthu Kalimuthu](https://arxiv.org/search/cs?searchtype=author&query=Kalimuthu%2C+M), [Aditya Mogadala](https://arxiv.org/search/cs?searchtype=author&query=Mogadala%2C+A), [Marius Mosbach](https://arxiv.org/search/cs?searchtype=author&query=Mosbach%2C+M), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Visual captioning aims to generate textual descriptions given images. Traditionally, the captioning models are trained on human annotated datasets such as Flickr30k and MS-COCO, which are limited in size and diversity. This limitation hinders the generalization capabilities of these models while also rendering them to often make mistakes. Language models can, however, be trained on vast amounts of freely available unlabelled data and have recently emerged as successful language encoders and coherent text generators. Meanwhile, several unimodal and multimodal fusion techniques have been proven to work well for natural language generation and automatic speech recognition. Building on these recent developments, and with an aim of improving the quality of generated captions, the contribution of our work in this paper is two-fold: First, we propose a generic multimodal model fusion framework for caption generation as well as emendation where we utilize different fusion strategies to integrate a pretrained Auxiliary Language Model (AuxLM) within the traditional encoder-decoder visual captioning frameworks. Next, we employ the same fusion strategies to integrate a pretrained Masked Language Model (MLM), namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for emending both syntactic and semantic errors in captions. Our caption emendation experiments on three benchmark image captioning datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the baseline, indicating the usefulness of our proposed multimodal fusion strategies. Further, we perform a preliminary qualitative analysis on the emended captions and identify error categories based on the type of corrections.

| Comments: | Under review at "Multi-Modal Deep Learning: Challenges and Applications", ICPR-2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2010.15251](https://arxiv.org/abs/2010.15251) [cs.CV] |
|           | (or [arXiv:2010.15251v1](https://arxiv.org/abs/2010.15251v1) [cs.CV] for this version) |

<h2 id="2020-10-30-2">2. CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models</h2>

Title: [CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models](https://arxiv.org/abs/2010.15266)

Authors: [Abhinav Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A), [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Guanghui Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+G), [Mahsa Yarmohammadi](https://arxiv.org/search/cs?searchtype=author&query=Yarmohammadi%2C+M), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B)

> Copy mechanisms are employed in sequence to sequence models (seq2seq) to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.

| Comments: | 4th Workshop on Structured Prediction for NLP (EMNLP 2020)   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2010.15266](https://arxiv.org/abs/2010.15266) [cs.CL] |
|           | (or [arXiv:2010.15266v1](https://arxiv.org/abs/2010.15266v1) [cs.CL] for this version) |

<h2 id="2020-10-30-3">3. Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection</h2>

Title: [Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection](https://arxiv.org/abs/2010.15360)

Authors: [Shaolei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.15360](https://arxiv.org/abs/2010.15360) [cs.CL] |
|           | (or [arXiv:2010.15360v1](https://arxiv.org/abs/2010.15360v1) [cs.CL] for this version) |

<h2 id="2020-10-30-4">4. Tilde at WMT 2020: News Task Systems</h2>

Title: [Tilde at WMT 2020: News Task Systems](https://arxiv.org/abs/2010.15423)

Authors: [Rihards Krišlauks](https://arxiv.org/search/cs?searchtype=author&query=Krišlauks%2C+R), [Mārcis Pinnis](https://arxiv.org/search/cs?searchtype=author&query=Pinnis%2C+M)

> This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions from the previous years and build our baseline systems to be morphologically motivated sub-word unit-based Transformer base models that we train using the Marian machine translation toolkit. Additionally, we experiment with different parallel and monolingual data selection schemes, as well as sampled back-translation. Our final models are ensembles of Transformer base and Transformer big models that feature right-to-left re-ranking.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.15423](https://arxiv.org/abs/2010.15423) [cs.CL] |
|           | (or [arXiv:2010.15423v1](https://arxiv.org/abs/2010.15423v1) [cs.CL] for this version) |

<h2 id="2020-10-30-5">5. Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model</h2>

Title: [Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model](https://arxiv.org/abs/2010.15437)

Authors: [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N), [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S)

> This paper presents a novel fusion method for integrating an external language model (LM) into the Transformer based sequence-to-sequence (seq2seq) model. While paired data are basically required to train the seq2seq model, the external LM can be trained with only unpaired data. Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data. However, the existing fusion methods assume that the LM is integrated with recurrent neural network-based seq2seq models instead of the Transformer. Therefore, this paper proposes a fusion method that can explicitly utilize network structures in the Transformer. The proposed method, called {\bf memory attentive fusion}, leverages the Transformer-style attention mechanism that repeats source-target attention in a multi-hop manner for reading the memorized knowledge in the LM. Our experiments on two text-style conversion tasks demonstrate that the proposed method performs better than conventional fusion methods.

| Comments: | Accepted as a short paper at INLG 2020                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.15437](https://arxiv.org/abs/2010.15437) [cs.CL] |
|           | (or [arXiv:2010.15437v1](https://arxiv.org/abs/2010.15437v1) [cs.CL] for this version) |

<h2 id="2020-10-30-6">6. Unbabel's Participation in the WMT20 Metrics Shared Task</h2>

Title: [Unbabel's Participation in the WMT20 Metrics Shared Task](https://arxiv.org/abs/2010.15535)

Authors: [Ricardo Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+R), [Craig Stewart](https://arxiv.org/search/cs?searchtype=author&query=Stewart%2C+C), [Catarina Farinha](https://arxiv.org/search/cs?searchtype=author&query=Farinha%2C+C), [Alon Lavie](https://arxiv.org/search/cs?searchtype=author&query=Lavie%2C+A)

> We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segment-level, document-level and system-level tracks on all language pairs, as well as the 'QE as a Metric' track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: We train several estimator models to regress on different human-generated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art.

| Comments: | WMT Metrics Shared Task 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.15535](https://arxiv.org/abs/2010.15535) [cs.CL] |
|           | (or [arXiv:2010.15535v1](https://arxiv.org/abs/2010.15535v1) [cs.CL] for this version) |

<h2 id="2020-10-30-7">7. Contextual BERT: Conditioning the Language Model Using a Global State</h2>

Title: [Contextual BERT: Conditioning the Language Model Using a Global State](https://arxiv.org/abs/2010.15778)

Authors: [Timo I. Denk](https://arxiv.org/search/cs?searchtype=author&query=Denk%2C+T+I), [Ana Peleteiro Ramallo](https://arxiv.org/search/cs?searchtype=author&query=Ramallo%2C+A+P)

> BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly.

| Comments: | Accepted at the TextGraphs-14 workshop at COLING'2020 - The 28th International Conference on Computational Linguistics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.15778](https://arxiv.org/abs/2010.15778) [cs.CL] |
|           | (or [arXiv:2010.15778v1](https://arxiv.org/abs/2010.15778v1) [cs.CL] for this version) |





# 2020-10-29

[Return to Index](#Index)



<h2 id="2020-10-29-1">1. The Volctrans Machine Translation System for WMT20</h2>

Title: [The Volctrans Machine Translation System for WMT20](https://arxiv.org/abs/2010.14806)

Authors: [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Yaoming Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> This paper describes our VolcTrans system on WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer, with several variants (wider or deeper Transformers, dynamic convolutions). The final system includes text pre-process, data selection, synthetic data generation, advanced model ensemble, and multilingual pre-training.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.14806](https://arxiv.org/abs/2010.14806) [cs.CL] |
|           | (or [arXiv:2010.14806v1](https://arxiv.org/abs/2010.14806v1) [cs.CL] for this version) |





<h2 id="2020-10-29-2">2. Bridging the Modality Gap for Speech-to-Text Translation</h2>

Title: [Bridging the Modality Gap for Speech-to-Text Translation](https://arxiv.org/abs/2010.14920)

Authors: [Yuchen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Junnan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> End-to-end speech translation aims to translate speech in one language into text in another language via an end-to-end way. Most existing methods employ an encoder-decoder structure with a single encoder to learn acoustic representation and semantic information simultaneously, which ignores the speech-and-text modality differences and makes the encoder overloaded, leading to great difficulty in learning such a model. To address these issues, we propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, we decouple the speech translation encoder into three parts and introduce a shrink mechanism to match the length of speech representation with that of the corresponding text transcription. To obtain better semantic representation, we completely integrate a text-based translation model into the STAST so that two tasks can be trained in the same latent space. Furthermore, we introduce a cross-modal adaptation method to close the distance between speech and text representation. Experimental results on English-French and English-German speech translation corpora have shown that our model significantly outperforms strong baselines, and achieves the new state-of-the-art performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.14920](https://arxiv.org/abs/2010.14920) [cs.CL] |
|           | (or [arXiv:2010.14920v1](https://arxiv.org/abs/2010.14920v1) [cs.CL] for this version) |









# 2020-10-28

[Return to Index](#Index)



<h2 id="2020-10-28-1">1. VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning</h2>

Title: [VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning](https://arxiv.org/abs/2010.13839)

Authors: [Thomas Carta](https://arxiv.org/search/cs?searchtype=author&query=Carta%2C+T), [Subhajit Chaudhury](https://arxiv.org/search/cs?searchtype=author&query=Chaudhury%2C+S), [Kartik Talamadupula](https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K), [Michiaki Tatsubori](https://arxiv.org/search/cs?searchtype=author&query=Tatsubori%2C+M)

> We present VisualHints, a novel environment for multimodal reinforcement learning (RL) involving text-based interactions along with visual hints (obtained from the environment). Real-life problems often demand that agents interact with the environment using both natural language information and visual perception towards solving a goal. However, most traditional RL environments either solve pure vision-based tasks like Atari games or video-based robotic manipulation; or entirely use natural language as a mode of interaction, like Text-based games and dialog systems. In this work, we aim to bridge this gap and unify these two approaches in a single environment for multimodal RL. We introduce an extension of the TextWorld cooking environment with the addition of visual clues interspersed throughout the environment. The goal is to force an RL agent to use both text and visual features to predict natural language action commands for solving the final task of cooking a meal. We enable variations and difficulties in our environment to emulate various interactive real-world scenarios. We present a baseline multimodal agent for solving such problems using CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We believe that our proposed visual-lingual environment will facilitate novel problem settings for the RL community.

| Comments: | Code is available at [this http URL](http://ibm.biz/VisualHints) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | [arXiv:2010.13839](https://arxiv.org/abs/2010.13839) [cs.LG] |
|           | (or [arXiv:2010.13839v1](https://arxiv.org/abs/2010.13839v1) [cs.LG] for this version) |





<h2 id="2020-10-28-2">2. Data Troubles in Sentence Level Confidence Estimation for Machine Translation</h2>

Title: [Data Troubles in Sentence Level Confidence Estimation for Machine Translation](https://arxiv.org/abs/2010.13856)

Authors: [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Junpei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Yuezhang](https://arxiv.org/search/cs?searchtype=author&query=Yuezhang) (Music)Li, [Hideto Kazawa](https://arxiv.org/search/cs?searchtype=author&query=Kazawa%2C+H), [Jeff Klingner](https://arxiv.org/search/cs?searchtype=author&query=Klingner%2C+J), [Mengmeng Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+M)

> The paper investigates the feasibility of confidence estimation for neural machine translation models operating at the high end of the performance spectrum. As a side product of the data annotation process necessary for building such models we propose sentence level accuracy SACC as a simple, self-explanatory evaluation metric for quality of translation.
> Experiments on two different annotator pools, one comprised of non-expert (crowd-sourced) and one of expert (professional) translators show that SACC can vary greatly depending on the translation proficiency of the annotators, despite the fact that both pools are about equally reliable according to Krippendorff's alpha metric; the relatively low values of inter-annotator agreement confirm the expectation that sentence-level binary labeling good / needs work for translation out of context is very hard.
> For an English-Spanish translation model operating at SACC=0.89 according to a non-expert annotator pool we can derive a confidence estimate that labels 0.5-0.6 of the good translations in an "in-domain" test set with 0.95 Precision. Switching to an expert annotator pool decreases SACC dramatically: 0.61 for English-Spanish, measured on the exact same data as above. This forces us to lower the CE model operating point to 0.9 Precision while labeling correctly about 0.20-0.25 of the good translations in the data.
> We find surprising the extent to which CE depends on the level of proficiency of the annotator pool used for labeling the data. This leads to an important recommendation we wish to make when tackling CE modeling in practice: it is critical to match the end-user expectation for translation quality in the desired domain with the demands of annotators assigning binary quality labels to CE training data.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.13856](https://arxiv.org/abs/2010.13856) [cs.CL] |
|           | (or [arXiv:2010.13856v1](https://arxiv.org/abs/2010.13856v1) [cs.CL] for this version) |





<h2 id="2020-10-28-3">3. Volctrans Parallel Corpus Filtering System for WMT 2020</h2>

Title: [Volctrans Parallel Corpus Filtering System for WMT 2020](https://arxiv.org/abs/2010.14029)

Authors: [Runxin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Zhuo Zhi](https://arxiv.org/search/cs?searchtype=author&query=Zhi%2C+Z), [Jun Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining module adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions, which is the highest among all submissions.

| Comments: | WMT 2020                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.14029](https://arxiv.org/abs/2010.14029) [cs.CL] |
|           | (or [arXiv:2010.14029v1](https://arxiv.org/abs/2010.14029v1) [cs.CL] for this version) |





<h2 id="2020-10-28-4">4. Multitask Training with Text Data for End-to-End Speech Recognition</h2>

Title: [Multitask Training with Text Data for End-to-End Speech Recognition](https://arxiv.org/abs/2010.14318)

Authors: [Peidong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Tara N. Sainath](https://arxiv.org/search/cs?searchtype=author&query=Sainath%2C+T+N), [Ron J. Weiss](https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+R+J)

> We propose a multitask training method for attention-based end-to-end speech recognition models to better incorporate language level information. We regularize the decoder in a sequence-to-sequence architecture by multitask training it on both the speech recognition task and a next-token prediction language modeling task. Trained on either the 100 hour subset of LibriSpeech or the full 960 hour dataset, the proposed method leads to an 11% relative performance improvement over the baseline and is comparable to language model shallow fusion, without requiring an additional neural network during decoding. Analyses of sample output sentences and the word error rate on rare words demonstrate that the proposed method can incorporate language level information effectively.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.14318](https://arxiv.org/abs/2010.14318) [cs.CL] |
|           | (or [arXiv:2010.14318v1](https://arxiv.org/abs/2010.14318v1) [cs.CL] for this version) |





<h2 id="2020-10-28-5">5. Evaluating Gender Bias in Speech Translation</h2>

Title: [Evaluating Gender Bias in Speech Translation](https://arxiv.org/abs/2010.14465)

Authors: [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R), [Christine Basta](https://arxiv.org/search/cs?searchtype=author&query=Basta%2C+C), [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I)

> The scientific community is more and more aware of the necessity to embrace pluralism and consistently represent major and minor social groups. In this direction, there is an urgent need to provide evaluation sets and protocols to measure existing biases in our automatic systems. This paper introduces WinoST, a new freely available challenge set for evaluating gender bias in speech translation. WinoST is the speech version of WinoMT which is an MT challenge set and both follow an evaluation protocol to measure gender accuracy. Using a state-of-the-art end-to-end speech translation system, we report the gender bias evaluation on 4 language pairs, and we show that gender accuracy in speech translation is more than 23% lower than in MT.

| Comments:    | Preprint, Submitted to ICASSP 2021                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | [arXiv:2010.14465](https://arxiv.org/abs/2010.14465) [cs.CL] |
|              | (or [arXiv:2010.14465v1](https://arxiv.org/abs/2010.14465v1) [cs.CL] for this version) |







# 2020-10-27

[Return to Index](#Index)



<h2 id="2020-10-27-1">1. Anchor-based Bilingual Word Embeddings for Low-Resource Languages</h2>

Title: [Anchor-based Bilingual Word Embeddings for Low-Resource Languages](https://arxiv.org/abs/2010.12627)

Authors: [Tobias Eder](https://arxiv.org/search/cs?searchtype=author&query=Eder%2C+T), [Viktor Hangya](https://arxiv.org/search/cs?searchtype=author&query=Hangya%2C+V), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Bilingual word embeddings (BWEs) are useful for many cross-lingual applications, such as bilingual lexicon induction (BLI) and cross-lingual transfer learning. While recent methods have led to good quality BWEs for different language pairs using only weak bilingual signals, they still rely on an abundance of monolingual training data in both languages for their performance. This becomes a problem especially in the case of low resource languages where neither parallel bilingual corpora nor large monolingual training data are available. This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point for training an embedding space for the low resource target language. By using the source vectors as anchors the vector spaces are automatically aligned. We evaluate the resulting BWEs on BLI and show the proposed method outperforms previous approaches in the low-resource setting by a large margin. We show strong results on the standard English-German test pair (using German to simulate low resource). We also show we can build useful BWEs for English-Hiligaynon, a true low-resource language, where previous approaches failed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12627](https://arxiv.org/abs/2010.12627) [cs.CL] |
|           | (or [arXiv:2010.12627v1](https://arxiv.org/abs/2010.12627v1) [cs.CL] for this version) |





<h2 id="2020-10-27-2">2. Rapid Domain Adaptation for Machine Translation with Monolingual Data</h2>

Title: [Rapid Domain Adaptation for Machine Translation with Monolingual Data](https://arxiv.org/abs/2010.12652)

Authors: [Mahdis Mahdieh](https://arxiv.org/search/cs?searchtype=author&query=Mahdieh%2C+M), [Mia Xu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M+X), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> One challenge of machine translation is how to quickly adapt to unseen domains in face of surging events like COVID-19, in which case timely and accurate translation of in-domain information into multiple languages is critical but little parallel data is available yet. In this paper, we propose an approach that enables rapid domain adaptation from the perspective of unsupervised translation. Our proposed approach only requires in-domain monolingual data and can be quickly applied to a preexisting translation system trained on general domain, reaching significant gains on in-domain translation quality with little or no drop on general-domain. We also propose an effective procedure of simultaneous adaptation for multiple domains and languages. To the best of our knowledge, this is the first attempt that aims to address unsupervised multilingual domain adaptation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12652](https://arxiv.org/abs/2010.12652) [cs.CL] |
|           | (or [arXiv:2010.12652v1](https://arxiv.org/abs/2010.12652v1) [cs.CL] for this version) |





<h2 id="2020-10-27-3">3. Dynamic Contextualized Word Embeddings</h2>

Title: [Dynamic Contextualized Word Embeddings](https://arxiv.org/abs/2010.12684)

Authors: [Valentin Hofmann](https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+V), [Janet B. Pierrehumbert](https://arxiv.org/search/cs?searchtype=author&query=Pierrehumbert%2C+J+B), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for various tasks in the computational social sciences. We highlight potential applications by means of qualitative and quantitative analyses.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12684](https://arxiv.org/abs/2010.12684) [cs.CL] |
|           | (or [arXiv:2010.12684v1](https://arxiv.org/abs/2010.12684v1) [cs.CL] for this version) |





<h2 id="2020-10-27-4">4. Improving Multilingual Models with Language-Clustered Vocabularies</h2>

Title: [Improving Multilingual Models with Language-Clustered Vocabularies](https://arxiv.org/abs/2010.12777)

Authors: [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Dan Garrette](https://arxiv.org/search/cs?searchtype=author&query=Garrette%2C+D), [Kiat Chuan Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+K+C), [Jason Riesa](https://arxiv.org/search/cs?searchtype=author&query=Riesa%2C+J)

> State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.

| Comments: | Published in the main conference of EMNLP 2020               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2010.12777](https://arxiv.org/abs/2010.12777) [cs.CL] |
|           | (or [arXiv:2010.12777v1](https://arxiv.org/abs/2010.12777v1) [cs.CL] for this version) |





<h2 id="2020-10-27-5">5. Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model</h2>

Title: [Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model](https://arxiv.org/abs/2010.12827)

Authors: [Amane Sugiyama](https://arxiv.org/search/cs?searchtype=author&query=Sugiyama%2C+A), [Naoki Yoshinaga](https://arxiv.org/search/cs?searchtype=author&query=Yoshinaga%2C+N)

> Although many context-aware neural machine translation models have been proposed to incorporate contexts in translation, most of those models are trained end-to-end on parallel documents aligned in sentence-level. Because only a few domains (and language pairs) have such document-level parallel data, we cannot perform accurate context-aware translation in most domains. We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder. Our context-aware decoder is built upon only a sentence-level parallel corpora and monolingual corpora; thus no document-level parallel data is needed. In a theoretical viewpoint, the core part of this work is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We show the effectiveness of our approach in three language pairs, English to French, English to Russian, and Japanese to English, by evaluation in \textsc{bleu} and contrastive tests for context-aware translation.

| Comments: | Under Review                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.12827](https://arxiv.org/abs/2010.12827) [cs.CL] |
|           | (or [arXiv:2010.12827v1](https://arxiv.org/abs/2010.12827v1) [cs.CL] for this version) |







<h2 id="2020-10-27-6">6. Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation</h2>

Title: [Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation](https://arxiv.org/abs/2010.12829)

Authors: [Chau Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+C), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> We propose an effective approach to utilize pretrained speech and text models to perform speech-to-text translation (ST). Our recipe to achieve cross-modal and cross-lingual transfer learning (XMTL) is simple and generalizable: using an adaptor module to bridge the modules pretrained in different modalities, and an efficient finetuning step which leverages the knowledge from pretrained modules yet making it work on a drastically different downstream task. With this approach, we built a multilingual speech-to-text translation model with pretrained audio encoder (wav2vec) and multilingual text decoder (mBART), which achieves new state-of-the-art on CoVoST 2 ST benchmark [1] for English into 15 languages as well as 6 Romance languages into English with on average +2.8 BLEU and +3.9 BLEU, respectively. On low-resource languages (with less than 10 hours training data), our approach significantly improves the quality of speech-to-text translation with +9.0 BLEU on Portuguese-English and +5.2 BLEU on Dutch-English.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12829](https://arxiv.org/abs/2010.12829) [cs.CL] |
|           | (or [arXiv:2010.12829v1](https://arxiv.org/abs/2010.12829v1) [cs.CL] for this version) |



<h2 id="2020-10-27-7">7. Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions</h2>

Title: [Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions](https://arxiv.org/abs/2010.12831)

Authors: [Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Haoxuan You](https://arxiv.org/search/cs?searchtype=author&query=You%2C+H), [Zhecan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Alireza Zareian](https://arxiv.org/search/cs?searchtype=author&query=Zareian%2C+A), [Shih-Fu Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)

> Pre-trained contextual vision-and-language (V&L) models have brought impressive performance improvement on various benchmarks. However, the paired text-image data required for pre-training are hard to collect and scale up. We investigate if a strong V&L representation model can be learned without text-image pairs. We propose Weakly-supervised VisualBERT with the key idea of conducting "mask-and-predict" pre-training on language-only and image-only corpora. Additionally, we introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. Evaluation on four V&L benchmarks shows that Weakly-supervised VisualBERT achieves similar performance with a model pre-trained with paired data. Besides, pre-training on more image-only data further improves a model that already has access to aligned data, suggesting the possibility of utilizing billions of raw images available to enhance V&L models.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12831](https://arxiv.org/abs/2010.12831) [cs.CL] |
|           | (or [arXiv:2010.12831v1](https://arxiv.org/abs/2010.12831v1) [cs.CL] for this version) |



<h2 id="2020-10-27-8">8. Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation</h2>

Title: [Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2010.12868)

Authors: [Yongchang Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Y), [Shilin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+S), [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Michael Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties and representations of source sentences. Therefore, we propose to adopt the multi-task learning to transfer the AT knowledge to NAT models through the encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English->German and WMT16 English->Romanian datasets show that the proposed multi-task NAT achieves significant improvements over the baseline NAT models. In addition, experimental results demonstrate that our multi-task NAT is complementary to the standard knowledge transfer method, knowledge distillation. Code is publicly available at [this https URL](https://github.com/yongchanghao/multi-task-nat)

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12868](https://arxiv.org/abs/2010.12868) [cs.CL] |
|           | (or [arXiv:2010.12868v1](https://arxiv.org/abs/2010.12868v1) [cs.CL] for this version) |



<h2 id="2020-10-27-9">9. Orthros: Non-autoregressive End-to-end Speech Translation with Dual-decoder</h2>

Title: [Orthros: Non-autoregressive End-to-end Speech Translation with Dual-decoder](https://arxiv.org/abs/2010.13047)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Yosuke Higuchi](https://arxiv.org/search/cs?searchtype=author&query=Higuchi%2C+Y), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> Fast inference speed is an important goal towards real-world deployment of speech translation (ST) systems. End-to-end (E2E) models based on the encoder-decoder architecture are more suitable for this goal than traditional cascaded systems, but their effectiveness regarding decoding speed has not been explored so far. Inspired by recent progress in non-autoregressive (NAR) methods in text-based translation, which generates target tokens in parallel by eliminating conditional dependencies, we study the problem of NAR decoding for E2E-ST. We propose a novel NAR E2E-ST framework, Orthoros, in which both NAR and autoregressive (AR) decoders are jointly trained on the shared speech encoder. The latter is used for selecting better translation among various length candidates generated from the former, which dramatically improves the effectiveness of a large length beam with negligible overhead. We further investigate effective length prediction methods from speech inputs and the impact of vocabulary sizes. Experiments on four benchmarks show the effectiveness of the proposed method in improving inference speed while maintaining competitive translation quality compared to state-of-the-art AR E2E-ST systems.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.13047](https://arxiv.org/abs/2010.13047) [cs.CL] |
|           | (or [arXiv:2010.13047v1](https://arxiv.org/abs/2010.13047v1) [cs.CL] for this version) |



<h2 id="2020-10-27-10">10. Autoencoding Improves Pre-trained Word Embeddings</h2>

Title: [Autoencoding Improves Pre-trained Word Embeddings](https://arxiv.org/abs/2010.13094)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Danushka Bollegala](https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D)

> Prior work investigating the geometry of pre-trained word embeddings have shown that word embeddings to be distributed in a narrow cone and by centering and projecting using principal component vectors one can increase the accuracy of a given set of pre-trained word embeddings. However, theoretically, this post-processing step is equivalent to applying a linear autoencoder to minimise the squared l2 reconstruction error. This result contradicts prior work (Mu and Viswanath, 2018) that proposed to remove the top principal components from pre-trained embeddings. We experimentally verify our theoretical claims and show that retaining the top principal components is indeed useful for improving pre-trained word embeddings, without requiring access to additional linguistic resources or labelled data.

| Comments: | COLING 2020                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.13094](https://arxiv.org/abs/2010.13094) [cs.CL] |
|           | (or [arXiv:2010.13094v1](https://arxiv.org/abs/2010.13094v1) [cs.CL] for this version) |



<h2 id="2020-10-27-11">11. Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding</h2>

Title: [Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding](https://arxiv.org/abs/2010.13105)

Authors: [Seongbin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Gyuwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G), [Seongjin Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+S), [Sangmin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> End-to-end approaches open a new way for more accurate and efficient spoken language understanding (SLU) systems by alleviating the drawbacks of traditional pipeline systems. Previous works exploit textual information for an SLU model via pre-training with automatic speech recognition or fine-tuning with knowledge distillation. To utilize textual information more effectively, this work proposes a two-stage textual knowledge distillation method that matches utterance-level representations and predicted logits of two modalities during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a speech encoder because it captures general and rich features. Furthermore, we improve the performance, especially in a low-resource scenario, with data augmentation methods by randomly masking spans of discrete audio tokens and contextualized hidden representations. Consequently, we push the state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy in the full dataset setting and 99.5% in the 10% subset setting. Throughout the ablation studies, we empirically verify that all used methods are crucial to the final performance, providing the best practice for spoken language understanding. Code to reproduce our results will be available upon publication.

| Comments: | Preprint; 5 pages, 1 figure                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2010.13105](https://arxiv.org/abs/2010.13105) [cs.CL] |
|           | (or [arXiv:2010.13105v1](https://arxiv.org/abs/2010.13105v1) [cs.CL] for this version) |



<h2 id="2020-10-27-12">12. The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task</h2>

Title: [The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task](https://arxiv.org/abs/2010.13192)

Authors: [Alexandra Chronopoulou](https://arxiv.org/search/cs?searchtype=author&query=Chronopoulou%2C+A), [Dario Stojanovski](https://arxiv.org/search/cs?searchtype=author&query=Stojanovski%2C+D), [Viktor Hangya](https://arxiv.org/search/cs?searchtype=author&query=Hangya%2C+V), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German<->Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian->German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.

| Comments: | WMT Unsupervised Shared Task 2020                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.13192](https://arxiv.org/abs/2010.13192) [cs.CL] |
|           | (or [arXiv:2010.13192v1](https://arxiv.org/abs/2010.13192v1) [cs.CL] for this version) |



<h2 id="2020-10-27-13">13. Constraint Translation Candidates: A Bridge between Neural Query Translation and Cross-lingual Information Retrieval</h2>

Title: [Constraint Translation Candidates: A Bridge between Neural Query Translation and Cross-lingual Information Retrieval](https://arxiv.org/abs/2010.13658)

Authors: [Tianchi Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+T), [Liang Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B)

> Query translation (QT) is a key component in cross-lingual information retrieval system (CLIR). With the help of deep learning, neural machine translation (NMT) has shown promising results on various tasks. However, NMT is generally trained with large-scale out-of-domain data rather than in-domain query translation pairs. Besides, the translation model lacks a mechanism at the inference time to guarantee the generated words to match the search index. The two shortages of QT result in readable texts for human but inadequate candidates for the downstream retrieval task. In this paper, we propose a novel approach to alleviate these problems by limiting the open target vocabulary search space of QT to a set of important words mined from search index database. The constraint translation candidates are employed at both of training and inference time, thus guiding the translation model to learn and generate well performing target queries. The proposed methods are exploited and examined in a real-word CLIR system--Aliexpress e-Commerce search engine. Experimental results demonstrate that our approach yields better performance on both translation quality and retrieval accuracy than the strong NMT baseline.

| Comments: | SIGIR eCom 2020                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | [arXiv:2010.13658](https://arxiv.org/abs/2010.13658) [cs.CL] |
|           | (or [arXiv:2010.13658v1](https://arxiv.org/abs/2010.13658v1) [cs.CL] for this version) |



<h2 id="2020-10-27-14">14. Exploiting Neural Query Translation into Cross Lingual Information Retrieval</h2>

Title: [Exploiting Neural Query Translation into Cross Lingual Information Retrieval](https://arxiv.org/abs/2010.13659)

Authors: [Liang Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B)

> As a crucial role in cross-language information retrieval (CLIR), query translation has three main challenges: 1) the adequacy of translation; 2) the lack of in-domain parallel training data; and 3) the requisite of low latency. To this end, existing CLIR systems mainly exploit statistical-based machine translation (SMT) rather than the advanced neural machine translation (NMT), limiting the further improvements on both translation and retrieval quality. In this paper, we investigate how to exploit neural query translation model into CLIR system. Specifically, we propose a novel data augmentation method that extracts query translation pairs according to user clickthrough data, thus to alleviate the problem of domain-adaptation in NMT. Then, we introduce an asynchronous strategy which is able to leverage the advantages of the real-time in SMT and the veracity in NMT. Experimental results reveal that the proposed approach yields better retrieval quality than strong baselines and can be well applied into a real-world CLIR system, i.e. Aliexpress e-Commerce search engine. Readers can examine and test their cases on our website: [this https URL](https://aliexpress.com/) .

| Comments: | SIGIR eCom 2020                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | [arXiv:2010.13659](https://arxiv.org/abs/2010.13659) [cs.CL] |
|           | (or [arXiv:2010.13659v1](https://arxiv.org/abs/2010.13659v1) [cs.CL] for this version) |





# 2020-10-26

[Return to Index](#Index)



<h2 id="2020-10-26-1">1. Multilingual BERT Post-Pretraining Alignment</h2>

Title: [Multilingual BERT Post-Pretraining Alignment](https://arxiv.org/abs/2010.12547)

Authors: [Lin Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+L), [Chung-Wei Hang](https://arxiv.org/search/cs?searchtype=author&query=Hang%2C+C), [Haode Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+H), [Abhishek Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+A), [Mo Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+M), [Saloni Potdar](https://arxiv.org/search/cs?searchtype=author&query=Potdar%2C+S)

> We propose a simple method to align multilingual contextual embeddings as a post-pretraining step for improved zero-shot cross-lingual transferability of the pretrained models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% less model parameters. On MLQA, our model outperforms XLM-R_Base that has 57% more parameters than ours.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12547](https://arxiv.org/abs/2010.12547) [cs.CL] |
|           | (or [arXiv:2010.12547v1](https://arxiv.org/abs/2010.12547v1) [cs.CL] for this version) |





<h2 id="2020-10-26-2">2. On the Transformer Growth for Progressive BERT Training</h2>

Title: [On the Transformer Growth for Progressive BERT Training](https://arxiv.org/abs/2010.12562)

Authors: [Xiaotao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+X), [Liyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Hongkun Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Jing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Chen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively--start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and large models respectively while achieving comparable performances. Code will be released for reproduction and future studies.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12562](https://arxiv.org/abs/2010.12562) [cs.CL] |
|           | (or [arXiv:2010.12562v1](https://arxiv.org/abs/2010.12562v1) [cs.CL] for this version) |





<h2 id="2020-10-26-3">3. DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries</h2>

Title: [DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries](https://arxiv.org/abs/2010.12566)

Authors: [Aditi Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+A), [Karthik Raman](https://arxiv.org/search/cs?searchtype=author&query=Raman%2C+K), [Krishna Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+K), [Jiecao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Pre-trained multilingual language models such as mBERT have shown immense gains for several natural language processing (NLP) tasks, especially in the zero-shot cross-lingual setting. Most, if not all, of these pre-trained models rely on the masked-language modeling (MLM) objective as the key language learning objective. The principle behind these approaches is that predicting the masked words with the help of the surrounding text helps learn potent contextualized representations. Despite the strong representation learning capability enabled by MLM, we demonstrate an inherent limitation of MLM for multilingual representation learning. In particular, by requiring the model to predict the language-specific token, the MLM objective disincentivizes learning a language-agnostic representation -- which is a key goal of multilingual pre-training. Therefore to encourage better cross-lingual representation learning we propose the DICT-MLM method. DICT-MLM works by incentivizing the model to be able to predict not just the original masked word, but potentially any of its cross-lingual synonyms as well. Our empirical analysis on multiple downstream tasks spanning 30+ languages, demonstrates the efficacy of the proposed approach and its ability to learn better multilingual representations.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.12566](https://arxiv.org/abs/2010.12566) [cs.CL] |
|           | (or [arXiv:2010.12566v1](https://arxiv.org/abs/2010.12566v1) [cs.CL] for this version) |





<h2 id="2020-10-26-4">4. Language-Conditioned Imitation Learning for Robot Manipulation Tasks</h2>

Title: [Language-Conditioned Imitation Learning for Robot Manipulation Tasks](https://arxiv.org/abs/2010.12083)

Authors: [Simon Stepputtis](https://arxiv.org/search/cs?searchtype=author&query=Stepputtis%2C+S), [Joseph Campbell](https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+J), [Mariano Phielipp](https://arxiv.org/search/cs?searchtype=author&query=Phielipp%2C+M), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Chitta Baral](https://arxiv.org/search/cs?searchtype=author&query=Baral%2C+C), [Heni Ben Amor](https://arxiv.org/search/cs?searchtype=author&query=Amor%2C+H+B)

> Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., "go to the large green bowl"). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods.

| Comments: | Accepted to the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada as spotlight presentation |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Robotics (cs.RO)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2010.12083](https://arxiv.org/abs/2010.12083) [cs.RO] |
|           | (or [arXiv:2010.12083v1](https://arxiv.org/abs/2010.12083v1) [cs.RO] for this version) |





<h2 id="2020-10-26-5">5. How Phonotactics Affect Multilingual and Zero-shot ASR Performance</h2>

Title: [How Phonotactics Affect Multilingual and Zero-shot ASR Performance](https://arxiv.org/abs/2010.12104)

Authors: [Siyuan Feng](https://arxiv.org/search/eess?searchtype=author&query=Feng%2C+S), [Piotr Żelasko](https://arxiv.org/search/eess?searchtype=author&query=Żelasko%2C+P), [Laureano Moro-Velázquez](https://arxiv.org/search/eess?searchtype=author&query=Moro-Velázquez%2C+L), [Ali Abavisani](https://arxiv.org/search/eess?searchtype=author&query=Abavisani%2C+A), [Mark Hasegawa-Johnson](https://arxiv.org/search/eess?searchtype=author&query=Hasegawa-Johnson%2C+M), [Odette Scharenborg](https://arxiv.org/search/eess?searchtype=author&query=Scharenborg%2C+O), [Najim Dehak](https://arxiv.org/search/eess?searchtype=author&query=Dehak%2C+N)

> The idea of combining multiple languages' recordings to train a single automatic speech recognition (ASR) model brings the promise of the emergence of universal speech representation. Recently, a Transformer encoder-decoder model has been shown to leverage multilingual data well in IPA transcriptions of languages presented during training. However, the representations it learned were not successful in zero-shot transfer to unseen languages. Because that model lacks an explicit factorization of the acoustic model (AM) and language model (LM), it is unclear to what degree the performance suffered from differences in pronunciation or the mismatch in phonotactics. To gain more insight into the factors limiting zero-shot ASR transfer, we replace the encoder-decoder with a hybrid ASR system consisting of a separate AM and LM. Then, we perform an extensive evaluation of monolingual, multilingual, and crosslingual (zero-shot) acoustic and language models on a set of 13 phonetically diverse languages. We show that the gain from modeling crosslingual phonotactics is limited, and imposing a too strong model can hurt the zero-shot transfer. Furthermore, we find that a multilingual LM hurts a multilingual ASR system's performance, and retaining only the target language's phonotactic data in LM training is preferable.

| Comments: | Submitted to ICASSP 2021. The first 2 authors contributed equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | [arXiv:2010.12104](https://arxiv.org/abs/2010.12104) [eess.AS] |
|           | (or [arXiv:2010.12104v1](https://arxiv.org/abs/2010.12104v1) [eess.AS] for this version) |





<h2 id="2020-10-26-6">6. A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios</h2>

Title: [A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios](https://arxiv.org/abs/2010.12309)

Authors: [Michael A. Hedderich](https://arxiv.org/search/cs?searchtype=author&query=Hedderich%2C+M+A), [Lukas Lange](https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+L), [Heike Adel](https://arxiv.org/search/cs?searchtype=author&query=Adel%2C+H), [Jannik Strötgen](https://arxiv.org/search/cs?searchtype=author&query=Strötgen%2C+J), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Current developments in natural language processing offer challenges and opportunities for low-resource languages and domains. Deep neural networks are known for requiring large amounts of training data which might not be available in resource-lean scenarios. However, there is also a growing body of works to improve the performance in low-resource settings. Motivated by fundamental changes towards neural models and the currently popular pre-train and fine-tune paradigm, we give an overview of promising approaches for low-resource natural language processing. After a discussion about the definition of low-resource scenarios and the different dimensions of data availability, we then examine methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. The survey closes with a brief look into methods suggested in non-NLP machine learning communities, which might be beneficial for NLP in low-resource scenarios

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.12309](https://arxiv.org/abs/2010.12309) [cs.CL] |
|           | (or [arXiv:2010.12309v1](https://arxiv.org/abs/2010.12309v1) [cs.CL] for this version) |







# 2020-10-23

[Return to Index](#Index)



<h2 id="2020-10-23-1">1. Similarity Analysis of Self-Supervised Speech Representations</h2>

Title: [Similarity Analysis of Self-Supervised Speech Representations](https://arxiv.org/abs/2010.11481)

Authors: [Yu-An Chung](https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+Y), [Yonatan Belinkov](https://arxiv.org/search/eess?searchtype=author&query=Belinkov%2C+Y), [James Glass](https://arxiv.org/search/eess?searchtype=author&query=Glass%2C+J)

> Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models' pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks (RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11481](https://arxiv.org/abs/2010.11481) [eess.AS] |
|           | (or [arXiv:2010.11481v1](https://arxiv.org/abs/2010.11481v1) [eess.AS] for this version) |





<h2 id="2020-10-23-2">2. Autoregressive Modeling is Misspecified for Some Sequence Distributions</h2>

Title: [Autoregressive Modeling is Misspecified for Some Sequence Distributions](https://arxiv.org/abs/2010.11939)

Authors: [Chu-Cheng Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C), [Aaron Jaech](https://arxiv.org/search/cs?searchtype=author&query=Jaech%2C+A), [Xin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Matt Gormley](https://arxiv.org/search/cs?searchtype=author&query=Gormley%2C+M), [Jason Eisner](https://arxiv.org/search/cs?searchtype=author&query=Eisner%2C+J)

> Should sequences be modeled autoregressively---one symbol at a time? How much computation is needed to predict the next symbol? While local normalization is cheap, this also limits its power. We point out that some probability distributions over discrete sequences cannot be well-approximated by any autoregressive model whose runtime and parameter size grow polynomially in the sequence length---even though their unnormalized sequence probabilities are efficient to compute exactly. Intuitively, the probability of the next symbol can be expensive to compute or approximate (even via randomized algorithms) when it marginalizes over exponentially many possible futures, which is in general NP-hard. Our result is conditional on the widely believed hypothesis that NP⊈P/poly (without which the polynomial hierarchy would collapse at the second level). This theoretical observation serves as a caution to the viewpoint that pumping up parameter size is a straightforward way to improve autoregressive models (e.g., in language modeling). It also suggests that globally normalized (energy-based) models may sometimes outperform locally normalized (autoregressive) models, as we demonstrate experimentally for language modeling.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11939](https://arxiv.org/abs/2010.11939) [cs.LG] |
|           | (or [arXiv:2010.11939v1](https://arxiv.org/abs/2010.11939v1) [cs.LG] for this version) |





<h2 id="2020-10-23-3">3. Improving Simultaneous Translation with Pseudo References
</h2>

Title: [Improving Simultaneous Translation with Pseudo References](https://arxiv.org/abs/2010.11247)

Authors: [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Atsuhito Kita](https://arxiv.org/search/cs?searchtype=author&query=Kita%2C+A), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large scale and publicly available simultaneous translation datasets, most simultaneous translation systems still train with ordinary full-sentence parallel corpora which are not suitable for the simultaneous scenario due to the existence of unnecessary long-distance reorderings. Instead of expensive, time-consuming annotation, we propose a novel method that rewrites the target side of existing full-sentence corpus into simultaneous-style translation. Experiments on Chinese-to-English translation demonstrate about +2.7 BLEU improvements with the addition of newly generated pseudo references.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.11247](https://arxiv.org/abs/2010.11247) [cs.CL] |
|           | (or [arXiv:2010.11247v1](https://arxiv.org/abs/2010.11247v1) [cs.CL] for this version) |





<h2 id="2020-10-23-4">4. A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks</h2>

Title: [A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks](https://arxiv.org/abs/2010.11338)

Authors: [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Dmitriy Genzel](https://arxiv.org/search/cs?searchtype=author&query=Genzel%2C+D)

> Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence. Its success heavily relies on the availability of large amounts of training data. This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition (ASR) and speech translation (ST). In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively. We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks. Our experiments show that the proposed method achieves a relative 10~15% word error rate reduction on the English Librispeech task, and improves the speech translation quality on the MuST-C tasks by 4.2~11.1 BLEU.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11338](https://arxiv.org/abs/2010.11338) [cs.CL] |
|           | (or [arXiv:2010.11338v1](https://arxiv.org/abs/2010.11338v1) [cs.CL] for this version) |





<h2 id="2020-10-23-5">5. MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation</h2>

Title: [MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation](https://arxiv.org/abs/2010.11445)

Authors: [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> End-to-end Speech-to-text Translation (E2E- ST), which directly translates source language speech to target language text, is widely useful in practice, but traditional cascaded approaches (ASR+MT) often suffer from error propagation in the pipeline. On the other hand, existing end-to-end solutions heavily depend on the source language transcriptions for pre-training or multi-task training with Automatic Speech Recognition (ASR). We instead propose a simple technique to learn a robust speech encoder in a self-supervised fashion only on the speech side, which can utilize speech data without transcription. This technique, termed Masked Acoustic Modeling (MAM), can also perform pre-training, for the first time, on any acoustic signals (including non-speech ones) without annotation. Compared with current state-of-the-art models on ST, our technique achieves +1.4 BLEU improvement without using transcriptions, and +1.2 BLEU using transcriptions. The pre-training of MAM with arbitrary acoustic signals also boosts the downstream speech-related tasks.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.11445](https://arxiv.org/abs/2010.11445) [cs.CL] |
|           | (or [arXiv:2010.11445v1](https://arxiv.org/abs/2010.11445v1) [cs.CL] for this version) |





<h2 id="2020-10-23-6">6. A Technical Report: BUT Speech Translation Systems</h2>

Title: [A Technical Report: BUT Speech Translation Systems](https://arxiv.org/abs/2010.11593)

Authors: [Hari Krishna Vydana](https://arxiv.org/search/cs?searchtype=author&query=Vydana%2C+H+K), [Lukas Burget](https://arxiv.org/search/cs?searchtype=author&query=Burget%2C+L), [Jan Cernocky](https://arxiv.org/search/cs?searchtype=author&query=Cernocky%2C+J)

> The paper describes the BUT's speech translation systems. The systems are English⟶German offline speech translation systems. The systems are based on our previous works \cite{Jointly_trained_transformers}. Though End-to-End and cascade~(ASR-MT) spoken language translation~(SLT) systems are reaching comparable performances, a large degradation is observed when translating ASR hypothesis compared to the oracle input text. To reduce this performance degradation, we have jointly-trained ASR and MT modules with ASR objective as an auxiliary loss. Both the networks are connected through the neural hidden representations. This model has an End-to-End differentiable path with respect to the final objective function and also utilizes the ASR objective for better optimization. During the inference both the modules(i.e., ASR and MT) are connected through the hidden representations corresponding to the n-best hypotheses. Ensembling with independently trained ASR and MT models have further improved the performance of the system.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11593](https://arxiv.org/abs/2010.11593) [cs.CL] |
|           | (or [arXiv:2010.11593v1](https://arxiv.org/abs/2010.11593v1) [cs.CL] for this version) |





<h2 id="2020-10-23-7">7. CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20</h2>

Title: [CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20](https://arxiv.org/abs/2010.11747)

Authors: [Ivana Kvapilíková](https://arxiv.org/search/cs?searchtype=author&query=Kvapilíková%2C+I), [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.

| Comments: | WMT20                                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2010.11747](https://arxiv.org/abs/2010.11747) [cs.CL] |
|           | (or [arXiv:2010.11747v1](https://arxiv.org/abs/2010.11747v1) [cs.CL] for this version) |







<h2 id="2020-10-23-8">8. Not all parameters are born equal: Attention is mostly what you need</h2>

Title: [Not all parameters are born equal: Attention is mostly what you need](https://arxiv.org/abs/2010.11859)

Authors: [Nikolay Bogoychev](https://arxiv.org/search/cs?searchtype=author&query=Bogoychev%2C+N)

> Transformers are widely used in state-of-the-art machine translation, but the key to their success is still unknown. To gain insight into this, we consider three groups of parameters: embeddings, attention, and feed forward neural network (FFN) layers. We examine the relative importance of each by performing an ablation study where we initialise them at random and freeze them, so that their weights do not change over the course of the training. Through this, we show that the attention and FFN are equally important and fulfil the same functionality in a model. We show that the decision about whether a component is frozen or allowed to train is at least as important for the final model performance as its number of parameters. At the same time, the number of parameters alone is not indicative of a component's importance. Finally, while the embedding layer is the least essential for machine translation tasks, it is the most important component for language modelling tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11859](https://arxiv.org/abs/2010.11859) [cs.CL] |
|           | (or [arXiv:2010.11859v1](https://arxiv.org/abs/2010.11859v1) [cs.CL] for this version) |





<h2 id="2020-10-23-9">9. mT5: A massively multilingual pre-trained text-to-text transformer</h2>

Title: [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)

Authors: [Linting Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+L), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Adam Roberts](https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+A), [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Rami Al-Rfou](https://arxiv.org/search/cs?searchtype=author&query=Al-Rfou%2C+R), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Aditya Barua](https://arxiv.org/search/cs?searchtype=author&query=Barua%2C+A), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C)

> The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11934](https://arxiv.org/abs/2010.11934) [cs.CL] |
|           | (or [arXiv:2010.11934v1](https://arxiv.org/abs/2010.11934v1) [cs.CL] for this version) |





<h2 id="2020-10-23-10">10. UniCase -- Rethinking Casing in Language Models</h2>

Title: [UniCase -- Rethinking Casing in Language Models](https://arxiv.org/abs/2010.11936)

Authors: [Rafal Powalski](https://arxiv.org/search/cs?searchtype=author&query=Powalski%2C+R), [Tomasz Stanislawek](https://arxiv.org/search/cs?searchtype=author&query=Stanislawek%2C+T)

> In this paper, we introduce a new approach to dealing with the problem of case-sensitiveness in Language Modelling (LM). We propose simple architecture modification to the RoBERTa language model, accompanied by a new tokenization strategy, which we named Unified Case LM (UniCase). We tested our solution on the GLUE benchmark, which led to increased performance by 0.42 points. Moreover, we prove that the UniCase model works much better when we have to deal with text data, where all tokens are uppercased (+5.88 point).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2010.11936](https://arxiv.org/abs/2010.11936) [cs.CL] |
|           | (or [arXiv:2010.11936v1](https://arxiv.org/abs/2010.11936v1) [cs.CL] for this version) |









# 2020-10-22

[Return to Index](#Index)



<h2 id="2020-10-22-1">1. Towards End-to-End In-Image Neural Machine Translation</h2>

Title: [Towards End-to-End In-Image Neural Machine Translation](https://arxiv.org/abs/2010.10648)

Authors: [Elman Mansimov](https://arxiv.org/search/cs?searchtype=author&query=Mansimov%2C+E), [Mitchell Stern](https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M), [Mia Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit%2C+J), [Puneet Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+P)

> In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.

| Comments: | Accepted as an oral presentation at EMNLP, NLP Beyond Text workshop, 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10648](https://arxiv.org/abs/2010.10648) [cs.CL]** |
|           | (or **[arXiv:2010.10648v1](https://arxiv.org/abs/2010.10648v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-2">2. Multi-Unit Transformer for Neural Machine Translation</h2>

Title: [Multi-Unit Transformer for Neural Machine Translation](https://arxiv.org/abs/2010.10743)

Authors: [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.

| Comments: | Accepted as a main conference paper in EMNLP 2020            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.10743](https://arxiv.org/abs/2010.10743) [cs.CL]** |
|           | (or **[arXiv:2010.10743v1](https://arxiv.org/abs/2010.10743v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-3">3. Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation</h2>

Title: [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://arxiv.org/abs/2010.10907)

Authors: [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I)

> In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.10907](https://arxiv.org/abs/2010.10907) [cs.CL]** |
|           | (or **[arXiv:2010.10907v1](https://arxiv.org/abs/2010.10907v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-4">4. What makes multilingual BERT multilingual?</h2>

Title: [What makes multilingual BERT multilingual?](https://arxiv.org/abs/2010.10938)

Authors: [Chi-Liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Tsung-Yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> Recently, multilingual BERT works remarkably well on cross-lingual transfer tasks, superior to static non-contextualized word embeddings. In this work, we provide an in-depth experimental study to supplement the existing literature of cross-lingual ability. We compare the cross-lingual ability of non-contextualized and contextualized representation model with the same data. We found that datasize and context window size are crucial factors to the transferability.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2004.09205](https://arxiv.org/abs/2004.09205) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10938](https://arxiv.org/abs/2010.10938) [cs.CL]** |
|           | (or **[arXiv:2010.10938v1](https://arxiv.org/abs/2010.10938v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-5">5. Token Drop mechanism for Neural Machine Translation</h2>

Title: [Token Drop mechanism for Neural Machine Translation](https://arxiv.org/abs/2010.11018)

Authors: [Huaao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Shigui Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+S), [Xiangyu Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+X), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M)

> Neural machine translation with millions of parameters is vulnerable to unfamiliar inputs. We propose Token Drop to improve generalization and avoid overfitting for the NMT model. Similar to word dropout, whereas we replace dropped token with a special token instead of setting zero to words. We further introduce two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Our method aims to force model generating target translation with less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.11018](https://arxiv.org/abs/2010.11018) [cs.CL]** |
|           | (or **[arXiv:2010.11018v1](https://arxiv.org/abs/2010.11018v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-6">6. NeuSpell: A Neural Spelling Correction Toolkit</h2>

Title: [NeuSpell: A Neural Spelling Correction Toolkit](https://arxiv.org/abs/2010.11085)

Authors: [Sai Muralidhar Jayanthi](https://arxiv.org/search/cs?searchtype=author&query=Jayanthi%2C+S+M), [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use contextual representations. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at [this http URL](http://neuspell.github.io/). Code and pretrained models are available at [this http URL](http://github.com/neuspell/neuspell).

| Comments: | Accepted at EMNLP 2020 (system demonstrations)               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.11085](https://arxiv.org/abs/2010.11085) [cs.CL]** |
|           | (or **[arXiv:2010.11085v1](https://arxiv.org/abs/2010.11085v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-7">7. Beyond English-Centric Multilingual Machine Translation</h2>

Title: [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125)

Authors: [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H), [Zhiyi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Siddharth Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+S), [Mandeep Baines](https://arxiv.org/search/cs?searchtype=author&query=Baines%2C+M), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Tom Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+T), [Vitaliy Liptchinsky](https://arxiv.org/search/cs?searchtype=author&query=Liptchinsky%2C+V), [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A)

> Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric by training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open source a training dataset that covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems of WMT. We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.11125](https://arxiv.org/abs/2010.11125) [cs.CL]** |
|           | (or **[arXiv:2010.11125v1](https://arxiv.org/abs/2010.11125v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-8">8. Sentence Boundary Augmentation For Neural Machine Translation Robustness</h2>

Title: [Sentence Boundary Augmentation For Neural Machine Translation Robustness](https://arxiv.org/abs/2010.11132)

Authors: [Daniel Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Te I](https://arxiv.org/search/cs?searchtype=author&query=I%2C+T), [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Dirk Padfield](https://arxiv.org/search/cs?searchtype=author&query=Padfield%2C+D)

> Neural Machine Translation (NMT) models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are provided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition (ASR), the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness. Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness.

| Comments: | 5 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.11132](https://arxiv.org/abs/2010.11132) [cs.CL]** |
|           | (or **[arXiv:2010.11132v1](https://arxiv.org/abs/2010.11132v1) [cs.CL]** for this version) |





<h2 id="2020-10-22-9">9. Cascaded Models With Cyclic Feedback For Direct Speech Translation</h2>

Title: [Cascaded Models With Cyclic Feedback For Direct Speech Translation](https://arxiv.org/abs/2010.11153)

Authors: [Tsz Kin Lam](https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+T+K), [Shigehiko Schamoni](https://arxiv.org/search/cs?searchtype=author&query=Schamoni%2C+S), [Stefan Riezler](https://arxiv.org/search/cs?searchtype=author&query=Riezler%2C+S)

> Direct speech translation describes a scenario where only speech inputs and corresponding translations are available. Such data are notoriously limited. We present a technique that allows cascades of automatic speech recognition (ASR) and machine translation (MT) to exploit in-domain direct speech translation data in addition to out-of-domain MT and ASR data. After pre-training MT and ASR, we use a feedback cycle where the downstream performance of the MT system is used as a signal to improve the ASR system by self-training, and the MT component is fine-tuned on multiple ASR outputs, making it more tolerant towards spelling variations. A comparison to end-to-end speech translation using components of identical architecture and the same data shows gains of up to 3.8 BLEU points on LibriVoxDeEn and up to 5.1 BLEU points on CoVoST for German-to-English speech translation.

| Comments: | 5 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.11153](https://arxiv.org/abs/2010.11153) [cs.CL]** |
|           | (or **[arXiv:2010.11153v1](https://arxiv.org/abs/2010.11153v1) [cs.CL]** for this version) |







# 2020-10-21

[Return to Index](#Index)



<h2 id="2020-10-21-1">1. Word Shape Matters: Robust Machine Translation with Visual Embedding</h2>

Title: [Word Shape Matters: Robust Machine Translation with Visual Embedding](https://arxiv.org/abs/2010.09997)

Authors: [Haohan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Peiyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+E+P)

> Neural machine translation has achieved remarkable empirical performance over standard benchmark datasets, yet recent evidence suggests that the models can still fail easily dealing with substandard inputs such as misspelled words, To overcome this issue, we introduce a new encoding heuristic of the input symbols for character-level NLP models: it encodes the shape of each character through the images depicting the letters when printed. We name this new strategy visual embedding and it is expected to improve the robustness of NLP models because humans also process the corpus visually through printed letters, instead of machinery one-hot vectors. Empirically, our method improves models' robustness against substandard inputs, even in the test scenario where the models are tested with the noises that are beyond what is available during the training phase.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.09997](https://arxiv.org/abs/2010.09997) [cs.CL]** |
|           | (or **[arXiv:2010.09997v1](https://arxiv.org/abs/2010.09997v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-2">2. Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization</h2>

Title: [Language Representation in Multilingual BERTand its applications to improve Cross-lingual Generalization](https://arxiv.org/abs/2010.10041)

Authors: [Chi-Liang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Tsung-Yuan Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+T), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> A token embedding in multilingual BERT (m-BERT) contains both language and semantic information. We find that representation of a language can be obtained by simply averaging the embeddings of the tokens of the language. With the language representation, we can control the output languages of multilingual BERT by manipulating the token embeddings and achieve unsupervised token translation. We further propose a computationally cheap but effective approach to improve the cross-lingual ability of m-BERT based on the observation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.10041](https://arxiv.org/abs/2010.10041) [cs.CL]** |
|           | (or **[arXiv:2010.10041v1](https://arxiv.org/abs/2010.10041v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-3">3. Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training</h2>

Title: [Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training](https://arxiv.org/abs/2010.10048)

Authors: [Zheng Renjie](https://arxiv.org/search/cs?searchtype=author&query=Renjie%2C+Z), [Ma Mingbo](https://arxiv.org/search/cs?searchtype=author&query=Mingbo%2C+M), [Zheng Baigong](https://arxiv.org/search/cs?searchtype=author&query=Baigong%2C+Z), [Liu Kaibo](https://arxiv.org/search/cs?searchtype=author&query=Kaibo%2C+L), [Yuan Jiahong](https://arxiv.org/search/cs?searchtype=author&query=Jiahong%2C+Y), [Church Kenneth](https://arxiv.org/search/cs?searchtype=author&query=Kenneth%2C+C), [Huang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+H)

> Simultaneous speech-to-speech translation is widely useful but extremely challenging, since it needs to generate target-language speech concurrently with the source-language speech, with only a few seconds delay. In addition, it needs to continuously translate a stream of sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches accumulate latencies progressively when the speaker talks faster, and introduce unnatural pauses when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation (SAT) which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech (as measured by the naturalness metric MOS) with substantially lower latency than the baseline, in both Zh <-> En directions.

| Comments:          | 10 pages, accepted by Findings of EMNLP 2020                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | Findings of EMNLP 2020                                       |
| Cite as:           | **[arXiv:2010.10048](https://arxiv.org/abs/2010.10048) [cs.CL]** |
|                    | (or **[arXiv:2010.10048v1](https://arxiv.org/abs/2010.10048v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-4">4. Complete Multilingual Neural Machine Translation</h2>

Title: [Complete Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.10239)

Authors: [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as the source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111*112=12,432 language pairs that provides competitive translation quality for all language pairs.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10239](https://arxiv.org/abs/2010.10239) [cs.CL]** |
|           | (or **[arXiv:2010.10239v1](https://arxiv.org/abs/2010.10239v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-5">5. Human-Paraphrased References Improve Neural Machine Translation</h2>

Title: [Human-Paraphrased References Improve Neural Machine Translation](https://arxiv.org/abs/2010.10245)

Authors: [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by Freitag et al. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10245](https://arxiv.org/abs/2010.10245) [cs.CL]** |
|           | (or **[arXiv:2010.10245v1](https://arxiv.org/abs/2010.10245v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-6">6. CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</h2>

Title: [CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](https://arxiv.org/abs/2010.10392)

Authors: [Hicham El Boukkouri](https://arxiv.org/search/cs?searchtype=author&query=Boukkouri%2C+H+E), [Olivier Ferret](https://arxiv.org/search/cs?searchtype=author&query=Ferret%2C+O), [Thomas Lavergne](https://arxiv.org/search/cs?searchtype=author&query=Lavergne%2C+T), [Hiroshi Noji](https://arxiv.org/search/cs?searchtype=author&query=Noji%2C+H), [Pierre Zweigenbaum](https://arxiv.org/search/cs?searchtype=author&query=Zweigenbaum%2C+P), [Junichi Tsujii](https://arxiv.org/search/cs?searchtype=author&query=Tsujii%2C+J)

> Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system even if it is not intrinsically linked to the notion of Transformer. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level and open-vocabulary representations.

| Comments: | 13 pages, 8 figures and 3 tables. Accepted at COLING 2020    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.10392](https://arxiv.org/abs/2010.10392) [cs.CL]** |
|           | (or **[arXiv:2010.10392v1](https://arxiv.org/abs/2010.10392v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-7">7. Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages</h2>

Title: [Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages](https://arxiv.org/abs/2010.10472)

Authors: [Yiyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> Spelling normalization for low resource languages is a challenging task because the patterns are hard to predict and large corpora are usually required to collect enough examples. This work shows a comparison of a neural model and character language models with varying amounts on target language data. Our usage scenario is interactive correction with nearly zero amounts of training examples, improving models as more data is collected, for example within a chat app. Such models are designed to be incrementally improved as feedback is given from users. In this work, we design a knowledge-base and prediction model embedded system for spelling correction in low-resource languages. Experimental results on multiple languages show that the model could become effective with a small amount of data. We perform experiments on both natural and synthetic data, as well as on data from two endangered languages (Ainu and Griko). Last, we built a prototype system that was used for a small case study on Hinglish, which further demonstrated the suitability of our approach in real world scenarios.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.10472](https://arxiv.org/abs/2010.10472) [cs.CL]** |
|           | (or **[arXiv:2010.10472v1](https://arxiv.org/abs/2010.10472v1) [cs.CL]** for this version) |





<h2 id="2020-10-21-8">8. Optimal Subarchitecture Extraction For BERT</h2>

Title: [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499)

Authors: [Adrian de Wynter](https://arxiv.org/search/cs?searchtype=author&query=de+Wynter%2C+A), [Daniel J. Perry](https://arxiv.org/search/cs?searchtype=author&query=Perry%2C+D+J)

> We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by applying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as "Bort", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the original BERT-large architecture, and 16% of the net size. Bort is also able to be pretrained in 288 GPU hours, which is 1.2% of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large (Liu et al., 2019), and about 33% of that of the world-record, in GPU hours, required to train BERT-large on the same hardware. It is also 7.9x faster on a CPU, as well as being better performing than other compressed variants of the architecture, and some of the non-compressed variants: it obtains performance improvements of between 0.3% and 31%, absolute, with respect to BERT-large, on multiple public natural language understanding (NLU) benchmarks.

| Comments: | Preprint. Under review                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.10499](https://arxiv.org/abs/2010.10499) [cs.CL]** |
|           | (or **[arXiv:2010.10499v1](https://arxiv.org/abs/2010.10499v1) [cs.CL]** for this version) |







# 2020-10-20

[Return to Index](#Index)



<h2 id="2020-10-20-1">1. Emerging Trends of Multimodal Research in Vision and Language</h2>

Title: [Emerging Trends of Multimodal Research in Vision and Language](https://arxiv.org/abs/2010.09522)

Authors: [Shagun Uppal](https://arxiv.org/search/cs?searchtype=author&query=Uppal%2C+S), [Sarthak Bhagat](https://arxiv.org/search/cs?searchtype=author&query=Bhagat%2C+S), [Devamanyu Hazarika](https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+D), [Navonil Majumdar](https://arxiv.org/search/cs?searchtype=author&query=Majumdar%2C+N), [Soujanya Poria](https://arxiv.org/search/cs?searchtype=author&query=Poria%2C+S), [Roger Zimmermann](https://arxiv.org/search/cs?searchtype=author&query=Zimmermann%2C+R), [Amir Zadeh](https://arxiv.org/search/cs?searchtype=author&query=Zadeh%2C+A)

> Deep Learning and its applications have cascaded impactful research and development with a diverse range of modalities present in the real-world data. More recently, this has enhanced research interests in the intersection of the Vision and Language arena with its numerous applications and fast-paced growth. In this paper, we present a detailed overview of the latest trends in research pertaining to visual and language modalities. We look at its applications in their task formulations and how to solve various problems related to semantic perception and content generation. We also address task-specific trends, along with their evaluation strategies and upcoming challenges. Moreover, we shed some light on multi-disciplinary patterns and insights that have emerged in the recent past, directing this field towards more modular and transparent intelligent systems. This survey identifies key trends gravitating recent literature in VisLang research and attempts to unearth directions that the field is heading towards.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.09522](https://arxiv.org/abs/2010.09522) [cs.CV]** |
|           | (or **[arXiv:2010.09522v1](https://arxiv.org/abs/2010.09522v1) [cs.CV]** for this version) |





<h2 id="2020-10-20-2">2. A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences</h2>

Title: [A Corpus for English-Japanese Multimodal Neural Machine Translation with Comparable Sentences](https://arxiv.org/abs/2010.08725)

Authors: [Andrew Merritt](https://arxiv.org/search/cs?searchtype=author&query=Merritt%2C+A), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Yuki Arase](https://arxiv.org/search/cs?searchtype=author&query=Arase%2C+Y)

> Multimodal neural machine translation (NMT) has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08725](https://arxiv.org/abs/2010.08725) [cs.CL]** |
|           | (or **[arXiv:2010.08725v1](https://arxiv.org/abs/2010.08725v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-3">3. Incorporate Semantic Structures into Machine Translation Evaluation via UCCA</h2>

Title: [Incorporate Semantic Structures into Machine Translation Evaluation via UCCA](https://arxiv.org/abs/2010.08728)

Authors: [Jin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yinuo Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Junfeng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J)

> Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.

| Comments: | WMT2020                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.08728](https://arxiv.org/abs/2010.08728) [cs.CL]** |
|           | (or **[arXiv:2010.08728v1](https://arxiv.org/abs/2010.08728v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-4">4. Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach</h2>

Title: [Capturing Longer Context for Document-level Neural Machine Translation: A Multi-resolutional Approach](https://arxiv.org/abs/2010.08961)

Authors: [Zewei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Discourse context has been proven useful when translating documents. It is quite a challenge to incorporate long document context in the prevailing neural machine translation models such as Transformer. In this paper, we propose multi-resolutional (MR) Doc2Doc, a method to train a neural sequence-to-sequence model for document-level translation. Our trained model can simultaneously translate sentence by sentence as well as a document as a whole. We evaluate our method and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that MR Doc2Doc outperforms sentence-level models and previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08961](https://arxiv.org/abs/2010.08961) [cs.CL]** |
|           | (or **[arXiv:2010.08961v1](https://arxiv.org/abs/2010.08961v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-5">5. Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation</h2>

Title: [Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation](https://arxiv.org/abs/2010.09046)

Authors: [Yunwon Tae](https://arxiv.org/search/cs?searchtype=author&query=Tae%2C+Y), [Cheonbok Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C), [Taehee Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T), [Soyoung Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Mohammad Azam Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+M+A), [Eunjeong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+E), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J)

> Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09046](https://arxiv.org/abs/2010.09046) [cs.CL]** |
|           | (or **[arXiv:2010.09046v1](https://arxiv.org/abs/2010.09046v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-6">6. Revisiting Modularized Multilingual NMT to Meet Industrial Demands</h2>

Title: [Revisiting Modularized Multilingual NMT to Meet Industrial Demands](https://arxiv.org/abs/2010.09402)

Authors: [Sungwon Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+S), [Bokyung Son](https://arxiv.org/search/cs?searchtype=author&query=Son%2C+B), [Kichang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Jaekyoung Bae](https://arxiv.org/search/cs?searchtype=author&query=Bae%2C+J)

> The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.

| Comments: | The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09402](https://arxiv.org/abs/2010.09402) [cs.CL]** |
|           | (or **[arXiv:2010.09402v1](https://arxiv.org/abs/2010.09402v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-7">7. Diving Deep into Context-Aware Neural Machine Translation</h2>

Title: [Diving Deep into Context-Aware Neural Machine Translation](https://arxiv.org/abs/2010.09482)

Authors: [Jingjing Huo](https://arxiv.org/search/cs?searchtype=author&query=Huo%2C+J), [Christian Herold](https://arxiv.org/search/cs?searchtype=author&query=Herold%2C+C), [Yingbo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Leonard Dahlmann](https://arxiv.org/search/cs?searchtype=author&query=Dahlmann%2C+L), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.

| Comments: | Accepted at 5th Conference on Machine Translation (WMT20)    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.09482](https://arxiv.org/abs/2010.09482) [cs.CL]** |
|           | (or **[arXiv:2010.09482v1](https://arxiv.org/abs/2010.09482v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-8">8. Cold-start Active Learning through Self-supervised Language Modeling</h2>

Title: [Cold-start Active Learning through Self-supervised Language Modeling](https://arxiv.org/abs/2010.09535)

Authors: [Michelle Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+M), [Hsuan-Tien Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

> Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.

| Comments: | Published in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.09535](https://arxiv.org/abs/2010.09535) [cs.CL]** |
|           | (or **[arXiv:2010.09535v1](https://arxiv.org/abs/2010.09535v1) [cs.CL]** for this version) |





<h2 id="2020-10-20-9">9. Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines</h2>

Title: [Subtitles to Segmentation: Improving Low-Resource Speech-to-Text Translation Pipelines](https://arxiv.org/abs/2010.09693)

Authors: [David Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+D), [Zhengping Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Chris Kedzie](https://arxiv.org/search/cs?searchtype=author&query=Kedzie%2C+C), [Elsbeth Turcan](https://arxiv.org/search/cs?searchtype=author&query=Turcan%2C+E), [Peter Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P), [Kathleen McKeown](https://arxiv.org/search/cs?searchtype=author&query=McKeown%2C+K)

> In this work, we focus on improving ASR output segmentation in the context of low-resource language speech-to-text translation. ASR output segmentation is crucial, as ASR systems segment the input audio using purely acoustic information and are not guaranteed to output sentence-like segments. Since most MT systems expect sentences as input, feeding in longer unsegmented passages can lead to sub-optimal performance. We explore the feasibility of using datasets of subtitles from TV shows and movies to train better ASR segmentation models. We further incorporate part-of-speech (POS) tag and dependency label information (derived from the unsegmented ASR outputs) into our segmentation model. We show that this noisy syntactic information can improve model accuracy. We evaluate our models intrinsically on segmentation quality and extrinsically on downstream MT performance, as well as downstream tasks including cross-lingual information retrieval (CLIR) tasks and human relevance assessments. Our model shows improved performance on downstream tasks for Lithuanian and Bulgarian.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | CLSST@LREC 2020 68-73                                        |
| Cite as:           | **[arXiv:2010.09693](https://arxiv.org/abs/2010.09693) [cs.CL]** |
|                    | (or **[arXiv:2010.09693v1](https://arxiv.org/abs/2010.09693v1) [cs.CL]** for this version) |







# 2020-10-19

[Return to Index](#Index)



<h2 id="2020-10-19-1">1. DiDi's Machine Translation System for WMT2020</h2>

Title: [DiDi's Machine Translation System for WMT2020](https://arxiv.org/abs/2010.08185)

Authors: [Tanfang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T), [Weiwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Wenyang Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+W), [Xing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X), [Xiangang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Jieping Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K)

> This paper describes DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model, and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese->English.

| Comments: | Accepted at WMT 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.08185](https://arxiv.org/abs/2010.08185) [cs.CL]** |
|           | (or **[arXiv:2010.08185v1](https://arxiv.org/abs/2010.08185v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-2">2. Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation</h2>

Title: [Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation](https://arxiv.org/abs/2010.08265)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method -- LayerDrop.

| Comments: | Accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.08265](https://arxiv.org/abs/2010.08265) [cs.CL]** |
|           | (or **[arXiv:2010.08265v1](https://arxiv.org/abs/2010.08265v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-3">3. It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT</h2>

Title: [It's not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT](https://arxiv.org/abs/2010.08275)

Authors: [Hila Gonen](https://arxiv.org/search/cs?searchtype=author&query=Gonen%2C+H), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations.

| Comments: | BlackboxNLP 2020                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.08275](https://arxiv.org/abs/2010.08275) [cs.CL]** |
|           | (or **[arXiv:2010.08275v1](https://arxiv.org/abs/2010.08275v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-4">4. Multi-Adversarial Learning for Cross-Lingual Word Embeddings</h2>

Title: [Multi-Adversarial Learning for Cross-Lingual Word Embeddings](https://arxiv.org/abs/2010.08432)

Authors: [Haozhou Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J), [Paola Merlo](https://arxiv.org/search/cs?searchtype=author&query=Merlo%2C+P)

> Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings -- maps of matching words across languages -- without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction show that this method improves performance over previous single-mapping methods, especially for distant languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.08432](https://arxiv.org/abs/2010.08432) [cs.CL]** |
|           | (or **[arXiv:2010.08432v1](https://arxiv.org/abs/2010.08432v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-5">5. Adaptive Feature Selection for End-to-End Speech Translation</h2>

Title: [Adaptive Feature Selection for End-to-End Speech Translation](https://arxiv.org/abs/2010.08518)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to SR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84% temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)

| Comments: | EMNLP2020 Findings; source code is at [this https URL](https://github.com/bzhangGo/zero) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.08518](https://arxiv.org/abs/2010.08518) [cs.CL]** |
|           | (or **[arXiv:2010.08518v1](https://arxiv.org/abs/2010.08518v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-6">6. Mischief: A Simple Black-Box Attack Against Transformer Architectures</h2>

Title: [Mischief: A Simple Black-Box Attack Against Transformer Architectures](https://arxiv.org/abs/2010.08542)

Authors: [Adrian de Wynter](https://arxiv.org/search/cs?searchtype=author&query=de+Wynter%2C+A)

> We introduce Mischief, a simple and lightweight method to produce a class of human-readable, realistic adversarial examples for language models. We perform exhaustive experimentations of our algorithm on four transformer-based architectures, across a variety of downstream tasks, as well as under varying concentrations of said examples. Our findings show that the presence of Mischief-generated adversarial samples in the test set significantly degrades (by up to 20%) the performance of these models with respect to their reported baselines. Nonetheless, we also demonstrate that, by including similar examples in the training set, it is possible to restore the baseline scores on the adversarial test set. Moreover, for certain tasks, the models trained with Mischief set show a modest increase on performance with respect to their original, non-adversarial baseline.

| Comments: | Technical report                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.08542](https://arxiv.org/abs/2010.08542) [cs.CL]** |
|           | (or **[arXiv:2010.08542v1](https://arxiv.org/abs/2010.08542v1) [cs.CL]** for this version) |





<h2 id="2020-10-19-7">7. Explicit Alignment Objectives for Multilingual Bidirectional Encoders</h2>

Title: [Explicit Alignment Objectives for Multilingual Bidirectional Encoders](https://arxiv.org/abs/2010.07972)

Authors: [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have proven to be impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLMR-large model which has 4.6x the parameters of AMBER.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.07972](https://arxiv.org/abs/2010.07972) [cs.CL]** |
|           | (or **[arXiv:2010.07972v1](https://arxiv.org/abs/2010.07972v1) [cs.CL]** for this version) |





# 2020-10-16

[Return to Index](#Index)



<h2 id="2020-10-16-1">1. Decoding Methods for Neural Narrative Generation</h2>

Title: [Decoding Methods for Neural Narrative Generation](https://arxiv.org/abs/2010.07375)

Authors:[Alexandra DeLucia](https://arxiv.org/search/cs?searchtype=author&query=DeLucia%2C+A), [Aaron Mueller](https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+A), [Xiang Lisa Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X+L), [João Sedoc](https://arxiv.org/search/cs?searchtype=author&query=Sedoc%2C+J)

> Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters---specifically, maximum mutual information---analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best within 0.7≤p≤0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.

| Comments: | 20 pages. Submitted to INLG 2020                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07375](https://arxiv.org/abs/2010.07375) [cs.CL]** |
|           | (or **[arXiv:2010.07375v1](https://arxiv.org/abs/2010.07375v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-2">2. Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses</h2>

Title: [Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses](https://arxiv.org/abs/2010.07574)

Authors:[Simon Flachs](https://arxiv.org/search/cs?searchtype=author&query=Flachs%2C+S), [Ophélie Lacroix](https://arxiv.org/search/cs?searchtype=author&query=Lacroix%2C+O), [Helen Yannakoudakis](https://arxiv.org/search/cs?searchtype=author&query=Yannakoudakis%2C+H), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A)

> Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.

| Comments: | Accepted at EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07574](https://arxiv.org/abs/2010.07574) [cs.CL]** |
|           | (or **[arXiv:2010.07574v1](https://arxiv.org/abs/2010.07574v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-3">3. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses</h2>

Title: [Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses](https://arxiv.org/abs/2010.07638)

Authors:[Prathyusha Jwalapuram](https://arxiv.org/search/cs?searchtype=author&query=Jwalapuram%2C+P), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Youlin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y)

> Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En. Code available at <[this https URL](https://github.com/ntunlp/pronoun-finetuning)>.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07638](https://arxiv.org/abs/2010.07638) [cs.CL]** |
|           | (or **[arXiv:2010.07638v1](https://arxiv.org/abs/2010.07638v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-4">4. Does Chinese BERT Encode Word Structure?</h2>

Title: [Does Chinese BERT Encode Word Structure?](https://arxiv.org/abs/2010.07711)

Authors:[Yile Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Leyang Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the features captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in BERT. However, little work has investigated word features for character-based languages such as Chinese. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT; (2) word-level features are mostly in the middle representation layers; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.

| Comments: | Accepted by COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07711](https://arxiv.org/abs/2010.07711) [cs.CL]** |
|           | (or **[arXiv:2010.07711v1](https://arxiv.org/abs/2010.07711v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-5">5. Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings</h2>

Title: [Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings](https://arxiv.org/abs/2010.07761)

Authors:[Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Julian Salazar](https://arxiv.org/search/cs?searchtype=author&query=Salazar%2C+J), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We describe an unsupervised method to create pseudo-parallel corpora for machine translation (MT) from unaligned text. We use multilingual BERT to create source and target sentence embeddings for nearest-neighbor search and adapt the model via self-training. We validate our technique by extracting parallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a 24.5 point increase (absolute) in F1 scores over previous unsupervised methods. We then improve an XLM-based unsupervised neural MT system pre-trained on Wikipedia by supplementing it with pseudo-parallel text mined from the same corpus, boosting unsupervised translation performance by up to 3.5 BLEU on the WMT'14 French-English and WMT'16 German-English tasks and outperforming the previous state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese corpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU improvement on the low-resource MT task. We demonstrate that unsupervised bitext mining is an effective way of augmenting MT datasets and complements existing techniques like initializing with pre-trained contextual embeddings.

| Comments: | To appear in the Transactions of the Association for Computational Linguistics |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07761](https://arxiv.org/abs/2010.07761) [cs.CL]** |
|           | (or **[arXiv:2010.07761v1](https://arxiv.org/abs/2010.07761v1) [cs.CL]** for this version) |





<h2 id="2020-10-16-6">6. Tokenization Repair in the Presence of Spelling Errors
</h2>

Title: [Tokenization Repair in the Presence of Spelling Errors](https://arxiv.org/abs/2010.07878)

Authors:[Hannah Bast](https://arxiv.org/search/cs?searchtype=author&query=Bast%2C+H), [Matthias Hertel](https://arxiv.org/search/cs?searchtype=author&query=Hertel%2C+M), [Mostafa M. Mohamed](https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+M+M)

> We consider the following tokenization repair problem: Given a natural language text with any combination of missing or spurious spaces, correct these. Spelling errors can be present, but it's not part of the problem to correct them. For example, given: "Tispa per isabout token izaionrep air", compute "Tis paper is about tokenizaion repair". It is tempting to think of this problem as a special case of spelling correction or to treat the two problems together. We make a case that tokenization repair and spelling correction should and can be treated as separate problems. We investigate a variety of neural models as well as a number of strong baselines. We identify three main ingredients to high-quality tokenization repair: deep language models with a bidirectional component, training the models on text with spelling errors, and making use of the space information already present. Our best methods can repair all tokenization errors on 97.5% of the correctly spelled test sentences and on 96.0% of the misspelled test sentences. With all spaces removed from the given text (the scenario from previous work), the accuracy falls to 94.5% and 90.1%, respectively. We conduct a detailed error analysis.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.07878](https://arxiv.org/abs/2010.07878) [cs.CL]** |
|           | (or **[arXiv:2010.07878v1](https://arxiv.org/abs/2010.07878v1) [cs.CL]** for this version) |









# 2020-10-15

[Return to Index](#Index)



<h2 id="2020-10-15-1">1. The EOS Decision and Length Extrapolation</h2>

Title: [The EOS Decision and Length Extrapolation](https://arxiv.org/abs/2010.07174)

Authors: [Benjamin Newman](https://arxiv.org/search/cs?searchtype=author&query=Newman%2C+B), [John Hewitt](https://arxiv.org/search/cs?searchtype=author&query=Hewitt%2C+J), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.

| Comments: | 16 page, 7 Figures, 9 Tables, Blackbox NLP Workshop at EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07174](https://arxiv.org/abs/2010.07174) [cs.CL]** |
|           | (or **[arXiv:2010.07174v1](https://arxiv.org/abs/2010.07174v1) [cs.CL]** for this version) |





<h2 id="2020-10-15-2">2. Dissecting the components and factors of Neural Text Generation</h2>

Title: [Dissecting the components and factors of Neural Text Generation](https://arxiv.org/abs/2010.07279)

Authors: [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks.

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.07279](https://arxiv.org/abs/2010.07279) [cs.CL]** |
|           | (or **[arXiv:2010.07279v1](https://arxiv.org/abs/2010.07279v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-3">3. Random Network Distillation as a Diversity Metric for Both Image and Text Generation</h2>

Title: [Random Network Distillation as a Diversity Metric for Both Image and Text Generation](https://arxiv.org/abs/2010.06715)

Authors: [Liam Fowl](https://arxiv.org/search/cs?searchtype=author&query=Fowl%2C+L), [Micah Goldblum](https://arxiv.org/search/cs?searchtype=author&query=Goldblum%2C+M), [Arjun Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Amr Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+A), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T)

> Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06715](https://arxiv.org/abs/2010.06715) [cs.LG]** |
|           | (or **[arXiv:2010.06715v1](https://arxiv.org/abs/2010.06715v1) [cs.LG]** for this version) |







<h2 id="2020-10-15-4">4. MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings</h2>

Title: [MulDE: Multi-teacher Knowledge Distillation for Low-dimensional Knowledge Graph Embeddings](https://arxiv.org/abs/2010.07152)

Authors: [Kai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Yu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Qian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Q), [Quan Z. Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+Q+Z)

> Link prediction based on knowledge graph embedding (KGE) aims to predict new triples to complete knowledge graphs (KGs) automatically. However, recent KGE models tend to improve performance by excessively increasing vector dimensions, which would cause enormous training costs and save storage in practical applications. To address this problem, we first theoretically analyze the capacity of low-dimensional space for KG embeddings based on the principle of minimum entropy. Then, we propose a novel knowledge distillation framework for knowledge graph embedding, utilizing multiple low-dimensional KGE models as teachers. Under a novel iterative distillation strategy, the MulDE model produces soft labels according to training epochs and student performance adaptively. The experimental results show that MulDE can effectively improve the performance and training speed of low-dimensional KGE models. The distilled 32-dimensional models are very competitive compared to some of state-or-the-art (SotA) high-dimensional methods on several commonly-used datasets.

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07152](https://arxiv.org/abs/2010.07152) [cs.AI]** |
|           | (or **[arXiv:2010.07152v1](https://arxiv.org/abs/2010.07152v1) [cs.AI]** for this version) |







<h2 id="2020-10-15-5">5. Memformer: The Memory-Augmented Transformer</h2>

Title: [Memformer: The Memory-Augmented Transformer](https://arxiv.org/abs/2010.06891)

Authors: [Qingyang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Zhenzhong Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Z), [Jing Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z)

> Transformer models have obtained remarkable accomplishments in various NLP tasks. However, these models have efficiency issues on long sequences, as the complexity of their self-attention module scales quadratically with the sequence length. To remedy the limitation, we present Memformer, a novel language model that utilizes a single unified memory to encode and retrieve past information. It includes a new optimization scheme, Memory Replay Back-Propagation, which promotes long-range back-propagation through time with a significantly reduced memory requirement. Memformer achieves (n) time complexity and (1) space complexity in processing long sequences, meaning that the model can handle an infinite length sequence during inference. Our model is also compatible with other self-supervised tasks to further improve the performance on language modeling. Experimental results show that Memformer outperforms the previous long-range sequence models on WikiText-103, including Transformer-XL and compressive Transformer.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06891](https://arxiv.org/abs/2010.06891) [cs.CL]** |
|           | (or **[arXiv:2010.06891v1](https://arxiv.org/abs/2010.06891v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-6">6. DA-Transformer: Distance-aware Transformer</h2>

Title: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06925](https://arxiv.org/abs/2010.06925) [cs.CL]** |
|           | (or **[arXiv:2010.06925v1](https://arxiv.org/abs/2010.06925v1) [cs.CL]** for this version) |







<h2 id="2020-10-15-7">7. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search</h2>

Title: [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://arxiv.org/abs/2010.07003)

Authors: [Gyuwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> Although transformers have achieved impressive accuracies in various tasks in natural language processing, they often come with a prohibitive computational cost, that prevents their use in scenarios with limited computational resources for inference. This need for computational efficiency in inference has been addressed by for instance PoWER-BERT (Goyal et al., 2020) which gradually decreases the length of a sequence as it is passed through layers. These approaches however often assume that the target computational complexity is known in advance at the time of training. This implies that a separate model must be trained for each inference scenario with its distinct computational budget. In this paper, we extend PoWER-BERT to address this issue of inefficiency and redundancy. The proposed extension enables us to train a large-scale transformer, called Length-Adaptive Transformer, once and uses it for various inference scenarios without re-training it. To do so, we train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. We then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification such as span-based question-answering, by introducing the idea of Drop-and-Restore. With Drop-and-Restore, word-vectors are dropped temporarily in intermediate layers and restored at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including SQuAD 1.1, MNLI-m, and SST-2. Code is available at [this https URL](https://github.com/clovaai/length-adaptive-transformer).

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.07003](https://arxiv.org/abs/2010.07003) [cs.CL]** |
|           | (or **[arXiv:2010.07003v1](https://arxiv.org/abs/2010.07003v1) [cs.CL]** for this version) |















# 2020-10-14

[Return to Index](#Index)



<h2 id="2020-10-14-1">1. Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation</h2>

Title: [Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation](https://arxiv.org/abs/2010.05997)

Authors: [Xing Jie Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+X+J), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

> Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for "attaching" dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.

| Comments: | Accepted for publication in Proceedings of WMT 2020          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.05997](https://arxiv.org/abs/2010.05997) [cs.CL]** |
|           | (or **[arXiv:2010.05997v1](https://arxiv.org/abs/2010.05997v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-2">2. Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model</h2>

Title: [Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model](https://arxiv.org/abs/2010.06040)

Authors: [Mingzhi Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+M), [Dinghan Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Lin Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+L)

> Masked Language Model (MLM) framework has been widely adopted for self-supervised language pre-training. In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. Thus, we theoretically quantify the gradient variance via correlating the gradient covariance with the Hamming distance between two different masks (given a certain text sequence). To reduce the variance due to the sampling of masks, we propose a fully-explored masking strategy, where a text sequence is divided into a certain number of non-overlapping segments. Thereafter, the tokens within one segment are masked for training. We prove, from a theoretical perspective, that the gradients derived from this new masking schema have a smaller variance and can lead to more efficient self-supervised training. We conduct extensive experiments on both continual pre-training and general pre-training from scratch. Empirical results confirm that this new masking strategy can consistently outperform standard random masking. Detailed efficiency analysis and ablation studies further validate the advantages of our fully-explored masking strategy under the MLM framework.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.06040](https://arxiv.org/abs/2010.06040) [cs.CL]** |
|           | (or **[arXiv:2010.06040v1](https://arxiv.org/abs/2010.06040v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-3">3. Towards Machine Translation for the Kurdish Language</h2>

Title: [Towards Machine Translation for the Kurdish Language](https://arxiv.org/abs/2010.06041)

Authors: [Sina Ahmadi](https://arxiv.org/search/cs?searchtype=author&query=Ahmadi%2C+S), [Mariam Masoud](https://arxiv.org/search/cs?searchtype=author&query=Masoud%2C+M)

> Machine translation is the task of translating texts from one language to another using computers. It has been one of the major tasks in natural language processing and computational linguistics and has been motivating to facilitate human communication. Kurdish, an Indo-European language, has received little attention in this realm due to the language being less-resourced. Therefore, in this paper, we are addressing the main issues in creating a machine translation system for the Kurdish language, with a focus on the Sorani dialect. We describe the available scarce parallel data suitable for training a neural machine translation model for Sorani Kurdish-English translation. We also discuss some of the major challenges in Kurdish language translation and demonstrate how fundamental text processing tasks, such as tokenization, can improve translation performance.

| Comments: | 12 pages - under review in the ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06041](https://arxiv.org/abs/2010.06041) [cs.CL]** |
|           | (or **[arXiv:2010.06041v1](https://arxiv.org/abs/2010.06041v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-4">4. Incorporating BERT into Parallel Sequence Decoding with Adapters</h2>

Title: [Incorporating BERT into Parallel Sequence Decoding with Adapters](https://arxiv.org/abs/2010.06138)

Authors: [Junliang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Linli Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Enhong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+E)

> While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49/33.57 BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60/43.56 BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models.

| Comments: | NeurIPS 2020                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06138](https://arxiv.org/abs/2010.06138) [cs.CL]** |
|           | (or **[arXiv:2010.06138v1](https://arxiv.org/abs/2010.06138v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-5">5. Mitigating Gender Bias in Machine Translation with Target Gender Annotations</h2>

Title: [Mitigating Gender Bias in Machine Translation with Target Gender Annotations](https://arxiv.org/abs/2010.06203)

Authors: [Toms Bergmanis](https://arxiv.org/search/cs?searchtype=author&query=Bergmanis%2C+T), [Artūrs Stafanovičs](https://arxiv.org/search/cs?searchtype=author&query=Stafanovičs%2C+A), [Mārcis Pinnis](https://arxiv.org/search/cs?searchtype=author&query=Pinnis%2C+M)

> When translating "The secretary asked for details." to a language with grammatical gender, it might be necessary to determine the gender of the subject "secretary". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.

| Comments: | EMNLP 2020 Fifth Conference on Machine Translation (WMT20)   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06203](https://arxiv.org/abs/2010.06203) [cs.CL]** |
|           | (or **[arXiv:2010.06203v1](https://arxiv.org/abs/2010.06203v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-6">6. CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations</h2>

Title: [CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations](https://arxiv.org/abs/2010.06351)

Authors: [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Pengcheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+P), [Shicheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

> Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6% absolute gain on GLUE benchmarks and 0.8% absolute increment on NLVR.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06351](https://arxiv.org/abs/2010.06351) [cs.CL]** |
|           | (or **[arXiv:2010.06351v1](https://arxiv.org/abs/2010.06351v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-7">7. The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT</h2>

Title: [The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT](https://arxiv.org/abs/2010.06354)

Authors: [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

> This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.

| Comments: | to be appear at the 5th Conference on Machine Translation (WMT20) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06354](https://arxiv.org/abs/2010.06354) [cs.CL]** |
|           | (or **[arXiv:2010.06354v1](https://arxiv.org/abs/2010.06354v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-8">8. Fine-grained linguistic evaluation for state-of-the-art Machine Translation</h2>

Title: [Fine-grained linguistic evaluation for state-of-the-art Machine Translation](https://arxiv.org/abs/2010.06359)

Authors: [Eleftherios Avramidis](https://arxiv.org/search/cs?searchtype=author&query=Avramidis%2C+E), [Vivien Macketanz](https://arxiv.org/search/cs?searchtype=author&query=Macketanz%2C+V), [Ursula Strohriegel](https://arxiv.org/search/cs?searchtype=author&query=Strohriegel%2C+U), [Aljoscha Burchardt](https://arxiv.org/search/cs?searchtype=author&query=Burchardt%2C+A), [Sebastian Möller](https://arxiv.org/search/cs?searchtype=author&query=Möller%2C+S)

> This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.

| Comments: | 11 pages, 1 figure, Fifth Conference of Machine Translation, WMT20 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.06359](https://arxiv.org/abs/2010.06359) [cs.CL]** |
|           | (or **[arXiv:2010.06359v1](https://arxiv.org/abs/2010.06359v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-9">9. Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language</h2>

Title: [Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language](https://arxiv.org/abs/2010.06447)

Authors: [Dan John Velasco](https://arxiv.org/search/cs?searchtype=author&query=Velasco%2C+D+J)

> Low-resource languages such as Filipino suffer from data scarcity which makes it challenging to develop NLP applications for Filipino language. The use of Transfer Learning (TL) techniques alleviates this problem in low-resource setting. In recent years, transformer-based models are proven to be effective in low-resource tasks but faces challenges in accessibility due to its high compute and memory requirements. There's a need for a cheaper but effective alternative. This paper has three contributions. First, release a pre-trained AWD LSTM language model for Filipino language. Second, benchmark AWD LSTM in the Hate Speech classification task and show that it performs on par with transformer-based models. Third, analyze the degradation rate of AWD-LSTM to smaller data using degradation test and compare it with transformer-based models.
> \-----
> Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer Learning (TL) techniques ay malaking tulong para sa mga pagkakataong gipit tayo sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay magastos sa resources. Kaya nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una, maglabas ng pre-trained AWD LSTM language model sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag benchmark ng AWD LSTM sa Hate Speech classification task at ipakita na kayang nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang degradation rate ng AWD-LSTM sa mas maliit na data gamit ang degradation test at ikumpara ito sa mga transformer-based models.

| Comments:    | 5 pages, 3 tables, 1 figure. in Filipino language            |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.06447](https://arxiv.org/abs/2010.06447) [cs.CL]** |
|              | (or **[arXiv:2010.06447v1](https://arxiv.org/abs/2010.06447v1) [cs.CL]** for this version) |





<h2 id="2020-10-14-10">10. Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!</h2>

Title: [Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!](https://arxiv.org/abs/2010.06572)

Authors: [Jack Hessel](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+J), [Lillian Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+L)

> Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.

| Subjects:          | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Published in EMNLP 2020                                      |
| Cite as:           | **[arXiv:2010.06572](https://arxiv.org/abs/2010.06572) [cs.CL]** |
|                    | (or **[arXiv:2010.06572v1](https://arxiv.org/abs/2010.06572v1) [cs.CL]** for this version) |









# 2020-10-13

[Return to Index](#Index)



<h2 id="2020-10-13-1">1. Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation</h2>

Title: [Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation](https://arxiv.org/abs/2010.05445)

Authors: [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Wray Buntine](https://arxiv.org/search/cs?searchtype=author&query=Buntine%2C+W), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

> Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation (NMT) models in bilingually low-resource scenarios. A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest. However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal. In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model. As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvement compared to strong baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05445](https://arxiv.org/abs/2010.05445) [cs.CL]** |
|           | (or **[arXiv:2010.05445v1](https://arxiv.org/abs/2010.05445v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-2">2. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</h2>

Title: [The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?](https://arxiv.org/abs/2010.05607)

Authors: [Jasmijn Bastings](https://arxiv.org/search/cs?searchtype=author&query=Bastings%2C+J), [Katja Filippova](https://arxiv.org/search/cs?searchtype=author&query=Filippova%2C+K)

> There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.

| Comments:          | Accepted at BlackboxNLP 2020                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP |
| Cite as:           | **[arXiv:2010.05607](https://arxiv.org/abs/2010.05607) [cs.CL]** |
|                    | (or **[arXiv:2010.05607v1](https://arxiv.org/abs/2010.05607v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-3">3. Load What You Need: Smaller Versions of Multilingual BERT</h2>

Title: [Load What You Need: Smaller Versions of Multilingual BERT](https://arxiv.org/abs/2010.05609)

Authors: [Amine Abdaoui](https://arxiv.org/search/cs?searchtype=author&query=Abdaoui%2C+A), [Camille Pradel](https://arxiv.org/search/cs?searchtype=author&query=Pradel%2C+C), [Grégoire Sigel](https://arxiv.org/search/cs?searchtype=author&query=Sigel%2C+G)

> Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to generate smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | SustaiNLP / EMNLP 2020                                       |
| Cite as:           | **[arXiv:2010.05609](https://arxiv.org/abs/2010.05609) [cs.CL]** |
|                    | (or **[arXiv:2010.05609v1](https://arxiv.org/abs/2010.05609v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-4">4. Controllable Paraphrasing and Translation with a Syntactic Exemplar</h2>

Title: [Controllable Paraphrasing and Translation with a Syntactic Exemplar](https://arxiv.org/abs/2010.05856)

Authors: [Mingda Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Sam Wiseman](https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

> Most prior work on exemplar-based syntactically controlled paraphrase generation relies on automatically-constructed large-scale paraphrase datasets. We sidestep this prerequisite by adapting models from prior work to be able to learn solely from bilingual text (bitext). Despite only using bitext for training, and in near zero-shot conditions, our single proposed model can perform four tasks: controlled paraphrase generation in both languages and controlled machine translation in both language directions. To evaluate these tasks quantitatively, we create three novel evaluation datasets. Our experimental results show that our models achieve competitive results on controlled paraphrase generation and strong performance on controlled machine translation. Analysis shows that our models learn to disentangle semantics and syntax in their latent representations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05856](https://arxiv.org/abs/2010.05856) [cs.CL]** |
|           | (or **[arXiv:2010.05856v1](https://arxiv.org/abs/2010.05856v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-5">5. Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models</h2>

Title: [Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models](https://arxiv.org/abs/2010.05874)

Authors: [Zirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y)

> Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05874](https://arxiv.org/abs/2010.05874) [cs.CL]** |
|           | (or **[arXiv:2010.05874v1](https://arxiv.org/abs/2010.05874v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-6">6. Do Language Embeddings Capture Scales?</h2>

Title: [Do Language Embeddings Capture Scales?](https://arxiv.org/abs/2010.05345)

Authors: [Xikun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Deepak Ramachandran](https://arxiv.org/search/cs?searchtype=author&query=Ramachandran%2C+D), [Ian Tenney](https://arxiv.org/search/cs?searchtype=author&query=Tenney%2C+I), [Yanai Elazar](https://arxiv.org/search/cs?searchtype=author&query=Elazar%2C+Y), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results.

| Comments:    | Accepted at EMNLP Findings 2020 and EMNLP BlackboxNLP workshop 2020 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.05345](https://arxiv.org/abs/2010.05345) [cs.CL]** |
|              | (or **[arXiv:2010.05345v1](https://arxiv.org/abs/2010.05345v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-7">7. Gradient-based Analysis of NLP Models is Manipulable</h2>

Title: [Gradient-based Analysis of NLP Models is Manipulable](https://arxiv.org/abs/2010.05419)

Authors: [Junlin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Jens Tuyls](https://arxiv.org/search/cs?searchtype=author&query=Tuyls%2C+J), [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Sameer Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

> Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, their faithfulness. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade that overwhelms the gradients without affecting the predictions. This Facade can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (text classification, NLI, and QA), we show that our method can manipulate numerous gradient-based analysis techniques: saliency maps, input reduction, and adversarial perturbations all identify unimportant or targeted tokens as being highly important. The code and a tutorial of this paper is available at [this http URL](http://ucinlp.github.io/facade).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.05419](https://arxiv.org/abs/2010.05419) [cs.CL]** |
|           | (or **[arXiv:2010.05419v1](https://arxiv.org/abs/2010.05419v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-8">8. It's not a Non-Issue: Negation as a Source of Error in Machine Translation</h2>

Title: [It's not a Non-Issue: Negation as a Source of Error in Machine Translation](https://arxiv.org/abs/2010.05432)

Authors: [Md Mosharaf Hossain](https://arxiv.org/search/cs?searchtype=author&query=Hossain%2C+M+M), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Eduardo Blanco](https://arxiv.org/search/cs?searchtype=author&query=Blanco%2C+E), [Alexis Palmer](https://arxiv.org/search/cs?searchtype=author&query=Palmer%2C+A)

> As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for modern MT systems using 17 translation directions as test bed. Through thorough analysis, we find that indeed the presence of negation can significantly impact downstream quality, in some cases resulting in quality reductions of more than 60%. We also provide a linguistically motivated analysis that directly explains the majority of our findings. We release our annotations and code to replicate our analysis here: [this https URL](https://github.com/mosharafhossain/negation-mt).

| Comments: | Accepted at the Findings of EMNLP2020                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.05432](https://arxiv.org/abs/2010.05432) [cs.CL]** |
|           | (or **[arXiv:2010.05432v1](https://arxiv.org/abs/2010.05432v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-9">9. Structural Knowledge Distillation</h2>

Title: [Structural Knowledge Distillation](https://arxiv.org/abs/2010.05010)

Authors: [Xinyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yong Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Zhaohui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Z), [Zixia Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Z), [Nguyen Bach](https://arxiv.org/search/cs?searchtype=author&query=Bach%2C+N), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Zhongqiang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Kewei Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+K)

> Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a smaller one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student's output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces smaller substructures than the teacher factorization; 3) the teacher factorization produces smaller substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible.

| Comments: | Under review as a conference paper of ICLR 2021. 15 pages    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.05010](https://arxiv.org/abs/2010.05010) [cs.CL]** |
|           | (or **[arXiv:2010.05010v1](https://arxiv.org/abs/2010.05010v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-10">10. SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task</h2>

Title: [SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task](https://arxiv.org/abs/2010.05122)

Authors: [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Masao Utiyama](https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

> In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.

| Comments: | WMT20                                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05122](https://arxiv.org/abs/2010.05122) [cs.CL]** |
|           | (or **[arXiv:2010.05122v1](https://arxiv.org/abs/2010.05122v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-11">11. fairseq S2T: Fast Speech-to-Text Modeling with fairseq</h2>

Title: [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Dmytro Okhonko](https://arxiv.org/search/cs?searchtype=author&query=Okhonko%2C+D), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T) modeling tasks such as end-to-end speech recognition and speech-to-text translation. It follows fairseq's careful design for scalability and extensibility. We provide end-to-end workflows from data pre-processing, model training to offline (online) inference. We implement state-of-the-art RNN-based as well as Transformer-based models and open-source detailed training recipes. Fairseq's machine translation models and language models can be seamlessly integrated into S2T workflows for multi-task learning or transfer learning. Fairseq S2T documentation and examples are available at [this https URL](https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text).

| Comments: | Accepted to AACL 2020 Demo                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.05171](https://arxiv.org/abs/2010.05171) [cs.CL]** |
|           | (or **[arXiv:2010.05171v1](https://arxiv.org/abs/2010.05171v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-12">12. Machine Translation of Mathematical Text</h2>

Title: [Machine Translation of Mathematical Text](https://arxiv.org/abs/2010.05229)

Authors: [Aditya Ohri](https://arxiv.org/search/cs?searchtype=author&query=Ohri%2C+A), [Tanya Schmah](https://arxiv.org/search/cs?searchtype=author&query=Schmah%2C+T)

> We have implemented a machine translation system, the PolyMath Translator, for LaTeX documents containing mathematical text. The current implementation translates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a held-out test corpus of mathematical sentences. It produces LaTeX documents that can be compiled to PDF without further editing. The system first converts the body of an input LaTeX document into English sentences containing math tokens, using the pandoc universal document converter to parse LaTeX input. We have trained a Transformer-based translator model, using OpenNMT, on a combined corpus containing a small proportion of domain-specific sentences. Our full system uses both this Transformer model and Google Translate, the latter being used as a backup to better handle linguistic features that do not appear in our training dataset. If the Transformer model does not have confidence in its translation, as determined by a high perplexity score, then we use Google Translate with a custom glossary. This backup was used 26% of the time on our test corpus of mathematical sentences. The PolyMath Translator is available as a web service at [this http URL](http://www.polymathtrans.ai/).

| Comments:    | 14 pages, 2 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2010.05229](https://arxiv.org/abs/2010.05229) [cs.CL]** |
|              | (or **[arXiv:2010.05229v1](https://arxiv.org/abs/2010.05229v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-13">13. Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It</h2>

Title: [Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It](https://arxiv.org/abs/2010.05332)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Rosie Sallis](https://arxiv.org/search/cs?searchtype=author&query=Sallis%2C+R), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> Neural Machine Translation (NMT) has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.
> In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, assessing on English-to-Spanish and English-to-German translation.
> We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest an effective alternative in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention in the inflected target language.

| Comments: | Workshop on Gender Bias in NLP, 2020                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05332](https://arxiv.org/abs/2010.05332) [cs.CL]** |
|           | (or **[arXiv:2010.05332v1](https://arxiv.org/abs/2010.05332v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-14">14. Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task</h2>

Title: [Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task](https://arxiv.org/abs/2010.05333)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence.
> The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.

| Comments: | WMT20 biomedical task                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.05333](https://arxiv.org/abs/2010.05333) [cs.CL]** |
|           | (or **[arXiv:2010.05333v1](https://arxiv.org/abs/2010.05333v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-15">15. ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization</h2>

Title: [ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization](https://arxiv.org/abs/2010.04791)

Authors: [Shiyue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Benjamin Frey](https://arxiv.org/search/cs?searchtype=author&query=Frey%2C+B), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively, and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization. Our data, code, and demo will be publicly available at [this https URL](https://github.com/ZhangShiyue/ChrEn)

| Comments: | EMNLP 2020 (19 pages)                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.04791](https://arxiv.org/abs/2010.04791) [cs.CL]** |
|           | (or **[arXiv:2010.04791v1](https://arxiv.org/abs/2010.04791v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-16">16. What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding</h2>

Title: [What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/abs/2010.04903)

Authors: [Yu-An Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.04903](https://arxiv.org/abs/2010.04903) [cs.CL]** |
|           | (or **[arXiv:2010.04903v1](https://arxiv.org/abs/2010.04903v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-17">17. On Long-Tailed Phenomena in Neural Machine Translation</h2>

Title: [On Long-Tailed Phenomena in Neural Machine Translation](https://arxiv.org/abs/2010.04924)

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Siddharth Dalmia](https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S), [Vivek Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+V), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F)

> State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04924](https://arxiv.org/abs/2010.04924) [cs.CL]** |
|           | (or **[arXiv:2010.04924v1](https://arxiv.org/abs/2010.04924v1) [cs.CL]** for this version) |





<h2 id="2020-10-13-18">18. Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns</h2>

Title: [Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns](https://arxiv.org/abs/2010.04989)

Authors: [Lei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Koichi Takeda](https://arxiv.org/search/cs?searchtype=author&query=Takeda%2C+K)

> This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the \textit{mismatching issue} when directly adopting BERTScore to QE. Specifically, there exist lots of mismatching errors between the source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross-lingual patterns, \textit{e.g.} word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.

| Comments: | To appear in WMT2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04989](https://arxiv.org/abs/2010.04989) [cs.CL]** |
|           | (or **[arXiv:2010.04989v1](https://arxiv.org/abs/2010.04989v1) [cs.CL]** for this version) |







# 2020-10-12

[Return to Index](#Index)



<h2 id="2020-10-12-1">1. Query-Key Normalization for Transformers</h2>

Title: [Query-Key Normalization for Transformers](https://arxiv.org/abs/2010.04245)

Authors: [Alex Henry](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+A), [Prudhvi Raj Dachapally](https://arxiv.org/search/cs?searchtype=author&query=Dachapally%2C+P+R), [Shubham Pawar](https://arxiv.org/search/cs?searchtype=author&query=Pawar%2C+S), [Yuxuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer's normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply ℓ2 normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT'15.

| Comments: | 8 pages, 2 figures, accepted at Findings of EMNLP 2020       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04245](https://arxiv.org/abs/2010.04245) [cs.CL]** |
|           | (or **[arXiv:2010.04245v1](https://arxiv.org/abs/2010.04245v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-2">2. Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task</h2>

Title: [Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task](https://arxiv.org/abs/2010.04297)

Authors: [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Amy Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+A), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Qijun Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Q), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Dipanjan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+D), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

> The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. Our submission is based on BLEURT, a previously published metric based on transfer learning. We extend the metric beyond English and evaluate it on 12 languages for which training examples are available, as well as four "zero-shot" languages, for which we have no fine-tuning data. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that BLEURT achieves competitive results on the WMT Metrics 2019 Shared Task, indicating its promise for the 2020 edition.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04297](https://arxiv.org/abs/2010.04297) [cs.CL]** |
|           | (or **[arXiv:2010.04297v1](https://arxiv.org/abs/2010.04297v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-3">3. Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning</h2>

Title: [Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](https://arxiv.org/abs/2010.04314)

Authors: [Xiaomian Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+X), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.

| Comments: | Accepted to EMNLP 2020 long paper                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04314](https://arxiv.org/abs/2010.04314) [cs.CL]** |
|           | (or **[arXiv:2010.04314v1](https://arxiv.org/abs/2010.04314v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-4">4. Token-level Adaptive Training for Neural Machine Translation</h2>

Title: [Token-level Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2010.04380)

Authors: [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.

| Comments: | 12 pages; Accepted by EMNLP 2020                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04380](https://arxiv.org/abs/2010.04380) [cs.CL]** |
|           | (or **[arXiv:2010.04380v1](https://arxiv.org/abs/2010.04380v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-5">5. A Survey of Knowledge-Enhanced Text Generation</h2>

Title: [A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/abs/2010.04389)

Authors: [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Zaitang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhiting Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Z), [Qingyun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M)

> The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.

| Comments: | 44 pages; Preprint; A paper and code collection is available at [this https URL](https://github.com/wyu97/KENLG-Reading) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04389](https://arxiv.org/abs/2010.04389) [cs.CL]** |
|           | (or **[arXiv:2010.04389v1](https://arxiv.org/abs/2010.04389v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-6">6. Uncertainty-Aware Semantic Augmentation for Neural Machine Translation</h2>

Title: [Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](https://arxiv.org/abs/2010.04411)

Authors: [Xiangpeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Yue Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Luxi Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.

| Comments: | Accepted to EMNLP 2020, 12 pages, 2 figures, 9 tables        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.04411](https://arxiv.org/abs/2010.04411) [cs.CL]** |
|           | (or **[arXiv:2010.04411v1](https://arxiv.org/abs/2010.04411v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-7">7. Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels</h2>

Title: [Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](https://arxiv.org/abs/2010.04438)

Authors: [Harris Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+H), [Jamie Kiros](https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J), [William Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W)

> A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.

| Comments: | 10 pages (+3 appendix), 11 figures, 5 tables. Accepted to Findings of EMNLP 2020 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2010.04438](https://arxiv.org/abs/2010.04438) [cs.CL]** |
|           | (or **[arXiv:2010.04438v1](https://arxiv.org/abs/2010.04438v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-8">8. Self-Paced Learning for Neural Machine Translation</h2>

Title: [Self-Paced Learning for Neural Machine Translation](https://arxiv.org/abs/2010.04505)

Authors: [Yu Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Yikai Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B)

> Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.

| Comments: | Accepted by EMNLP2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.04505](https://arxiv.org/abs/2010.04505) [cs.CL]** |
|           | (or **[arXiv:2010.04505v1](https://arxiv.org/abs/2010.04505v1) [cs.CL]** for this version) |





<h2 id="2020-10-12-9">9. Recursive Top-Down Production for Sentence Generation with Latent Trees</h2>

Title: [Recursive Top-Down Production for Sentence Generation with Latent Trees](https://arxiv.org/abs/2010.04704)

Authors: [Shawn Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Timothy J. O'Donnell](https://arxiv.org/search/cs?searchtype=author&query=O'Donnell%2C+T+J), [Alessandro Sordoni](https://arxiv.org/search/cs?searchtype=author&query=Sordoni%2C+A), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville%2C+A)

> We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017), where it outperforms previous models on the LENGTH split, and English question formation (McCoy et al., 2020), where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset (Elliott et al., 2016), and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.04704](https://arxiv.org/abs/2010.04704) [cs.CL]** |
|           | (or **[arXiv:2010.04704v1](https://arxiv.org/abs/2010.04704v1) [cs.CL]** for this version) |







# 2020-10-09

[Return to Index](#Index)



<h2 id="2020-10-09-1">1. Shallow-to-Deep Training for Neural Machine Translation</h2>

Title: [Shallow-to-Deep Training for Neural Machine Translation](https://arxiv.org/abs/2010.03737)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Ziyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Yufan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Huizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is 1.4 × faster than training from scratch, and achieves a BLEU score of 30.33 and 43.29 on two tasks. The code is publicly available at [this https URL](https://github.com/libeineu/SDT-Training/).

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03737](https://arxiv.org/abs/2010.03737) [cs.CL]** |
|           | (or **[arXiv:2010.03737v1](https://arxiv.org/abs/2010.03737v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-2">2. Improving Attention Mechanism with Query-Value Interaction</h2>

Title: [Improving Attention Mechanism with Query-Value Interaction](https://arxiv.org/abs/2010.03766)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Attention mechanism has played critical roles in various state-of-the-art NLP models such as Transformer and BERT. It can be formulated as a ternary function that maps the input queries, keys and values into an output by using a summation of values weighted by the attention weights derived from the interactions between queries and keys. Similar with query-key interactions, there is also inherent relatedness between queries and values, and incorporating query-value interactions has the potential to enhance the output by learning customized values according to the characteristics of queries. However, the query-value interactions are ignored by existing attention methods, which may be not optimal. In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions. We propose a query-value interaction function which can learn query-aware attention values, and combine them with the original values and attention weights to form the final output. Extensive experiments on four datasets for different tasks show that our approach can consistently improve the performance of many attention-based models by incorporating query-value interactions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03766](https://arxiv.org/abs/2010.03766) [cs.CL]** |
|           | (or **[arXiv:2010.03766v1](https://arxiv.org/abs/2010.03766v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-3">3. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning</h2>

Title: [ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768)

Authors: [Mohit Shridhar](https://arxiv.org/search/cs?searchtype=author&query=Shridhar%2C+M), [Xingdi Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+X), [Marc-Alexandre Côté](https://arxiv.org/search/cs?searchtype=author&query=Côté%2C+M), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Adam Trischler](https://arxiv.org/search/cs?searchtype=author&query=Trischler%2C+A), [Matthew Hausknecht](https://arxiv.org/search/cs?searchtype=author&query=Hausknecht%2C+M)

> Given a simple request (e.g., Put a washed apple in the kitchen fridge), humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, visual scene understanding, and so forth).

| Comments: | Data, code, and videos are available at [this http URL](http://alfworld.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO) |
| Cite as:  | **[arXiv:2010.03768](https://arxiv.org/abs/2010.03768) [cs.CL]** |
|           | (or **[arXiv:2010.03768v1](https://arxiv.org/abs/2010.03768v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-4">4. What Can We Do to Improve Peer Review in NLP?</h2>

Title: [What Can We Do to Improve Peer Review in NLP?](https://arxiv.org/abs/2010.03863)

Authors: [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I)

> Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.

| Comments: | To appear at Findings of EMNLP                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.03863](https://arxiv.org/abs/2010.03863) [cs.CL]** |
|           | (or **[arXiv:2010.03863v1](https://arxiv.org/abs/2010.03863v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-5">5. Dense Relational Image Captioning via Multi-task Triple-Stream Networks</h2>

Title: [Dense Relational Image Captioning via Multi-task Triple-Stream Networks](https://arxiv.org/abs/2010.03855)

Authors: [Dong-Jin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Tae-Hyun oh](https://arxiv.org/search/cs?searchtype=author&query=oh%2C+T), [Jinsoo Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J), [In So Kweon](https://arxiv.org/search/cs?searchtype=author&query=Kweon%2C+I+S)

> We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions of each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS, i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to not only learn to generate captions but also predict the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. We additionally extend analysis to an ablation study, applications on holistic image captioning, scene graph generation, and retrieval tasks

| Comments: | Journal extension of our CVPR 2019 paper [arXiv:1903.05942](https://arxiv.org/abs/1903.05942) . Source code : [this https URL](https://github.com/Dong-JinKim/DenseRelationalCaptioning) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2010.03855](https://arxiv.org/abs/2010.03855) [cs.CV]** |
|           | (or **[arXiv:2010.03855v1](https://arxiv.org/abs/2010.03855v1) [cs.CV]** for this version) |





<h2 id="2020-10-09-6">6. Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations</h2>

Title: [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://arxiv.org/abs/2010.03644)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [Pradyumna Narayana](https://arxiv.org/search/cs?searchtype=author&query=Narayana%2C+P), [Kazoo Sone](https://arxiv.org/search/cs?searchtype=author&query=Sone%2C+K), [Sugato Basu](https://arxiv.org/search/cs?searchtype=author&query=Basu%2C+S), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2010.03644](https://arxiv.org/abs/2010.03644) [cs.CL]** |
|           | (or **[arXiv:2010.03644v1](https://arxiv.org/abs/2010.03644v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-7">7. Cross-Thought for Sentence Encoder Pre-training</h2>

Title: [Cross-Thought for Sentence Encoder Pre-training](https://arxiv.org/abs/2010.03652)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03652](https://arxiv.org/abs/2010.03652) [cs.CL]** |
|           | (or **[arXiv:2010.03652v1](https://arxiv.org/abs/2010.03652v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-8">8. Leveraging Discourse Rewards for Document-Level Neural Machine Translation</h2>

Title: [Leveraging Discourse Rewards for Document-Level Neural Machine Translation](https://arxiv.org/abs/2010.03732)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Nazanin Esmaili](https://arxiv.org/search/cs?searchtype=author&query=Esmaili%2C+N), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion (LC) and coherence (COH), by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F_BERT.

| Comments: | Accepted at COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03732](https://arxiv.org/abs/2010.03732) [cs.CL]** |
|           | (or **[arXiv:2010.03732v1](https://arxiv.org/abs/2010.03732v1) [cs.CL]** for this version) |





<h2 id="2020-10-09-9">9. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</h2>

Title: [Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines](https://arxiv.org/abs/2010.03790)

Authors: [Keerthiram Murugesan](https://arxiv.org/search/cs?searchtype=author&query=Murugesan%2C+K), [Mattia Atzeni](https://arxiv.org/search/cs?searchtype=author&query=Atzeni%2C+M), [Pavan Kapanipathi](https://arxiv.org/search/cs?searchtype=author&query=Kapanipathi%2C+P), [Pushkar Shukla](https://arxiv.org/search/cs?searchtype=author&query=Shukla%2C+P), [Sadhana Kumaravel](https://arxiv.org/search/cs?searchtype=author&query=Kumaravel%2C+S), [Gerald Tesauro](https://arxiv.org/search/cs?searchtype=author&query=Tesauro%2C+G), [Kartik Talamadupula](https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K), [Mrinmaya Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+M), [Murray Campbell](https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M)

> Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.03790](https://arxiv.org/abs/2010.03790) [cs.AI]** |
|           | (or **[arXiv:2010.03790v1](https://arxiv.org/abs/2010.03790v1) [cs.AI]** for this version) |





# 2020-10-08

[Return to Index](#Index)



<h2 id="2020-10-08-1">1. Plug and Play Autoencoders for Conditional Text Generation</h2>

Title: [Plug and Play Autoencoders for Conditional Text Generation](https://arxiv.org/abs/2010.02983)

Authors: [Florian Mai](https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+F) (1 and 2), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N) (3), [Ivan Montero](https://arxiv.org/search/cs?searchtype=author&query=Montero%2C+I) (3), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A) (3 and 4), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J) (1) ((1) Idiap Research Institute, (2) EPFL, (3) University of Washington, (4) Allen Institute for Artificial Intelligence)

> Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.

| Comments: | To be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02983](https://arxiv.org/abs/2010.02983) [cs.CL]** |
|           | (or **[arXiv:2010.02983v1](https://arxiv.org/abs/2010.02983v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-2">2. Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</h2>

Title: [Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](https://arxiv.org/abs/2010.03142)

Authors: [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple low-resource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pre-training corpus. Code, data, and pre-trained models are available at [this https URL](https://github.com/linzehui/mRASP).

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03142](https://arxiv.org/abs/2010.03142) [cs.CL]** |
|           | (or **[arXiv:2010.03142v1](https://arxiv.org/abs/2010.03142v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-3">3. A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction</h2>

Title: [A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](https://arxiv.org/abs/2010.03155)

Authors: [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Jun Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+J), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found that there is a non-negligible amount of "noise" where errors were inappropriately edited or left uncorrected. To address this, we designed a self-refinement method where the key idea is to denoise these datasets by leveraging the prediction consistency of existing models, and outperformed strong denoising baseline methods. We further applied task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. We then analyzed the effect of the proposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance.

| Comments: | accepted by EMNLP 2020 (Findings)                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03155](https://arxiv.org/abs/2010.03155) [cs.CL]** |
|           | (or **[arXiv:2010.03155v1](https://arxiv.org/abs/2010.03155v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-4">4. Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages</h2>

Title: [Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](https://arxiv.org/abs/2010.03179)

Authors: [Michael A. Hedderich](https://arxiv.org/search/cs?searchtype=author&query=Hedderich%2C+M+A), [David Adelani](https://arxiv.org/search/cs?searchtype=author&query=Adelani%2C+D), [Dawei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+D), [Jesujoba Alabi](https://arxiv.org/search/cs?searchtype=author&query=Alabi%2C+J), [Udia Markus](https://arxiv.org/search/cs?searchtype=author&query=Markus%2C+U), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and Yorùbá on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.

| Comments: | Accepted at EMNLP'20                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03179](https://arxiv.org/abs/2010.03179) [cs.CL]** |
|           | (or **[arXiv:2010.03179v1](https://arxiv.org/abs/2010.03179v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-5">5. Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction</h2>

Title: [Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](https://arxiv.org/abs/2010.03260)

Authors: [Mengyun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Xingxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.

| Comments: | Accepted by EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03260](https://arxiv.org/abs/2010.03260) [cs.CL]** |
|           | (or **[arXiv:2010.03260v1](https://arxiv.org/abs/2010.03260v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-6">6. Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation</h2>

Title: [Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](https://arxiv.org/abs/2010.03412)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Xing Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+X), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.

| Comments: | Accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.03412](https://arxiv.org/abs/2010.03412) [cs.CL]** |
|           | (or **[arXiv:2010.03412v1](https://arxiv.org/abs/2010.03412v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-7">7. WER we are and WER we think we are</h2>

Title: [WER we are and WER we think we are](https://arxiv.org/abs/2010.03432)

Authors: [Piotr Szymański](https://arxiv.org/search/cs?searchtype=author&query=Szymański%2C+P), [Piotr Żelasko](https://arxiv.org/search/cs?searchtype=author&query=Żelasko%2C+P), [Mikolaj Morzy](https://arxiv.org/search/cs?searchtype=author&query=Morzy%2C+M), [Adrian Szymczak](https://arxiv.org/search/cs?searchtype=author&query=Szymczak%2C+A), [Marzena Żyła-Hoppe](https://arxiv.org/search/cs?searchtype=author&query=Żyła-Hoppe%2C+M), [Joanna Banaszczak](https://arxiv.org/search/cs?searchtype=author&query=Banaszczak%2C+J), [Lukasz Augustyniak](https://arxiv.org/search/cs?searchtype=author&query=Augustyniak%2C+L), [Jan Mizgajski](https://arxiv.org/search/cs?searchtype=author&query=Mizgajski%2C+J), [Yishay Carmiel](https://arxiv.org/search/cs?searchtype=author&query=Carmiel%2C+Y)

> Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.

| Comments: | Accepted to EMNLP Findings                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2010.03432](https://arxiv.org/abs/2010.03432) [cs.CL]** |
|           | (or **[arXiv:2010.03432v1](https://arxiv.org/abs/2010.03432v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-8">8. Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation</h2>

Title: [Improving Sentiment Analysis over non-English Tweets using Multilingual Transformers and Automatic Translation for Data-Augmentation](https://arxiv.org/abs/2010.03486)

Authors: [Valentin Barriere](https://arxiv.org/search/cs?searchtype=author&query=Barriere%2C+V), [Alexandra Balahur](https://arxiv.org/search/cs?searchtype=author&query=Balahur%2C+A)

> Tweets are specific text data when compared to general text. Although sentiment analysis over tweets has become very popular in the last decade for English, it is still difficult to find huge annotated corpora for non-English languages. The recent rise of the transformer models in Natural Language Processing allows to achieve unparalleled performances in many tasks, but these models need a consequent quantity of text to adapt to the tweet domain. We propose the use of a multilingual transformer model, that we pre-train over English tweets and apply data-augmentation using automatic translation to adapt the model to non-English languages. Our experiments in French, Spanish, German and Italian suggest that the proposed technique is an efficient way to improve the results of the transformers over small corpora of tweets in a non-English language.

| Comments: | Accepted to COLING2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03486](https://arxiv.org/abs/2010.03486) [cs.CL]** |
|           | (or **[arXiv:2010.03486v1](https://arxiv.org/abs/2010.03486v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-9">9. TeaForN: Teacher-Forcing with N-grams</h2>

Title: [TeaForN: Teacher-Forcing with N-grams](https://arxiv.org/abs/2010.03494)

Authors: [Sebastian Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S), [Nan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Radu Soricut](https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R)

> Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.

| Comments: | to be published in EMNLP 2020                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03494](https://arxiv.org/abs/2010.03494) [cs.CL]** |
|           | (or **[arXiv:2010.03494v1](https://arxiv.org/abs/2010.03494v1) [cs.CL]** for this version) |





<h2 id="2020-10-08-10">10. Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models</h2>

Title: [Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models](https://arxiv.org/abs/2010.03542)

Authors: [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y)

> This paper describes Galileo's performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.

| Comments: | 8 pages, 2 figures, 6 tables. Accepted at Proceedings of 14th International Workshop on Semantic Evaluation (SemEval-2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.03542](https://arxiv.org/abs/2010.03542) [cs.CL]** |
|           | (or **[arXiv:2010.03542v1](https://arxiv.org/abs/2010.03542v1) [cs.CL]** for this version) |



# 2020-10-07

[Return to Index](#Index)



<h2 id="2020-10-07-1">1. Multi-task Learning for Multilingual Neural Machine Translation</h2>

Title: [Multi-task Learning for Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.02523)

Authors: [Yiren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [ChengXiang Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+C), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H)

> While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02523](https://arxiv.org/abs/2010.02523) [cs.CL]** |
|           | (or **[arXiv:2010.02523v1](https://arxiv.org/abs/2010.02523v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-2">2. Do Explicit Alignments Robustly Improve Multilingual Encoders?</h2>

Title: [Do Explicit Alignments Robustly Improve Multilingual Encoders?](https://arxiv.org/abs/2010.02537)

Authors: [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M)

> Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02537](https://arxiv.org/abs/2010.02537) [cs.CL]** |
|           | (or **[arXiv:2010.02537v1](https://arxiv.org/abs/2010.02537v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-3">3. The Multilingual Amazon Reviews Corpus</h2>

Title: [The Multilingual Amazon Reviews Corpus](https://arxiv.org/abs/2010.02573)

Authors: [Phillip Keung](https://arxiv.org/search/cs?searchtype=author&query=Keung%2C+P), [Yichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [György Szarvas](https://arxiv.org/search/cs?searchtype=author&query=Szarvas%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.

| Comments: | To appear in EMNLP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02573](https://arxiv.org/abs/2010.02573) [cs.CL]** |
|           | (or **[arXiv:2010.02573v1](https://arxiv.org/abs/2010.02573v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-4">4. On the Sparsity of Neural Machine Translation Models</h2>

Title: [On the Sparsity of Neural Machine Translation Models](https://arxiv.org/abs/2010.02646)

Authors: [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02646](https://arxiv.org/abs/2010.02646) [cs.CL]** |
|           | (or **[arXiv:2010.02646v1](https://arxiv.org/abs/2010.02646v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-5">5. On the Sub-Layer Functionalities of Transformer Decoder</h2>

Title: [On the Sub-Layer Functionalities of Transformer Decoder](https://arxiv.org/abs/2010.02648)

Authors: [Yilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Prasad Tadepalli](https://arxiv.org/search/cs?searchtype=author&query=Tadepalli%2C+P), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages -- developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance -- a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.

| Comments: | Findings of the 2020 Conference on Empirical Methods in Natural Language Processing (Long) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.02648](https://arxiv.org/abs/2010.02648) [cs.CL]** |
|           | (or **[arXiv:2010.02648v1](https://arxiv.org/abs/2010.02648v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-6">6. Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</h2>

Title: [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://arxiv.org/abs/2010.02684)

Authors: [Alvin Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+A), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A)

> This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a 'backdoor poisoning' attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier's predictions can be steered to the poison target class with success rates of >80% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.

| Comments: | Accepted in EMNLP-Findings 2020, Camera Ready Version        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2010.02684](https://arxiv.org/abs/2010.02684) [cs.CL]** |
|           | (or **[arXiv:2010.02684v1](https://arxiv.org/abs/2010.02684v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-7">7. Analyzing Individual Neurons in Pre-trained Language Models</h2>

Title: [Analyzing Individual Neurons in Pre-trained Language Models](https://arxiv.org/abs/2010.02695)

Authors: [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

> While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.

| Comments: | Accepted in EMNLP 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02695](https://arxiv.org/abs/2010.02695) [cs.CL]** |
|           | (or **[arXiv:2010.02695v1](https://arxiv.org/abs/2010.02695v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-8">8. Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</h2>

Title: [Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](https://arxiv.org/abs/2010.02705)

Authors: [Minki Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+M), [Moonsu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Sung Ju Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J)

> We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.

| Comments: | 19 pages, 9 figures, EMNLP 2020                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02705](https://arxiv.org/abs/2010.02705) [cs.CL]** |
|           | (or **[arXiv:2010.02705v1](https://arxiv.org/abs/2010.02705v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-9">9. Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs</h2>

Title: [Robustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs](https://arxiv.org/abs/2010.02847)

Authors: [Haiyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Alison Sneyd](https://arxiv.org/search/cs?searchtype=author&query=Sneyd%2C+A), [Mark Stevenson](https://arxiv.org/search/cs?searchtype=author&query=Stevenson%2C+M)

> It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy "man is to computer-programmer as woman is to homemaker" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.

| Comments: | Accepted at AACL-IJCNLP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02847](https://arxiv.org/abs/2010.02847) [cs.CL]** |
|           | (or **[arXiv:2010.02847v1](https://arxiv.org/abs/2010.02847v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-10">10. PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</h2>

Title: [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://arxiv.org/abs/2010.02301)

Authors: [Xinyu Hua](https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X), [Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L)

> Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often "rambling" without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.

| Comments: | Accepted at EMNLP 2020 as a long paper                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02301](https://arxiv.org/abs/2010.02301) [cs.CL]** |
|           | (or **[arXiv:2010.02301v1](https://arxiv.org/abs/2010.02301v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-11">11. We Don't Speak the Same Language: Interpreting Polarization through Machine Translation</h2>

Title: [We Don't Speak the Same Language: Interpreting Polarization through Machine Translation](https://arxiv.org/abs/2010.02339)

Authors: [Ashiqur R. KhudaBukhsh](https://arxiv.org/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R), [Rupak Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+R), [Mark S. Kamlet](https://arxiv.org/search/cs?searchtype=author&query=Kamlet%2C+M+S), [Tom M. Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell%2C+T+M)

> Polarization among US political parties, media and elites is a widely studied topic. Prominent lines of prior research across multiple disciplines have observed and analyzed growing polarization in social media. In this paper, we present a new methodology that offers a fresh perspective on interpreting polarization through the lens of machine translation. With a novel proposition that two sub-communities are speaking in two different \emph{languages}, we demonstrate that modern machine translation methods can provide a simple yet powerful and interpretable framework to understand the differences between two (or more) large-scale social media discussion data sets at the granularity of words. Via a substantial corpus of 86.6 million comments by 6.5 million users on over 200,000 news videos hosted by YouTube channels of four prominent US news networks, we demonstrate that simple word-level and phrase-level translation pairs can reveal deep insights into the current political divide -- what is \emph{black lives matter} to one can be \emph{all lives matter} to the other.

| Subjects: | **Computation and Language (cs.CL)**; Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02339](https://arxiv.org/abs/2010.02339) [cs.CL]** |
|           | (or **[arXiv:2010.02339v1](https://arxiv.org/abs/2010.02339v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-12">12. Inference Strategies for Machine Translation with Conditional Masking</h2>

Title: [Inference Strategies for Machine Translation with Conditional Masking](https://arxiv.org/abs/2010.02352)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard "mask-predict" algorithm, and provide analyses of its behavior on machine translation tasks.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02352](https://arxiv.org/abs/2010.02352) [cs.CL]** |
|           | (or **[arXiv:2010.02352v1](https://arxiv.org/abs/2010.02352v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-13">13. Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</h2>

Title: [Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks](https://arxiv.org/abs/2010.02394)

Authors: [Lichao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L), [Congying Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+C), [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Tingting Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+T), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S), [Lifang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+L)

> Mixup is the latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT. To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named "mixup-transformer", for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUE benchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.

| Comments: | Accepted by COLING 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02394](https://arxiv.org/abs/2010.02394) [cs.CL]** |
|           | (or **[arXiv:2010.02394v1](https://arxiv.org/abs/2010.02394v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-14">14. Guiding Attention for Self-Supervised Learning with Transformers</h2>

Title: [Guiding Attention for Self-Supervised Learning with Transformers](https://arxiv.org/abs/2010.02399)

Authors: [Ameet Deshpande](https://arxiv.org/search/cs?searchtype=author&query=Deshpande%2C+A), [Karthik Narasimhan](https://arxiv.org/search/cs?searchtype=author&query=Narasimhan%2C+K)

> In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.

| Comments: | Accepted to Findings of EMNLP, 2020                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02399](https://arxiv.org/abs/2010.02399) [cs.CL]** |
|           | (or **[arXiv:2010.02399v1](https://arxiv.org/abs/2010.02399v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-15">15. Adversarial Grammatical Error Correction</h2>

Title: [Adversarial Grammatical Error Correction](https://arxiv.org/abs/2010.02407)

Authors: [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V), [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D)

> Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs) have been successful in generating realistic texts across many different tasks by learning to directly minimize the difference between human-generated and synthetic text. In this work, we present an adversarial learning approach to GEC, using the generator-discriminator framework. The generator is a Transformer model, trained to produce grammatically correct sentences given grammatically incorrect ones. The discriminator is a sentence-pair classification model, trained to judge a given pair of grammatically incorrect-correct sentences on the quality of grammatical correction. We pre-train both the discriminator and the generator on parallel texts and then fine-tune them further using a policy gradient method that assigns high rewards to sentences which could be true corrections of the grammatically incorrect text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that Adversarial-GEC can achieve competitive GEC quality compared to NMT-based baselines.

| Comments: | 13 Pages, EMNLP 2020                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02407](https://arxiv.org/abs/2010.02407) [cs.CL]** |
|           | (or **[arXiv:2010.02407v1](https://arxiv.org/abs/2010.02407v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-16">16. Efficient Inference For Neural Machine Translation</h2>

Title: [Efficient Inference For Neural Machine Translation](https://arxiv.org/abs/2010.02416)

Authors: [Yi-Te Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+Y), [Sarthak Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S), [Yi-Hsiu Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Y), [Ilya Chatsviorkin](https://arxiv.org/search/cs?searchtype=author&query=Chatsviorkin%2C+I)

> Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.02416](https://arxiv.org/abs/2010.02416) [cs.CL]** |
|           | (or **[arXiv:2010.02416v1](https://arxiv.org/abs/2010.02416v1) [cs.CL]** for this version) |



<h2 id="2020-10-07-17">17. Iterative Domain-Repaired Back-Translation</h2>

Title: [Iterative Domain-Repaired Back-Translation](https://arxiv.org/abs/2010.02473)

Authors: [Hao-Ran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02473](https://arxiv.org/abs/2010.02473) [cs.CL]** |
|           | (or **[arXiv:2010.02473v1](https://arxiv.org/abs/2010.02473v1) [cs.CL]** for this version) |







# 2020-10-06

[Return to Index](#Index)



<h2 id="2020-10-06-1">1. A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples</h2>

Title: [A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples](https://arxiv.org/abs/2010.01345)

Authors: [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.

| Comments: | COLING 2020 Long Paper                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01345](https://arxiv.org/abs/2010.01345) [cs.CL]** |
|           | (or **[arXiv:2010.01345v1](https://arxiv.org/abs/2010.01345v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-2">2. Transformer-Based Neural Text Generation with Syntactic Guidance</h2>

Title: [Transformer-Based Neural Text Generation with Syntactic Guidance](https://arxiv.org/abs/2010.01737)

Authors: [Yinghao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y) (Georgia Institute of Technology), [Rui Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+R) (Georgia Institute of Technology), [Isaac Rehg](https://arxiv.org/search/cs?searchtype=author&query=Rehg%2C+I) (Georgia Institute of Technology), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C) (Georgia Institute of Technology)

> We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.

| Comments: | 11 pages, 4 figures and 5 tables                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01737](https://arxiv.org/abs/2010.01737) [cs.CL]** |
|           | (or **[arXiv:2010.01737v1](https://arxiv.org/abs/2010.01737v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-3">3. Second-Order NLP Adversarial Examples</h2>

Title: [Second-Order NLP Adversarial Examples](https://arxiv.org/abs/2010.01770)

Authors: [John X. Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J+X)

> Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at [this https URL](https://github.com/jxmorris12/second-order-adversarial-examples).

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01770](https://arxiv.org/abs/2010.01770) [cs.CL]** |
|           | (or **[arXiv:2010.01770v2](https://arxiv.org/abs/2010.01770v2) [cs.CL]** for this version) |





<h2 id="2020-10-06-4">4. GenAug: Data Augmentation for Finetuning Text Generators</h2>

Title: [GenAug: Data Augmentation for Finetuning Text Generators](https://arxiv.org/abs/2010.01794)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Dongyeop Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+D), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.

| Comments: | EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available at [this https URL](https://github.com/styfeng/GenAug) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.01794](https://arxiv.org/abs/2010.01794) [cs.CL]** |
|           | (or **[arXiv:2010.01794v1](https://arxiv.org/abs/2010.01794v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-5">5. Lifelong Language Knowledge Distillation</h2>

Title: [Lifelong Language Knowledge Distillation](https://arxiv.org/abs/2010.02123)

Authors: [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Shang-Yu Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+S), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y)

> It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.

| Comments: | EMNLP 2020 long paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.02123](https://arxiv.org/abs/2010.02123) [cs.CL]** |
|           | (or **[arXiv:2010.02123v1](https://arxiv.org/abs/2010.02123v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-6">6. A Streaming Approach For Efficient Batched Beam Search</h2>

Title: [A Streaming Approach For Efficient Batched Beam Search](https://arxiv.org/abs/2010.02164)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Violet Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+V), [John DeNero](https://arxiv.org/search/cs?searchtype=author&query=DeNero%2C+J), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically ``refills" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF) |
| Cite as:  | **[arXiv:2010.02164](https://arxiv.org/abs/2010.02164) [cs.CL]** |
|           | (or **[arXiv:2010.02164v1](https://arxiv.org/abs/2010.02164v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-7">7. Self-training Improves Pre-training for Natural Language Understanding</h2>

Title: [Self-training Improves Pre-training for Natural Language Understanding](https://arxiv.org/abs/2010.02194)

Authors: [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Beliz Gunel](https://arxiv.org/search/cs?searchtype=author&query=Gunel%2C+B), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Onur Celebi](https://arxiv.org/search/cs?searchtype=author&query=Celebi%2C+O), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.02194](https://arxiv.org/abs/2010.02194) [cs.CL]** |
|           | (or **[arXiv:2010.02194v1](https://arxiv.org/abs/2010.02194v1) [cs.CL]** for this version) |





<h2 id="2020-10-06-8">8. Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation</h2>

Title: [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.01667)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> To improve the performance of Neural Machine Translation~(NMT) for low-resource languages~(LRL), one effective strategy is to leverage parallel data from a related high-resource language~(HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate \emph{into} the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.

| Comments: | Accepted to Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.01667](https://arxiv.org/abs/2010.01667) [cs.CL]** |
|           | (or **[arXiv:2010.01667v1](https://arxiv.org/abs/2010.01667v1) [cs.CL]** for this version) |





# 2020-10-05

[Return to Index](#Index)



<h2 id="2020-10-05-1">1. Nearest Neighbor Machine Translation</h2>

Title: [Nearest Neighbor Machine Translation](https://arxiv.org/abs/2010.00710)

Authors: [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> We introduce k-nearest-neighbor machine translation (kNN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2010.00710](https://arxiv.org/abs/2010.00710) [cs.CL]** |
|           | (or **[arXiv:2010.00710v1](https://arxiv.org/abs/2010.00710v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-2">2. A Survey of the State of Explainable AI for Natural Language Processing</h2>

Title: [A Survey of the State of Explainable AI for Natural Language Processing](https://arxiv.org/abs/2010.00711)

Authors: [Marina Danilevsky](https://arxiv.org/search/cs?searchtype=author&query=Danilevsky%2C+M), [Kun Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+K), [Ranit Aharonov](https://arxiv.org/search/cs?searchtype=author&query=Aharonov%2C+R), [Yannis Katsis](https://arxiv.org/search/cs?searchtype=author&query=Katsis%2C+Y), [Ban Kawas](https://arxiv.org/search/cs?searchtype=author&query=Kawas%2C+B), [Prithviraj Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+P)

> Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.

| Comments:    | To appear in AACL-IJCNLP 2020                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2010.00711](https://arxiv.org/abs/2010.00711) [cs.CL]** |
|              | (or **[arXiv:2010.00711v1](https://arxiv.org/abs/2010.00711v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-3">3. An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</h2>

Title: [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://arxiv.org/abs/2010.00784)

Authors: [Kristjan Arumae](https://arxiv.org/search/cs?searchtype=author&query=Arumae%2C+K), [Qing Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Parminder Bhatia](https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+P)

> Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.

| Comments: | arXiv admin note: text overlap with [arXiv:2004.03794](https://arxiv.org/abs/2004.03794) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2010.00784](https://arxiv.org/abs/2010.00784) [cs.CL]** |
|           | (or **[arXiv:2010.00784v1](https://arxiv.org/abs/2010.00784v1) [cs.CL]** for this version) |





<h2 id="2020-10-05-4">4. Which *BERT? A Survey Organizing Contextualized Encoders</h2>

Title: [Which *BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/abs/2010.00854)

Authors: [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B)

> Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.

| Comments: | EMNLP 2020                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2010.00854](https://arxiv.org/abs/2010.00854) [cs.CL]** |
|           | (or **[arXiv:2010.00854v1](https://arxiv.org/abs/2010.00854v1) [cs.CL]** for this version) |



# 2020-10-02

[Return to Index](#Index)



<h2 id="2020-10-02-1">1. WeChat Neural Machine Translation Systems for WMT20</h2>

Title: [WeChat Neural Machine Translation Systems for WMT20](https://arxiv.org/abs/2010.00247)

Authors: [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Xianfeng Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Qinsong Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Q), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Sifan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H)

> We participate in the WMT 2020 shared news translation task on Chinese to English. Our system is based on the Transformer (Vaswani et al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese to English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.

| Comments: | Accepted at WMT 2020. Our Chinese to English system achieved the highest case-sensitive BLEU score among all submissions |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2010.00247](https://arxiv.org/abs/2010.00247) [cs.CL]** |
|           | (or **[arXiv:2010.00247v1](https://arxiv.org/abs/2010.00247v1) [cs.CL]** for this version) |



# 2020-10-01

[Return to Index](#Index)



<h2 id="2020-10-01-1">1. Rethinking Attention with Performers</h2>

Title: [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)

Authors: [Krzysztof Choromanski](https://arxiv.org/search/cs?searchtype=author&query=Choromanski%2C+K), [Valerii Likhosherstov](https://arxiv.org/search/cs?searchtype=author&query=Likhosherstov%2C+V), [David Dohan](https://arxiv.org/search/cs?searchtype=author&query=Dohan%2C+D), [Xingyou Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Andreea Gane](https://arxiv.org/search/cs?searchtype=author&query=Gane%2C+A), [Tamas Sarlos](https://arxiv.org/search/cs?searchtype=author&query=Sarlos%2C+T), [Peter Hawkins](https://arxiv.org/search/cs?searchtype=author&query=Hawkins%2C+P), [Jared Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+J), [Afroz Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+A), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [David Belanger](https://arxiv.org/search/cs?searchtype=author&query=Belanger%2C+D), [Lucy Colwell](https://arxiv.org/search/cs?searchtype=author&query=Colwell%2C+L), [Adrian Weller](https://arxiv.org/search/cs?searchtype=author&query=Weller%2C+A)

> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.

| Comments: | 36 pages. This is an updated version of a previous submission which can be found at [arXiv:2006.03555](https://arxiv.org/abs/2006.03555). See [this https URL](https://github.com/google-research/google-research/tree/master/protein_lm) for protein language model code, and [this https URL](https://github.com/google-research/google-research/tree/master/performer) for Performer code |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2009.14794](https://arxiv.org/abs/2009.14794) [cs.LG]** |
|           | (or **[arXiv:2009.14794v1](https://arxiv.org/abs/2009.14794v1) [cs.LG]** for this version) |





<h2 id="2020-10-01-2">2. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</h2>

Title: [Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](https://arxiv.org/abs/2009.14304)

Authors: [Saurabh Kulshreshtha](https://arxiv.org/search/cs?searchtype=author&query=Kulshreshtha%2C+S), [José Luis Redondo-García](https://arxiv.org/search/cs?searchtype=author&query=Redondo-García%2C+J+L), [Ching-Yun Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C)

> Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work proposes several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs. dictionary-based supervision and rotational vs. fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.

| Comments: | Accepted as a long paper in Findings of EMNLP 2020           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14304](https://arxiv.org/abs/2009.14304) [cs.CL]** |
|           | (or **[arXiv:2009.14304v1](https://arxiv.org/abs/2009.14304v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-3">3. Can Automatic Post-Editing Improve NMT?</h2>

Title: [Can Automatic Post-Editing Improve NMT?](https://arxiv.org/abs/2009.14395)

Authors: [Shamil Chollampatt](https://arxiv.org/search/cs?searchtype=author&query=Chollampatt%2C+S), [Raymond Hendy Susanto](https://arxiv.org/search/cs?searchtype=author&query=Susanto%2C+R+H), [Liling Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L), [Ewa Szymanska](https://arxiv.org/search/cs?searchtype=author&query=Szymanska%2C+E)

> Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at [this https URL](https://github.com/shamilcm/pedra).

| Comments: | In EMNLP 2020                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14395](https://arxiv.org/abs/2009.14395) [cs.CL]** |
|           | (or **[arXiv:2009.14395v1](https://arxiv.org/abs/2009.14395v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-4">4. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment</h2>

Title: [Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](https://arxiv.org/abs/2009.14510)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data.

| Comments: | EMNLP-2020 Long Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2009.14510](https://arxiv.org/abs/2009.14510) [cs.CL]** |
|           | (or **[arXiv:2009.14510v1](https://arxiv.org/abs/2009.14510v1) [cs.CL]** for this version) |





<h2 id="2020-10-01-5">5. On Romanization for Model Transfer Between Scripts in Neural Machine Translation</h2>

Title: [On Romanization for Model Transfer Between Scripts in Neural Machine Translation](https://arxiv.org/abs/2009.14824)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.

| Comments: | accepted at Findings of EMNLP 2020                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2009.14824](https://arxiv.org/abs/2009.14824) [cs.CL]** |
|           | (or **[arXiv:2009.14824v1](https://arxiv.org/abs/2009.14824v1) [cs.CL]** for this version) |

