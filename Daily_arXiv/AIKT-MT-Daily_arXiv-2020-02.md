# Daily arXiv: Machine Translation - Feb., 2020

# Index

- [2020-02-28](#2020-02-28)
  - [1. SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech Translation](#2020-02-28-1)
  - [2. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](#2020-02-28-2)
  - [3. Echo State Neural Machine Translation](#2020-02-28-3)
  - [4. Binarized PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces](#2020-02-28-4)
  - [5. Improving cross-lingual model transfer by chunking](#2020-02-28-5)
- [2020-02-27](#2020-02-27)
  - [1. Marathi To English Neural Machine Translation With Near Perfect Corpus And Transformers](#2020-02-27-1)
- [2020-02-26](#2020-02-26)
  - [1. MuST-Cinema: a Speech-to-Subtitles corpus](#2020-02-26-1)
  - [2. Language-Independent Tokenisation Rivals Language-Specific Tokenisation for Word Similarity Prediction](#2020-02-26-2)
- [2020-02-25](#2020-02-25)
  - [1. Modelling Latent Skills for Multitask Language Generation](#2020-02-25-1)
  - [2. Machine Translation System Selection from Bandit Feedback](#2020-02-25-2)
  - [3. GRET: Global Representation Enhanced Transformer](#2020-02-25-3)
  - [4. Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation](#2020-02-25-4)
  - [5. Word Embeddings Inherently Recover the Conceptual Organization of the Human Mind](#2020-02-25-5)
  - [6. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation](#2020-02-25-6)
- [2020-02-24](#2020-02-24)
  - [1. Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration](#2020-02-24-1)
  - [2. Accessing Higher-level Representations in Sequential Transformers with Feedback Memory](#2020-02-24-2)
  - [3. Refinement of Unsupervised Cross-Lingual Word Embeddings](#2020-02-24-3)
  - [4. Is Aligning Embedding Spaces a Challenging Task? An Analysis of the Existing Methods](#2020-02-24-4)
- [2020-02-21](#2020-02-21)
  - [1. Balancing Cost and Benefit with Tied-Multi Transformers](#2020-02-21-1)
  - [2. Contextual Lensing of Universal Sentence Representations](#2020-02-21-2)
  - [3. Compositional Neural Machine Translation by Removing the Lexicon from Syntax](#2020-02-21-3)
- [2020-02-20](#2020-02-20)
  - [1. The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding](#2020-02-20-1)
  - [2. Toward Making the Most of Context in Neural Machine Translation](#2020-02-20-2)
- [2020-02-19](#2020-02-19)
  - [1. From English To Foreign Languages: Transferring Pre-trained Language Models](#2020-02-19-1)
  - [2. A Survey of Deep Learning Techniques for Neural Machine Translation](#2020-02-19-2)
- [2020-02-18](#2020-02-18)
  - [1. Supervised Phrase-boundary Embeddings](#2020-02-18-1)
  - [2. Neural Machine Translation with Joint Representation](#2020-02-18-2)
  - [3. Multi-layer Representation Fusion for Neural Machine Translation](#2020-02-18-3)
  - [4. Incorporating BERT into Neural Machine Translation](#2020-02-18-4)
- [2020-02-17](#2020-02-17)
  - [1. Transformers as Soft Reasoners over Language](#2020-02-17-1)
  - [2. Transformer on a Diet](#2020-02-17-2)
- [2020-02-13](#2020-02-13)
  - [1. Superbloom: Bloom filter meets Transformer](#2020-02-13-1)
  - [2. On Layer Normalization in the Transformer Architecture](#2020-02-13-2)
- [2020-02-12](#2020-02-12)
  - [1. Learning Coupled Policies for Simultaneous Machine Translation](#2020-02-12-1)
- [2020-02-11](#2020-02-11)
  - [1. Blank Language Models](#2020-02-11-1)
  - [2. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention](#2020-02-11-2)
  - [3. Multilingual Alignment of Contextual Word Representations](#2020-02-11-3)


- [2020-02-10](#2020-02-10)

  - [1. Neural Machine Translation System of Indic Languages -- An Attention based Approach](#2020-02-10-1)
  - [2. A Multilingual View of Unsupervised Machine Translation](#2020-02-10-2)
- [2020-02-06](#2020-02-06)
- [1. Multilingual acoustic word embedding models for processing zero-resource languages](#2020-02-06-1)
  - [2. Irony Detection in a Multilingual Context](#2020-02-06-2)
- [2020-02-05](#2020-02-05)

  - [1. CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus](#2020-02-05-1)
- [2020-02-04](#2020-02-04)

  - [1. Unsupervised Bilingual Lexicon Induction Across Writing Systems](#2020-02-04-1)
  - [2. Citation Text Generation](#2020-02-04-2)
  - [3. Unsupervised Multilingual Alignment using Wasserstein Barycenter](#2020-02-04-3)
  - [4. Joint Contextual Modeling for ASR Correction and Language Understanding](#2020-02-04-4)
  - [5. FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications](#2020-02-04-5)
  - [6. Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance](#2020-02-04-6)
- [2020-02-03](#2020-02-03)

  - [1. Self-Adversarial Learning with Comparative Discrimination for Text Generation](#2020-02-03-1)
  - [2. Teaching Machines to Converse](#2020-02-03-2)
- [2020-01](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2020-01.md)
- [2019-12](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-12.md)
- [2019-11](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-11.md)
- [2019-10](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-10.md)
- [2019-09](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-09.md)
- [2019-08](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-08.md)
- [2019-07](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-07.md)
- [2019-06](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-06.md)
- [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
- [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
- [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)



# 2020-02-28

[Return to Index](#Index)



<h2 id="2020-02-28-1">1. SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech Translation</h2>

Title: [SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech Translation](https://arxiv.org/abs/2002.12231)

Authors: [Arya D. McCarthy](https://arxiv.org/search/eess?searchtype=author&query=McCarthy%2C+A+D), [Liezl Puzon](https://arxiv.org/search/eess?searchtype=author&query=Puzon%2C+L), [Juan Pino](https://arxiv.org/search/eess?searchtype=author&query=Pino%2C+J)

*(Submitted on 27 Feb 2020)*

> We propose autoencoding speaker conversion for training data augmentation in automatic speech translation. This technique directly transforms an audio sequence, resulting in audio synthesized to resemble another speaker's voice. Our method compares favorably to SpecAugment on English*[Math Processing Error]*French and English*[Math Processing Error]*Romanian automatic speech translation (AST) tasks as well as on a low-resource English automatic speech recognition (ASR) task. Further, in ablations, we show the benefits of both quantity and diversity in augmented data. Finally, we show that we can combine our approach with augmentation by machine-translated transcripts to obtain a competitive end-to-end AST model that outperforms a very strong cascade model on an English*[Math Processing Error]*French AST task. Our method is sufficiently general that it can be applied to other speech generation and analysis tasks.

| Comments: | Accepted to ICASSP 2020                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | [arXiv:2002.12231](https://arxiv.org/abs/2002.12231) [eess.AS] |
|           | (or [arXiv:2002.12231v1](https://arxiv.org/abs/2002.12231v1) [eess.AS] for this version) |





<h2 id="2020-02-28-2">2. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</h2>

Title: [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)

Authors: [Zhuohan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Eric Wallace](https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E), [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Kevin Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D), [Joseph E. Gonzalez](https://arxiv.org/search/cs?searchtype=author&query=Gonzalez%2C+J+E)

*(Submitted on 26 Feb 2020)*

> Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations.
> This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.11794](https://arxiv.org/abs/2002.11794) [cs.CL] |
|           | (or [arXiv:2002.11794v1](https://arxiv.org/abs/2002.11794v1) [cs.CL] for this version) |





<h2 id="2020-02-28-3">3. Echo State Neural Machine Translation</h2>

Title: [Echo State Neural Machine Translation](https://arxiv.org/abs/2002.11847)

Authors: [Ankush Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+A), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Qi Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Q)

*(Submitted on 27 Feb 2020)*

> We present neural machine translation (NMT) models inspired by echo state network (ESN), named Echo State NMT (ESNMT), in which the encoder and decoder layer weights are randomly generated then fixed throughout training. We show that even with this extremely simple model construction and training procedure, ESNMT can already reach 70-80% quality of fully trainable baselines. We examine how spectral radius of the reservoir, a key quantity that characterizes the model, determines the model behavior. Our findings indicate that randomized networks can work well even for complicated sequence-to-sequence prediction NLP tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.11847](https://arxiv.org/abs/2002.11847) [cs.CL] |
|           | (or [arXiv:2002.11847v1](https://arxiv.org/abs/2002.11847v1) [cs.CL] for this version) |





<h2 id="2020-02-28-4">4. Binarized PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces</h2>

Title: [Binarized PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces](https://arxiv.org/abs/2002.12005)

Authors: [Zhenisbek Assylbekov](https://arxiv.org/search/cs?searchtype=author&query=Assylbekov%2C+Z), [Alibi Jangeldin](https://arxiv.org/search/cs?searchtype=author&query=Jangeldin%2C+A)

*(Submitted on 27 Feb 2020)*

> We show analytically that removing sigmoid transformation in the SGNS objective does not harm the quality of word vectors significantly and at the same time is related to factorizing a binarized PMI matrix which, in turn, can be treated as an adjacency matrix of a certain graph. Empirically, such graph is a complex network, i.e. it has strong clustering and scale-free degree distribution, and is tightly connected with hyperbolic spaces. In short, we show the connection between static word embeddings and hyperbolic spaces through the binarized PMI matrix using analytical and empirical methods.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.12005](https://arxiv.org/abs/2002.12005) [cs.CL] |
|           | (or [arXiv:2002.12005v1](https://arxiv.org/abs/2002.12005v1) [cs.CL] for this version) |





<h2 id="2020-02-28-5">5. Improving cross-lingual model transfer by chunking</h2>

Title: [Improving cross-lingual model transfer by chunking](https://arxiv.org/abs/2002.12097)

Authors: [Ayan Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+A), [Sudeshna Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+S)

*(Submitted on 27 Feb 2020)*

> We present a shallow parser guided cross-lingual model transfer approach in order to address the syntactic differences between source and target languages more effectively. In this work, we assume the chunks or phrases in a sentence as transfer units in order to address the syntactic differences between the source and target languages arising due to the differences in ordering of words in the phrases and the ordering of phrases in a sentence separately.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.12097](https://arxiv.org/abs/2002.12097) [cs.CL] |
|           | (or [arXiv:2002.12097v1](https://arxiv.org/abs/2002.12097v1) [cs.CL] for this version) |





# 2020-02-27

[Return to Index](#Index)



<h2 id="2020-02-27-1">1. Marathi To English Neural Machine Translation With Near Perfect Corpus And Transformers</h2>

Title: [Marathi To English Neural Machine Translation With Near Perfect Corpus And Transformers](https://arxiv.org/abs/2002.11643)

Authors: [Swapnil Ashok Jadhav](https://arxiv.org/search/cs?searchtype=author&query=Jadhav%2C+S+A)

*(Submitted on 26 Feb 2020)*

> There have been very few attempts to benchmark performances of state-of-the-art algorithms for Neural Machine Translation task on Indian Languages. Google, Bing, Facebook and Yandex are some of the very few companies which have built translation systems for few of the Indian Languages. Among them, translation results from Google are supposed to be better, based on general inspection. Bing-Translator do not even support Marathi language which has around 95 million speakers and ranks 15th in the world in terms of combined primary and secondary speakers. In this exercise, we trained and compared variety of Neural Machine Marathi to English Translators trained with BERT-tokenizer by huggingface and various Transformer based architectures using Facebook's Fairseq platform with limited but almost correct parallel corpus to achieve better BLEU scores than Google on Tatoeba and Wikimedia open datasets.

| Comments: | 5 pages, 5 tables. This report is based on applied research work done at Dailyhunt |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.11643](https://arxiv.org/abs/2002.11643) [cs.CL] |
|           | (or [arXiv:2002.11643v1](https://arxiv.org/abs/2002.11643v1) [cs.CL] for this version) |







# 2020-02-26

[Return to Index](#Index)



<h2 id="2020-02-26-1">1. MuST-Cinema: a Speech-to-Subtitles corpus</h2>

Title: [MuST-Cinema: a Speech-to-Subtitles corpus](https://arxiv.org/abs/2002.10829)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

*(Submitted on 25 Feb 2020)*

> Growing needs in localising audiovisual content in multiple languages through subtitles call for the development of automatic solutions for human subtitling. Neural Machine Translation (NMT) can contribute to the automatisation of subtitling, facilitating the work of human subtitlers and reducing turn-around times and related costs. NMT requires high-quality, large, task-specific training data. The existing subtitling corpora, however, are missing both alignments to the source language audio and important information about subtitle breaks. This poses a significant limitation for developing efficient automatic approaches for subtitling, since the length and form of a subtitle directly depends on the duration of the utterance. In this work, we present MuST-Cinema, a multilingual speech translation corpus built from TED subtitles. The corpus is comprised of (audio, transcription, translation) triplets. Subtitle breaks are preserved by inserting special symbols. We show that the corpus can be used to build models that efficiently segment sentences into subtitles and propose a method for annotating existing subtitling corpora with subtitle breaks, conforming to the constraint of length.

| Comments: | Accepted at LREC 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.10829](https://arxiv.org/abs/2002.10829) [cs.CL] |
|           | (or [arXiv:2002.10829v1](https://arxiv.org/abs/2002.10829v1) [cs.CL] for this version) |







<h2 id="2020-02-26-2">2. Language-Independent Tokenisation Rivals Language-Specific Tokenisation for Word Similarity Prediction</h2>

Title: [Language-Independent Tokenisation Rivals Language-Specific Tokenisation for Word Similarity Prediction](https://arxiv.org/abs/2002.11004)

Authors: [Danushka Bollegala](https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D), [Ryuichi Kiryo](https://arxiv.org/search/cs?searchtype=author&query=Kiryo%2C+R), [Kosuke Tsujino](https://arxiv.org/search/cs?searchtype=author&query=Tsujino%2C+K), [Haruki Yukawa](https://arxiv.org/search/cs?searchtype=author&query=Yukawa%2C+H)

*(Submitted on 25 Feb 2020)*

> Language-independent tokenisation (LIT) methods that do not require labelled language resources or lexicons have recently gained popularity because of their applicability in resource-poor languages. Moreover, they compactly represent a language using a fixed size vocabulary and can efficiently handle unseen or rare words. On the other hand, language-specific tokenisation (LST) methods have a long and established history, and are developed using carefully created lexicons and training resources. Unlike subtokens produced by LIT methods, LST methods produce valid morphological subwords. Despite the contrasting trade-offs between LIT vs. LST methods, their performance on downstream NLP tasks remain unclear. In this paper, we empirically compare the two approaches using semantic similarity measurement as an evaluation task across a diverse set of languages. Our experimental results covering eight languages show that LST consistently outperforms LIT when the vocabulary size is large, but LIT can produce comparable or better results than LST in many languages with comparatively smaller (i.e. less than 100K words) vocabulary sizes, encouraging the use of LIT when language-specific resources are unavailable, incomplete or a smaller model is required. Moreover, we find that smoothed inverse frequency (SIF) to be an accurate method to create word embeddings from subword embeddings for multilingual semantic similarity prediction tasks. Further analysis of the nearest neighbours of tokens show that semantically and syntactically related tokens are closely embedded in subword embedding spaces

| Comments: | To appear in the 12th Language Resources and Evaluation (LREC 2020) Conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.11004](https://arxiv.org/abs/2002.11004) [cs.CL] |
|           | (or [arXiv:2002.11004v1](https://arxiv.org/abs/2002.11004v1) [cs.CL] for this version) |





# 2020-02-25

[Return to Index](#Index)



<h2 id="2020-02-25-1">1. Modelling Latent Skills for Multitask Language Generation</h2>

Title: [Modelling Latent Skills for Multitask Language Generation](https://arxiv.org/abs/2002.09543)

Authors:  [Kris Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+K), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D)

*(Submitted on 21 Feb 2020)*

> We present a generative model for multitask conditional language generation. Our guiding hypothesis is that a shared set of latent skills underlies many disparate language generation tasks, and that explicitly modelling these skills in a task embedding space can help with both positive transfer across tasks and with efficient adaptation to new tasks. We instantiate this task embedding space as a latent variable in a latent variable sequence-to-sequence model. We evaluate this hypothesis by curating a series of monolingual text-to-text language generation datasets - covering a broad range of tasks and domains - and comparing the performance of models both in the multitask and few-shot regimes. We show that our latent task variable model outperforms other sequence-to-sequence baselines on average across tasks in the multitask setting. In the few-shot learning setting on an unseen test dataset (i.e., a new task), we demonstrate that model adaptation based on inference in the latent task space is more robust than standard fine-tuning based parameter adaptation and performs comparably in terms of overall performance. Finally, we examine the latent task representations learnt by our model and show that they cluster tasks in a natural way.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.09543](https://arxiv.org/abs/2002.09543) [cs.CL] |
|           | (or [arXiv:2002.09543v1](https://arxiv.org/abs/2002.09543v1) [cs.CL] for this version) |





<h2 id="2020-02-25-2">2. Machine Translation System Selection from Bandit Feedback</h2>

Title: [Machine Translation System Selection from Bandit Feedback](https://arxiv.org/abs/2002.09646)

Authors:  [Jason Naradowsky](https://arxiv.org/search/cs?searchtype=author&query=Naradowsky%2C+J), [Xuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K)

*(Submitted on 22 Feb 2020)*

> Adapting machine translation systems in the real world is a difficult problem. In contrast to offline training, users cannot provide the type of fine-grained feedback typically used for improving the system. Moreover, users have different translation needs, and even a single user's needs may change over time.
> In this work we take a different approach, treating the problem of adapting as one of selection. Instead of adapting a single system, we train many translation systems using different architectures and data partitions. Using bandit learning techniques on simulated user feedback, we learn a policy to choose which system to use for a particular translation task. We show that our approach can (1) quickly adapt to address domain changes in translation tasks, (2) outperform the single best system in mixed-domain translation tasks, and (3) make effective instance-specific decisions when using contextual bandit strategies.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.09646](https://arxiv.org/abs/2002.09646) [cs.CL] |
|           | (or [arXiv:2002.09646v1](https://arxiv.org/abs/2002.09646v1) [cs.CL] for this version) |





<h2 id="2020-02-25-3">3. GRET: Global Representation Enhanced Transformer</h2>

Title: [GRET: Global Representation Enhanced Transformer](https://arxiv.org/abs/2002.10101)

Authors:  [Rongxiang Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R), [Haoran Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Lidong Bing](https://arxiv.org/search/cs?searchtype=author&query=Bing%2C+L), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

*(Submitted on 24 Feb 2020)*

> Transformer, based on the encoder-decoder framework, has achieved state-of-the-art performance on several natural language generation tasks. The encoder maps the words in the input sentence into a sequence of hidden states, which are then fed into the decoder to generate the output sentence. These hidden states usually correspond to the input words and focus on capturing local information. However, the global (sentence level) information is seldom explored, leaving room for the improvement of generation quality. In this paper, we propose a novel global representation enhanced Transformer (GRET) to explicitly model global representation in the Transformer network. Specifically, in the proposed model, an external state is generated for the global representation from the encoder. The global representation is then fused into the decoder during the decoding process to improve generation quality. We conduct experiments in two text generation tasks: machine translation and text summarization. Experimental results on four WMT machine translation tasks and LCSTS text summarization task demonstrate the effectiveness of the proposed approach on natural language generation.

| Comments: | Accepted by AAAI 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.10101](https://arxiv.org/abs/2002.10101) [cs.CL] |
|           | (or [arXiv:2002.10101v1](https://arxiv.org/abs/2002.10101v1) [cs.CL] for this version) |





<h2 id="2020-02-25-4">4. Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation</h2>

Title: [Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation](https://arxiv.org/abs/2002.10260)

Authors:  [Alessandro Raganato](https://arxiv.org/search/cs?searchtype=author&query=Raganato%2C+A), [Yves Scherrer](https://arxiv.org/search/cs?searchtype=author&query=Scherrer%2C+Y), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

*(Submitted on 24 Feb 2020)*

> Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that attention heads learn simple positional patterns which are often redundant. In this paper, we propose to replace all but one attention head of each encoder layer with fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.10260](https://arxiv.org/abs/2002.10260) [cs.CL] |
|           | (or [arXiv:2002.10260v1](https://arxiv.org/abs/2002.10260v1) [cs.CL] for this version) |





<h2 id="2020-02-25-5">5. Word Embeddings Inherently Recover the Conceptual Organization of the Human Mind</h2>

Title: [Word Embeddings Inherently Recover the Conceptual Organization of the Human Mind](https://arxiv.org/abs/2002.10284)

Authors:  [Victor Swift](https://arxiv.org/search/cs?searchtype=author&query=Swift%2C+V)

*(Submitted on 6 Feb 2020)*

> Machine learning is a means to uncover deep patterns from rich sources of data. Here, we find that machine learning can recover the conceptual organization of the human mind when applied to the natural language use of millions of people. Utilizing text from billions of webpages, we recover most of the concepts contained in English, Dutch, and Japanese, as represented in large scale Word Association networks. Our results justify machine learning as a means to probe the human mind, at a depth and scale that has been unattainable using self-report and observational methods. Beyond direct psychological applications, our methods may prove useful for projects concerned with defining, assessing, relating, or uncovering concepts in any scientific field.

| Comments: | 12 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neurons and Cognition (q-bio.NC) |
| Cite as:  | [arXiv:2002.10284](https://arxiv.org/abs/2002.10284) [cs.CL] |
|           | (or [arXiv:2002.10284v1](https://arxiv.org/abs/2002.10284v1) [cs.CL] for this version) |





<h2 id="2020-02-25-6">6. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation
</h2>

Title: [Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation](https://arxiv.org/abs/2002.10345)

Authors:  [Yige Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X), [Ligao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

*(Submitted on 24 Feb 2020)*

> Fine-tuning pre-trained language models like BERT has become an effective way in NLP and yields state-of-the-art results on many downstream tasks. Recent studies on adapting BERT to new tasks mainly focus on modifying the model structure, re-designing the pre-train tasks, and leveraging external data and knowledge. The fine-tuning strategy itself has yet to be fully explored. In this paper, we improve the fine-tuning of BERT with two effective mechanisms: self-ensemble and self-distillation. The experiments on text classification and natural language inference tasks show our proposed methods can significantly improve the adaption of BERT without any external data or knowledge.

| Comments: | 7 pages, 6 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.10345](https://arxiv.org/abs/2002.10345) [cs.CL] |
|           | (or [arXiv:2002.10345v1](https://arxiv.org/abs/2002.10345v1) [cs.CL] for this version) |





# 2020-02-24

[Return to Index](#Index)



<h2 id="2020-02-24-1">1. Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration</h2>

Title: [Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration](https://arxiv.org/abs/2002.09253)

Authors: [Cédric Colas](https://arxiv.org/search/cs?searchtype=author&query=Colas%2C+C), [Tristan Karch](https://arxiv.org/search/cs?searchtype=author&query=Karch%2C+T), [Nicolas Lair](https://arxiv.org/search/cs?searchtype=author&query=Lair%2C+N), [Jean-Michel Dussoux](https://arxiv.org/search/cs?searchtype=author&query=Dussoux%2C+J), [Clément Moulin-Frier](https://arxiv.org/search/cs?searchtype=author&query=Moulin-Frier%2C+C), [Peter Ford Dominey](https://arxiv.org/search/cs?searchtype=author&query=Dominey%2C+P+F), [Pierre-Yves Oudeyer](https://arxiv.org/search/cs?searchtype=author&query=Oudeyer%2C+P)

*(Submitted on 21 Feb 2020)*

> Autonomous reinforcement learning agents must be intrinsically motivated to explore their environment, discover potential goals, represent them and learn how to achieve them. As children do the same, they benefit from exposure to language, using it to formulate goals and imagine new ones as they learn their meaning. In our proposed learning architecture (IMAGINE), the agent freely explores its environment and turns natural language descriptions of interesting interactions from a social partner into potential goals. IMAGINE learns to represent goals by jointly learning a language model and a goal-conditioned reward function. Just like humans, our agent uses language compositionality to generate new goals by composing known ones. Leveraging modular model architectures based on Deep Sets and gated-attention mechanisms, IMAGINE autonomously builds a repertoire of behaviors and shows good zero-shot generalization properties for various types of generalization. When imagining its own goals, the agent leverages zero-shot generalization of the reward function to further train on imagined goals and refine its behavior. We present experiments in a simulated domain where the agent interacts with procedurally generated scenes containing objects of various types and colors, discovers goals, imagines others and learns to achieve them.

| Comments: | Contains main article and supplementaries                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | [arXiv:2002.09253](https://arxiv.org/abs/2002.09253) [cs.AI] |
|           | (or [arXiv:2002.09253v1](https://arxiv.org/abs/2002.09253v1) [cs.AI] for this version) |





<h2 id="2020-02-24-2">2. Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</h2>

Title: [Accessing Higher-level Representations in Sequential Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)

Authors: [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [Thibaut Lavril](https://arxiv.org/search/cs?searchtype=author&query=Lavril%2C+T), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave%2C+E), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A), [Sainbayar Sukhbaatar](https://arxiv.org/search/cs?searchtype=author&query=Sukhbaatar%2C+S)

*(Submitted on 21 Feb 2020)*

> Transformers are feedforward networks that can process input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input - the representation at a given layer can only access representations from lower layers, rather than the higher level representations already built in previous time steps. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, neural machine translation, summarization, and reinforcement learning that the increased representation capacity can improve over Transformer baselines.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.09402](https://arxiv.org/abs/2002.09402) [cs.LG] |
|           | (or [arXiv:2002.09402v1](https://arxiv.org/abs/2002.09402v1) [cs.LG] for this version) |





<h2 id="2020-02-24-3">3. Refinement of Unsupervised Cross-Lingual Word Embeddings</h2>

Title: [Refinement of Unsupervised Cross-Lingual Word Embeddings](https://arxiv.org/abs/2002.09213)

Authors: [Magdalena Biesialska](https://arxiv.org/search/cs?searchtype=author&query=Biesialska%2C+M), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

*(Submitted on 21 Feb 2020)*

> Cross-lingual word embeddings aim to bridge the gap between high-resource and low-resource languages by allowing to learn multilingual word representations even without using any direct bilingual signal. The lion's share of the methods are projection-based approaches that map pre-trained embeddings into a shared latent space. These methods are mostly based on the orthogonal transformation, which assumes language vector spaces to be isomorphic. However, this criterion does not necessarily hold, especially for morphologically-rich languages. In this paper, we propose a self-supervised method to refine the alignment of unsupervised bilingual word embeddings. The proposed model moves vectors of words and their corresponding translations closer to each other as well as enforces length- and center-invariance, thus allowing to better align cross-lingual embeddings. The experimental results demonstrate the effectiveness of our approach, as in most cases it outperforms state-of-the-art methods in a bilingual lexicon induction task.

| Comments: | Accepted at the 24th European Conference on Artificial Intelligence (ECAI 2020) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.09213](https://arxiv.org/abs/2002.09213) [cs.CL] |
|           | (or [arXiv:2002.09213v1](https://arxiv.org/abs/2002.09213v1) [cs.CL] for this version) |





<h2 id="2020-02-24-4">4. Is Aligning Embedding Spaces a Challenging Task? An Analysis of the Existing Methods</h2>

Title: [Is Aligning Embedding Spaces a Challenging Task? An Analysis of the Existing Methods](https://arxiv.org/abs/2002.09247)

Authors: [Russa Biswas](https://arxiv.org/search/cs?searchtype=author&query=Biswas%2C+R), [Mehwish Alam](https://arxiv.org/search/cs?searchtype=author&query=Alam%2C+M), [Harald Sack](https://arxiv.org/search/cs?searchtype=author&query=Sack%2C+H)

*(Submitted on 21 Feb 2020)*

> Representation Learning of words and Knowledge Graphs (KG) into low dimensional vector spaces along with its applications to many real-world scenarios have recently gained momentum. In order to make use of multiple KG embeddings for knowledge-driven applications such as question answering, named entity disambiguation, knowledge graph completion, etc., alignment of different KG embedding spaces is necessary. In addition to multilinguality and domain-specific information, different KGs pose the problem of structural differences making the alignment of the KG embeddings more challenging. This paper provides a theoretical analysis and comparison of the state-of-the-art alignment methods between two embedding spaces representing entity-entity and entity-word. This paper also aims at assessing the capability and short-comings of the existing alignment methods on the pretext of different applications.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.09247](https://arxiv.org/abs/2002.09247) [cs.CL] |
|           | (or [arXiv:2002.09247v1](https://arxiv.org/abs/2002.09247v1) [cs.CL] for this version) |





# 2020-02-21

[Return to Index](#Index)



<h2 id="2020-02-21-1">1. Balancing Cost and Benefit with Tied-Multi Transformers</h2>

Title: [Balancing Cost and Benefit with Tied-Multi Transformers](https://arxiv.org/abs/2002.08614)

Authors:  [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Raphael Rubino](https://arxiv.org/search/cs?searchtype=author&query=Rubino%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

*(Submitted on 20 Feb 2020)*

> We propose and evaluate a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In sequence-to-sequence modeling, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes NxM models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. We then propose a mechanism to choose a priori the number of encoder and decoder layers for faster decoding, and also explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality.

| Comments: | Extended version of our previous manuscript available at [arXiv:1908.10118](https://arxiv.org/abs/1908.10118) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.08614](https://arxiv.org/abs/2002.08614) [cs.CL] |
|           | (or [arXiv:2002.08614v1](https://arxiv.org/abs/2002.08614v1) [cs.CL] for this version) |





<h2 id="2020-02-21-2">2. Contextual Lensing of Universal Sentence Representations</h2>

Title: [Contextual Lensing of Universal Sentence Representations](https://arxiv.org/abs/2002.08866)

Authors:  [Jamie Kiros](https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J)

*(Submitted on 20 Feb 2020)*

> What makes a universal sentence encoder universal? The notion of a generic encoder of text appears to be at odds with the inherent contextualization and non-permanence of language use in a dynamic world. However, mapping sentences into generic fixed-length vectors for downstream similarity and retrieval tasks has been fruitful, particularly for multilingual applications. How do we manage this dilemma? In this work we propose Contextual Lensing, a methodology for inducing context-oriented universal sentence vectors. We break the construction of universal sentence vectors into a core, variable length, sentence matrix representation equipped with an adaptable `lens' from which fixed-length vectors can be induced as a function of the lens context. We show that it is possible to focus notions of language similarity into a small number of lens parameters given a core universal matrix representation. For example, we demonstrate the ability to encode translation similarity of sentences across several languages into a single weight matrix, even when the core encoder has not seen parallel data.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.08866](https://arxiv.org/abs/2002.08866) [cs.CL] |
|           | (or [arXiv:2002.08866v1](https://arxiv.org/abs/2002.08866v1) [cs.CL] for this version) |





<h2 id="2020-02-21-3">3. Compositional Neural Machine Translation by Removing the Lexicon from Syntax</h2>

Title: [Compositional Neural Machine Translation by Removing the Lexicon from Syntax](https://arxiv.org/abs/2002.08899)

Authors:  [Tristan Thrush](https://arxiv.org/search/cs?searchtype=author&query=Thrush%2C+T)

*(Submitted on 6 Feb 2020)*

> The meaning of a natural language utterance is largely determined from its syntax and words. Additionally, there is evidence that humans process an utterance by separating knowledge about the lexicon from syntax knowledge. Theories from semantics and neuroscience claim that complete word meanings are not encoded in the representation of syntax. In this paper, we propose neural units that can enforce this constraint over an LSTM encoder and decoder. We demonstrate that our model achieves competitive performance across a variety of domains including semantic parsing, syntactic parsing, and English to Mandarin Chinese translation. In these cases, our model outperforms the standard LSTM encoder and decoder architecture on many or all of our metrics. To demonstrate that our model achieves the desired separation between the lexicon and syntax, we analyze its weights and explore its behavior when different neural modules are damaged. When damaged, we find that the model displays the knowledge distortions that aphasics are evidenced to have.

| Comments: | natural language processing; adversarial neural networks; machine translation; aphasia; neural attention |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.08899](https://arxiv.org/abs/2002.08899) [cs.CL] |
|           | (or [arXiv:2002.08899v1](https://arxiv.org/abs/2002.08899v1) [cs.CL] for this version) |





# 2020-02-20

[Return to Index](#Index)



<h2 id="2020-02-20-1">1. The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding</h2>

Title: [The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding]()

Authors: [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Yu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Jianshu Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+J), [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Xueyun Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X), [Emmanuel Awa](https://arxiv.org/search/cs?searchtype=author&query=Awa%2C+E), [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Hoifung Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H), [Guihong Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+G), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

*(Submitted on 19 Feb 2020)*

> We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at [this https URL](https://github.com/namisan/mt-dnn).

| Comments: | 9 pages, 3 figures and 3 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.07972](https://arxiv.org/abs/2002.07972) [cs.CL] |
|           | (or [arXiv:2002.07972v1](https://arxiv.org/abs/2002.07972v1) [cs.CL] for this version) |





<h2 id="2020-02-20-2">2. Toward Making the Most of Context in Neural Machine Translation</h2>

Title: [Toward Making the Most of Context in Neural Machine Translation](https://arxiv.org/abs/2002.07982)

Authors: [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Xiang Yue](https://arxiv.org/search/cs?searchtype=author&query=Yue%2C+X), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

*(Submitted on 19 Feb 2020)*

> Document-level machine translation manages to outperform sentence level models by a small margin, but have failed to be widely adopted. We argue that previous research did not make a clear use of the global context, and propose a new document-level NMT framework that deliberately models the local context of each sentence with the awareness of the global context of the document in both source and target languages. We specifically design the model to be able to deal with documents containing any number of sentences, including single sentences. This unified approach allows our model to be trained elegantly on standard datasets without needing to train on sentence and document level data separately. Experimental results demonstrate that our model outperforms Transformer baselines and previous document-level NMT models with substantial margins of up to 2.1 BLEU on state-of-the-art baselines. We also provide analyses which show the benefit of context far beyond the neighboring two or three sentences, which previous studies have typically incorporated.

| Comments: | Submitted to a conference                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.07982](https://arxiv.org/abs/2002.07982) [cs.CL] |
|           | (or [arXiv:2002.07982v1](https://arxiv.org/abs/2002.07982v1) [cs.CL] for this version) |





# 2020-02-19

[Return to Index](#Index)



<h2 id="2020-02-19-1">1. From English To Foreign Languages: Transferring Pre-trained Language Models</h2>

Title: [From English To Foreign Languages: Transferring Pre-trained Language Models]()

Authors: [Ke Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+K)

*(Submitted on 18 Feb 2020)*

> Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT base model within a day and a foreign BERT large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.07306](https://arxiv.org/abs/2002.07306) [cs.CL] |
|           | (or [arXiv:2002.07306v1](https://arxiv.org/abs/2002.07306v1) [cs.CL] for this version) |





<h2 id="2020-02-19-2">2. A Survey of Deep Learning Techniques for Neural Machine Translation</h2>

Title: [A Survey of Deep Learning Techniques for Neural Machine Translation](https://arxiv.org/abs/2002.07526)

Authors: [Shuoheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Yuxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xiaowen Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+X)

*(Submitted on 18 Feb 2020)*

> In recent years, natural language processing (NLP) has got great development with deep learning techniques. In the sub-field of machine translation, a new approach named Neural Machine Translation (NMT) has emerged and got massive attention from both academia and industry. However, with a significant number of researches proposed in the past several years, there is little work in investigating the development process of this new technology trend. This literature survey traces back the origin and principal development timeline of NMT, investigates the important branches, categorizes different research orientations, and discusses some future research trends in this field.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.07526](https://arxiv.org/abs/2002.07526) [cs.CL] |
|           | (or [arXiv:2002.07526v1](https://arxiv.org/abs/2002.07526v1) [cs.CL] for this version) |



# 2020-02-18

[Return to Index](#Index)



<h2 id="2020-02-18-1">1. Supervised Phrase-boundary Embeddings</h2>

Title: [Supervised Phrase-boundary Embeddings](https://arxiv.org/abs/2002.06450)

Authors: [Manni Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M), [David Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+D), [Mark Levene](https://arxiv.org/search/cs?searchtype=author&query=Levene%2C+M)

*(Submitted on 15 Feb 2020)*

> We propose a new word embedding model, called SPhrase, that incorporates supervised phrase information. Our method modifies traditional word embeddings by ensuring that all target words in a phrase have exactly the same context. We demonstrate that including this information within a context window produces superior embeddings for both intrinsic evaluation tasks and downstream extrinsic tasks.

| Comments: | 12 pages, 3 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | [arXiv:2002.06450](https://arxiv.org/abs/2002.06450) [cs.AI] |
|           | (or [arXiv:2002.06450v1](https://arxiv.org/abs/2002.06450v1) [cs.AI] for this version) |





<h2 id="2020-02-18-2">2. Neural Machine Translation with Joint Representation</h2>

Title: [Neural Machine Translation with Joint Representation](https://arxiv.org/abs/2002.06546)

Authors: [YanYang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Tongran Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 16 Feb 2020)*

> Though early successes of Statistical Machine Translation (SMT) systems are attributed in part to the explicit modelling of the interaction between any two source and target units, e.g., alignment, the recent Neural Machine Translation (NMT) systems resort to the attention which partially encodes the interaction for efficiency. In this paper, we employ Joint Representation that fully accounts for each possible interaction. We sidestep the inefficiency issue by refining representations with the proposed efficient attention operation. The resulting Reformer models offer a new Sequence-to- Sequence modelling paradigm besides the Encoder-Decoder framework and outperform the Transformer baseline in either the small scale IWSLT14 German-English, English-German and IWSLT15 Vietnamese-English or the large scale NIST12 Chinese-English translation tasks by about 1 BLEU point.We also propose a systematic model scaling approach, allowing the Reformer model to beat the state-of-the-art Transformer in IWSLT14 German-English and NIST12 Chinese-English with about 50% fewer parameters. The code is publicly available at [this https URL](https://github.com/lyy1994/reformer).

| Comments: | AAAI 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06546](https://arxiv.org/abs/2002.06546) [cs.CL] |
|           | (or [arXiv:2002.06546v1](https://arxiv.org/abs/2002.06546v1) [cs.CL] for this version) |





<h2 id="2020-02-18-3">3. Multi-layer Representation Fusion for Neural Machine Translation</h2>

Title: [Multi-layer Representation Fusion for Neural Machine Translation](https://arxiv.org/abs/2002.06714)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Fuxue Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Yanyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yinqiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 16 Feb 2020)*

> Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation.

| Comments: | COLING 2018                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06714](https://arxiv.org/abs/2002.06714) [cs.CL] |
|           | (or [arXiv:2002.06714v1](https://arxiv.org/abs/2002.06714v1) [cs.CL] for this version) |





<h2 id="2020-02-18-4">4. Incorporating BERT into Neural Machine Translation</h2>

Title: [Incorporating BERT into Neural Machine Translation](https://arxiv.org/abs/2002.06823)

Authors: [Jinhua Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Yingce Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y), [Lijun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Wengang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Houqiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 17 Feb 2020)*

> The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \url{[this https URL](https://github.com/bert-nmt/bert-nmt)}.

| Comments: | Accepted to ICLR-2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.06823](https://arxiv.org/abs/2002.06823) [cs.CL] |
|           | (or [arXiv:2002.06823v1](https://arxiv.org/abs/2002.06823v1) [cs.CL] for this version) |



# 2020-02-17

[Return to Index](#Index)



<h2 id="2020-02-17-1">1. Transformers as Soft Reasoners over Language</h2>

Title: [Transformers as Soft Reasoners over Language](https://arxiv.org/abs/2002.05867)

Authors: [Peter Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P), [Oyvind Tafjord](https://arxiv.org/search/cs?searchtype=author&query=Tafjord%2C+O), [Kyle Richardson](https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+K)

*(Submitted on 14 Feb 2020)*

> AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at [this http URL](http://rule-reasoning.apps.allenai.org/)

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.05867](https://arxiv.org/abs/2002.05867) [cs.CL] |
|           | (or [arXiv:2002.05867v1](https://arxiv.org/abs/2002.05867v1) [cs.CL] for this version) |





<h2 id="2020-02-17-2">2. Transformer on a Diet</h2>

Title: [Transformer on a Diet](https://arxiv.org/abs/2002.06170)

Authors: [Chenguang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Zihao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A), [Zheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J)

*(Submitted on 14 Feb 2020)*

> Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available.

| Comments: | 6 pages, 2 tables, 1 figure                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.06170](https://arxiv.org/abs/2002.06170) [cs.CL] |
|           | (or [arXiv:2002.06170v1](https://arxiv.org/abs/2002.06170v1) [cs.CL] for this version) |





# 2020-02-13

[Return to Index](#Index)



<h2 id="2020-02-13-1">1. Superbloom: Bloom filter meets Transformer</h2>

Title: [Superbloom: Bloom filter meets Transformer](https://arxiv.org/abs/2002.04723)

Authors: [John Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+J), [Qingqing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q), [Walid Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+W), [Steffen Rendle](https://arxiv.org/search/cs?searchtype=author&query=Rendle%2C+S), [Li Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

*(Submitted on 11 Feb 2020)*

> We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.04723](https://arxiv.org/abs/2002.04723) [cs.LG] |
|           | (or [arXiv:2002.04723v1](https://arxiv.org/abs/2002.04723v1) [cs.LG] for this version) |





<h2 id="2020-02-13-2">2. On Layer Normalization in the Transformer Architecture</h2>

Title: [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)

Authors:[Ruibin Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+R), [Yunchang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Di He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+D), [Kai Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+K), [Shuxin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S), [Chen Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+C), [Huishuai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yanyan Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Y), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 12 Feb 2020)*

> The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.04745](https://arxiv.org/abs/2002.04745) [cs.LG] |
|           | (or [arXiv:2002.04745v1](https://arxiv.org/abs/2002.04745v1) [cs.LG] for this version) |



# 2020-02-12

[Return to Index](#Index)



<h2 id="2020-02-12-1">1. Learning Coupled Policies for Simultaneous Machine Translation</h2>

Title: [Learning Coupled Policies for Simultaneous Machine Translation](https://arxiv.org/abs/2002.04306)

Authors: [Philip Arthur](https://arxiv.org/search/cs?searchtype=author&query=Arthur%2C+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

*(Submitted on 11 Feb 2020)*

> Abstract: In simultaneous machine translation, the system needs to incrementally generate the output translation before the input sentence ends. This is a coupled decision process consisting of a programmer and interpreter. The programmer's policy decides about when to WRITE the next output or READ the next input, and the interpreter's policy decides what word to write. We present an imitation learning (IL) approach to efficiently learn effective coupled programmer-interpreter policies. To enable IL, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. We attribute the effectiveness of the learned coupled policies to (i) scheduled sampling addressing the coupled exposure bias, and (ii) quality of oracle actions capturing enough information from the partial input before writing the output. Experiments show our method outperforms strong baselines in terms of translation quality and delay, when translating from German/Arabic/Czech/Bulgarian/Romanian to English.

| Comments: | 7 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.04306](https://arxiv.org/abs/2002.04306) [cs.CL] |
|           | (or [arXiv:2002.04306v1](https://arxiv.org/abs/2002.04306v1) [cs.CL] for this version) |





# 2020-02-11

[Return to Index](#Index)



<h2 id="2020-02-11-1">1. Blank Language Models</h2>

Title: [Blank Language Models](https://arxiv.org/abs/2002.03079)

Authors: [Tianxiao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+T), [Victor Quach](https://arxiv.org/search/cs?searchtype=author&query=Quach%2C+V), [Regina Barzilay](https://arxiv.org/search/cs?searchtype=author&query=Barzilay%2C+R), [Tommi Jaakkola](https://arxiv.org/search/cs?searchtype=author&query=Jaakkola%2C+T)

*(Submitted on 8 Feb 2020)*

> We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.03079](https://arxiv.org/abs/2002.03079) [cs.CL] |
|           | (or [arXiv:2002.03079v1](https://arxiv.org/abs/2002.03079v1) [cs.CL] for this version) |





<h2 id="2020-02-11-2">2. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention</h2>

Title: [LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention](https://arxiv.org/abs/2002.03084)

Authors: [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Arianna Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

*(Submitted on 8 Feb 2020)*

> Non-autoregressive translation (NAT) models generate multiple tokens in one forward pass and is highly efficient at inference stage compared with autoregressive translation (AT) methods. However, NAT models often suffer from the multimodality problem, i.e., generating duplicated tokens or missing tokens. In this paper, we propose two novel methods to address this issue, the Look-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The Look-Around strategy predicts the neighbor tokens in order to predict the current token, and the Vocabulary Attention models long-term token dependencies inside the decoder by attending the whole vocabulary for each position to acquire knowledge of which token is about to generate. %We also propose a dynamic bidirectional decoding approach to accelerate the inference process of the LAVA model while preserving the high-quality of the generated output. Our proposed model uses significantly less time during inference compared with autoregressive models and most other NAT models. Our experiments on four benchmarks (WMT14 En*[Math Processing Error]*De, WMT14 De*[Math Processing Error]*En, WMT16 Ro*[Math Processing Error]*En and IWSLT14 De*[Math Processing Error]*En) show that the proposed model achieves competitive performance compared with the state-of-the-art non-autoregressive and autoregressive models while significantly reducing the time cost in inference phase.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.03084](https://arxiv.org/abs/2002.03084) [cs.CL] |
|           | (or [arXiv:2002.03084v1](https://arxiv.org/abs/2002.03084v1) [cs.CL] for this version) |





<h2 id="2020-02-11-3">3. Multilingual Alignment of Contextual Word Representations</h2>

Title: [Multilingual Alignment of Contextual Word Representations](https://arxiv.org/abs/2002.03518)

Authors: [Steven Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S), [Nikita Kitaev](https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

*(Submitted on 10 Feb 2020)*

> We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models.

| Comments: | ICLR 2020                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2002.03518](https://arxiv.org/abs/2002.03518) [cs.CL] |
|           | (or [arXiv:2002.03518v1](https://arxiv.org/abs/2002.03518v1) [cs.CL] for this version) |



# 2020-02-10

[Return to Index](#Index)



<h2 id="2020-02-10-1">1. Neural Machine Translation System of Indic Languages -- An Attention based Approach</h2>

Title: [Neural Machine Translation System of Indic Languages -- An Attention based Approach]()

Authors: [Parth Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+P), [Vishvajit Bakrola](https://arxiv.org/search/cs?searchtype=author&query=Bakrola%2C+V)

*(Submitted on 2 Feb 2020)*

> Neural machine translation (NMT) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques. Proposed neural machine translation model developed for the Gujarati language contains encoder-decoder with attention mechanism. In India, almost all the languages are originated from their ancestral language - Sanskrit. They are having inevitable similarities including lexical and named entity similarity. Translating into Indic languages is always be a challenging task. In this paper, we have presented the neural machine translation system (NMT) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58.49 percentage of total speakers in the country. We have compared the performance of our NMT model with automatic evaluation matrices such as BLEU, perplexity and TER matrix. The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 BLEU score on English-Gujarati translation.

| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP), Gangtok, India, 2019, pp. 1-5 |
| DOI:               | [10.1109/ICACCP.2019.8882969](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FICACCP.2019.8882969&v=a230a5bc) |
| Cite as:           | [arXiv:2002.02758](https://arxiv.org/abs/2002.02758) [cs.CL] |
|                    | (or [arXiv:2002.02758v1](https://arxiv.org/abs/2002.02758v1) [cs.CL] for this version) |







<h2 id="2020-02-10-2">2. A Multilingual View of Unsupervised Machine Translation</h2>

Title: [A Multilingual View of Unsupervised Machine Translation](https://arxiv.org/abs/2002.02955)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Pierre Foret](https://arxiv.org/search/cs?searchtype=author&query=Foret%2C+P), [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P)

*(Submitted on 7 Feb 2020)*

> We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT'14 English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in most directions. In particular, we obtain a +1.65 BLEU advantage over the best-performing unsupervised model in the Romanian-English direction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.02955](https://arxiv.org/abs/2002.02955) [cs.CL] |
|           | (or [arXiv:2002.02955v1](https://arxiv.org/abs/2002.02955v1) [cs.CL] for this version) |







# 2020-02-06

[Return to Index](#Index)



<h2 id="2020-02-06-1">1. Multilingual acoustic word embedding models for processing zero-resource languages</h2>

Title: [Multilingual acoustic word embedding models for processing zero-resource languages](https://arxiv.org/abs/2002.02109)

Authors: [Herman Kamper](https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H), [Yevgen Matusevych](https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y), [Sharon Goldwater](https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S)

*(Submitted on 6 Feb 2020)*

> Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. In settings where unlabelled speech is the only available resource, such embeddings can be used in "zero-resource" speech search, indexing and discovery systems. Here we propose to train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. For this transfer learning approach, we consider two multilingual recurrent neural network models: a discriminative classifier trained on the joint vocabularies of all training languages, and a correspondence autoencoder trained to reconstruct word pairs. We test these using a word discrimination task on six target zero-resource languages. When trained on seven well-resourced languages, both models perform similarly and outperform unsupervised models trained on the zero-resource languages. With just a single training language, the second model works better, but performance depends more on the particular training--testing language pair.

| Comments: | 5 pages, 4 figures, 1 table; accepted to ICASSP 2020. arXiv admin note: text overlap with [arXiv:1811.00403](https://arxiv.org/abs/1811.00403) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2002.02109](https://arxiv.org/abs/2002.02109) [cs.CL] |
|           | (or [arXiv:2002.02109v1](https://arxiv.org/abs/2002.02109v1) [cs.CL] for this version) |





<h2 id="2020-02-06-2">2. Irony Detection in a Multilingual Context</h2>

Title: [Irony Detection in a Multilingual Context](https://arxiv.org/abs/2002.02427)

Authors: [Bilal Ghanem](https://arxiv.org/search/cs?searchtype=author&query=Ghanem%2C+B), [Jihen Karoui](https://arxiv.org/search/cs?searchtype=author&query=Karoui%2C+J), [Farah Benamara](https://arxiv.org/search/cs?searchtype=author&query=Benamara%2C+F), [Paolo Rosso](https://arxiv.org/search/cs?searchtype=author&query=Rosso%2C+P), [Véronique Moriceau](https://arxiv.org/search/cs?searchtype=author&query=Moriceau%2C+V)

*(Submitted on 6 Feb 2020)*

> This paper proposes the first multilingual (French, English and Arabic) and multicultural (Indo-European languages vs. less culturally close languages) irony detection system. We employ both feature-based models and neural architectures using monolingual word representation. We compare the performance of these systems with state-of-the-art systems to identify their capabilities. We show that these monolingual models trained separately on different languages using multilingual word representation or text-based features can open the door to irony detection in languages that lack of annotated data for irony.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.02427](https://arxiv.org/abs/2002.02427) [cs.CL] |
|           | (or [arXiv:2002.02427v1](https://arxiv.org/abs/2002.02427v1) [cs.CL] for this version) |



# 2020-02-05

[Return to Index](#Index)



<h2 id="2020-02-05-1">1. CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus</h2>

Title: [CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus](https://arxiv.org/abs/2002.01320)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J)

*(Submitted on 4 Feb 2020)*

> Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets involve language pairs with English as a source language, involve very specific domains or are low resource. We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free to use. We also provide additional evaluation data derived from Tatoeba under CC licenses.

| Comments: | Submitted to LREC 2020                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.01320](https://arxiv.org/abs/2002.01320) [cs.CL] |
|           | (or [arXiv:2002.01320v1](https://arxiv.org/abs/2002.01320v1) [cs.CL] for this version) |



# 2020-02-04

[Return to Index](#Index)



<h2 id="2020-02-04-1">1. Unsupervised Bilingual Lexicon Induction Across Writing Systems</h2>

Title: [Unsupervised Bilingual Lexicon Induction Across Writing Systems](https://arxiv.org/abs/2002.00037)

Authors: [Parker Riley](https://arxiv.org/search/cs?searchtype=author&query=Riley%2C+P), [Daniel Gildea](https://arxiv.org/search/cs?searchtype=author&query=Gildea%2C+D)

*(Submitted on 31 Jan 2020)*

> Recent embedding-based methods in unsupervised bilingual lexicon induction have shown good results, but generally have not leveraged orthographic (spelling) information, which can be helpful for pairs of related languages. This work augments a state-of-the-art method with orthographic features, and extends prior work in this space by proposing methods that can learn and utilize orthographic correspondences even between languages with different scripts. We demonstrate this by experimenting on three language pairs with different scripts and varying degrees of lexical similarity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00037](https://arxiv.org/abs/2002.00037) [cs.CL] |
|           | (or [arXiv:2002.00037v1](https://arxiv.org/abs/2002.00037v1) [cs.CL] for this version) |





<h2 id="2020-02-04-2">2. Citation Text Generation</h2>

Title: [Citation Text Generation](https://arxiv.org/abs/2002.00317)

Authors: [Kelvin Luu](https://arxiv.org/search/cs?searchtype=author&query=Luu%2C+K), [Rik Koncel-Kedziorski](https://arxiv.org/search/cs?searchtype=author&query=Koncel-Kedziorski%2C+R), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Isabel Cachola](https://arxiv.org/search/cs?searchtype=author&query=Cachola%2C+I), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 2 Feb 2020)*

> We introduce the task of citation text generation: given a pair of scientific documents, explain their relationship in natural language text in the manner of a citation from one text to the other. This task encourages systems to learn rich relationships between scientific texts and to express them concretely in natural language. Models for citation text generation will require robust document understanding including the capacity to quickly adapt to new vocabulary and to reason about document content. We believe this challenging direction of research will benefit high-impact applications such as automatic literature review or scientific writing assistance systems. In this paper we establish the task of citation text generation with a standard evaluation corpus and explore several baseline models.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2002.00317](https://arxiv.org/abs/2002.00317) [cs.CL] |
|           | (or [arXiv:2002.00317v1](https://arxiv.org/abs/2002.00317v1) [cs.CL] for this version) |





<h2 id="2020-02-04-3">3. Unsupervised Multilingual Alignment using Wasserstein Barycenter</h2>

Title: [Unsupervised Multilingual Alignment using Wasserstein Barycenter](https://arxiv.org/abs/2002.00743)

Authors: [Xin Lian](https://arxiv.org/search/cs?searchtype=author&query=Lian%2C+X), [Kshitij Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+K), [Jakub Truszkowski](https://arxiv.org/search/cs?searchtype=author&query=Truszkowski%2C+J), [Pascal Poupart](https://arxiv.org/search/cs?searchtype=author&query=Poupart%2C+P), [Yaoliang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y)

*(Submitted on 28 Jan 2020)*

> We study unsupervised multilingual alignment, the problem of finding word-to-word translations between multiple languages without using any parallel data. One popular strategy is to reduce multilingual alignment to the much simplified bilingual setting, by picking one of the input languages as the pivot language that we transit through. However, it is well-known that transiting through a poorly chosen pivot language (such as English) may severely degrade the translation quality, since the assumed transitive relations among all pairs of languages may not be enforced in the training process. Instead of going through a rather arbitrarily chosen pivot language, we propose to use the Wasserstein barycenter as a more informative ''mean'' language: it encapsulates information from all languages and minimizes all pairwise transportation costs. We evaluate our method on standard benchmarks and demonstrate state-of-the-art performances.

| Comments: | Work in progress; comments welcome!                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | [arXiv:2002.00743](https://arxiv.org/abs/2002.00743) [cs.CL] |
|           | (or [arXiv:2002.00743v1](https://arxiv.org/abs/2002.00743v1) [cs.CL] for this version) |





<h2 id="2020-02-04-4">4. Joint Contextual Modeling for ASR Correction and Language Understanding</h2>

Title: [Joint Contextual Modeling for ASR Correction and Language Understanding](https://arxiv.org/abs/2002.00750)

Authors: [Yue Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+Y), [Sai Sumanth Miryala](https://arxiv.org/search/cs?searchtype=author&query=Miryala%2C+S+S), [Chandra Khatri](https://arxiv.org/search/cs?searchtype=author&query=Khatri%2C+C), [Runze Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Huaixiu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Piero Molino](https://arxiv.org/search/cs?searchtype=author&query=Molino%2C+P), [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [Alexandros Papangelis](https://arxiv.org/search/cs?searchtype=author&query=Papangelis%2C+A), [Hugh Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+H), [Franziska Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+F), [Gokhan Tur](https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G)

*(Submitted on 28 Jan 2020)*

> The quality of automatic speech recognition (ASR) is critical to Dialogue Systems as ASR errors propagate to and directly impact downstream tasks such as language understanding (LU). In this paper, we propose multi-task neural approaches to perform contextual language correction on ASR outputs jointly with LU to improve the performance of both tasks simultaneously. To measure the effectiveness of this approach we used a public benchmark, the 2nd Dialogue State Tracking (DSTC2) corpus. As a baseline approach, we trained task-specific Statistical Language Models (SLM) and fine-tuned state-of-the-art Generalized Pre-training (GPT) Language Model to re-rank the n-best ASR hypotheses, followed by a model to identify the dialog act and slots. i) We further trained ranker models using GPT and Hierarchical CNN-RNN models with discriminatory losses to detect the best output given n-best hypotheses. We extended these ranker models to first select the best ASR output and then identify the dialogue act and slots in an end to end fashion. ii) We also proposed a novel joint ASR error correction and LU model, a word confusion pointer network (WCN-Ptr) with multi-head self-attention on top, which consumes the word confusions populated from the n-best. We show that the error rates of off the shelf ASR and following LU systems can be reduced significantly by 14% relative with joint models trained using small amounts of in-domain data.

| Comments: | Accepted at IEEE ICASSP 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | [arXiv:2002.00750](https://arxiv.org/abs/2002.00750) [cs.CL] |
|           | (or [arXiv:2002.00750v1](https://arxiv.org/abs/2002.00750v1) [cs.CL] for this version) |





<h2 id="2020-02-04-5">5. FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications</h2>

Title: [FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications](https://arxiv.org/abs/2002.00760)

Authors: [Dou Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+D), [Lv Zhonghou](https://arxiv.org/search/cs?searchtype=author&query=Zhonghou%2C+L), [Wang minghua](https://arxiv.org/search/cs?searchtype=author&query=minghua%2C+W)

*(Submitted on 31 Jan 2020)*

> In this paper, we present a novel algorithm, FastWordBug, to efficiently generate small text perturbations in a black-box setting that forces a sentiment analysis or text classification mode to make an incorrect prediction. By combining the part of speech attributes of words, we propose a scoring method that can quickly identify important words that affect text classification. We evaluate FastWordBug on three real-world text datasets and two state-of-the-art machine learning models under black-box setting. The results show that our method can significantly reduce the accuracy of the model, and at the same time, we can call the model as little as possible, with the highest attack efficiency. We also attack two popular real-world cloud services of NLP, and the results show that our method works as well.

| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00760](https://arxiv.org/abs/2002.00760) [cs.CL] |
|           | (or [arXiv:2002.00760v1](https://arxiv.org/abs/2002.00760v1) [cs.CL] for this version) |





<h2 id="2020-02-04-6">6. Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance</h2>

Title: [Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance](https://arxiv.org/abs/2002.00761)

Authors: [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F)

*(Submitted on 31 Jan 2020)*

> Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel bitexts for machine translation training. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low, mid, and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents, we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs, 15% on mid-resource language pairs, and 22% on low-resource language pairs

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:2002.00761](https://arxiv.org/abs/2002.00761) [cs.CL] |
|           | (or [arXiv:2002.00761v1](https://arxiv.org/abs/2002.00761v1) [cs.CL] for this version) |



# 2020-02-03
[Return to Index](#Index)



<h2 id="2020-02-03-1">1. Self-Adversarial Learning with Comparative Discrimination for Text Generation</h2>

Title: [Self-Adversarial Learning with Comparative Discrimination for Text Generation](https://arxiv.org/abs/2001.11691)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

*(Submitted on 31 Jan 2020)*

> Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.

| Comments: | to be published in ICLR 2020                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | [arXiv:2001.11691](https://arxiv.org/abs/2001.11691) [cs.CL] |
|           | (or [arXiv:2001.11691v1](https://arxiv.org/abs/2001.11691v1) [cs.CL] for this version) |





<h2 id="2020-02-03-2">2. Teaching Machines to Converse</h2>

Title: [Teaching Machines to Converse](https://arxiv.org/abs/2001.11701)

Authors: [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

*(Submitted on 31 Jan 2020)*

> The ability of a machine to communicate with humans has long been associated with the general success of AI. This dates back to Alan Turing's epoch-making work in the early 1950s, which proposes that a machine's intelligence can be tested by how well it, the machine, can fool a human into believing that the machine is a human through dialogue conversations. Many systems learn generation rules from a minimal set of authored rules or labels on top of hand-coded rules or templates, and thus are both expensive and difficult to extend to open-domain scenarios. Recently, the emergence of neural network models the potential to solve many of the problems in dialogue learning that earlier systems cannot tackle: the end-to-end neural frameworks offer the promise of scalability and language-independence, together with the ability to track the dialogue state and then mapping between states and dialogue actions in a way not possible with conventional systems. On the other hand, neural systems bring about new challenges: they tend to output dull and generic responses; they lack a consistent or a coherent persona; they are usually optimized through single-turn conversations and are incapable of handling the long-term success of a conversation; and they are not able to take the advantage of the interactions with humans. This dissertation attempts to tackle these challenges: Contributions are two-fold: (1) we address new challenges presented by neural network models in open-domain dialogue generation systems; (2) we develop interactive question-answering dialogue systems by (a) giving the agent the ability to ask questions and (b) training a conversation agent through interactions with humans in an online fashion, where a bot improves through communicating with humans and learning from the mistakes that it makes.

| Comments: | phd thesis                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | [arXiv:2001.11701](https://arxiv.org/abs/2001.11701) [cs.CL] |
|           | (or [arXiv:2001.11701v1](https://arxiv.org/abs/2001.11701v1) [cs.CL] for this version) |


