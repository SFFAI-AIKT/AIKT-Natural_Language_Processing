# Daily arXiv: Machine Translation - April, 2021

# Index

- [2021-04-30](#2021-04-30)
  - [1. Dynabench: Rethinking Benchmarking in NLP](#2021-04-30-1)
  - [2. Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation](#2021-04-30-2)
  - [3. Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation](#2021-04-30-3)
- [2021-04-29](#2021-04-29)
  - [1. Gradient-based Adversarial Attacks against Text Transformers](#2021-04-29-1)
- [2021-04-28](#2021-04-28)
  - [1. SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text Style Transfer](#2021-04-28-1)
  - [2. Teaching a Massive Open Online Course on Natural Language Processing](#2021-04-28-2)
  - [3. Morph Call: Probing Morphosyntactic Content of Multilingual Transformers](#2021-04-28-3)
- [2021-04-27](#2021-04-27)
  - [1. Modeling Coverage for Non-Autoregressive Neural Machine Translation](#2021-04-27-1)
  - [2. Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation](#2021-04-27-2)
  - [3. Automatic Post-Editing for Translating Chinese Novels to Vietnamese](#2021-04-27-3)
  - [4. XLM-T: A Multilingual Language Model Toolkit for Twitter](#2021-04-27-4)
  - [5. Reranking Machine Translation Hypotheses with Structured and Web-based Language Models](#2021-04-27-5)
  - [6. PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](#2021-04-27-6)
  - [7. Attention vs non-attention for a Shapley-based explanation method](#2021-04-27-7)
  - [8. Easy and Efficient Transformer : Scalable Inference Solution For large NLP mode](#2021-04-27-8)
- [2021-04-26](#2021-04-26)
  - [1. Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation](#2021-04-26-1)
  - [2. LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech](#2021-04-26-2)
- [2021-04-23](#2021-04-23)
  - [1. Disfluency Detection with Unlabeled Data and Small BERT Models](#2021-04-23-1)
  - [2. Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?](#2021-04-23-2)
- [2021-04-22](#2021-04-22)
  - [1. Revisiting Document Representations for Large-Scale Zero-Shot Learning](#2021-04-22-1)
  - [2. Discriminative Self-training for Punctuation Prediction](#2021-04-22-2)
  - [3. Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning](#2021-04-22-3)
  - [4. End-to-end Speech Translation via Cross-modal Progressive Training](#2021-04-22-4)
  - [5. On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs](#2021-04-22-5)
  - [6. Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?](#2021-04-22-6)
  - [7. Improving BERT Pretraining with Syntactic Supervision](#2021-04-22-7)
  - [8. Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models](#2021-04-22-8)
- [2021-04-21](#2021-04-21)
  - [1. Can Latent Alignments Improve Autoregressive Machine Translation?](#2021-04-21-1)
  - [2. Efficient pre-training objectives for Transformers](#2021-04-21-2)
  - [3. Problems and Countermeasures in Natural Language Processing Evaluation](#2021-04-21-3)
  - [4. Addressing the Vulnerability of NMT in Input Perturbations](#2021-04-21-4)
  - [5. Grammatical Error Generation Based on Translated Fragments](#2021-04-21-5)
  - [6. Towards Solving Multimodal Comprehension](#2021-04-21-6)
- [2021-04-20](#2021-04-20)
  - [1. Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training](#2021-04-20-1)
  - [2. Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models](#2021-04-20-2)
  - [3. Improving Neural Machine Translation with Compact Word Embedding Tables](#2021-04-20-3)
  - [4. mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs](#2021-04-20-4)
  - [5. Knowledge Neurons in Pretrained Transformers](#2021-04-20-5)
  - [6. Zero-shot Cross-lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders](#2021-04-20-6)
  - [7. On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation](#2021-04-20-7)
  - [8. Chinese Sentences Similarity via Cross-Attention Based Siamese Network](#2021-04-20-8)
  - [9. Lifelong Learning of Few-shot Learners across NLP Tasks](#2021-04-20-9)
  - [10. Contrastive Out-of-Distribution Detection for Pretrained Transformers](#2021-04-20-10)
  - [11. Stream-level Latency Evaluation for Simultaneous Machine Translation](#2021-04-20-11)
  - [12. SimCSE: Simple Contrastive Learning of Sentence Embeddings](#2021-04-20-12)
  - [13. GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation](#2021-04-20-13)
  - [14. CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP](#2021-04-20-14)
  - [15. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](#2021-04-20-15)
  - [16. Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction](#2021-04-20-16)
  - [17. Probing for Bridging Inference in Transformer Language Models](#2021-04-20-17)
  - [18. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding](#2021-04-20-18)
  - [19. Frequency-based Distortions in Contextualized Word Embeddings](#2021-04-20-19)
  - [20. Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation](#2021-04-20-20)
  - [21. Sentence Alignment with Parallel Documents Helps Biomedical Machine Translation](#2021-04-20-21)
  - [22. Embodying Pre-Trained Word Embeddings Through Robot Actions](#2021-04-20-22)
  - [23. Dual-View Distilled BERT for Sentence Embedding](#2021-04-20-23)
  - [24. CLIPScore: A Reference-free Evaluation Metric for Image Captioning](#2021-04-20-24)
  - [25. "Wikily" Neural Machine Translation Tailored to Cross-Lingual Tasks](#2021-04-20-25)
- [2021-04-19](#2021-04-19)
  - [1. Improving Gender Translation Accuracy with Filtered Self-Training](#2021-04-19-1)
  - [2. Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation](#2021-04-19-2)
  - [3. Investigating Failures of Automatic Translation in the Case of Unambiguous Gender](#2021-04-19-3)
  - [4. Comparison of Grammatical Error Correction Using Back-Translation Models](#2021-04-19-4)
  - [5. Segmenting Subtitles for Correcting ASR Segmentation Errors](#2021-04-19-5)
  - [6. Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research](#2021-04-19-6)
  - [7. Generating Bug-Fixes Using Pretrained Transformers](#2021-04-19-7)
  - [8. MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning](#2021-04-19-8)
  - [9. Language Models are Few-Shot Butlers](#2021-04-19-9)
  - [10. Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders](#2021-04-19-10)
  - [11. Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models](#2021-04-19-11)
  - [12. Towards Variable-Length Textual Adversarial Attacks](#2021-04-19-12)
  - [13. Serial or Parallel? Plug-able Adapter for multilingual machine translation](#2021-04-19-13)
  - [14. Robust Open-Vocabulary Translation from Visual Text Representations](#2021-04-19-14)
  - [15. Is Your Language Model Ready for Dense Representation Fine-tuning?](#2021-04-19-15)
  - [16. Context-Adaptive Document-Level Neural Machine Translation](#2021-04-19-16)
- [2021-04-16](#2021-04-16)
  - [1. What Makes a Scientific Paper be Accepted for Publication?](#2021-04-16-1)
  - [2. An Interpretability Illusion for BERT](#2021-04-16-2)
  - [3. An Alignment-Agnostic Model for Chinese Text Error Correction](#2021-04-16-3)
  - [4. Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](#2021-04-16-4)
  - [5. Sentence-Permuted Paragraph Generation](#2021-04-16-5)
  - [6. Adaptive Sparse Transformer for Multilingual Translation](#2021-04-16-6)
  - [7. Simultaneous Multi-Pivot Neural Machine Translation](#2021-04-16-7)
  - [8. First the worst: Finding better gender translations during beam search](#2021-04-16-8)
  - [9. Effect of Post-processing on Contextualized Word Representations](#2021-04-16-9)
  - [10. IndT5: A Text-to-Text Transformer for 10 Indigenous Languages](#2021-04-16-10)
  - [11. Generating Datasets with Pretrained Language Models](#2021-04-16-11)
  - [12. Reward Optimization for Neural Machine Translation with Learned Metrics](#2021-04-16-12)
  - [13. Hierarchical Learning for Generation with Long Source Sequences](#2021-04-16-13)
  - [14. Sometimes We Want Translationese](#2021-04-16-14)
  - [15. Demystify Optimization Challenges in Multilingual Transformers](#2021-04-16-15)
  - [16. Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](#2021-04-16-16)
- [2021-04-15](#2021-04-15)
  - [1. Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation](#2021-04-15-1)
  - [2. Large-Scale Self- and Semi-Supervised Learning for Speech Translation](#2021-04-15-2)
  - [3. The Curious Case of Hallucinations in Neural Machine Translation](#2021-04-15-3)
  - [4. Sentence Embeddings by Ensemble Distillation](#2021-04-15-4)
  - [5. Distributed Word Representation in Tsetlin Machine](#2021-04-15-5)
  - [6. Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey](#2021-04-15-6)
  - [7. TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](#2021-04-15-7)
  - [8. Sparse Attention with Linear Units](#2021-04-15-8 )
- [2021-04-14](#2021-04-14)
  - [1. Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique](#2021-04-14-1)
  - [2. Targeted Adversarial Training for Natural Language Understanding](#2021-04-14-2)
  - [3. Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation](#2021-04-14-3)
  - [4. Discourse Probing of Pretrained Language Models](#2021-04-14-4)
  - [5. Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation](#2021-04-14-5)
  - [6. Gender Bias in Machine Translation](#2021-04-14-6)
  - [7. Lessons on Parameter Sharing across Layers in Transformers](#2021-04-14-7)
  - [8. What's in your Head? Emergent Behaviour in Multi-Task Transformer Models](#2021-04-14-8)
  - [9. Understanding Hard Negatives in Noise Contrastive Estimation](#2021-04-14-9)
  - [10. Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling](#2021-04-14-10)
  - [11. EXPLAINABOARD: An Explainable Leaderboard for NLP](#2021-04-14-11)
  - [12. Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding](#2021-04-14-12)
- [2021-04-13](#2021-04-13)
  - [1. Achieving Model Robustness through Discrete Adversarial Training](#2021-04-13-1)
  - [2. TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation](#2021-04-13-2)
  - [3. Not All Attention Is All You Need](#2021-04-13-3)
  - [4. Sentiment-based Candidate Selection for NMT](#2021-04-13-4)
  - [5. Disentangled Contrastive Learning for Learning Robust Textual Representations](#2021-04-13-5)
  - [6. Assessing Reference-Free Peer Evaluation for Machine Translation](#2021-04-13-6)
  - [7. FUDGE: Controlled Text Generation With Future Discriminators](#2021-04-13-7)
  - [8. Machine Translation Decoding beyond Beam Search](#2021-04-13-8)
  - [9. Self-Training with Weak Supervision](#2021-04-13-9)
  - [10. Survey on reinforcement learning for language processing](#2021-04-13-10)
  - [11. Backtranslation Feedback Improves User Confidence in MT, Not Quality](#2021-04-13-11)
- [2021-04-12](#2021-04-12)
  - [1. Video-aided Unsupervised Grammar Induction](#2021-04-12-1)
  - [2. Design and Implementation of English To Yoruba Verb Phrase Machine Translation System](#2021-04-12-2)
  - [3. Efficient Large-Scale Language Model Training on GPU Clusters](#2021-04-12-3)
  - [4. Chinese Character Decomposition for Neural MT with Multi-Word Expressions](#2021-04-12-4)
- [2021-04-09](#2021-04-09)
  - [1. Extended Parallel Corpus for Amharic-English Machine Translation](#2021-04-09-1)
  - [2. BSTC: A Large-Scale Chinese-English Speech Translation Dataset](#2021-04-09-2)
  - [3. A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](#2021-04-09-3)
  - [4. Probing BERT in Hyperbolic Spaces](#2021-04-09-4)
- [2021-04-08](#2021-04-08)
  - [1. VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations](#2021-04-08-1)
  - [2. Better Neural Machine Translation by Extracting Linguistic Information from BERT](#2021-04-08-2)
  - [3. GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education](#2021-04-08-3)
- [2021-04-07](#2021-04-07)
  - [1. Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding](#2021-04-07-1)
  - [2. ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation](#2021-04-07-2)
- [2021-04-06](#2021-04-06)
  - [1. TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition](#2021-04-06-1)
  - [2. Attention Forcing for Machine Translation](#2021-04-06-2)
  - [3. WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](#2021-04-06-3)
  - [4. Rethinking Perturbations in Encoder-Decoders for Fast Training](#2021-04-06-4)
- [2021-04-02](#2021-04-02)
  - [1. Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](#2021-04-02-1)
	- [2. Domain-specific MT for Low-resource Languages: The case of Bambara-French](#2021-04-02-2)
  - [3. Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation](#2021-04-02-3)
  - [4. Detecting over/under-translation errors for determining adequacy in human translations](#2021-04-02-4)
  - [5. Many-to-English Machine Translation Tools, Data, and Pretrained Models](#2021-04-02-5)
  - [6. Low-Resource Neural Machine Translation for South-Eastern African Languages](#2021-04-02-6)
  - [7. WakaVT: A Sequential Variational Transformer for Waka Generation](#2021-04-02-7)
  - [8. Sampling and Filtering of Neural Machine Translation Distillation Data](#2021-04-02-8)
- [2021-04-01](#2021-04-01)
  - [1. An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation](#2021-04-01-1)
	- [2. Few-shot learning through contextual data augmentation](#2021-04-01-2)
  - [3. UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language](#2021-04-01-3)
  - [4. Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources](#2021-04-01-4)
  - [5. Leveraging Neural Machine Translation for Word Alignment](#2021-04-01-5)
- [2021-03-31](#2021-03-31)	
  - [1. Diagnosing Vision-and-Language Navigation: What Really Matters](#2021-03-31-1)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-04-30

[Return to Index](#Index)



<h2 id="2021-04-30-1">1. Dynabench: Rethinking Benchmarking in NLP
</h2>

Title: [Dynabench: Rethinking Benchmarking in NLP](https://arxiv.org/abs/2104.14337)

Authors:[Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D), [Max Bartolo](https://arxiv.org/search/cs?searchtype=author&query=Bartolo%2C+M), [Yixin Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y), [Divyansh Kaushik](https://arxiv.org/search/cs?searchtype=author&query=Kaushik%2C+D), [Atticus Geiger](https://arxiv.org/search/cs?searchtype=author&query=Geiger%2C+A), [Zhengxuan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Bertie Vidgen](https://arxiv.org/search/cs?searchtype=author&query=Vidgen%2C+B), [Grusha Prasad](https://arxiv.org/search/cs?searchtype=author&query=Prasad%2C+G), [Amanpreet Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A), [Pratik Ringshia](https://arxiv.org/search/cs?searchtype=author&query=Ringshia%2C+P), [Zhiyi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z), [Tristan Thrush](https://arxiv.org/search/cs?searchtype=author&query=Thrush%2C+T), [Sebastian Riedel](https://arxiv.org/search/cs?searchtype=author&query=Riedel%2C+S), [Zeerak Waseem](https://arxiv.org/search/cs?searchtype=author&query=Waseem%2C+Z), [Pontus Stenetorp](https://arxiv.org/search/cs?searchtype=author&query=Stenetorp%2C+P), [Robin Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts%2C+C), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.14337](https://arxiv.org/abs/2104.14337) [cs.CL]** |
|           | (or **[arXiv:2104.14337v1](https://arxiv.org/abs/2104.14337v1) [cs.CL]** for this version) |





<h2 id="2021-04-30-2">2. Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation
</h2>

Title: [Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation](https://arxiv.org/abs/2104.14470)

Authors:[Ha Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Yannick Estève](https://arxiv.org/search/cs?searchtype=author&query=Estève%2C+Y), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Boosted by the simultaneous translation shared task at IWSLT 2020, promising end-to-end online speech translation approaches were recently proposed. They consist in incrementally encoding a speech input (in a source language) and decoding the corresponding text (in a target language) with the best possible trade-off between latency and translation quality. This paper investigates two key aspects of end-to-end simultaneous speech translation: (a) how to encode efficiently the continuous speech flow, and (b) how to segment the speech flow in order to alternate optimally between reading (R: encoding input) and writing (W: decoding output) operations. We extend our previously proposed end-to-end online decoding strategy and show that while replacing BLSTM by ULSTM encoding degrades performance in offline mode, it actually improves both efficiency and performance in online mode. We also measure the impact of different methods to segment the speech signal (using fixed interval boundaries, oracle word boundaries or randomly set boundaries) and show that our best end-to-end online decoding strategy is surprisingly the one that alternates R/W operations on fixed size blocks on our English-German speech translation setup.

| Comments: | Submitted to Interspeech 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.14470](https://arxiv.org/abs/2104.14470) [cs.CL]** |
|           | (or **[arXiv:2104.14470v1](https://arxiv.org/abs/2104.14470v1) [cs.CL]** for this version) |





<h2 id="2021-04-30-3">3. Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation
</h2>

Title: [Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation](https://arxiv.org/abs/2104.14478)

Authors:[Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D), [Viresh Ratnakar](https://arxiv.org/search/cs?searchtype=author&query=Ratnakar%2C+V), [Qijun Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Q), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

> Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.14478](https://arxiv.org/abs/2104.14478) [cs.CL]** |
|           | (or **[arXiv:2104.14478v1](https://arxiv.org/abs/2104.14478v1) [cs.CL]** for this version) |



# 2021-04-29

[Return to Index](#Index)



<h2 id="2021-04-29-1">1. Gradient-based Adversarial Attacks against Text Transformers
</h2>

Title: [Gradient-based Adversarial Attacks against Text Transformers](https://arxiv.org/abs/2104.13733)

Authors: [Chuan Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+C), [Alexandre Sablayrolles](https://arxiv.org/search/cs?searchtype=author&query=Sablayrolles%2C+A), [Hervé Jégou](https://arxiv.org/search/cs?searchtype=author&query=Jégou%2C+H), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.13733](https://arxiv.org/abs/2104.13733) [cs.CL]** |
|           | (or **[arXiv:2104.13733v1](https://arxiv.org/abs/2104.13733v1) [cs.CL]** for this version) |







# 2021-04-28

[Return to Index](#Index)



<h2 id="2021-04-28-1">1. SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text Style Transfer
</h2>

Title: [SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text Style Transfer](https://arxiv.org/abs/2104.12977)

Authors: [Jicheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jiao Ou](https://arxiv.org/search/cs?searchtype=author&query=Ou%2C+J)

> Text style transfer aims to change the style of sentences while preserving the semantic meanings. Due to the lack of parallel data, the Denoising Auto-Encoder (DAE) is widely used in this task to model distributions of different sentence styles. However, because of the conflict between the target of the conventional denoising procedure and the target of style transfer task, the vanilla DAE can not produce satisfying enough results. To improve the transferability of the model, most of the existing works combine DAE with various complicated unsupervised networks, which makes the whole system become over-complex. In this work, we design a novel DAE model named Style-Enhanced DAE (SE-DAE), which is specifically designed for the text style transfer task. Compared with previous complicated style-transfer models, our model do not consist of any complicated unsupervised networks, but only relies on the high-quality pseudo-parallel data generated by a novel data refinement mechanism. Moreover, to alleviate the conflict between the targets of the conventional denoising procedure and the style transfer task, we propose another novel style denoising mechanism, which is more compatible with the target of the style transfer task. We validate the effectiveness of our model on two style benchmark datasets. Both automatic evaluation and human evaluation show that our proposed model is highly competitive compared with previous strong the state of the art (SOTA) approaches and greatly outperforms the vanilla DAE.

| Comments: | Accepted by the 2021 International Joint Conference on Neural Networks (IJCNN 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12977](https://arxiv.org/abs/2104.12977) [cs.CL]** |
|           | (or **[arXiv:2104.12977v1](https://arxiv.org/abs/2104.12977v1) [cs.CL]** for this version) |





<h2 id="2021-04-28-2">2. Teaching a Massive Open Online Course on Natural Language Processing
</h2>

Title: [Teaching a Massive Open Online Course on Natural Language Processing](https://arxiv.org/abs/2104.12846)

Authors: [Ekaterina Artemova](https://arxiv.org/search/cs?searchtype=author&query=Artemova%2C+E), [Murat Apishev](https://arxiv.org/search/cs?searchtype=author&query=Apishev%2C+M), [Veronika Sarkisyan](https://arxiv.org/search/cs?searchtype=author&query=Sarkisyan%2C+V), [Sergey Aksenov](https://arxiv.org/search/cs?searchtype=author&query=Aksenov%2C+S), [Denis Kirjanov](https://arxiv.org/search/cs?searchtype=author&query=Kirjanov%2C+D), [Oleg Serikov](https://arxiv.org/search/cs?searchtype=author&query=Serikov%2C+O)

> This paper presents a new Massive Open Online Course on Natural Language Processing, targeted at non-English speaking students. The course lasts 12 weeks; every week consists of lectures, practical sessions, and quiz assignments. Three weeks out of 12 are followed by Kaggle-style coding assignments.
> Our course intends to serve multiple purposes: (i) familiarize students with the core concepts and methods in NLP, such as language modeling or word or sentence representations, (ii) show that recent advances, including pre-trained Transformer-based models, are built upon these concepts; (iii) introduce architectures for most demanded real-life applications, (iv) develop practical skills to process texts in multiple languages. The course was prepared and recorded during 2020, launched by the end of the year, and in early 2021 has received positive feedback.

| Comments: | To appear in the Proceedings of the Fifth Workshop on Teaching NLP @ NAACL |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12846](https://arxiv.org/abs/2104.12846) [cs.CL]** |
|           | (or **[arXiv:2104.12846v1](https://arxiv.org/abs/2104.12846v1) [cs.CL]** for this version) |





<h2 id="2021-04-28-3">3. Morph Call: Probing Morphosyntactic Content of Multilingual Transformers
</h2>

Title: [Morph Call: Probing Morphosyntactic Content of Multilingual Transformers](https://arxiv.org/abs/2104.12847)

Authors: [Vladislav Mikhailov](https://arxiv.org/search/cs?searchtype=author&query=Mikhailov%2C+V), [Oleg Serikov](https://arxiv.org/search/cs?searchtype=author&query=Serikov%2C+O), [Ekaterina Artemova](https://arxiv.org/search/cs?searchtype=author&query=Artemova%2C+E)

> The outstanding performance of transformer-based language models on a great variety of NLP and NLU tasks has stimulated interest in exploring their inner workings. Recent research has focused primarily on higher-level and complex linguistic phenomena such as syntax, semantics, world knowledge, and common sense. The majority of the studies are anglocentric, and little remains known regarding other languages, precisely their morphosyntactic properties. To this end, our work presents Morph Call, a suite of 46 probing tasks for four Indo-European languages of different morphology: English, French, German and Russian. We propose a new type of probing task based on the detection of guided sentence perturbations. We use a combination of neuron-, layer- and representation-level introspection techniques to analyze the morphosyntactic content of four multilingual transformers, including their less explored distilled versions. Besides, we examine how fine-tuning for POS-tagging affects the model knowledge. The results show that fine-tuning can improve and decrease the probing performance and change how morphosyntactic knowledge is distributed across the model. The code and data are publicly available, and we hope to fill the gaps in the less studied aspect of transformers.

| Comments: | To appear in the Proceedings of the 3rd Workshop on Research in Computational Typology and Multilingual NLP (SIGTYP, NAACL) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12847](https://arxiv.org/abs/2104.12847) [cs.CL]** |
|           | (or **[arXiv:2104.12847v1](https://arxiv.org/abs/2104.12847v1) [cs.CL]** for this version) |








# 2021-04-27

[Return to Index](#Index)



<h2 id="2021-04-27-1">1. Modeling Coverage for Non-Autoregressive Neural Machine Translation
</h2>

Title: [Modeling Coverage for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2104.11897)

Authors: [Yong Shan](https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+Y), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C)

> Non-Autoregressive Neural Machine Translation (NAT) has achieved significant inference speedup by generating all tokens simultaneously. Despite its high efficiency, NAT usually suffers from two kinds of translation errors: over-translation (e.g. repeated tokens) and under-translation (e.g. missing translations), which eventually limits the translation quality. In this paper, we argue that these issues of NAT can be addressed through coverage modeling, which has been proved to be useful in autoregressive decoding. We propose a novel Coverage-NAT to model the coverage information directly by a token-level coverage iterative refinement mechanism and a sentence-level coverage agreement, which can remind the model if a source token has been translated or not and improve the semantics consistency between the translation and the source, respectively. Experimental results on WMT14 En-De and WMT16 En-Ro translation tasks show that our method can alleviate those errors and achieve strong improvements over the baseline system.

| Comments: | Accepted by the 2021 International Joint Conference on Neural Networks (IJCNN 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.11897](https://arxiv.org/abs/2104.11897) [cs.CL]** |
|           | (or **[arXiv:2104.11897v1](https://arxiv.org/abs/2104.11897v1) [cs.CL]** for this version) |





<h2 id="2021-04-27-2">2. Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation
</h2>

Title: [Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation](https://arxiv.org/abs/2104.11928)

Authors: [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Zhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xiao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Task-agnostic knowledge distillation, a teacher-student framework, has been proved effective for BERT compression. Although achieving promising results on NLP tasks, it requires enormous computational resources. In this paper, we propose Extract Then Distill (ETD), a generic and flexible strategy to reuse the teacher's parameters for efficient and effective task-agnostic distillation, which can be applied to students of any size. Specifically, we introduce two variants of ETD, ETD-Rand and ETD-Impt, which extract the teacher's parameters in a random manner and by following an importance metric respectively. In this way, the student has already acquired some knowledge at the beginning of the distillation process, which makes the distillation process converge faster. We demonstrate the effectiveness of ETD on the GLUE benchmark and SQuAD. The experimental results show that: (1) compared with the baseline without an ETD strategy, ETD can save 70\% of computation cost. Moreover, it achieves better results than the baseline when using the same computing resource. (2) ETD is generic and has been proven effective for different distillation methods (e.g., TinyBERT and MiniLM) and students of different sizes. The source code will be publicly available upon publication.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.11928](https://arxiv.org/abs/2104.11928) [cs.CL]** |
|           | (or **[arXiv:2104.11928v1](https://arxiv.org/abs/2104.11928v1) [cs.CL]** for this version) |





<h2 id="2021-04-27-3">3. Automatic Post-Editing for Translating Chinese Novels to Vietnamese
</h2>

Title: [Automatic Post-Editing for Translating Chinese Novels to Vietnamese](https://arxiv.org/abs/2104.12128)

Authors: [Thanh Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Dai Quoc Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+D+Q)

> Automatic post-editing (APE) is an important remedy for reducing errors of raw translated texts that are produced by machine translation (MT) systems or software-aided translation. In this paper, we present the first attempt to tackle the APE task for Vietnamese. Specifically, we construct the first large-scale dataset of 5M Vietnamese translated and corrected sentence pairs. We then apply strong neural MT models to handle the APE task, using our constructed dataset. Experimental results from both automatic and human evaluations show the effectiveness of the neural MT models in handling the Vietnamese APE task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.12128](https://arxiv.org/abs/2104.12128) [cs.CL]** |
|           | (or **[arXiv:2104.12128v1](https://arxiv.org/abs/2104.12128v1) [cs.CL]** for this version) |





<h2 id="2021-04-27-4">4. XLM-T: A Multilingual Language Model Toolkit for Twitter
</h2>

Title: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250)

Authors: [Francesco Barbieri](https://arxiv.org/search/cs?searchtype=author&query=Barbieri%2C+F), [Luis Espinosa Anke](https://arxiv.org/search/cs?searchtype=author&query=Anke%2C+L+E), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J)

> Language models are ubiquitous in current NLP, and their multilingual capacity has recently attracted considerable attention. However, current analyses have almost exclusively focused on (multilingual variants of) standard benchmarks, and have relied on clean pre-training and task-specific corpora as multilingual signals. In this paper, we introduce XLM-T, a framework for using and evaluating multilingual language models in Twitter. This framework features two main assets: (1) a strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020) model pre-trained on millions of tweets in over thirty languages, alongside starter code to subsequently fine-tune on a target task; and (2) a set of unified sentiment analysis Twitter datasets in eight different languages. This is a modular framework that can easily be extended to additional tasks, as well as integrated with recent efforts also aimed at the homogenization of Twitter-specific datasets (Barbieri et al. 2020).

| Comments: | Submitted to ACL demo. Code and data available at [this https URL](https://github.com/cardiffnlp/xlm-t) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12250](https://arxiv.org/abs/2104.12250) [cs.CL]** |
|           | (or **[arXiv:2104.12250v1](https://arxiv.org/abs/2104.12250v1) [cs.CL]** for this version) |





<h2 id="2021-04-27-5">5. Reranking Machine Translation Hypotheses with Structured and Web-based Language Models
</h2>

Title: [Reranking Machine Translation Hypotheses with Structured and Web-based Language Models](https://arxiv.org/abs/2104.12277)

Authors: [Wen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Andreas Stolcke](https://arxiv.org/search/cs?searchtype=author&query=Stolcke%2C+A), [Jing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+J)

> In this paper, we investigate the use of linguistically motivated and computationally efficient structured language models for reranking N-best hypotheses in a statistical machine translation system. These language models, developed from Constraint Dependency Grammar parses, tightly integrate knowledge of words, morphological and lexical features, and syntactic dependency constraints. Two structured language models are applied for N-best rescoring, one is an almost-parsing language model, and the other utilizes more syntactic features by explicitly modeling syntactic dependencies between words. We also investigate effective and efficient language modeling methods to use N-grams extracted from up to 1 teraword of web documents. We apply all these language models for N-best re-ranking on the NIST and DARPA GALE program 2006 and 2007 machine translation evaluation tasks and find that the combination of these language models increases the BLEU score up to 1.6% absolutely on blind test sets.

| Comments:          | With a correction to the math in Figure 1 caption            |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proc. 2007 IEEE ASRU Workshop, pp. 159-164                   |
| DOI:               | [10.1109/ASRU.2007.4430102](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FASRU.2007.4430102&v=b42f7934) |
| Cite as:           | **[arXiv:2104.12277](https://arxiv.org/abs/2104.12277) [cs.CL]** |
|                    | (or **[arXiv:2104.12277v1](https://arxiv.org/abs/2104.12277v1) [cs.CL]** for this version) |







<h2 id="2021-04-27-6">6. PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation
</h2>

Title: [PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369)

Authors: [Wei Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W), [Xiaozhe Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Teng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+T), [Hui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Yi Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Y), [Zhiwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [ZhenZhang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Kaisheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Xiaoda Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Chen Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Ziyan Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Z), [Yifan Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Xinjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Jianfeng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Qi Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Q), [Yue Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Yan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Hengtao Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+H), [Dasen Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+D), [Zexuan Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+Z), [Fang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+F), [Fangqing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+F), [Han Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Lingfeng Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+L), [Yehong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Zhe Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Shaojie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Mingyue Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Shanzhi Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Gaojun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+G), [Yaowei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xuefeng Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Yonghong Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y)

> Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-α, with up to 200 billion parameters. PanGu-α is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-α in performing various tasks under few-shot or zero-shot settings.

| Comments: | The technique report for PanGu-α                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12369](https://arxiv.org/abs/2104.12369) [cs.CL]** |
|           | (or **[arXiv:2104.12369v1](https://arxiv.org/abs/2104.12369v1) [cs.CL]** for this version) |





<h2 id="2021-04-27-7">7. Attention vs non-attention for a Shapley-based explanation method
</h2>

Title: [Attention vs non-attention for a Shapley-based explanation method](https://arxiv.org/abs/2104.12424)

Authors: [Tom Kersten](https://arxiv.org/search/cs?searchtype=author&query=Kersten%2C+T), [Hugh Mee Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+H+M), [Jaap Jumelet](https://arxiv.org/search/cs?searchtype=author&query=Jumelet%2C+J), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D)

> The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods -- that are often proposed and tested in the domain of computer vision -- are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) -- a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models -- and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.

| Comments: | Accepted for publication at DeeLIO 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.12424](https://arxiv.org/abs/2104.12424) [cs.CL]** |
|           | (or **[arXiv:2104.12424v1](https://arxiv.org/abs/2104.12424v1) [cs.CL]** for this version) |



<h2 id="2021-04-27-8">8. Easy and Efficient Transformer : Scalable Inference Solution For large NLP mode
</h2>

Title: [Easy and Efficient Transformer : Scalable Inference Solution For large NLP mode](https://arxiv.org/abs/2104.12470)

Authors: [Gongzheng li](https://arxiv.org/search/cs?searchtype=author&query=li%2C+G), [Yadong Xi](https://arxiv.org/search/cs?searchtype=author&query=Xi%2C+Y), [Jingzhen Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+J), [Duan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Bai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Changjie Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Xiaoxi Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Zeng Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z)

> The ultra-large-scale pre-training model can effectively improve the effect of a variety of tasks, and it also brings a heavy computational burden to inference. This paper introduces a series of ultra-large-scale pre-training model optimization methods that combine algorithm characteristics and GPU processor hardware characteristics, and on this basis, propose an inference engine -- Easy and Efficient Transformer (EET), Which has a significant performance improvement over the existing schemes.
> We firstly introduce a pre-padding decoding mechanism that improves token parallelism for generation tasks. Then we design high optimized kernels to remove sequence masks and achieve cost-free calculation for padding tokens, as well as support long sequence and long embedding sizes. Thirdly a user-friendly inference system with an easy service pipeline was introduced which greatly reduces the difficulty of engineering deployment with high throughput. Compared to Faster Transformer's implementation for GPT-2 on A100, EET achieves a 1.5-15x state-of-art speedup varying with context length.EET is available [this https URL](https://github.com/NetEase-FuXi/EET).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.12470](https://arxiv.org/abs/2104.12470) [cs.CL]** |
|           | (or **[arXiv:2104.12470v1](https://arxiv.org/abs/2104.12470v1) [cs.CL]** for this version) |





# 2021-04-26

[Return to Index](#Index)



<h2 id="2021-04-26-1">1. Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation
</h2>

Title: [Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation](https://arxiv.org/abs/2104.11710)

Authors:[Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> The audio segmentation mismatch between training data and those seen at run-time is a major problem in direct speech translation. Indeed, while systems are usually trained on manually segmented corpora, in real use cases they are often presented with continuous audio requiring automatic (and sub-optimal) segmentation. After comparing existing techniques (VAD-based, fixed-length and hybrid segmentation methods), in this paper we propose enhanced hybrid solutions to produce better results without sacrificing latency. Through experiments on different domains and language pairs, we show that our methods outperform all the other techniques, reducing by at least 30% the gap between the traditional VAD-based approach and optimal manual segmentation.

| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.11710](https://arxiv.org/abs/2104.11710) [cs.SD]** |
|           | (or **[arXiv:2104.11710v1](https://arxiv.org/abs/2104.11710v1) [cs.SD]** for this version) |





<h2 id="2021-04-26-2">2. LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech
</h2>

Title: [LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech](https://arxiv.org/abs/2104.11462)

Authors:[Solene Evain](https://arxiv.org/search/cs?searchtype=author&query=Evain%2C+S), [Ha Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Marcely Zanon Boito](https://arxiv.org/search/cs?searchtype=author&query=Boito%2C+M+Z), [Salima Mdhaffar](https://arxiv.org/search/cs?searchtype=author&query=Mdhaffar%2C+S), [Sina Alisamir](https://arxiv.org/search/cs?searchtype=author&query=Alisamir%2C+S), [Ziyi Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+Z), [Natalia Tomashenko](https://arxiv.org/search/cs?searchtype=author&query=Tomashenko%2C+N), [Marco Dinarelli](https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M), [Titouan Parcollet](https://arxiv.org/search/cs?searchtype=author&query=Parcollet%2C+T), [Alexandre Allauzen](https://arxiv.org/search/cs?searchtype=author&query=Allauzen%2C+A), [Yannick Esteve](https://arxiv.org/search/cs?searchtype=author&query=Esteve%2C+Y), [Benjamin Lecouteux](https://arxiv.org/search/cs?searchtype=author&query=Lecouteux%2C+B), [Francois Portet](https://arxiv.org/search/cs?searchtype=author&query=Portet%2C+F), [Solange Rossato](https://arxiv.org/search/cs?searchtype=author&query=Rossato%2C+S), [Fabien Ringeval](https://arxiv.org/search/cs?searchtype=author&query=Ringeval%2C+F), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This renders difficult the objective comparison between SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose LeBenchmark: a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also target speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact. LeBenchmark is shared with the scientific community for reproducible research in SSL from speech.

| Comments: | Submitted to Interspeech 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.11462](https://arxiv.org/abs/2104.11462) [cs.CL]** |
|           | (or **[arXiv:2104.11462v1](https://arxiv.org/abs/2104.11462v1) [cs.CL]** for this version) |






# 2021-04-23

[Return to Index](#Index)



<h2 id="2021-04-23-1">1. Disfluency Detection with Unlabeled Data and Small BERT Models
</h2>

Title: [Disfluency Detection with Unlabeled Data and Small BERT Models](https://arxiv.org/abs/2104.10769)

Authors: [Johann C. Rocholl](https://arxiv.org/search/cs?searchtype=author&query=Rocholl%2C+J+C), [Vicky Zayats](https://arxiv.org/search/cs?searchtype=author&query=Zayats%2C+V), [Daniel D. Walker](https://arxiv.org/search/cs?searchtype=author&query=Walker%2C+D+D), [Noah B. Murad](https://arxiv.org/search/cs?searchtype=author&query=Murad%2C+N+B), [Aaron Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+A), [Daniel J. Liebling](https://arxiv.org/search/cs?searchtype=author&query=Liebling%2C+D+J)

> Disfluency detection models now approach high accuracy on English text. However, little exploration has been done in improving the size and inference time of the model. At the same time, automatic speech recognition (ASR) models are moving from server-side inference to local, on-device inference. Supporting models in the transcription pipeline (like disfluency detection) must follow suit. In this work we concentrate on the disfluency detection task, focusing on small, fast, on-device models based on the BERT architecture. We demonstrate it is possible to train disfluency detection models as small as 1.3 MiB, while retaining high performance. We build on previous work that showed the benefit of data augmentation approaches such as self-training. Then, we evaluate the effect of domain mismatch between conversational and written text on model performance. We find that domain adaptation and data augmentation strategies have a more pronounced effect on these smaller models, as compared to conventional BERT models.

| Comments: | Submitted to INTERSPEECH 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.10769](https://arxiv.org/abs/2104.10769) [cs.CL]** |
|           | (or **[arXiv:2104.10769v1](https://arxiv.org/abs/2104.10769v1) [cs.CL]** for this version) |





<h2 id="2021-04-23-2">2. Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?
</h2>

Title: [Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?](https://arxiv.org/abs/2104.10809)

Authors: [William Merrill](https://arxiv.org/search/cs?searchtype=author&query=Merrill%2C+W), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever "understand" raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of "assertions": contexts within raw text that provide indirect clues about underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation if all expressions in the language are referentially transparent. However, if the language uses non-transparent patterns like variable binding, we show that emulation can become an uncomputable problem. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to "understand".

| Comments: | Accepted at TACL; pre-MIT Press publication version          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.10809](https://arxiv.org/abs/2104.10809) [cs.CL]** |
|           | (or **[arXiv:2104.10809v1](https://arxiv.org/abs/2104.10809v1) [cs.CL]** for this version) |







# 2021-04-22

[Return to Index](#Index)



<h2 id="2021-04-22-1">1. Revisiting Document Representations for Large-Scale Zero-Shot Learning
</h2>

Title: [Revisiting Document Representations for Large-Scale Zero-Shot Learning](https://arxiv.org/abs/2104.10355)

Authors: [Jihyung Kil](https://arxiv.org/search/cs?searchtype=author&query=Kil%2C+J), [Wei-Lun Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+W)

> Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.10355](https://arxiv.org/abs/2104.10355) [cs.CV]** |
|           | (or **[arXiv:2104.10355v1](https://arxiv.org/abs/2104.10355v1) [cs.CV]** for this version) |





<h2 id="2021-04-22-2">2. Discriminative Self-training for Punctuation Prediction
</h2>

Title: [Discriminative Self-training for Punctuation Prediction](https://arxiv.org/abs/2104.10339)

Authors: [Qian Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Wen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Mengzhe Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Qinglin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q)

> Punctuation prediction for automatic speech recognition (ASR) output transcripts plays a crucial role for improving the readability of the ASR transcripts and for improving the performance of downstream natural language processing applications. However, achieving good performance on punctuation prediction often requires large amounts of labeled speech transcripts, which is expensive and laborious. In this paper, we propose a Discriminative Self-Training approach with weighted loss and discriminative label smoothing to exploit unlabeled speech transcripts. Experimental results on the English IWSLT2011 benchmark test set and an internal Chinese spoken language dataset demonstrate that the proposed approach achieves significant improvement on punctuation prediction accuracy over strong baselines including BERT, RoBERTa, and ELECTRA models. The proposed Discriminative Self-Training approach outperforms the vanilla self-training approach. We establish a new state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current SOTA model by 1.3% absolute gain on F1.

| Comments: | Submitted to INTERSPEECH 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.10339](https://arxiv.org/abs/2104.10339) [cs.CL]** |
|           | (or **[arXiv:2104.10339v1](https://arxiv.org/abs/2104.10339v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-3">3. Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning
</h2>

Title: [Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning](https://arxiv.org/abs/2104.10357)

Authors: [Qian Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Wen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Qinglin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q)

> In the traditional cascading architecture for spoken language understanding (SLU), it has been observed that automatic speech recognition errors could be detrimental to the performance of natural language understanding. End-to-end (E2E) SLU models have been proposed to directly map speech input to desired semantic frame with a single model, hence mitigating ASR error propagation. Recently, pre-training technologies have been explored for these E2E models. In this paper, we propose a novel joint textual-phonetic pre-training approach for learning spoken language representations, aiming at exploring the full potentials of phonetic information to improve SLU robustness to ASR errors. We explore phoneme labels as high-level speech features, and design and compare pre-training tasks based on conditional masked language model objectives and inter-sentence relation objectives. We also investigate the efficacy of combining textual and phonetic information during fine-tuning. Experimental results on spoken language understanding benchmarks, Fluent Speech Commands and SNIPS, show that the proposed approach significantly outperforms strong baseline models and improves robustness of spoken language understanding to ASR errors.

| Comments: | Submitted to INTERSPEECH 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.10357](https://arxiv.org/abs/2104.10357) [cs.CL]** |
|           | (or **[arXiv:2104.10357v1](https://arxiv.org/abs/2104.10357v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-4">4. End-to-end Speech Translation via Cross-modal Progressive Training
</h2>

Title: [End-to-end Speech Translation via Cross-modal Progressive Training](https://arxiv.org/abs/2104.10380)

Authors: [Rong Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+R), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> End-to-end speech translation models have become a new trend in the research due to their potential of reducing error propagation. However, these models still suffer from the challenge of data scarcity. How to effectively make use of unlabeled or other parallel corpora from machine translation is promising but still an open problem. In this paper, we propose Cross Speech-Text Network (XSTNet), an end-to-end model for speech-to-text translation. XSTNet takes both speech and text as input and outputs both transcription and translation text. The model benefits from its three key design aspects: a self supervising pre-trained sub-network as the audio encoder, a multi-task training objective to exploit additional parallel bilingual text, and a progressive training procedure. We evaluate the performance of XSTNet and baselines on the MuST-C En-De/Fr/Ru datasets. XSTNet achieves state-of-the-art results on all three language directions with an average BLEU of 27.8, outperforming the previous best method by 3.7 BLEU. The code and the models will be released to the public.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.10380](https://arxiv.org/abs/2104.10380) [cs.CL]** |
|           | (or **[arXiv:2104.10380v1](https://arxiv.org/abs/2104.10380v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-5">5. On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs
</h2>

Title: [On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs](https://arxiv.org/abs/2104.10408)

Authors: [Roman Grundkiewicz](https://arxiv.org/search/cs?searchtype=author&query=Grundkiewicz%2C+R), [Marcin Junczys-Dowmunt](https://arxiv.org/search/cs?searchtype=author&query=Junczys-Dowmunt%2C+M), [Christian Federmann](https://arxiv.org/search/cs?searchtype=author&query=Federmann%2C+C), [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T)

> Recent studies emphasize the need of document context in human evaluation of machine translations, but little research has been done on the impact of user interfaces on annotator productivity and the reliability of assessments. In this work, we compare human assessment data from the last two WMT evaluation campaigns collected via two different methods for document-level evaluation. Our analysis shows that a document-centric approach to evaluation where the annotator is presented with the entire document context on a screen leads to higher quality segment and document level assessments. It improves the correlation between segment and document scores and increases inter-annotator agreement for document scores but is considerably more time consuming for annotators.

| Comments: | Presented at HumEval, EACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.10408](https://arxiv.org/abs/2104.10408) [cs.CL]** |
|           | (or **[arXiv:2104.10408v1](https://arxiv.org/abs/2104.10408v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-6">6. Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?
</h2>

Title: [Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?](https://arxiv.org/abs/2104.10441)

Authors: [Tim Isbister](https://arxiv.org/search/cs?searchtype=author&query=Isbister%2C+T), [Fredrik Carlsson](https://arxiv.org/search/cs?searchtype=author&query=Carlsson%2C+F), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> Most work in NLP makes the assumption that it is desirable to develop solutions in the native language in question. There is consequently a strong trend towards building native language models even for low-resource languages. This paper questions this development, and explores the idea of simply translating the data into English, thereby enabling the use of pretrained, and large-scale, English language models. We demonstrate empirically that a large English language model coupled with modern machine translation outperforms native language models in most Scandinavian languages. The exception to this is Finnish, which we assume is due to inferior translation quality. Our results suggest that machine translation is a mature technology, which raises a serious counter-argument for training native language models for low-resource languages. This paper therefore strives to make a provocative but important point. As English language models are improving at an unprecedented pace, which in turn improves machine translation, it is from an empirical and environmental stand-point more effective to translate data from low-resource languages into English, than to build language models for such languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.10441](https://arxiv.org/abs/2104.10441) [cs.CL]** |
|           | (or **[arXiv:2104.10441v1](https://arxiv.org/abs/2104.10441v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-7">7. Improving BERT Pretraining with Syntactic Supervision
</h2>

Title: [Improving BERT Pretraining with Syntactic Supervision](https://arxiv.org/abs/2104.10516)

Authors: [Giorgos Tziafas](https://arxiv.org/search/cs?searchtype=author&query=Tziafas%2C+G), [Konstantinos Kogkalidis](https://arxiv.org/search/cs?searchtype=author&query=Kogkalidis%2C+K), [Gijs Wijnholds](https://arxiv.org/search/cs?searchtype=author&query=Wijnholds%2C+G), [Michael Moortgat](https://arxiv.org/search/cs?searchtype=author&query=Moortgat%2C+M)

> Bidirectional masked Transformers have become the core theme in the current NLP landscape. Despite their impressive benchmarks, a recurring theme in recent research has been to question such models' capacity for syntactic generalization. In this work, we seek to address this question by adding a supervised, token-level supertagging objective to standard unsupervised pretraining, enabling the explicit incorporation of syntactic biases into the network's training dynamics. Our approach is straightforward to implement, induces a marginal computational overhead and is general enough to adapt to a variety of settings. We apply our methodology on Lassy Large, an automatically annotated corpus of written Dutch. Our experiments suggest that our syntax-aware model performs on par with established baselines, despite Lassy Large being one order of magnitude smaller than commonly used corpora.

| Comments: | 4 pages, rejected by IWCS due to "not fitting the conference theme" |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.10516](https://arxiv.org/abs/2104.10516) [cs.CL]** |
|           | (or **[arXiv:2104.10516v1](https://arxiv.org/abs/2104.10516v1) [cs.CL]** for this version) |





<h2 id="2021-04-22-8">8. Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models
</h2>

Title: [Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models](https://arxiv.org/abs/2104.10658)

Authors: [Dewayne Whitfield](https://arxiv.org/search/cs?searchtype=author&query=Whitfield%2C+D)

> Classification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training. It is becoming common practice to utilize synthetic data to boost the performance of Machine Learning Models. It is reported that Shell is using synthetic data to build models to detect problems that rarely occur; for example Shell created synthetic data to help models to identify deteriorating oil lines. It is common practice for Machine Learning Practitioners to generate synthetic data by rotating, flipping, and cropping images to increase the volume of image data to train Convolutional Neural Networks. The purpose of this paper is to explore creating and utilizing synthetic NLP data to improve the performance of Natural Language Processing Machine Learning Classification Models. In this paper I used a Yelp pizza restaurant reviews dataset and transfer learning to fine-tune a pre-trained GPT-2 Transformer Model to generate synthetic pizza reviews data. I then combined this synthetic data with the original genuine data to create a new joint dataset. The new combined model significantly outperformed the original model in accuracy and precision.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.10658](https://arxiv.org/abs/2104.10658) [cs.CL]** |
|           | (or **[arXiv:2104.10658v1](https://arxiv.org/abs/2104.10658v1) [cs.CL]** for this version) |







# 2021-04-21

[Return to Index](#Index)



<h2 id="2021-04-21-1">1. Can Latent Alignments Improve Autoregressive Machine Translation?
</h2>

Title: [Can Latent Alignments Improve Autoregressive Machine Translation?](https://arxiv.org/abs/2104.09554)

Authors: [Adi Haviv](https://arxiv.org/search/cs?searchtype=author&query=Haviv%2C+A), [Lior Vassertail](https://arxiv.org/search/cs?searchtype=author&query=Vassertail%2C+L), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.09554](https://arxiv.org/abs/2104.09554) [cs.CL]** |
|           | (or **[arXiv:2104.09554v1](https://arxiv.org/abs/2104.09554v1) [cs.CL]** for this version) |





<h2 id="2021-04-21-2">2. Efficient pre-training objectives for Transformers
</h2>

Title: [Efficient pre-training objectives for Transformers](https://arxiv.org/abs/2104.09694)

Authors: [Luca Di Liello](https://arxiv.org/search/cs?searchtype=author&query=Di+Liello%2C+L), [Matteo Gabburo](https://arxiv.org/search/cs?searchtype=author&query=Gabburo%2C+M), [Alessandro Moschitti](https://arxiv.org/search/cs?searchtype=author&query=Moschitti%2C+A)

> The Transformer architecture deeply changed the natural language processing, outperforming all previous state-of-the-art models. However, well-known Transformer models like BERT, RoBERTa, and GPT-2 require a huge compute budget to create a high quality contextualised representation. In this paper, we study several efficient pre-training objectives for Transformers-based models. By testing these objectives on different tasks, we determine which of the ELECTRA model's new features is the most relevant. We confirm that Transformers pre-training is improved when the input does not contain masked tokens and that the usage of the whole output to compute the loss reduces training time. Moreover, inspired by ELECTRA, we study a model composed of two blocks; a discriminator and a simple generator based on a statistical model with no impact on the computational performances. Besides, we prove that eliminating the MASK token and considering the whole output during the loss computation are essential choices to improve performance. Furthermore, we show that it is possible to efficiently train BERT-like models using a discriminative approach as in ELECTRA but without a complex generator, which is expensive. Finally, we show that ELECTRA benefits heavily from a state-of-the-art hyper-parameters search.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.09694](https://arxiv.org/abs/2104.09694) [cs.CL]** |
|           | (or **[arXiv:2104.09694v1](https://arxiv.org/abs/2104.09694v1) [cs.CL]** for this version) |





<h2 id="2021-04-21-3">3. Problems and Countermeasures in Natural Language Processing Evaluation
</h2>

Title: [Problems and Countermeasures in Natural Language Processing Evaluation](https://arxiv.org/abs/2104.09712)

Authors: [Qingxiu Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z), [Weidong Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+W), [Baobao Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B)

> Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluation. Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.09712](https://arxiv.org/abs/2104.09712) [cs.CL]** |
|           | (or **[arXiv:2104.09712v1](https://arxiv.org/abs/2104.09712v1) [cs.CL]** for this version) |





<h2 id="2021-04-21-4">4. Addressing the Vulnerability of NMT in Input Perturbations
</h2>

Title: [Addressing the Vulnerability of NMT in Input Perturbations](https://arxiv.org/abs/2104.09810)

Authors: [Weiwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Ai Ti Aw](https://arxiv.org/search/cs?searchtype=author&query=Aw%2C+A+T), [Yang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y), [Kui Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+K), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation.

| Comments: | Accepted by NAACL 2021 Industry Track                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.09810](https://arxiv.org/abs/2104.09810) [cs.CL]** |
|           | (or **[arXiv:2104.09810v1](https://arxiv.org/abs/2104.09810v1) [cs.CL]** for this version) |





<h2 id="2021-04-21-5">5. Grammatical Error Generation Based on Translated Fragments
</h2>

Title: [Grammatical Error Generation Based on Translated Fragments](https://arxiv.org/abs/2104.09933)

Authors: [Eetu Sjöblom](https://arxiv.org/search/cs?searchtype=author&query=Sjöblom%2C+E), [Mathias Creutz](https://arxiv.org/search/cs?searchtype=author&query=Creutz%2C+M), [Teemu Vahtola](https://arxiv.org/search/cs?searchtype=author&query=Vahtola%2C+T)

> We perform neural machine translation of sentence fragments in order to create large amounts of training data for English grammatical error correction. Our method aims at simulating mistakes made by second language learners, and produces a wider range of non-native style language in comparison to state-of-the-art synthetic data creation methods. In addition to purely grammatical errors, our approach generates other types of errors, such as lexical errors. We perform grammatical error correction experiments using neural sequence-to-sequence models, and carry out quantitative and qualitative evaluation. A model trained on data created using our proposed method is shown to outperform a baseline model on test data with a high proportion of errors.

| Comments: | Accepted for NoDaLiDa 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.09933](https://arxiv.org/abs/2104.09933) [cs.CL]** |
|           | (or **[arXiv:2104.09933v1](https://arxiv.org/abs/2104.09933v1) [cs.CL]** for this version) |





<h2 id="2021-04-21-6">6. Towards Solving Multimodal Comprehension
</h2>

Title: [Towards Solving Multimodal Comprehension](https://arxiv.org/abs/2104.10139)

Authors: [Pritish Sahu](https://arxiv.org/search/cs?searchtype=author&query=Sahu%2C+P), [Karan Sikka](https://arxiv.org/search/cs?searchtype=author&query=Sikka%2C+K), [Ajay Divakaran](https://arxiv.org/search/cs?searchtype=author&query=Divakaran%2C+A)

> This paper targets the problem of procedural multimodal machine comprehension (M3C). This task requires an AI to comprehend given steps of multimodal instructions and then answer questions. Compared to vanilla machine comprehension tasks where an AI is required only to understand a textual input, procedural M3C is more challenging as the AI needs to comprehend both the temporal and causal factors along with multimodal inputs. Recently Yagcioglu et al. [35] introduced RecipeQA dataset to evaluate M3C. Our first contribution is the introduction of two new M3C datasets- WoodworkQA and DecorationQA with 16K and 10K instructional procedures, respectively. We then evaluate M3C using a textual cloze style question-answering task and highlight an inherent bias in the question answer generation method from [35] that enables a naive baseline to cheat by learning from only answer choices. This naive baseline performs similar to a popular method used in question answering- Impatient Reader [6] that uses attention over both the context and the query. We hypothesized that this naturally occurring bias present in the dataset affects even the best performing model. We verify our proposed hypothesis and propose an algorithm capable of modifying the given dataset to remove the bias elements. Finally, we report our performance on the debiased dataset with several strong baselines. We observe that the performance of all methods falls by a margin of 8% - 16% after correcting for the bias. We hope these datasets and the analysis will provide valuable benchmarks and encourage further research in this area.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.10139](https://arxiv.org/abs/2104.10139) [cs.CL]** |
|           | (or **[arXiv:2104.10139v1](https://arxiv.org/abs/2104.10139v1) [cs.CL]** for this version) |







# 2021-04-20

[Return to Index](#Index)



<h2 id="2021-04-20-1">1. Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training
</h2>

Title: [Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training](https://arxiv.org/abs/2104.08645)

Authors: [Kuan-Hao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+K), [Wasi Uddin Ahmad](https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+W+U), [Nanyun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+N), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)

> In recent years, pre-trained multilingual language models, such as multilingual BERT and XLM-R, exhibit good performance on zero-shot cross-lingual transfer learning. However, since their multilingual contextual embedding spaces for different languages are not perfectly aligned, the difference between representations of different languages might cause zero-shot cross-lingual transfer failed in some cases. In this work, we draw connections between those failed cases and adversarial examples. We then propose to use robust training methods to train a robust model that can tolerate some noise in input embeddings. We study two widely used robust training methods: adversarial training and randomized smoothing. The experimental results demonstrate that robust training can improve zero-shot cross-lingual transfer for text classification. The performance improvements become significant when the distance between the source language and the target language increases.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08645](https://arxiv.org/abs/2104.08645) [cs.CL]** |
|           | (or **[arXiv:2104.08645v1](https://arxiv.org/abs/2104.08645v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-2">2. Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models
</h2>

Title: [Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2104.08666)

Authors: [Tejas Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+T), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y)

> Numerous works have analyzed biases in vision and pre-trained language models individually - however, less attention has been paid to how these biases interact in multimodal settings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intra- and inter-modality associations and biases learned by these models. Specifically, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these findings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08666](https://arxiv.org/abs/2104.08666) [cs.CL]** |
|           | (or **[arXiv:2104.08666v1](https://arxiv.org/abs/2104.08666v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-3">3. Improving Neural Machine Translation with Compact Word Embedding Tables
</h2>

Title: [Improving Neural Machine Translation with Compact Word Embedding Tables](https://arxiv.org/abs/2104.08677)

Authors: [Krtin Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+K), [Mehdi Rezagholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Rezagholizadeh%2C+M), [Yiu Sing Lau](https://arxiv.org/search/cs?searchtype=author&query=Lau%2C+Y+S), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Embedding matrices are key components in neural natural language processing (NLP) models that are responsible to provide numerical representations of input tokens.\footnote{In this paper words and subwords are referred to as \textit{tokens} and the term \textit{embedding} only refers to embeddings of inputs.} In this paper, we analyze the impact and utility of such matrices in the context of neural machine translation (NMT). We show that detracting syntactic and semantic information from word embeddings and running NMT systems with random embeddings is not as damaging as it initially sounds. We also show how incorporating only a limited amount of task-specific knowledge from fully-trained embeddings can boost the performance NMT systems. Our findings demonstrate that in exchange for negligible deterioration in performance, any NMT model can be run with partially random embeddings. Working with such structures means a minimal memory requirement as there is no longer need to store large embedding tables, which is a significant gain in industrial and on-device settings. We evaluated our embeddings in translating {English} into {German} and {French} and achieved a 5.3x compression rate. Despite having a considerably smaller architecture, our models in some cases are even able to outperform state-of-the-art baselines.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08677](https://arxiv.org/abs/2104.08677) [cs.CL]** |
|           | (or **[arXiv:2104.08677v1](https://arxiv.org/abs/2104.08677v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-4">4. mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs
</h2>

Title: [mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs](https://arxiv.org/abs/2104.08692)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Shaohan Huang Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+S+H+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed mT6 improves cross-lingual transferability over mT5.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08692](https://arxiv.org/abs/2104.08692) [cs.CL]** |
|           | (or **[arXiv:2104.08692v1](https://arxiv.org/abs/2104.08692v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-5">5. Knowledge Neurons in Pretrained Transformers
</h2>

Title: [Knowledge Neurons in Pretrained Transformers](https://arxiv.org/abs/2104.08696)

Authors: [Damai Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Yaru Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Y), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we explore how implicit knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We present that the activation of such knowledge neurons is highly correlated to the expression of their corresponding facts. In addition, even without fine-tuning, we can leverage knowledge neurons to explicitly edit (such as update, and erase) specific factual knowledge for pretrained Transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08696](https://arxiv.org/abs/2104.08696) [cs.CL]** |
|           | (or **[arXiv:2104.08696v1](https://arxiv.org/abs/2104.08696v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-6">6. Zero-shot Cross-lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders
</h2>

Title: [Zero-shot Cross-lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders](https://arxiv.org/abs/2104.08757)

Authors: [Guanhua Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Jia Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+J), [Wenping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Previous works mainly focus on improving cross-lingual transfer for NLU tasks with multilingual pretrained encoder (MPE), or improving the translation performance on NMT task with BERT. However, how to improve the cross-lingual transfer of NMT model with multilingual pretrained encoder is under-explored. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with one parallel dataset and an off-the-shelf MPE, then is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. The SixT model leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. The extensive experiments prove that SixT significantly improves the translation quality of the unseen languages. With much less computation cost and training data, our model achieves better performance on many-to-English testsets than CRISS and m2m-100, two strong multilingual NMT baselines.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08757](https://arxiv.org/abs/2104.08757) [cs.CL]** |
|           | (or **[arXiv:2104.08757v1](https://arxiv.org/abs/2104.08757v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-7">7. On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation
</h2>

Title: [On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation](https://arxiv.org/abs/2104.08771)

Authors: [Mozhdeh Gheini](https://arxiv.org/search/cs?searchtype=author&query=Gheini%2C+M), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

> We study the power of cross-attention in the Transformer architecture within the context of machine translation. In transfer learning experiments, where we fine-tune a translation model on a dataset with one new language, we find that, apart from the new language's embeddings, only the cross-attention parameters need to be fine-tuned to obtain competitive BLEU performance. We provide insights into why this is the case and further find that limiting fine-tuning in this manner yields cross-lingually aligned type embeddings. The implications of this finding include a mitigation of catastrophic forgetting in the network and the potential for zero-shot translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08771](https://arxiv.org/abs/2104.08771) [cs.CL]** |
|           | (or **[arXiv:2104.08771v1](https://arxiv.org/abs/2104.08771v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-8">8. Chinese Sentences Similarity via Cross-Attention Based Siamese Network
</h2>

Title: [Chinese Sentences Similarity via Cross-Attention Based Siamese Network](https://arxiv.org/abs/2104.08787)

Authors: [Zhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xiangxie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Yicong Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Y)

> Measuring sentence similarity is a key research area nowadays as it allows machines to better understand human languages. In this paper, we proposed a Cross-Attention Siamese Network (CATsNet) to carry out the task of learning the semantic meanings of Chinese sentences and comparing the similarity between two sentences. This novel model is capable of catching non-local features. Additionally, we also tried to apply the long short-term memory (LSTM) network in the model to improve its performance. The experiments were conducted on the LCQMC dataset and the results showed that our model could achieve a higher accuracy than previous work.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08787](https://arxiv.org/abs/2104.08787) [cs.CL]** |
|           | (or **[arXiv:2104.08787v1](https://arxiv.org/abs/2104.08787v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-9">9. Lifelong Learning of Few-shot Learners across NLP Tasks
</h2>

Title: [Lifelong Learning of Few-shot Learners across NLP Tasks](https://arxiv.org/abs/2104.08808)

Authors: [Xisen Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+X), [Mohammad Rostami](https://arxiv.org/search/cs?searchtype=author&query=Rostami%2C+M), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Recent advances in large pre-trained language models have greatly improved the performance on a broad set of NLP tasks. However, adapting an existing model to new tasks often requires (repeated) re-training over enormous labeled data that is prohibitively expensive to obtain. Moreover, models learned on new tasks may gradually "forget" about the knowledge learned from earlier tasks (i.e., catastrophic forgetting). In this paper, we study the challenge of lifelong learning to few-shot learn over a sequence of diverse NLP tasks, through continuously fine-tuning a language model. We investigate the model's ability of few-shot generalization to new tasks while retaining its performance on the previously learned tasks. We explore existing continual learning methods in solving this problem and propose a continual meta-learning approach which learns to generate adapter weights from a few examples while regularizing changes of the weights to mitigate catastrophic forgetting. We demonstrate our approach preserves model performance over training tasks and leads to positive knowledge transfer when the future tasks are learned.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08808](https://arxiv.org/abs/2104.08808) [cs.CL]** |
|           | (or **[arXiv:2104.08808v1](https://arxiv.org/abs/2104.08808v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-10">10. Contrastive Out-of-Distribution Detection for Pretrained Transformers
</h2>

Title: [Contrastive Out-of-Distribution Detection for Pretrained Transformers](https://arxiv.org/abs/2104.08812)

Authors: [Wenxuan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Muhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M)

> Pretrained transformers achieve remarkable performance when the test data follows the same distribution as the training data. However, in real-world NLU tasks, the model often faces out-of-distribution (OoD) instances. Such instances can cause the severe semantic shift problem to inference, hence they are supposed to be identified and rejected by the model. In this paper, we study the OoD detection problem for pretrained transformers using only in-distribution data in training. We observe that such instances can be found using the Mahalanobis distance in the penultimate layer. We further propose a contrastive loss that improves the compactness of representations, such that OoD instances can be better differentiated from in-distribution ones. Experiments on the GLUE benchmark demonstrate the effectiveness of the proposed methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08812](https://arxiv.org/abs/2104.08812) [cs.CL]** |
|           | (or **[arXiv:2104.08812v1](https://arxiv.org/abs/2104.08812v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-11">11. Stream-level Latency Evaluation for Simultaneous Machine Translation
</h2>

Title: [Stream-level Latency Evaluation for Simultaneous Machine Translation](https://arxiv.org/abs/2104.08817)

Authors: [Javier Iranzo-Sánchez](https://arxiv.org/search/cs?searchtype=author&query=Iranzo-Sánchez%2C+J), [Jorge Civera](https://arxiv.org/search/cs?searchtype=author&query=Civera%2C+J), [Alfons Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+A)

> Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08817](https://arxiv.org/abs/2104.08817) [cs.CL]** |
|           | (or **[arXiv:2104.08817v1](https://arxiv.org/abs/2104.08817v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-12">12. SimCSE: Simple Contrastive Learning of Sentence Embeddings
</h2>

Title: [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)

Authors: [Tianyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Xingcheng Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+X), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We hypothesize that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we draw inspiration from the recent success of learning sentence embeddings from natural language inference (NLI) datasets and incorporate annotated pairs from NLI datasets into contrastive learning by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT-base achieve an average of 74.5% and 81.6% Spearman's correlation respectively, a 7.9 and 4.6 points improvement compared to previous best results. We also show that contrastive learning theoretically regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08821](https://arxiv.org/abs/2104.08821) [cs.CL]** |
|           | (or **[arXiv:2104.08821v1](https://arxiv.org/abs/2104.08821v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-13">13. GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation
</h2>

Title: [GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation](https://arxiv.org/abs/2104.08826)

Authors: [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Dongju Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Jaewook Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J), [Sang-Woo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Woomyeong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+W)

> Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach.

| Comments: | 11 pages, 7 tables, 2 figures                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.08826](https://arxiv.org/abs/2104.08826) [cs.CL]** |
|           | (or **[arXiv:2104.08826v1](https://arxiv.org/abs/2104.08826v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-14">14. CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP
</h2>

Title: [CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP](https://arxiv.org/abs/2104.08835)

Authors: [Qinyuan Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Q), [Bill Yuchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B+Y), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Humans can learn a new language task more efficiently than machines, conceivably by leveraging their prior experience and knowledge in learning other tasks. In this paper, we explore whether such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a task setup for studying cross-task few-shot learning ability, which standardizes seen/unseen task splits, data access during different learning stages, and the evaluation protocols. In addition, we present NLP Few-shot Gym, a repository of 160 few-shot NLP tasks, covering diverse task categories and applications, and converted to a unified text-to-text format. Our empirical analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. Additionally, the advantage lasts into medium-resource scenarios when thousands of training examples are available. We also observe that selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08835](https://arxiv.org/abs/2104.08835) [cs.CL]** |
|           | (or **[arXiv:2104.08835v1](https://arxiv.org/abs/2104.08835v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-15">15. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding
</h2>

Title: [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)

Authors: [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Tengchao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+T), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Guoxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Yijuan Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Dinei Florencio](https://arxiv.org/search/cs?searchtype=author&query=Florencio%2C+D), [Cha Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual form understanding benchmark dataset named XFUN, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUN dataset. The pre-trained LayoutXLM model and the XFUN dataset will be publicly available at [this https URL](https://aka.ms/layoutxlm).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08836](https://arxiv.org/abs/2104.08836) [cs.CL]** |
|           | (or **[arXiv:2104.08836v1](https://arxiv.org/abs/2104.08836v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-16">16. Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction
</h2>

Title: [Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction](https://arxiv.org/abs/2104.08874)

Authors: [Federico Bianchi](https://arxiv.org/search/cs?searchtype=author&query=Bianchi%2C+F), [Ciro Greco](https://arxiv.org/search/cs?searchtype=author&query=Greco%2C+C), [Jacopo Tagliabue](https://arxiv.org/search/cs?searchtype=author&query=Tagliabue%2C+J)

> We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.

| Comments: | Published as a conference paper at NAACL2021                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08874](https://arxiv.org/abs/2104.08874) [cs.CL]** |
|           | (or **[arXiv:2104.08874v1](https://arxiv.org/abs/2104.08874v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-17">17. Probing for Bridging Inference in Transformer Language Models
</h2>

Title: [Probing for Bridging Inference in Transformer Language Models](https://arxiv.org/abs/2104.09400)

Authors: [Onkar Pandit](https://arxiv.org/search/cs?searchtype=author&query=Pandit%2C+O), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y)

> We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any fine-tuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.09400](https://arxiv.org/abs/2104.09400) [cs.CL]** |
|           | (or **[arXiv:2104.09400v1](https://arxiv.org/abs/2104.09400v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-18">18. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding
</h2>

Title: [LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding](https://arxiv.org/abs/2104.08405)

Authors: [Te-Lin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T), [Cheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Mingyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Tao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T), [Spurthi Amba Hombaiah](https://arxiv.org/search/cs?searchtype=author&query=Hombaiah%2C+S+A), [Michael Bendersky](https://arxiv.org/search/cs?searchtype=author&query=Bendersky%2C+M)

> Document layout comprises both structural and visual (eg. font-sizes) information that is vital but often ignored by machine learning models. The few existing models which do use layout information only consider textual contents, and overlook the existence of contents in other modalities such as images. Additionally, spatial interactions of presented contents in a layout were never really fully exploited. To bridge this gap, we parse a document into content blocks (eg. text, table, image) and propose a novel layout-aware multimodal hierarchical framework, LAMPreT, to model the blocks and the whole document. Our LAMPreT encodes each block with a multimodal transformer in the lower-level and aggregates the block-level representations and connections utilizing a specifically designed transformer at the higher-level. We design hierarchical pretraining objectives where the lower-level model is trained similarly to multimodal grounding models, and the higher-level model is trained with our proposed novel layout-aware objectives. We evaluate the proposed model on two layout-aware tasks -- text block filling and image suggestion and show the effectiveness of our proposed hierarchical architecture as well as pretraining techniques.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08405](https://arxiv.org/abs/2104.08405) [cs.CL]** |
|           | (or **[arXiv:2104.08405v1](https://arxiv.org/abs/2104.08405v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-19">19. Frequency-based Distortions in Contextualized Word Embeddings
</h2>

Title: [Frequency-based Distortions in Contextualized Word Embeddings](https://arxiv.org/abs/2104.08465)

Authors: [Kaitlyn Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+K), [Kawin Ethayarajh](https://arxiv.org/search/cs?searchtype=author&query=Ethayarajh%2C+K), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D)

> How does word frequency in pre-training data affect the behavior of similarity metrics in contextualized BERT embeddings? Are there systematic ways in which some word relationships are exaggerated or understated? In this work, we explore the geometric characteristics of contextualized word embeddings with two novel tools: (1) an identity probe that predicts the identity of a word using its embedding; (2) the minimal bounding sphere for a word's contextualized representations. Our results reveal that words of high and low frequency differ significantly with respect to their representational geometry. Such differences introduce distortions: when compared to human judgments, point estimates of embedding similarity (e.g., cosine similarity) can over- or under-estimate the semantic similarity of two words, depending on the frequency of those words in the training data. This has downstream societal implications: BERT-Base has more trouble differentiating between South American and African countries than North American and European ones. We find that these distortions persist when using BERT-Multilingual, suggesting that they cannot be easily fixed with additional data, which in turn introduces new distortions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08465](https://arxiv.org/abs/2104.08465) [cs.CL]** |
|           | (or **[arXiv:2104.08465v1](https://arxiv.org/abs/2104.08465v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-20">20. Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation
</h2>

Title: [Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation](https://arxiv.org/abs/2104.08478)

Authors: [Seiichiro Kondo](https://arxiv.org/search/cs?searchtype=author&query=Kondo%2C+S), [Kengo Hotate](https://arxiv.org/search/cs?searchtype=author&query=Hotate%2C+K), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

> Neural machine translation (NMT) has recently gained widespread attention because of its high translation accuracy. However, it shows poor performance in the translation of long sentences, which is a major issue in low-resource languages. It is assumed that this issue is caused by insufficient number of long sentences in the training data. Therefore, this study proposes a simple data augmentation method to handle long sentences. In this method, we use only the given parallel corpora as the training data and generate long sentences by concatenating two sentences. Based on the experimental results, we confirm improvements in long sentence translation by the proposed data augmentation method, despite its simplicity. Moreover, the translation quality is further improved by the proposed method, when combined with back-translation.

| Comments: | 7 pages; camera-ready for NAACL Student Research Workshop 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08478](https://arxiv.org/abs/2104.08478) [cs.CL]** |
|           | (or **[arXiv:2104.08478v1](https://arxiv.org/abs/2104.08478v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-21">21. Sentence Alignment with Parallel Documents Helps Biomedical Machine Translation
</h2>

Title: [Sentence Alignment with Parallel Documents Helps Biomedical Machine Translation](https://arxiv.org/abs/2104.08588)

Authors: [Shengxuan Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+S), [Huaiyuan Ying](https://arxiv.org/search/cs?searchtype=author&query=Ying%2C+H), [Sheng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+S)

> The existing neural machine translation system has achieved near human-level performance in general domain in some languages, but the lack of parallel corpora poses a key problem in specific domains. In biomedical domain, the parallel corpus is less accessible. This work presents a new unsupervised sentence alignment method and explores features in training biomedical neural machine translation (NMT) systems. We use a simple but effective way to build bilingual word embeddings (BWEs) to evaluate bilingual word similarity and transferred the sentence alignment problem into an extended earth mover's distance (EMD) problem. The proposed method achieved high accuracy in both 1-to-1 and many-to-many cases. Pre-training in general domain, the larger in-domain dataset and n-to-m sentence pairs benefit the NMT model. Fine-tuning in domain corpus helps the translation model learns more terminology and fits the in-domain style of text.

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.08588](https://arxiv.org/abs/2104.08588) [cs.CL]** |
|           | (or **[arXiv:2104.08588v1](https://arxiv.org/abs/2104.08588v1) [cs.CL]** for this version) |





<h2 id="2021-04-20-22">22. Embodying Pre-Trained Word Embeddings Through Robot Actions
</h2>

Title: [Embodying Pre-Trained Word Embeddings Through Robot Actions](https://arxiv.org/abs/2104.08521)

Authors: [Minori Toyoda](https://arxiv.org/search/cs?searchtype=author&query=Toyoda%2C+M), [Kanata Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+K), [Hiroki Mori](https://arxiv.org/search/cs?searchtype=author&query=Mori%2C+H), [Yoshihiko Hayashi](https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+Y), [Tetsuya Ogata](https://arxiv.org/search/cs?searchtype=author&query=Ogata%2C+T)

> We propose a promising neural network model with which to acquire a grounded representation of robot actions and the linguistic descriptions thereof. Properly responding to various linguistic expressions, including polysemous words, is an important ability for robots that interact with people via linguistic dialogue. Previous studies have shown that robots can use words that are not included in the action-description paired datasets by using pre-trained word embeddings. However, the word embeddings trained under the distributional hypothesis are not grounded, as they are derived purely from a text corpus. In this letter, we transform the pre-trained word embeddings to embodied ones by using the robot's sensory-motor experiences. We extend a bidirectional translation model for actions and descriptions by incorporating non-linear layers that retrofit the word embeddings. By training the retrofit layer and the bidirectional translation model alternately, our proposed model is able to transform the pre-trained word embeddings to adapt to a paired action-description dataset. Our results demonstrate that the embeddings of synonyms form a semantic cluster by reflecting the experiences (actions and environments) of a robot. These embeddings allow the robot to properly generate actions from unseen words that are not paired with actions in a dataset.

| Comments:          | To appear in IEEE Robotics and Automation Letters (RA-L) and IEEE International Conference on Robotics and Automation (ICRA 2021) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Robotics (cs.RO)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Journal reference: | IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 4225-4232, 2021 |
| DOI:               | [10.1109/LRA.2021.3067862](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FLRA.2021.3067862&v=ee623705) |
| Cite as:           | **[arXiv:2104.08521](https://arxiv.org/abs/2104.08521) [cs.RO]** |
|                    | (or **[arXiv:2104.08521v1](https://arxiv.org/abs/2104.08521v1) [cs.RO]** for this version) |





<h2 id="2021-04-20-23">23. Dual-View Distilled BERT for Sentence Embedding
</h2>

Title: [Dual-View Distilled BERT for Sentence Embedding](https://arxiv.org/abs/2104.08675)

Authors: [Xingyi Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+X)

> Recently, BERT realized significant progress for sentence matching via word-level cross sentence attention. However, the performance significantly drops when using siamese BERT-networks to derive two sentence embeddings, which fall short in capturing the global semantic since the word-level attention between two sentences is absent. In this paper, we propose a Dual-view distilled BERT~(DvBERT) for sentence matching with sentence embeddings. Our method deals with a sentence pair from two distinct views, i.e., Siamese View and Interaction View. Siamese View is the backbone where we generate sentence embeddings. Interaction View integrates the cross sentence interaction as multiple teachers to boost the representation ability of sentence embeddings. Experiments on six STS tasks show that our method outperforms the state-of-the-art sentence embedding methods significantly.

| Comments: | Accepted at SIGIR 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.08675](https://arxiv.org/abs/2104.08675) [cs.AI]** |
|           | (or **[arXiv:2104.08675v1](https://arxiv.org/abs/2104.08675v1) [cs.AI]** for this version) |





<h2 id="2021-04-20-24">24. CLIPScore: A Reference-free Evaluation Metric for Image Captioning
</h2>

Title: [CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://arxiv.org/abs/2104.08718)

Authors: [Jack Hessel](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+J), [Ari Holtzman](https://arxiv.org/search/cs?searchtype=author&query=Holtzman%2C+A), [Maxwell Forbes](https://arxiv.org/search/cs?searchtype=author&query=Forbes%2C+M), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in stark contrast to the reference-free manner in which humans assess caption quality.
> In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker vs reference-based metrics, e.g., news captions that require richer contextual knowledge.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08718](https://arxiv.org/abs/2104.08718) [cs.CV]** |
|           | (or **[arXiv:2104.08718v1](https://arxiv.org/abs/2104.08718v1) [cs.CV]** for this version) |





<h2 id="2021-04-20-25">25. "Wikily" Neural Machine Translation Tailored to Cross-Lingual Tasks
</h2>

Title: ["Wikily" Neural Machine Translation Tailored to Cross-Lingual Tasks](https://arxiv.org/abs/2104.08384)

Authors: [Mohammad Sadegh Rasooli](https://arxiv.org/search/cs?searchtype=author&query=Rasooli%2C+M+S), [Chris Callison-Burch](https://arxiv.org/search/cs?searchtype=author&query=Callison-Burch%2C+C), [Derry Tanti Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D+T)

> We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily translation models to unsupervised image captioning and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a wikily translation of the English captioning data. Our captioning results in Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08384](https://arxiv.org/abs/2104.08384) [cs.CL]** |
|           | (or **[arXiv:2104.08384v1](https://arxiv.org/abs/2104.08384v1) [cs.CL]** for this version) |





# 2021-04-19

[Return to Index](#Index)



<h2 id="2021-04-19-1">1. Improving Gender Translation Accuracy with Filtered Self-Training
</h2>

Title: [Improving Gender Translation Accuracy with Filtered Self-Training](https://arxiv.org/abs/2104.07695)

Authors: [Prafulla Kumar Choubey](https://arxiv.org/search/cs?searchtype=author&query=Choubey%2C+P+K), [Anna Currey](https://arxiv.org/search/cs?searchtype=author&query=Currey%2C+A), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Georgiana Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+G)

> Targeted evaluations have found that machine translation systems often output incorrect gender, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose a gender-filtered self-training technique to improve gender translation accuracy on unambiguously gendered inputs. This approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then added to the training data. We filter the gender-specific corpora on the source and target sides to ensure that sentence pairs contain and correctly translate the specified gender. We evaluate our approach on translation from English into five languages, finding that our models improve gender translation accuracy without any cost to generic translation quality. In addition, we show the viability of our approach on several settings, including re-training from scratch, fine-tuning, controlling the balance of the training data, forward translation, and back-translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07695](https://arxiv.org/abs/2104.07695) [cs.CL]** |
|           | (or **[arXiv:2104.07695v1](https://arxiv.org/abs/2104.07695v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-2">2. Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation
</h2>

Title: [Cross-lingual Entity Alignment with Adversarial Kernel Embedding and Adversarial Knowledge Translation](https://arxiv.org/abs/2104.07837)

Authors: [Gong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+G), [Yang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Sixing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Zeru Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Dejing Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+D)

> Cross-lingual entity alignment, which aims to precisely connect the same entities in different monolingual knowledge bases (KBs) together, often suffers challenges from feature inconsistency to sequence context unawareness. This paper presents a dual adversarial learning framework for cross-lingual entity alignment, DAEA, with two original contributions. First, in order to address the structural and attribute feature inconsistency between entities in two knowledge graphs (KGs), an adversarial kernel embedding technique is proposed to extract graph-invariant information in an unsupervised manner, and project two KGs into the common embedding space. Second, in order to further improve successful rate of entity alignment, we propose to produce multiple random walks through each entity to be aligned and mask these entities in random walks. With the guidance of known aligned entities in the context of multiple random walks, an adversarial knowledge translation model is developed to fill and translate masked entities in pairwise random walks from two KGs. Extensive experiments performed on real-world datasets show that DAEA can well solve the feature inconsistency and sequence context unawareness issues and significantly outperforms thirteen state-of-the-art entity alignment methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07837](https://arxiv.org/abs/2104.07837) [cs.CL]** |
|           | (or **[arXiv:2104.07837v1](https://arxiv.org/abs/2104.07837v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-3">3. Investigating Failures of Automatic Translation in the Case of Unambiguous Gender
</h2>

Title: [Investigating Failures of Automatic Translation in the Case of Unambiguous Gender](https://arxiv.org/abs/2104.07838)

Authors: [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> Transformer based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by transformer based models with regards to translating from a language that doesn't mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no transformer based model we tested was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of transformer based NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.

| Comments: | 10 pages, 2 figures, 4 tables, submitting to EMNLP 2021      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07838](https://arxiv.org/abs/2104.07838) [cs.CL]** |
|           | (or **[arXiv:2104.07838v1](https://arxiv.org/abs/2104.07838v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-4">4. Comparison of Grammatical Error Correction Using Back-Translation Models
</h2>

Title: [Comparison of Grammatical Error Correction Using Back-Translation Models](https://arxiv.org/abs/2104.07848)

Authors: [Aomi Koyama](https://arxiv.org/search/cs?searchtype=author&query=Koyama%2C+A), [Kengo Hotate](https://arxiv.org/search/cs?searchtype=author&query=Hotate%2C+K), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

> Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Therefore, GEC studies have developed various methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous GEC studies using BT have employed the same architecture for both GEC and BT models. However, GEC models have different correction tendencies depending on their architectures. Thus, in this study, we compare the correction tendencies of the GEC models trained on pseudo data generated by different BT models, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. Additionally, we examine the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the F_0.5 scores of each error type compared with that of single BT models with different seeds.

| Comments: | 10 pages; camera-ready for NAACL Student Research Workshop 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07848](https://arxiv.org/abs/2104.07848) [cs.CL]** |
|           | (or **[arXiv:2104.07848v1](https://arxiv.org/abs/2104.07848v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-5">5. Segmenting Subtitles for Correcting ASR Segmentation Errors
</h2>

Title: [Segmenting Subtitles for Correcting ASR Segmentation Errors](https://arxiv.org/abs/2104.07868)

Authors: [David Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+D), [Chris Kedzie](https://arxiv.org/search/cs?searchtype=author&query=Kedzie%2C+C), [Faisal Ladhak](https://arxiv.org/search/cs?searchtype=author&query=Ladhak%2C+F), [Elsbeth Turcan](https://arxiv.org/search/cs?searchtype=author&query=Turcan%2C+E), [Petra Galuščáková](https://arxiv.org/search/cs?searchtype=author&query=Galuščáková%2C+P), [Elena Zotkina](https://arxiv.org/search/cs?searchtype=author&query=Zotkina%2C+E), [Zhengping Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Peter Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P), [Kathleen McKeown](https://arxiv.org/search/cs?searchtype=author&query=McKeown%2C+K)

> Typical ASR systems segment the input audio into utterances using purely acoustic information, which may not resemble the sentence-like units that are expected by conventional machine translation (MT) systems for Spoken Language Translation. In this work, we propose a model for correcting the acoustic segmentation of ASR models for low-resource languages to improve performance on downstream tasks. We propose the use of subtitles as a proxy dataset for correcting ASR acoustic segmentation, creating synthetic acoustic utterances by modeling common error modes. We train a neural tagging model for correcting ASR acoustic segmentation and show that it improves downstream performance on MT and audio-document cross-language information retrieval (CLIR).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07868](https://arxiv.org/abs/2104.07868) [cs.CL]** |
|           | (or **[arXiv:2104.07868v1](https://arxiv.org/abs/2104.07868v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-6">6. Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research
</h2>

Title: [Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research](https://arxiv.org/abs/2104.07874)

Authors: [Denis Newman-Griffis](https://arxiv.org/search/cs?searchtype=author&query=Newman-Griffis%2C+D), [Jill Fain Lehman](https://arxiv.org/search/cs?searchtype=author&query=Lehman%2C+J+F), [Carolyn Rosé](https://arxiv.org/search/cs?searchtype=author&query=Rosé%2C+C), [Harry Hochheiser](https://arxiv.org/search/cs?searchtype=author&query=Hochheiser%2C+H)

> Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.

| Comments: | Accepted to NAACL-HLT 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.07874](https://arxiv.org/abs/2104.07874) [cs.CL]** |
|           | (or **[arXiv:2104.07874v1](https://arxiv.org/abs/2104.07874v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-7">7. Generating Bug-Fixes Using Pretrained Transformers
</h2>

Title: [Generating Bug-Fixes Using Pretrained Transformers](https://arxiv.org/abs/2104.07896)

Authors: [Dawn Drain](https://arxiv.org/search/cs?searchtype=author&query=Drain%2C+D), [Chen Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Alexey Svyatkovskiy](https://arxiv.org/search/cs?searchtype=author&query=Svyatkovskiy%2C+A), [Neel Sundaresan](https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+N)

> Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.

| Subjects: | **Computation and Language (cs.CL)**; Programming Languages (cs.PL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07896](https://arxiv.org/abs/2104.07896) [cs.CL]** |
|           | (or **[arXiv:2104.07896v1](https://arxiv.org/abs/2104.07896v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-8">8. MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning
</h2>

Title: [MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning](https://arxiv.org/abs/2104.07908)

Authors: [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Milad Shokouhi](https://arxiv.org/search/cs?searchtype=author&query=Shokouhi%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages - without access to large-scale monolingual corpora or large amounts of labeled data - for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at [this http URL](http://github.com/microsoft/MetaXL).

| Comments: | 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.07908](https://arxiv.org/abs/2104.07908) [cs.CL]** |
|           | (or **[arXiv:2104.07908v1](https://arxiv.org/abs/2104.07908v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-9">9. Language Models are Few-Shot Butlers
</h2>

Title: [Language Models are Few-Shot Butlers](https://arxiv.org/abs/2104.07972)

Authors: [Vincent Micheli](https://arxiv.org/search/cs?searchtype=author&query=Micheli%2C+V), [François Fleuret](https://arxiv.org/search/cs?searchtype=author&query=Fleuret%2C+F)

> Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine-tuned with only 1.2% of the expert demonstrations and a simple reinforcement learning algorithm achieve a 51% absolute improvement in success rate over existing methods in the ALFWorld environment.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07972](https://arxiv.org/abs/2104.07972) [cs.CL]** |
|           | (or **[arXiv:2104.07972v1](https://arxiv.org/abs/2104.07972v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-10">10. Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders
</h2>

Title: [Fast, Effective and Self-Supervised: Transforming Masked LanguageModels into Universal Lexical and Sentence Encoders](https://arxiv.org/abs/2104.08027)

Authors: [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in less than a minute without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective univeral lexical and sentence encoders.

| Comments: | 11 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.08027](https://arxiv.org/abs/2104.08027) [cs.CL]** |
|           | (or **[arXiv:2104.08027v1](https://arxiv.org/abs/2104.08027v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-11">11. Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models
</h2>

Title: [Effect of Vision-and-Language Extensions on Natural Language Understanding in Vision-and-Language Models](https://arxiv.org/abs/2104.08066)

Authors: [Taichi Iki](https://arxiv.org/search/cs?searchtype=author&query=Iki%2C+T), [Akiko Aizawa](https://arxiv.org/search/cs?searchtype=author&query=Aizawa%2C+A)

> Extending language models with structural modifications and vision-and-language (V&L) pretraining are successful ways of making V&L models that can ground vision and language. Potential applications of these advanced models include multi-modal machine reading comprehension models and multi-modal dialogue models, which require language ability upon grounding. Although language capability is crucial for such applications, the impact of extending their visual capabilities on their language capabilities is not fully understood. This paper investigates how visual extension affects the language capability of V&L models using the GLUE benchmark. We found that visual extension causes some decreases in language capability and that V&L pretraining has a greater impact than structural modifications on the decreases. Our results suggest the need for further study on pretraining that can maintain or, if possible, improve a model's language capability.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08066](https://arxiv.org/abs/2104.08066) [cs.CL]** |
|           | (or **[arXiv:2104.08066v1](https://arxiv.org/abs/2104.08066v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-12">12. Towards Variable-Length Textual Adversarial Attacks
</h2>

Title: [Towards Variable-Length Textual Adversarial Attacks](https://arxiv.org/abs/2104.08139)

Authors: [Junliang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Linlin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Linli Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Enhong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+E), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Adversarial attacks have shown the vulnerability of machine learning models, however, it is non-trivial to conduct textual adversarial attacks on natural language processing tasks due to the discreteness of data. Most previous approaches conduct attacks with the atomic \textit{replacement} operation, which usually leads to fixed-length adversarial examples and therefore limits the exploration on the decision space. In this paper, we propose variable-length textual adversarial attacks~(VL-Attack) and integrate three atomic operations, namely \textit{insertion}, \textit{deletion} and \textit{replacement}, into a unified framework, by introducing and manipulating a special \textit{blank} token while attacking. In this way, our approach is able to more comprehensively find adversarial examples around the decision boundary and effectively conduct adversarial attacks. Specifically, our method drops the accuracy of IMDB classification by 96% with only editing 1.3% tokens while attacking a pre-trained BERT model. In addition, fine-tuning the victim model with generated adversarial samples can improve the robustness of the model without hurting the performance, especially for length-sensitive models. On the task of non-autoregressive machine translation, our method can achieve 33.18 BLEU score on IWSLT14 German-English translation, achieving an improvement of 1.47 over the baseline model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08139](https://arxiv.org/abs/2104.08139) [cs.CL]** |
|           | (or **[arXiv:2104.08139v1](https://arxiv.org/abs/2104.08139v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-13">13. Serial or Parallel? Plug-able Adapter for multilingual machine translation
</h2>

Title: [Serial or Parallel? Plug-able Adapter for multilingual machine translation](https://arxiv.org/abs/2104.08154)

Authors: [Yaoming Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Developing a unified multilingual translation model is a key topic in machine translation research. However, existing approaches suffer from performance degradation: multilingual models yield inferior performance compared to the ones trained separately on rich bilingual data. We attribute the performance degradation to two issues: multilingual embedding conflation and multilingual fusion effects. To address the two issues, we propose PAM, a Transformer model augmented with defusion adaptation for multilingual machine translation. Specifically, PAM consists of embedding and layer adapters to shift the word and intermediate representations towards language-specific ones. Extensive experiment results on IWSLT, OPUS-100, and WMT benchmarks show that \method outperforms several strong competitors, including series adapter and multilingual knowledge distillation.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.08154](https://arxiv.org/abs/2104.08154) [cs.CL]** |
|           | (or **[arXiv:2104.08154v1](https://arxiv.org/abs/2104.08154v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-14">14. Robust Open-Vocabulary Translation from Visual Text Representations
</h2>

Title: [Robust Open-Vocabulary Translation from Visual Text Representations](https://arxiv.org/abs/2104.08211)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [David Etter](https://arxiv.org/search/cs?searchtype=author&query=Etter%2C+D), [Matt Post](https://arxiv.org/search/cs?searchtype=author&query=Post%2C+M)

> Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an 'open-vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text. We show that models using visual text representations approach or match performance of text baselines on clean TED datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German--English task where subword models degrade to 1.9.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08211](https://arxiv.org/abs/2104.08211) [cs.CL]** |
|           | (or **[arXiv:2104.08211v1](https://arxiv.org/abs/2104.08211v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-15">15. Is Your Language Model Ready for Dense Representation Fine-tuning?
</h2>

Title: [Is Your Language Model Ready for Dense Representation Fine-tuning?](https://arxiv.org/abs/2104.08253)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Jamie Callan](https://arxiv.org/search/cs?searchtype=author&query=Callan%2C+J)

> Pre-trained language models (LM) have become go-to text representation encoders. Prior research used deep LMs to encode text sequences such as sentences and passages into single dense vector representations. These dense representations have been used in efficient text comparison and embedding-based retrieval. However, dense encoders suffer in low resource situations. Many techniques have been developed to solve this problem. Despite their success, not much is known about why this happens. This paper shows that one cause lies in the readiness of the LM to expose its knowledge through dense representation in fine-tuning, which we term Optimization Readiness. To validate the theory, we present Condenser, a general pre-training architecture based on Transformer LMs, to improve dense optimization readiness. We show that fine-tuning from Condenser significantly improves performance for small and/or noisy training sets.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08253](https://arxiv.org/abs/2104.08253) [cs.CL]** |
|           | (or **[arXiv:2104.08253v1](https://arxiv.org/abs/2104.08253v1) [cs.CL]** for this version) |





<h2 id="2021-04-19-16">16. Context-Adaptive Document-Level Neural Machine Translation
</h2>

Title: [Context-Adaptive Document-Level Neural Machine Translation](https://arxiv.org/abs/2104.08259)

Authors: [Linlin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

> Most existing document-level neural machine translation (NMT) models leverage a fixed number of the previous or all global source sentences to handle the context-independent problem in standard NMT. However, the translating of each source sentence benefits from various sizes of context, and inappropriate context may harm the translation performance. In this work, we introduce a data-adaptive method that enables the model to adopt the necessary and useful context. Specifically, we introduce a light predictor into two document-level translation models to select the explicit context. Experiments demonstrate the proposed approach can significantly improve the performance over the previous methods with a gain up to 1.99 BLEU points.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.08259](https://arxiv.org/abs/2104.08259) [cs.CL]** |
|           | (or **[arXiv:2104.08259v1](https://arxiv.org/abs/2104.08259v1) [cs.CL]** for this version) |






# 2021-04-16

[Return to Index](#Index)



<h2 id="2021-04-16-1">1. What Makes a Scientific Paper be Accepted for Publication?
</h2>

Title: [What Makes a Scientific Paper be Accepted for Publication?](https://arxiv.org/abs/2104.07112)

Authors: [Panagiotis Fytas](https://arxiv.org/search/cs?searchtype=author&query=Fytas%2C+P), [Georgios Rizos](https://arxiv.org/search/cs?searchtype=author&query=Rizos%2C+G), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer review process. We start by extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peer-review dataset. Second, since such global explanations do not justify causal interpretations, we provide a methodology for detecting confounding effects in natural language in order to generate causal explanations, under assumptions, in the form of lexicons. Our proposed linguistic explanation methodology indicates the following on a case dataset of ICLR submissions: a) the organising committee follows, for the most part, the recommendations of reviewers, and, b) the paper's main characteristics that led to reviewers recommending acceptance for publication are originality, clarity and substance.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2104.07112](https://arxiv.org/abs/2104.07112) [cs.CL]** |
|              | (or **[arXiv:2104.07112v1](https://arxiv.org/abs/2104.07112v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-2">2. An Interpretability Illusion for BERT
</h2>

Title: [An Interpretability Illusion for BERT](https://arxiv.org/abs/2104.07143)

Authors: [Tolga Bolukbasi](https://arxiv.org/search/cs?searchtype=author&query=Bolukbasi%2C+T), [Adam Pearce](https://arxiv.org/search/cs?searchtype=author&query=Pearce%2C+A), [Ann Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A), [Andy Coenen](https://arxiv.org/search/cs?searchtype=author&query=Coenen%2C+A), [Emily Reif](https://arxiv.org/search/cs?searchtype=author&query=Reif%2C+E), [Fernanda Viégas](https://arxiv.org/search/cs?searchtype=author&query=Viégas%2C+F), [Martin Wattenberg](https://arxiv.org/search/cs?searchtype=author&query=Wattenberg%2C+M)

> We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07143](https://arxiv.org/abs/2104.07143) [cs.CL]** |
|           | (or **[arXiv:2104.07143v1](https://arxiv.org/abs/2104.07143v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-3">3. An Alignment-Agnostic Model for Chinese Text Error Correction
</h2>

Title: [An Alignment-Agnostic Model for Chinese Text Error Correction](https://arxiv.org/abs/2104.07190)

Authors: [Liying Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+L), [Yue Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Weishun Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+W), [Liang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Jing Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J)

> This paper investigates how to correct Chinese text errors with types of mistaken, missing and redundant characters, which is common for Chinese native speakers. Most existing models based on detect-correct framework can correct mistaken characters errors, but they cannot deal with missing or redundant characters. The reason is that lengths of sentences before and after correction are not the same, leading to the inconsistence between model inputs and outputs. Although the Seq2Seq-based or sequence tagging methods provide solutions to the problem and achieved relatively good results on English context, but they do not perform well in Chinese context according to our experimental results. In our work, we propose a novel detect-correct framework which is alignment-agnostic, meaning that it can handle both text aligned and non-aligned occasions, and it can also serve as a cold start model when there are no annotated data provided. Experimental results on three datasets demonstrate that our method is effective and achieves the best performance among existing published models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07190](https://arxiv.org/abs/2104.07190) [cs.CL]** |
|           | (or **[arXiv:2104.07190v1](https://arxiv.org/abs/2104.07190v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-4">4. Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models
</h2>

Title: [Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](https://arxiv.org/abs/2104.07204)

Authors: [Yuxuan Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Y), [Yijia Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yansong Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Dongyan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+D)

> Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at [this https URL](https://github.com/alibaba/pretrained-language-models/LatticeBERT).

| Comments: | Accepted at NAACL 2021, 16 pages                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07204](https://arxiv.org/abs/2104.07204) [cs.CL]** |
|           | (or **[arXiv:2104.07204v1](https://arxiv.org/abs/2104.07204v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-5">5. Sentence-Permuted Paragraph Generation
</h2>

Title: [Sentence-Permuted Paragraph Generation](https://arxiv.org/abs/2104.07228)

Authors: [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Tong Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Zhichun Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M)

> Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for training, decoding, and candidate ranking in the sentence-permuted generation. Experiments on three paragraph generation benchmarks demonstrate PermGen generates more diverse outputs with a higher quality than existing models.

| Comments: | 19 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.07228](https://arxiv.org/abs/2104.07228) [cs.CL]** |
|           | (or **[arXiv:2104.07228v1](https://arxiv.org/abs/2104.07228v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-6">6. Adaptive Sparse Transformer for Multilingual Translation
</h2>

Title: [Adaptive Sparse Transformer for Multilingual Translation](https://arxiv.org/abs/2104.07358)

Authors: [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Dmitriy Genzel](https://arxiv.org/search/cs?searchtype=author&query=Genzel%2C+D)

> Multilingual machine translation has attracted much attention recently due to its support of knowledge transfer among languages and the low cost of training and deployment compared with numerous bilingual models. A known challenge of multilingual models is the negative language interference. In order to enhance the translation quality, deeper and wider architectures are applied to multilingual modeling for larger model capacity, which suffers from the increased inference cost at the same time. It has been pointed out in recent studies that parameters shared among languages are the cause of interference while they may also enable positive transfer. Based on these insights, we propose an adaptive and sparse architecture for multilingual modeling, and train the model to learn shared and language-specific parameters to improve the positive transfer and mitigate the interference. The sparse architecture only activates a subnetwork which preserves inference efficiency, and the adaptive design selects different subnetworks based on the input languages. Evaluated on multilingual translation across multiple public datasets, our model outperforms strong baselines in terms of translation quality without increasing the inference cost.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07358](https://arxiv.org/abs/2104.07358) [cs.CL]** |
|           | (or **[arXiv:2104.07358v1](https://arxiv.org/abs/2104.07358v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-7">7. Simultaneous Multi-Pivot Neural Machine Translation
</h2>

Title: [Simultaneous Multi-Pivot Neural Machine Translation](https://arxiv.org/abs/2104.07410)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Abhisek Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+A)

> Parallel corpora are indispensable for training neural machine translation (NMT) models, and parallel corpora for most language pairs do not exist or are scarce. In such cases, pivot language NMT can be helpful where a pivot language is used such that there exist parallel corpora between the source and pivot and pivot and target languages. Naturally, the quality of pivot language translation is more inferior to what could be achieved with a direct parallel corpus of a reasonable size for that pair. In a real-time simultaneous translation setting, the quality of pivot language translation deteriorates even further given that the model has to output translations the moment a few source words become available. To solve this issue, we propose multi-pivot translation and apply it to a simultaneous translation setting involving pivot languages. Our approach involves simultaneously translating a source language into multiple pivots, which are then simultaneously translated together into the target language by leveraging multi-source NMT. Our experiments in a low-resource setting using the N-way parallel UN corpus for Arabic to English NMT via French and Spanish as pivots reveals that in a simultaneous pivot NMT setting, using two pivot languages can lead to an improvement of up to 5.8 BLEU.

| Comments: | preliminary work. pardon the messy writing and mistakes. will be submitted to emnlp after major overhaul |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07410](https://arxiv.org/abs/2104.07410) [cs.CL]** |
|           | (or **[arXiv:2104.07410v1](https://arxiv.org/abs/2104.07410v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-8">8. First the worst: Finding better gender translations during beam search
</h2>

Title: [First the worst: Finding better gender translations during beam search](https://arxiv.org/abs/2104.07429)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Rosie Sallis](https://arxiv.org/search/cs?searchtype=author&query=Sallis%2C+R), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

> Neural machine translation inference procedures like beam search generate the most likely output under the model. This can exacerbate any demographic biases exhibited by the model. We focus on gender bias resulting from systematic errors in grammatical gender translation, which can lead to human referents being misrepresented or misgendered.
> Most approaches to this problem adjust the training data or the model. By contrast, we experiment with simply adjusting the inference procedure. We experiment with reranking nbest lists using gender features obtained automatically from the source sentence, and applying gender constraints while decoding to improve nbest list gender diversity. We find that a combination of these techniques allows large gains in WinoMT accuracy without requiring additional bilingual data or an additional NMT model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07429](https://arxiv.org/abs/2104.07429) [cs.CL]** |
|           | (or **[arXiv:2104.07429v1](https://arxiv.org/abs/2104.07429v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-9">9. Effect of Post-processing on Contextualized Word Representations
</h2>

Title: [Effect of Post-processing on Contextualized Word Representations](https://arxiv.org/abs/2104.07456)

Authors: [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Firoj Alam](https://arxiv.org/search/cs?searchtype=author&query=Alam%2C+F), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N)

> Post-processing of static embedding has beenshown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principle components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy)and sequence classification tasks. Our findings raise interesting points in relation to theresearch studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07456](https://arxiv.org/abs/2104.07456) [cs.CL]** |
|           | (or **[arXiv:2104.07456v1](https://arxiv.org/abs/2104.07456v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-10">10. IndT5: A Text-to-Text Transformer for 10 Indigenous Languages
</h2>

Title: [IndT5: A Text-to-Text Transformer for 10 Indigenous Languages](https://arxiv.org/abs/2104.07483)

Authors: [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Wei-Rui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Hasan Cavusogl](https://arxiv.org/search/cs?searchtype=author&query=Cavusogl%2C+H)

> Transformer language models have become fundamental components of NLP based pipelines. Although several Transformer have been introduced to serve many languages, there is a shortage of models pre-trained for low-resource and Indigenous languages in particular. In this work, we introduce IndT5, the first Transformer language model for Indigenous languages. To train IndT5, we build IndCorpus, a new corpus for 10 Indigenous languages and Spanish. We also present the application of IndT5 to machine translation by investigating different approaches to translate between Spanish and the Indigenous languages as part of our contribution to the AmericasNLP 2021 Shared Task on Open Machine Translation. IndT5 and IndCorpus are publicly available for research.

| Comments: | AmericasNLP 2021                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07483](https://arxiv.org/abs/2104.07483) [cs.CL]** |
|           | (or **[arXiv:2104.07483v1](https://arxiv.org/abs/2104.07483v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-11">11. Generating Datasets with Pretrained Language Models
</h2>

Title: [Generating Datasets with Pretrained Language Models](https://arxiv.org/abs/2104.07540)

Authors: [Timo Schick](https://arxiv.org/search/cs?searchtype=author&query=Schick%2C+T), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> To obtain high-quality sentence embeddings from pretrained language models, they must either be augmented with additional pretraining objectives or finetuned on large amounts of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how large pretrained language models can be leveraged to obtain high-quality embeddings without requiring any labeled data, finetuning or modifications to their pretraining objective: We utilize their generative abilities to generate entire datasets of labeled text pairs from scratch, which can then be used for regular finetuning of much smaller models. Our fully unsupervised approach outperforms strong baselines on several English semantic textual similarity datasets.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07540](https://arxiv.org/abs/2104.07540) [cs.CL]** |
|           | (or **[arXiv:2104.07540v1](https://arxiv.org/abs/2104.07540v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-12">12. Reward Optimization for Neural Machine Translation with Learned Metrics
</h2>

Title: [Reward Optimization for Neural Machine Translation with Learned Metrics](https://arxiv.org/abs/2104.07541)

Authors: [Raphael Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+R), [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Jung-Woo Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+J)

> Neural machine translation (NMT) models are conventionally trained with token-level negative log-likelihood (NLL), which does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. Multiple approaches are proposed to train NMT with BLEU as the reward, in order to directly improve the metric. However, it was reported that the gain in BLEU does not translate to real quality improvement, limiting the application in industry. Recently, it became clear to the community that BLEU has a low correlation with human judgment when dealing with state-of-the-art models. This leads to the emerging of model-based evaluation metrics. These new metrics are shown to have a much higher human correlation. In this paper, we investigate whether it is beneficial to optimize NMT models with the state-of-the-art model-based metric, BLEURT. We propose a contrastive-margin loss for fast and stable reward optimization suitable for large NMT models. In experiments, we perform automatic and human evaluations to compare models trained with smoothed BLEU and BLEURT to the baseline models. Results show that the reward optimization with BLEURT is able to increase the metric scores by a large margin, in contrast to limited gain when training with smoothed BLEU. The human evaluation shows that models trained with BLEURT improve adequacy and coverage of translations. Code is available via [this https URL](https://github.com/naver-ai/MetricMT).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07541](https://arxiv.org/abs/2104.07541) [cs.CL]** |
|           | (or **[arXiv:2104.07541v1](https://arxiv.org/abs/2104.07541v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-13">13. Hierarchical Learning for Generation with Long Source Sequences
</h2>

Title: [Hierarchical Learning for Generation with Long Source Sequences](https://arxiv.org/abs/2104.07545)

Authors: [Tobias Rohde](https://arxiv.org/search/cs?searchtype=author&query=Rohde%2C+T), [Xiaoxia Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yinhan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> One of the challenges for current sequence to sequence (seq2seq) models is processing long sequences, such as those in summarization and document level machine translation tasks. These tasks require the model to reason at the token level as well as the sentence and paragraph level. We design and study a new Hierarchical Attention Transformer-based architecture (HAT) that outperforms standard Transformers on several sequence to sequence tasks. In particular, our model achieves stateof-the-art results on four summarization tasks, including ArXiv, CNN/DM, SAMSum, and AMI, and we push PubMed R1 & R2 SOTA further. Our model significantly outperforms our document-level machine translation baseline by 28 BLEU on the WMT19 EN-DE document translation task. We also investigate what the hierarchical layers learn by visualizing the hierarchical encoder-decoder attention. Finally, we study hierarchical learning on encoder-only pre-training and analyze its performance on classification downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07545](https://arxiv.org/abs/2104.07545) [cs.CL]** |
|           | (or **[arXiv:2104.07545v1](https://arxiv.org/abs/2104.07545v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-14">14. Sometimes We Want Translationese
</h2>

Title: [Sometimes We Want Translationese](https://arxiv.org/abs/2104.07623)

Authors: [Prasanna Parthasarathi](https://arxiv.org/search/cs?searchtype=author&query=Parthasarathi%2C+P), [Koustuv Sinha](https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+K), [Joelle Pineau](https://arxiv.org/search/cs?searchtype=author&query=Pineau%2C+J), [Adina Williams](https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+A)

> Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case.

| Comments: | 16 pages, 11 figures and 3 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07623](https://arxiv.org/abs/2104.07623) [cs.CL]** |
|           | (or **[arXiv:2104.07623v1](https://arxiv.org/abs/2104.07623v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-15">15. Demystify Optimization Challenges in Multilingual Transformers
</h2>

Title: [Demystify Optimization Challenges in Multilingual Transformers](https://arxiv.org/abs/2104.07639)

Authors: [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H)

> Multilingual Transformer improves parameter efficiency and crosslingual transfer. How to effectively train multilingual models has not been well studied. Using multilingual machine translation as a testbed, we study optimization challenges from loss landscape and parameter plasticity perspectives. We found that imbalanced training data poses task interference between high and low resource languages, characterized by nearly orthogonal gradients for major parameters and the optimization trajectory being mostly dominated by high resource. We show that local curvature of the loss surface affects the degree of interference, and existing heuristics of data subsampling implicitly reduces the sharpness, although still face a trade-off between high and low resource languages. We propose a principled multi-objective optimization algorithm, Curvature Aware Task Scaling (CATS), which improves both optimization and generalization especially for low resource. Experiments on TED, WMT and OPUS-100 benchmarks demonstrate that CATS advances the Pareto front of accuracy while being efficient to apply to massive multilingual settings at the scale of 100 languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07639](https://arxiv.org/abs/2104.07639) [cs.CL]** |
|           | (or **[arXiv:2104.07639v1](https://arxiv.org/abs/2104.07639v1) [cs.CL]** for this version) |





<h2 id="2021-04-16-16">16. Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining
</h2>

Title: [Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](https://arxiv.org/abs/2104.07642)

Authors: [Chih-chan Tien](https://arxiv.org/search/cs?searchtype=author&query=Tien%2C+C), [Shane Steinert-Threlkeld](https://arxiv.org/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S)

> This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. And such transfer from bilingual alignment to multilingual alignment is a dual-pivot transfer from two pivot languages to other language pairs. To study this theory, we train an unsupervised model with unpaired sentences and another single-pair supervised model with bitexts, both based on the unsupervised language model XLM-R. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with higher multilingual alignment.

| Comments: | 10 pages, 2 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.07642](https://arxiv.org/abs/2104.07642) [cs.CL]** |
|           | (or **[arXiv:2104.07642v1](https://arxiv.org/abs/2104.07642v1) [cs.CL]** for this version) |





# 2021-04-15

[Return to Index](#Index)



<h2 id="2021-04-15-1">1. Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation
</h2>

Title: [Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation](https://arxiv.org/abs/2104.06457)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Tatsuya Kawahara](https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.

| Comments: | Accepted at NAACL-HLT 2021 (short paper)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.06457](https://arxiv.org/abs/2104.06457) [cs.CL]** |
|           | (or **[arXiv:2104.06457v1](https://arxiv.org/abs/2104.06457v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-2">2. Large-Scale Self- and Semi-Supervised Learning for Speech Translation
</h2>

Title: [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Anne Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06678](https://arxiv.org/abs/2104.06678) [cs.CL]** |
|           | (or **[arXiv:2104.06678v1](https://arxiv.org/abs/2104.06678v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-3">3. The Curious Case of Hallucinations in Neural Machine Translation
</h2>

Title: [The Curious Case of Hallucinations in Neural Machine Translation](https://arxiv.org/abs/2104.06683)

Authors: [Vikas Raunak](https://arxiv.org/search/cs?searchtype=author&query=Raunak%2C+V), [Arul Menezes](https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+A), [Marcin Junczys-Dowmunt](https://arxiv.org/search/cs?searchtype=author&query=Junczys-Dowmunt%2C+M)

> In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman (2020), and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06683](https://arxiv.org/abs/2104.06683) [cs.CL]** |
|           | (or **[arXiv:2104.06683v1](https://arxiv.org/abs/2104.06683v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-4">4. Sentence Embeddings by Ensemble Distillation
</h2>

Title: [Sentence Embeddings by Ensemble Distillation](https://arxiv.org/abs/2104.06719)

Authors: [Fredrik Carlsson Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+F+C+M)

> This paper contributes a new State Of The Art (SOTA) for Semantic Textual Similarity (STS). We compare and combine a number of recently proposed sentence embedding methods for STS, and propose a novel and simple ensemble knowledge distillation scheme that improves on previous approaches. Our experiments demonstrate that a model trained to learn the average embedding space from multiple ensemble students outperforms all the other individual models with high robustness. Utilizing our distillation method in combination with previous methods, we significantly improve on the SOTA unsupervised STS, and by proper hyperparameter tuning of previous methods we improve the supervised SOTA scores.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06719](https://arxiv.org/abs/2104.06719) [cs.CL]** |
|           | (or **[arXiv:2104.06719v1](https://arxiv.org/abs/2104.06719v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-5">5. Distributed Word Representation in Tsetlin Machine
</h2>

Title: [Distributed Word Representation in Tsetlin Machine](https://arxiv.org/abs/2104.06901)

Authors: [Rohan Kumar Yadav](https://arxiv.org/search/cs?searchtype=author&query=Yadav%2C+R+K), [Lei Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+L), [Ole-Christoffer Granmo](https://arxiv.org/search/cs?searchtype=author&query=Granmo%2C+O), [Morten Goodwin](https://arxiv.org/search/cs?searchtype=author&query=Goodwin%2C+M)

> Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based on propositional logic. The algorithm has demonstrated competitive performance in many Natural Language Processing (NLP) tasks, including sentiment analysis, text classification, and Word Sense Disambiguation (WSD). To obtain human-level interpretability, legacy TM employs Boolean input features such as bag-of-words (BOW). However, the BOW representation makes it difficult to use any pre-trained information, for instance, word2vec and GloVe word representations. This restriction has constrained the performance of TM compared to deep neural networks (DNNs) in NLP. To reduce the performance gap, in this paper, we propose a novel way of using pre-trained word representations for TM. The approach significantly enhances the TM performance and maintains interpretability at the same time. We achieve this by extracting semantically related words from pre-trained word representations as input features to the TM. Our experiments show that the accuracy of the proposed approach is significantly higher than the previous BOW-based TM, reaching the level of DNN-based models.

| Comments: | 9 pages, 13 figures, and 4 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06901](https://arxiv.org/abs/2104.06901) [cs.CL]** |
|           | (or **[arXiv:2104.06901v1](https://arxiv.org/abs/2104.06901v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-6">6. Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
</h2>

Title: [Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey](https://arxiv.org/abs/2104.06951)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D)

> The development of deep learning techniques has allowed Neural Machine Translation (NMT) models to become extremely powerful, given sufficient training data and training time. However, systems struggle when translating text from a new domain with a distinct style or vocabulary. Tuning on a representative training corpus allows good in-domain translation, but such data-centric approaches can cause over-fitting to new data and `catastrophic forgetting' of previously learned behaviour.
> We concentrate on more robust approaches to domain adaptation for NMT, particularly the case where a system may need to translate sentences from multiple domains. We divide techniques into those relating to data selection, model architecture, parameter adaptation procedure, and inference procedure. We finally highlight the benefits of domain adaptation and multi-domain adaptation techniques to other lines of NMT research.

| Comments: | 39 pages + references                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.06951](https://arxiv.org/abs/2104.06951) [cs.CL]** |
|           | (or **[arXiv:2104.06951v1](https://arxiv.org/abs/2104.06951v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-7">7. TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning
</h2>

Title: [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](https://arxiv.org/abs/2104.06979)

Authors: [Kexin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Nils Reimers](https://arxiv.org/search/cs?searchtype=author&query=Reimers%2C+N), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Learning sentence embeddings often requires large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong pre-training method for learning sentence embeddings, significantly outperforming other approaches like Masked Language Model.
> A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06979](https://arxiv.org/abs/2104.06979) [cs.CL]** |
|           | (or **[arXiv:2104.06979v1](https://arxiv.org/abs/2104.06979v1) [cs.CL]** for this version) |





<h2 id="2021-04-15-8">8. Sparse Attention with Linear Units
</h2>

Title: [Sparse Attention with Linear Units](https://arxiv.org/abs/2104.07012)

Authors: [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.07012](https://arxiv.org/abs/2104.07012) [cs.CL]** |
|           | (or **[arXiv:2104.07012v1](https://arxiv.org/abs/2104.07012v1) [cs.CL]** for this version) |







# 2021-04-14

[Return to Index](#Index)



<h2 id="2021-04-14-1">1. Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique
</h2>

Title: [Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique](https://arxiv.org/abs/2104.05753)

Authors: [Felermino D. M. A. Ali](https://arxiv.org/search/cs?searchtype=author&query=Ali%2C+F+D+M+A), [Andrew Caines](https://arxiv.org/search/cs?searchtype=author&query=Caines%2C+A), [Jaimito L. A. Malavi](https://arxiv.org/search/cs?searchtype=author&query=Malavi%2C+J+L+A)

> Major advancement in the performance of machine translation models has been made possible in part thanks to the availability of large-scale parallel corpora. But for most languages in the world, the existence of such corpora is rare. Emakhuwa, a language spoken in Mozambique, is like most African languages low-resource in NLP terms. It lacks both computational and linguistic resources and, to the best of our knowledge, few parallel corpora including Emakhuwa already exist. In this paper we describe the creation of the Emakhuwa-Portuguese parallel corpus, which is a collection of texts from the Jehovah's Witness website and a variety of other sources including the African Story Book website, the Universal Declaration of Human Rights and Mozambican legal documents. The dataset contains 47,415 sentence pairs, amounting to 699,976 word tokens of Emakhuwa and 877,595 word tokens in Portuguese. After normalization processes which remain to be completed, the corpus will be made freely available for research use.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05753](https://arxiv.org/abs/2104.05753) [cs.CL]** |
|           | (or **[arXiv:2104.05753v1](https://arxiv.org/abs/2104.05753v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-2">2. Targeted Adversarial Training for Natural Language Understanding
</h2>

Title: [Targeted Adversarial Training for Natural Language Understanding](https://arxiv.org/abs/2104.05847)

Authors: [Lis Pereira](https://arxiv.org/search/cs?searchtype=author&query=Pereira%2C+L), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Hoifung Poon](https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Ichiro Kobayashi](https://arxiv.org/search/cs?searchtype=author&query=Kobayashi%2C+I)

> We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: [this https URL](https://github.com/namisan/mt-dnn).

| Comments: | 9 pages, 4 tables, 3 figurers, NAACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05847](https://arxiv.org/abs/2104.05847) [cs.CL]** |
|           | (or **[arXiv:2104.05847v1](https://arxiv.org/abs/2104.05847v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-3">3. Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation
</h2>

Title: [Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation](https://arxiv.org/abs/2104.05848)

Authors: [Zhong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

> We translate a closed text that is known in advance into a severely low resource language by leveraging massive source parallelism. Our contribution is four-fold. Firstly, we rank 124 source languages empirically to determine their closeness to the low resource language and select the top few. We call the linguistic definition of language family Family of Origin (FAMO), and we call the empirical definition of higher-ranked languages using our metrics Family of Choice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual Order-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines (~3.5\%) of low resource data from the Bible dataset and the medical EMEA dataset. Using English as a hypothetical low resource language to translate from Spanish, we obtain a +24.7 BLEU increase over a multilingual baseline, and a +10.2 BLEU increase over our asymmetric baseline. Thirdly, we also use a real severely low resource Mayan language, Eastern Pokomchi. Finally, we add an order-preserving lexiconized component to translate named entities accurately. We build a massive lexicon table for 2,939 Bible named entities in 124 source languages, and include many that occur once and covers more than 66 severely low resource languages. Training on randomly sampled 1,093 lines of low resource data, we reach a 30.3 BLEU score for Spanish-English translation testing on 30,022 lines of Bible, and a 42.8 BLEU score for Portuguese-English translation on the medical EMEA dataset.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05848](https://arxiv.org/abs/2104.05848) [cs.CL]** |
|           | (or **[arXiv:2104.05848v1](https://arxiv.org/abs/2104.05848v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-4">4. Discourse Probing of Pretrained Language Models
</h2>

Title: [Discourse Probing of Pretrained Language Models](https://arxiv.org/abs/2104.05882)

Authors: [Fajri Koto](https://arxiv.org/search/cs?searchtype=author&query=Koto%2C+F), [Jey Han Lau](https://arxiv.org/search/cs?searchtype=author&query=Lau%2C+J+H), [Timothy Baldwin](https://arxiv.org/search/cs?searchtype=author&query=Baldwin%2C+T)

> Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -- but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.

| Comments: | Accepted at NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05882](https://arxiv.org/abs/2104.05882) [cs.CL]** |
|           | (or **[arXiv:2104.05882v1](https://arxiv.org/abs/2104.05882v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-5">5. Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation
</h2>

Title: [Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation](https://arxiv.org/abs/2104.05964)

Authors: [Kyeongpil Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+K), [Kyohoon Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+K), [Soyoung Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Sujin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+S), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J), [Yougbin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y)

> Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a self-attention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.

| Comments: | Accepted to NAACL 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.05964](https://arxiv.org/abs/2104.05964) [cs.CL]** |
|           | (or **[arXiv:2104.05964v1](https://arxiv.org/abs/2104.05964v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-6">6. Gender Bias in Machine Translation
</h2>

Title: [Gender Bias in Machine Translation](https://arxiv.org/abs/2104.06001)

Authors: [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i)critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii)discuss the mitigating strategies proposed so far, and iv)point toward potential directions for future work.

| Comments: | Accepted for publication in Transaction of the Association for Computational Linguistics (TACL), 2021. Pre-MIT Press publication version |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.06001](https://arxiv.org/abs/2104.06001) [cs.CL]** |
|           | (or **[arXiv:2104.06001v1](https://arxiv.org/abs/2104.06001v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-7">7. Lessons on Parameter Sharing across Layers in Transformers
</h2>

Title: [Lessons on Parameter Sharing across Layers in Transformers](https://arxiv.org/abs/2104.06022)

Authors: [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S)

> We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06022](https://arxiv.org/abs/2104.06022) [cs.CL]** |
|           | (or **[arXiv:2104.06022v1](https://arxiv.org/abs/2104.06022v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-8">8. What's in your Head? Emergent Behaviour in Multi-Task Transformer Models
</h2>

Title: [What's in your Head? Emergent Behaviour in Multi-Task Transformer Models](https://arxiv.org/abs/2104.06129)

Authors: [Mor Geva](https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M), [Uri Katz](https://arxiv.org/search/cs?searchtype=author&query=Katz%2C+U), [Aviv Ben-Arie](https://arxiv.org/search/cs?searchtype=author&query=Ben-Arie%2C+A), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06129](https://arxiv.org/abs/2104.06129) [cs.CL]** |
|           | (or **[arXiv:2104.06129v1](https://arxiv.org/abs/2104.06129v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-9">9. Understanding Hard Negatives in Noise Contrastive Estimation
</h2>

Title: [Understanding Hard Negatives in Noise Contrastive Estimation](https://arxiv.org/abs/2104.06245)

Authors: [Wenzheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Karl Stratos](https://arxiv.org/search/cs?searchtype=author&query=Stratos%2C+K)

> The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives -- highest-scoring incorrect examples under the model -- are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.06245](https://arxiv.org/abs/2104.06245) [cs.CL]** |
|           | (or **[arXiv:2104.06245v1](https://arxiv.org/abs/2104.06245v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-10">10. Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling
</h2>

Title: [Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling](https://arxiv.org/abs/2104.06268)

Authors: [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I)

> In this thesis, we address the data scarcity and limitations of linguistic theory by proposing language-agnostic multi-task training methods. First, we introduce a meta-learning-based approach, meta-transfer learning, in which information is judiciously extracted from high-resource monolingual speech data to the code-switching domain. The meta-transfer learning quickly adapts the model to the code-switching task from a number of monolingual tasks by learning to learn in a multi-task learning fashion. Second, we propose a novel multilingual meta-embeddings approach to effectively represent code-switching data by acquiring useful knowledge learned in other languages, learning the commonalities of closely related languages and leveraging lexical composition. The method is far more efficient compared to contextualized pre-trained multilingual models. Third, we introduce multi-task learning to integrate syntactic information as a transfer learning strategy to a language model and learn where to code-switch. To further alleviate the aforementioned issues, we propose a data augmentation method using Pointer-Gen, a neural network using a copy mechanism to teach the model the code-switch points from monolingual parallel sentences. We disentangle the need for linguistic theory, and the model captures code-switching points by attending to input words and aligning the parallel words, without requiring any word alignments or constituency parsers. More importantly, the model can be effectively used for languages that are syntactically different, and it outperforms the linguistic theory-based models.

| Comments: | HKUST PhD Thesis. 120 pages                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2104.06268](https://arxiv.org/abs/2104.06268) [cs.CL]** |
|           | (or **[arXiv:2104.06268v1](https://arxiv.org/abs/2104.06268v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-11">11. EXPLAINABOARD: An Explainable Leaderboard for NLP
</h2>

Title: [EXPLAINABOARD: An Explainable Leaderboard for NLP](https://arxiv.org/abs/2104.06387)

Authors: [Pengfei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Jinlan Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Yang Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [Weizhe Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+W), [Shuaicheng Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Junqi Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Yixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Zihuiwen Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B, C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or and in what contexts do particular errors occur?). ExplainaBoard has been deployed at \url{[this http URL](http://explainaboard.nlpedia.ai/)}, and we have additionally released our interpretable evaluation code at \url{[this https URL](https://github.com/neulab/ExplainaBoard)} and output files from more than 300 systems, 40 datasets, and 9 tasks to motivate the "output-driven" research in the future.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.06387](https://arxiv.org/abs/2104.06387) [cs.CL]** |
|           | (or **[arXiv:2104.06387v1](https://arxiv.org/abs/2104.06387v1) [cs.CL]** for this version) |





<h2 id="2021-04-14-12">12. Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding
</h2>

Title: [Bridging the Gap Between Clean Data Training and Real-World Inference for Spoken Language Understanding](https://arxiv.org/abs/2104.06393)

Authors: [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Yiren Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Spoken language understanding (SLU) system usually consists of various pipeline components, where each component heavily relies on the results of its upstream ones. For example, Intent detection (ID), and slot filling (SF) require its upstream automatic speech recognition (ASR) to transform the voice into text. In this case, the upstream perturbations, e.g. ASR errors, environmental noise and careless user speaking, will propagate to the ID and SF models, thus deteriorating the system performance. Therefore, the well-performing SF and ID models are expected to be noise resistant to some extent. However, existing models are trained on clean data, which causes a \textit{gap between clean data training and real-world inference.} To bridge the gap, we propose a method from the perspective of domain adaptation, by which both high- and low-quality samples are embedding into similar vector space. Meanwhile, we design a denoising generation model to reduce the impact of the low-quality samples. Experiments on the widely-used dataset, i.e. Snips, and large scale in-house dataset (10 million training examples) demonstrate that this method not only outperforms the baseline models on real-world (noisy) corpus but also enhances the robustness, that is, it produces high-quality results under a noisy environment. The source code will be released.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.06393](https://arxiv.org/abs/2104.06393) [cs.CL]** |
|           | (or **[arXiv:2104.06393v1](https://arxiv.org/abs/2104.06393v1) [cs.CL]** for this version) |





# 2021-04-13

[Return to Index](#Index)



<h2 id="2021-04-13-1">1. Achieving Model Robustness through Discrete Adversarial Training
</h2>

Title: [Achieving Model Robustness through Discrete Adversarial Training](https://arxiv.org/abs/2104.05062)

Authors: [Maor Ivgi](https://arxiv.org/search/cs?searchtype=author&query=Ivgi%2C+M), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only, i.e., given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every step, adapting to the changing nature of the model. We also consider efficient attacks based on random sampling, that unlike prior work are not based on expensive search-based procedures. As a second contribution, we provide a general formulation for multiple search-based attacks from past work, and propose a new attack based on best-first search. Surprisingly, we find that random sampling leads to impressive gains in robustness, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher training cost, significantly improving robustness on three datasets. Last, we show that our proposed algorithm substantially improves robustness compared to prior methods.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05062](https://arxiv.org/abs/2104.05062) [cs.LG]** |
|           | (or **[arXiv:2104.05062v1](https://arxiv.org/abs/2104.05062v1) [cs.LG]** for this version) |





<h2 id="2021-04-13-2">2. TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation
</h2>

Title: [TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation](https://arxiv.org/abs/2104.04632)

Authors: [Hansi Hettiarachchi](https://arxiv.org/search/cs?searchtype=author&query=Hettiarachchi%2C+H), [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T)

> Identifying whether a word carries the same meaning or different meaning in two contexts is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. Most of the previous work in this area rely on language-specific resources making it difficult to generalise across languages. Considering this limitation, our approach to SemEval-2021 Task 2 is based only on pretrained transformer models and does not use any language-specific processing and resources. Despite that, our best model achieves 0.90 accuracy for English-English subtask which is very compatible compared to the best result of the subtask; 0.93 accuracy. Our approach also achieves satisfactory results in other monolingual and cross-lingual language pairs as well.

| Comments: | Accepted to SemEval-2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.04632](https://arxiv.org/abs/2104.04632) [cs.CL]** |
|           | (or **[arXiv:2104.04632v1](https://arxiv.org/abs/2104.04632v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-3">3. Not All Attention Is All You Need
</h2>

Title: [Not All Attention Is All You Need](https://arxiv.org/abs/2104.04692)

Authors: [Hongqiu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M)

> Self-attention based models have achieved remarkable success in natural language processing. However, the self-attention network design is questioned as suboptimal in recent studies, due to its veiled validity and high redundancy. In this paper, we focus on pre-trained language models with self-pruning training design on task-specific tuning. We demonstrate that the lighter state-of-the-art models with nearly 80% of self-attention layers pruned, may achieve even better results on multiple tasks, including natural language understanding, document classification, named entity recognition and POS tagging, with nearly twice faster inference.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.04692](https://arxiv.org/abs/2104.04692) [cs.CL]** |
|           | (or **[arXiv:2104.04692v1](https://arxiv.org/abs/2104.04692v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-4">4. Sentiment-based Candidate Selection for NMT
</h2>

Title: [Sentiment-based Candidate Selection for NMT](https://arxiv.org/abs/2104.04840)

Authors: [Alex Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+A), [Derry Tanti Wijaya](https://arxiv.org/search/cs?searchtype=author&query=Wijaya%2C+D+T)

> The explosion of user-generated content (UGC)--e.g. social media posts, comments, and reviews--has motivated the development of NLP applications tailored to these types of informal texts. Prevalent among these applications have been sentiment analysis and machine translation (MT). Grounded in the observation that UGC features highly idiomatic, sentiment-charged language, we propose a decoder-side approach that incorporates automatic sentiment scoring into the MT candidate selection process. We train separate English and Spanish sentiment classifiers, then, using n-best candidates generated by a baseline MT model with beam search, select the candidate that minimizes the absolute difference between the sentiment score of the source sentence and that of the translation, and perform a human evaluation to assess the produced translations. Unlike previous work, we select this minimally divergent translation by considering the sentiment scores of the source sentence and translation on a continuous interval, rather than using e.g. binary classification, allowing for more fine-grained selection of translation candidates. The results of human evaluations show that, in comparison to the open-source MT baseline model on top of which our sentiment-based pipeline is built, our pipeline produces more accurate translations of colloquial, sentiment-heavy source texts.

| Comments:    | 14 pages, 1 figure                                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2104.04840](https://arxiv.org/abs/2104.04840) [cs.CL]** |
|              | (or **[arXiv:2104.04840v1](https://arxiv.org/abs/2104.04840v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-5">5. Disentangled Contrastive Learning for Learning Robust Textual Representations
</h2>

Title: [Disentangled Contrastive Learning for Learning Robust Textual Representations](https://arxiv.org/abs/2104.04907)

Authors: [Xiang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Xin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+X), [Zhen Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+Z), [Hongbin Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H), [Shumin Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Although the self-supervised pre-training of transformer models has resulted in the revolutionizing of natural language processing (NLP) applications and the achievement of state-of-the-art results with regard to various benchmarks, this process is still vulnerable to small and imperceptible permutations originating from legitimate inputs. Intuitively, the representations should be similar in the feature space with subtle input permutations, while large variations occur with different meanings. This motivates us to investigate the learning of robust textual representation in a contrastive manner. However, it is non-trivial to obtain opposing semantic instances for textual samples. In this study, we propose a disentangled contrastive learning method that separately optimizes the uniformity and alignment of representations without negative sampling. Specifically, we introduce the concept of momentum representation consistency to align features and leverage power normalization while conforming the uniformity. Our experimental results for the NLP benchmarks demonstrate that our approach can obtain better results compared with the baselines, as well as achieve promising improvements with invariance tests and adversarial attacks. The code is available in [this https URL](https://github.com/zjunlp/DCL).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.04907](https://arxiv.org/abs/2104.04907) [cs.CL]** |
|           | (or **[arXiv:2104.04907v1](https://arxiv.org/abs/2104.04907v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-6">6. Assessing Reference-Free Peer Evaluation for Machine Translation
</h2>

Title: [Assessing Reference-Free Peer Evaluation for Machine Translation](https://arxiv.org/abs/2104.05146)

Authors: [Sweta Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+S), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.

| Comments: | NAACL 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.05146](https://arxiv.org/abs/2104.05146) [cs.CL]** |
|           | (or **[arXiv:2104.05146v1](https://arxiv.org/abs/2104.05146v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-7">7. FUDGE: Controlled Text Generation With Future Discriminators
</h2>

Title: [FUDGE: Controlled Text Generation With Future Discriminators](https://arxiv.org/abs/2104.05218)

Authors: [Kevin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+K), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D)

> We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in language generation, and formality change in machine translation -- and observe gains in all three tasks.

| Comments: | To appear at NAACL 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.05218](https://arxiv.org/abs/2104.05218) [cs.CL]** |
|           | (or **[arXiv:2104.05218v1](https://arxiv.org/abs/2104.05218v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-8">8. Machine Translation Decoding beyond Beam Search
</h2>

Title: [Machine Translation Decoding beyond Beam Search](https://arxiv.org/abs/2104.05336)

Authors: [Rémi Leblond](https://arxiv.org/search/cs?searchtype=author&query=Leblond%2C+R), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac%2C+J), [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L), [Miruna Pislar](https://arxiv.org/search/cs?searchtype=author&query=Pislar%2C+M), [Jean-Baptiste Lespiau](https://arxiv.org/search/cs?searchtype=author&query=Lespiau%2C+J), [Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&query=Antonoglou%2C+I), [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals%2C+O)

> Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.

| Comments: | 23 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.05336](https://arxiv.org/abs/2104.05336) [cs.CL]** |
|           | (or **[arXiv:2104.05336v1](https://arxiv.org/abs/2104.05336v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-9">9. Self-Training with Weak Supervision
</h2>

Title: [Self-Training with Weak Supervision](https://arxiv.org/abs/2104.05514)

Authors: [Giannis Karamanolakis](https://arxiv.org/search/cs?searchtype=author&query=Karamanolakis%2C+G), [Subhabrata Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+S), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind.
> In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.

| Comments: | Accepted to NAACL 2021 (Long Paper)                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2104.05514](https://arxiv.org/abs/2104.05514) [cs.CL]** |
|           | (or **[arXiv:2104.05514v1](https://arxiv.org/abs/2104.05514v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-10">10. Survey on reinforcement learning for language processing
</h2>

Title: [Survey on reinforcement learning for language processing](https://arxiv.org/abs/2104.05565)

Authors: [Victor Uc-Cetina](https://arxiv.org/search/cs?searchtype=author&query=Uc-Cetina%2C+V), [Nicolas Navarro-Guerrero](https://arxiv.org/search/cs?searchtype=author&query=Navarro-Guerrero%2C+N), [Anabel Martin-Gonzalez](https://arxiv.org/search/cs?searchtype=author&query=Martin-Gonzalez%2C+A), [Cornelius Weber](https://arxiv.org/search/cs?searchtype=author&query=Weber%2C+C), [Stefan Wermter](https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S)

> In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of natural language processing, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in natural language processing that might benefit from reinforcement learning.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.05565](https://arxiv.org/abs/2104.05565) [cs.CL]** |
|           | (or **[arXiv:2104.05565v1](https://arxiv.org/abs/2104.05565v1) [cs.CL]** for this version) |







<h2 id="2021-04-13-11">11. Backtranslation Feedback Improves User Confidence in MT, Not Quality
</h2>

Title: [Backtranslation Feedback Improves User Confidence in MT, Not Quality](https://arxiv.org/abs/2104.05688)

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Michal Novák](https://arxiv.org/search/cs?searchtype=author&query=Novák%2C+M), [Matúš Žilinec](https://arxiv.org/search/cs?searchtype=author&query=Žilinec%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O), [Mateo Obregón](https://arxiv.org/search/cs?searchtype=author&query=Obregón%2C+M), [Robin L. Hill](https://arxiv.org/search/cs?searchtype=author&query=Hill%2C+R+L), [Frédéric Blain](https://arxiv.org/search/cs?searchtype=author&query=Blain%2C+F), [Marina Fomicheva](https://arxiv.org/search/cs?searchtype=author&query=Fomicheva%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L), [Lisa Yankovskaya](https://arxiv.org/search/cs?searchtype=author&query=Yankovskaya%2C+L)

> Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.

| Comments: | 9 pages (excluding references); to appear at NAACL-HWT 2021  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2104.05688](https://arxiv.org/abs/2104.05688) [cs.CL]** |
|           | (or **[arXiv:2104.05688v1](https://arxiv.org/abs/2104.05688v1) [cs.CL]** for this version) |













# 2021-04-12

[Return to Index](#Index)



<h2 id="2021-04-12-1">1. Video-aided Unsupervised Grammar Induction
</h2>

Title: [Video-aided Unsupervised Grammar Induction](https://arxiv.org/abs/2104.04369)

Authors: [Songyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Linfeng Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L), [Lifeng Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+L), [Kun Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Jiebo Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on learning syntactic grammars from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.

| Comments: | This paper is accepted by NAACL'21                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.04369](https://arxiv.org/abs/2104.04369) [cs.CV]** |
|           | (or **[arXiv:2104.04369v1](https://arxiv.org/abs/2104.04369v1) [cs.CV]** for this version) |





<h2 id="2021-04-12-2">2. Design and Implementation of English To Yoruba Verb Phrase Machine Translation System
</h2>

Title: [Design and Implementation of English To Yoruba Verb Phrase Machine Translation System](https://arxiv.org/abs/2104.04125)

Authors: [Safiriyu Eludiora](https://arxiv.org/search/cs?searchtype=author&query=Eludiora%2C+S), [Benjamin Ajibade](https://arxiv.org/search/cs?searchtype=author&query=Ajibade%2C+B)

> We aim to develop an English to Yoruba machine translation system which can translate English verb phrase text to its Yoruba equivalent.Words from both languages Source Language and Target Language were collected for the verb phrase group in the home domain.The lexical translation is done by assigning values of the matching word in the dictionary.The syntax of the two languages was realized using Context-Free Grammar,we validated the rewrite rules with finite state automata.The human evaluation method was used and expert fluency scored.The evaluation shows the system performed better than that of sampled Google translation with over 70 percent of the response matching that of the system's output.

| Comments: | 9 pages, 9 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.04125](https://arxiv.org/abs/2104.04125) [cs.CL]** |
|           | (or **[arXiv:2104.04125v1](https://arxiv.org/abs/2104.04125v1) [cs.CL]** for this version) |





<h2 id="2021-04-12-3">3. Efficient Large-Scale Language Model Training on GPU Clusters
</h2>

Title: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473)

Authors: [Deepak Narayanan](https://arxiv.org/search/cs?searchtype=author&query=Narayanan%2C+D), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Jared Casper](https://arxiv.org/search/cs?searchtype=author&query=Casper%2C+J), [Patrick LeGresley](https://arxiv.org/search/cs?searchtype=author&query=LeGresley%2C+P), [Mostofa Patwary](https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M), [Vijay Korthikanti](https://arxiv.org/search/cs?searchtype=author&query=Korthikanti%2C+V), [Dmitri Vainbrand](https://arxiv.org/search/cs?searchtype=author&query=Vainbrand%2C+D), [Prethvi Kashinkunti](https://arxiv.org/search/cs?searchtype=author&query=Kashinkunti%2C+P), [Julie Bernauer](https://arxiv.org/search/cs?searchtype=author&query=Bernauer%2C+J), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B), [Amar Phanishayee](https://arxiv.org/search/cs?searchtype=author&query=Phanishayee%2C+A), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia%2C+M)

> Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these large models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on a single GPU or even on a multi-GPU server; and b) the number of compute operations required to train these models can result in unrealistically long training times. New methods of model parallelism such as tensor and pipeline parallelism have been proposed to address these challenges; unfortunately, naive usage leads to fundamental scaling issues at thousands of GPUs due to various reasons, e.g., expensive cross-node communication or idle periods waiting on other devices.
> In this work, we show how to compose different types of parallelism methods (tensor, pipeline, and data paralleism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models we can efficiently train compared to existing systems. We discuss various implementations of pipeline parallelism and propose a novel schedule that can improve throughput by more than 10% with comparable memory footprint compared to previously-proposed approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. The composition of these techniques allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of peak; previous efforts to train similar-sized models achieve much lower throughput (36% of theoretical peak). Our code has been open-sourced at [this https URL](https://github.com/nvidia/megatron-lm).

| Subjects: | **Computation and Language (cs.CL)**; Distributed, Parallel, and Cluster Computing (cs.DC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.04473](https://arxiv.org/abs/2104.04473) [cs.CL]** |
|           | (or **[arXiv:2104.04473v1](https://arxiv.org/abs/2104.04473v1) [cs.CL]** for this version) |





<h2 id="2021-04-12-4">4. Chinese Character Decomposition for Neural MT with Multi-Word Expressions
</h2>

Title: [Chinese Character Decomposition for Neural MT with Multi-Word Expressions](https://arxiv.org/abs/2104.04497)

Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Gareth J. F. Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+G+J+F), [Alan F. Smeaton](https://arxiv.org/search/cs?searchtype=author&query=Smeaton%2C+A+F), [Paolo Bolzoni](https://arxiv.org/search/cs?searchtype=author&query=Bolzoni%2C+P)

> Chinese character decomposition has been used as a feature to enhance Machine Translation (MT) models, combining radicals into character and word level models. Recent work has investigated ideograph or stroke level embedding. However, questions remain about different decomposition levels of Chinese character representations, radical and strokes, best suited for MT. To investigate the impact of Chinese decomposition embedding in detail, i.e., radical, stroke, and intermediate levels, and how well these decompositions represent the meaning of the original character sequences, we carry out analysis with both automated and human evaluation of MT. Furthermore, we investigate if the combination of decomposed Multiword Expressions (MWEs) can enhance the model learning. MWE integration into MT has seen more than a decade of exploration. However, decomposed MWEs has not previously been explored.

| Comments: | Accepted to publish in NoDaLiDa2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.04497](https://arxiv.org/abs/2104.04497) [cs.CL]** |
|           | (or **[arXiv:2104.04497v1](https://arxiv.org/abs/2104.04497v1) [cs.CL]** for this version) |










# 2021-04-09

[Return to Index](#Index)



<h2 id="2021-04-09-1">1. Extended Parallel Corpus for Amharic-English Machine Translation
</h2>

Title: [Extended Parallel Corpus for Amharic-English Machine Translation](https://arxiv.org/abs/2104.03543)

Authors: [Andargachew Mekonnen Gezmu](https://arxiv.org/search/cs?searchtype=author&query=Gezmu%2C+A+M), [Andreas Nürnberger](https://arxiv.org/search/cs?searchtype=author&query=Nürnberger%2C+A), [Tesfaye Bayu Bati](https://arxiv.org/search/cs?searchtype=author&query=Bati%2C+T+B)

> This paper describes the acquisition, preprocessing, segmentation, and alignment of an Amharic-English parallel corpus. It will be useful for machine translation of an under-resourced language, Amharic. The corpus is larger than previously compiled corpora; it is released for research purposes. We trained neural machine translation and phrase-based statistical machine translation models using the corpus. In the automatic evaluation, neural machine translation models outperform phrase-based statistical machine translation models.

| Comments: | Accepted to AfricanNLP workshop under EACL 2021              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.03543](https://arxiv.org/abs/2104.03543) [cs.CL]** |
|           | (or **[arXiv:2104.03543v1](https://arxiv.org/abs/2104.03543v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-2">2. BSTC: A Large-Scale Chinese-English Speech Translation Dataset
</h2>

Title: [BSTC: A Large-Scale Chinese-English Speech Translation Dataset](https://arxiv.org/abs/2104.03575)

Authors: [Ruiqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Xiyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Chuanqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Zhongjun HeHua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z+H), [Zhi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Ying Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Qinfei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q)

> This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale Chinese-English speech translation dataset. This dataset is constructed based on a collection of licensed videos of talks or lectures, including about 68 hours of Mandarin data, their manual transcripts and translations into English, as well as automated transcripts by an automatic speech recognition (ASR) model. We have further asked three experienced interpreters to simultaneously interpret the testing talks in a mock conference setting. This corpus is expected to promote the research of automatic simultaneous translation as well as the development of practical systems. We have organized simultaneous translation tasks and used this corpus to evaluate automatic simultaneous translation systems.

| Comments: | 8 pages, 6 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.03575](https://arxiv.org/abs/2104.03575) [cs.CL]** |
|           | (or **[arXiv:2104.03575v1](https://arxiv.org/abs/2104.03575v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-3">3. A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders
</h2>

Title: [A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](https://arxiv.org/abs/2104.03630)

Authors: [Maarten De Raedt](https://arxiv.org/search/cs?searchtype=author&query=De+Raedt%2C+M), [Fréderic Godin](https://arxiv.org/search/cs?searchtype=author&query=Godin%2C+F), [Pieter Buteneers](https://arxiv.org/search/cs?searchtype=author&query=Buteneers%2C+P), [Chris Develder](https://arxiv.org/search/cs?searchtype=author&query=Develder%2C+C), [Thomas Demeester](https://arxiv.org/search/cs?searchtype=author&query=Demeester%2C+T)

> Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. We investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and cross-lingual settings.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.03630](https://arxiv.org/abs/2104.03630) [cs.CL]** |
|           | (or **[arXiv:2104.03630v1](https://arxiv.org/abs/2104.03630v1) [cs.CL]** for this version) |





<h2 id="2021-04-09-4">4. Probing BERT in Hyperbolic Spaces
</h2>

Title: [Probing BERT in Hyperbolic Spaces](https://arxiv.org/abs/2104.03869)

Authors: [Boli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Yao Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Guangwei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G), [Pengjun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+P), [Chuanqi Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C), [Mosha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Liping Jing](https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+L)

> Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a Poincare probe, a structural probe projecting these embeddings into a Poincare subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincare probe via extensive experiments and visualization. Our results can be reproduced at [this https URL](https://github.com/FranxYao/PoincareProbe).

| Comments: | ICLR 2021 Camera ready                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.03869](https://arxiv.org/abs/2104.03869) [cs.CL]** |
|           | (or **[arXiv:2104.03869v1](https://arxiv.org/abs/2104.03869v1) [cs.CL]** for this version) |





# 2021-04-08

[Return to Index](#Index)



<h2 id="2021-04-08-1">1. VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations
</h2>

Title: [VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations](https://arxiv.org/abs/2104.02797)

Authors: [Archit Rathore](https://arxiv.org/search/cs?searchtype=author&query=Rathore%2C+A), [Sunipa Dev](https://arxiv.org/search/cs?searchtype=author&query=Dev%2C+S), [Jeff M. Phillips](https://arxiv.org/search/cs?searchtype=author&query=Phillips%2C+J+M), [Vivek Srikumar](https://arxiv.org/search/cs?searchtype=author&query=Srikumar%2C+V), [Yan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Chin-Chia Michael Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+C+M), [Junpeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Bei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B)

> Word vector embeddings have been shown to contain and amplify biases in data they are extracted from. Consequently, many techniques have been proposed to identify, mitigate, and attenuate these biases in word representations. In this paper, we utilize interactive visualization to increase the interpretability and accessibility of a collection of state-of-the-art debiasing techniques. To aid this, we present Visualization of Embedding Representations for deBiasing system ("VERB"), an open-source web-based visualization tool that helps the users gain a technical understanding and visual intuition of the inner workings of debiasing techniques, with a focus on their geometric properties. In particular, VERB offers easy-to-follow use cases in exploring the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, VERB decomposes each technique into interpretable sequences of primitive transformations and highlights their effect on the word vectors using dimensionality reduction and interactive visual exploration. VERB is designed to target natural language processing (NLP) practitioners who are designing decision-making systems on top of word embeddings, and also researchers working with fairness and ethics of machine learning systems in NLP. It can also serve as a visual medium for education, which helps an NLP novice to understand and mitigate biases in word embeddings.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2104.02797](https://arxiv.org/abs/2104.02797) [cs.CL]** |
|           | (or **[arXiv:2104.02797v1](https://arxiv.org/abs/2104.02797v1) [cs.CL]** for this version) |





<h2 id="2021-04-08-2">2. Better Neural Machine Translation by Extracting Linguistic Information from BERT
</h2>

Title: [Better Neural Machine Translation by Extracting Linguistic Information from BERT](https://arxiv.org/abs/2104.02831)

Authors: [Hassan S. Shavarani](https://arxiv.org/search/cs?searchtype=author&query=Shavarani%2C+H+S), [Anoop Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+A)

> Adding linguistic information (syntax or semantics) to neural machine translation (NMT) has mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT (Devlin et al., 2019) has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer-based NMT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.02831](https://arxiv.org/abs/2104.02831) [cs.CL]** |
|           | (or **[arXiv:2104.02831v1](https://arxiv.org/abs/2104.02831v1) [cs.CL]** for this version) |



<h2 id="2021-04-08-3">3. GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education
</h2>

Title: [GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for Language Education](https://arxiv.org/abs/2104.03190)

Authors: [Masato Hagiwara](https://arxiv.org/search/cs?searchtype=author&query=Hagiwara%2C+M), [Joshua Tanner](https://arxiv.org/search/cs?searchtype=author&query=Tanner%2C+J), [Keisuke Sakaguchi](https://arxiv.org/search/cs?searchtype=author&query=Sakaguchi%2C+K)

> We present GrammarTagger, an open-source grammar profiler which, given an input text, identifies grammatical features useful for language education. The model architecture enables it to learn from a small amount of texts annotated with spans and their labels, which 1) enables easier and more intuitive annotation, 2) supports overlapping spans, and 3) is less prone to error propagation, compared to complex hand-crafted rules defined on constituency/dependency parses. We show that we can bootstrap a grammar profiler model with F1≈0.6 from only a couple hundred sentences both in English and Chinese, which can be further boosted via learning a multilingual model. With GrammarTagger, we also build Octanove Learn, a search engine of language learning materials indexed by their reading difficulty and grammatical features. The code and pretrained models are publicly available at \url{[this https URL](https://github.com/octanove/grammartagger)}.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.03190](https://arxiv.org/abs/2104.03190) [cs.CL]** |
|           | (or **[arXiv:2104.03190v1](https://arxiv.org/abs/2104.03190v1) [cs.CL]** for this version) |









# 2021-04-07

[Return to Index](#Index)



<h2 id="2021-04-07-1">1. Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding
</h2>

Title: [Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding](https://arxiv.org/abs/2104.02138)

Authors: [Suyoun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Abhinav Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+A), [Duc Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+D), [Ching-Feng Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+C), [Christian Fuegen](https://arxiv.org/search/cs?searchtype=author&query=Fuegen%2C+C), [Ozlem Kalinli](https://arxiv.org/search/cs?searchtype=author&query=Kalinli%2C+O), [Michael L. Seltzer](https://arxiv.org/search/cs?searchtype=author&query=Seltzer%2C+M+L)

> Word Error Rate (WER) has been the predominant metric used to evaluate the performance of automatic speech recognition (ASR) systems. However, WER is sometimes not a good indicator for downstream Natural Language Understanding (NLU) tasks, such as intent recognition, slot filling, and semantic parsing in task-oriented dialog systems. This is because WER takes into consideration only literal correctness instead of semantic correctness, the latter of which is typically more important for these downstream tasks. In this study, we propose a novel Semantic Distance (SemDist) measure as an alternative evaluation metric for ASR systems to address this issue. We define SemDist as the distance between a reference and hypothesis pair in a sentence-level embedding space. To represent the reference and hypothesis as a sentence embedding, we exploit RoBERTa, a state-of-the-art pre-trained deep contextualized language model based on the transformer architecture. We demonstrate the effectiveness of our proposed metric on various downstream tasks, including intent recognition, semantic parsing, and named entity recognition.

| Comments: | submitted to Interspeech 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2104.02138](https://arxiv.org/abs/2104.02138) [cs.CL]** |
|           | (or **[arXiv:2104.02138v1](https://arxiv.org/abs/2104.02138v1) [cs.CL]** for this version) |





<h2 id="2021-04-07-2">2. ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation
</h2>

Title: [ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation](https://arxiv.org/abs/2104.02308)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Quan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Tao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T), [Shuhan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S), [Xin Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> It has been found that residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODEs). In this paper, we explore a deeper relationship between Transformer and numerical methods of ODEs. We show that a residual block of layers in Transformer can be described as a higher-order solution to ODEs. This leads us to design a new architecture (call it ODE Transformer) analogous to the Runge-Kutta method that is well motivated in ODEs. As a natural extension to Transformer, ODE Transformer is easy to implement and parameter efficient. Our experiments on three WMT tasks demonstrate the genericity of this model, and large improvements in performance over several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the WMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the WMT'14 En-Fr task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.02308](https://arxiv.org/abs/2104.02308) [cs.CL]** |
|           | (or **[arXiv:2104.02308v1](https://arxiv.org/abs/2104.02308v1) [cs.CL]** for this version) |







# 2021-04-06

[Return to Index](#Index)



<h2 id="2021-04-06-1">1. TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition
</h2>

Title: [TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition](https://arxiv.org/abs/2104.01522)

Authors: [Zhengkun Tian](https://arxiv.org/search/eess?searchtype=author&query=Tian%2C+Z), [Jiangyan Yi](https://arxiv.org/search/eess?searchtype=author&query=Yi%2C+J), [Jianhua Tao](https://arxiv.org/search/eess?searchtype=author&query=Tao%2C+J), [Ye Bai](https://arxiv.org/search/eess?searchtype=author&query=Bai%2C+Y), [Shuai Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+S), [Zhengqi Wen](https://arxiv.org/search/eess?searchtype=author&query=Wen%2C+Z), [Xuefei Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X)

> The autoregressive (AR) models, such as attention-based encoder-decoder models and RNN-Transducer, have achieved great success in speech recognition. They predict the output sequence conditioned on the previous tokens and acoustic encoded states, which is inefficient on GPUs. The non-autoregressive (NAR) models can get rid of the temporal dependency between the output tokens and predict the entire output tokens in at least one step. However, the NAR model still faces two major problems. On the one hand, there is still a great gap in performance between the NAR models and the advanced AR models. On the other hand, it's difficult for most of the NAR models to train and converge. To address these two problems, we propose a new model named the two-step non-autoregressive transformer(TSNAT), which improves the performance and accelerating the convergence of the NAR model by learning prior knowledge from a parameters-sharing AR model. Furthermore, we introduce the two-stage method into the inference process, which improves the model performance greatly. All the experiments are conducted on a public Chinese mandarin dataset ASIEHLL-1. The results show that the TSNAT can achieve a competitive performance with the AR model and outperform many complicated NAR models.

| Comments: | Submitted to Interspeech2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2104.01522](https://arxiv.org/abs/2104.01522) [eess.AS]** |
|           | (or **[arXiv:2104.01522v1](https://arxiv.org/abs/2104.01522v1) [eess.AS]** for this version) |





<h2 id="2021-04-06-2">2. Attention Forcing for Machine Translation
</h2>

Title: [Attention Forcing for Machine Translation](https://arxiv.org/abs/2104.01264)

Authors: [Qingyun Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Q), [Yiting Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Potsawee Manakul](https://arxiv.org/search/cs?searchtype=author&query=Manakul%2C+P), [Xixin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Mark J. F. Gales](https://arxiv.org/search/cs?searchtype=author&query=Gales%2C+M+J+F)

> Auto-regressive sequence-to-sequence models with attention mechanisms have achieved state-of-the-art performance in various tasks including Text-To-Speech (TTS) and Neural Machine Translation (NMT). The standard training approach, teacher forcing, guides a model with the reference output history. At inference stage, the generated output history must be used. This mismatch can impact performance. However, it is highly challenging to train the model using the generated output. Several approaches have been proposed to address this problem, normally by selectively using the generated output history. To make training stable, these approaches often require a heuristic schedule or an auxiliary classifier. This paper introduces attention forcing for NMT. This approach guides the model with the generated output history and reference attention, and can reduce the training-inference mismatch without a schedule or a classifier. Attention forcing has been successful in TTS, but its application to NMT is more challenging, due to the discrete and multi-modal nature of the output space. To tackle this problem, this paper adds a selection scheme to vanilla attention forcing, which automatically selects a suitable training approach for each pair of training data. Experiments show that attention forcing can improve the overall translation quality and the diversity of the translations.

| Comments: | arXiv admin note: text overlap with [arXiv:1909.12289](https://arxiv.org/abs/1909.12289) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.01264](https://arxiv.org/abs/2104.01264) [cs.CL]** |
|           | (or **[arXiv:2104.01264v1](https://arxiv.org/abs/2104.01264v1) [cs.CL]** for this version) |







<h2 id="2021-04-06-3">3. WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach
</h2>

Title: [WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](https://arxiv.org/abs/2104.01767)

Authors: [Junjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Wanjun Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+W), [Shuai Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Linjun Shou](https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L), [Ming Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N)

> Producing the embedding of a sentence in an unsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have there main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top andbottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.01767](https://arxiv.org/abs/2104.01767) [cs.CL]** |
|           | (or **[arXiv:2104.01767v1](https://arxiv.org/abs/2104.01767v1) [cs.CL]** for this version) |







<h2 id="2021-04-06-4">4. Rethinking Perturbations in Encoder-Decoders for Fast Training
</h2>

Title: [Rethinking Perturbations in Encoder-Decoders for Fast Training](https://arxiv.org/abs/2104.01853)

Authors: [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Shun Kiyono](https://arxiv.org/search/cs?searchtype=author&query=Kiyono%2C+S)

> We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster. Our code is publicly available at [this https URL](https://github.com/takase/rethink_perturbations).

| Comments: | Accepted at NAACL-HLT 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.01853](https://arxiv.org/abs/2104.01853) [cs.CL]** |
|           | (or **[arXiv:2104.01853v1](https://arxiv.org/abs/2104.01853v1) [cs.CL]** for this version) |







# 2021-04-02

[Return to Index](#Index)



<h2 id="2021-04-02-1">1. Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study
</h2>

Title: [Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://arxiv.org/abs/2104.00676)

Authors: [Zhiqiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Zechun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Dejia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+D), [Zitian Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Kwang-Ting Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+K), [Marios Savvides](https://arxiv.org/search/cs?searchtype=author&query=Savvides%2C+M)

> This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation. We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness. Project page: [this http URL](http://zhiqiangshen.com/projects/LS_and_KD/index.html).

| Comments: | ICLR 2021. Project page: [this http URL](http://zhiqiangshen.com/projects/LS_and_KD/index.html) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2104.00676](https://arxiv.org/abs/2104.00676) [cs.LG]** |
|           | (or **[arXiv:2104.00676v1](https://arxiv.org/abs/2104.00676v1) [cs.LG]** for this version) |





<h2 id="2021-04-02-2">2. Domain-specific MT for Low-resource Languages: The case of Bambara-French
</h2>

Title: [Domain-specific MT for Low-resource Languages: The case of Bambara-French](https://arxiv.org/abs/2104.00041)

Authors: [Allahsera Auguste Tapo](https://arxiv.org/search/cs?searchtype=author&query=Tapo%2C+A+A), [Michael Leventhal](https://arxiv.org/search/cs?searchtype=author&query=Leventhal%2C+M), [Sarah Luger](https://arxiv.org/search/cs?searchtype=author&query=Luger%2C+S), [Christopher M. Homan](https://arxiv.org/search/cs?searchtype=author&query=Homan%2C+C+M), [Marcos Zampieri](https://arxiv.org/search/cs?searchtype=author&query=Zampieri%2C+M)

> Translating to and from low-resource languages is a challenge for machine translation (MT) systems due to a lack of parallel data. In this paper we address the issue of domain-specific MT for Bambara, an under-resourced Mande language spoken in Mali. We present the first domain-specific parallel dataset for MT of Bambara into and from French. We discuss challenges in working with small quantities of domain-specific data for a low-resource language and we present the results of machine learning experiments on this data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00041](https://arxiv.org/abs/2104.00041) [cs.CL]** |
|           | (or **[arXiv:2104.00041v1](https://arxiv.org/abs/2104.00041v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-3">3. Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation
</h2>

Title: [Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation](https://arxiv.org/abs/2104.00106)

Authors: [Aviral Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+A), [Chengzhi Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Har Simrat Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+H+S)

> This work focuses on comparing different solutions for machine translation on low resource language pairs, namely, with zero-shot transfer learning and unsupervised machine translation. We discuss how the data size affects the performance of both unsupervised MT and transfer learning. Additionally we also look at how the domain of the data affects the result of unsupervised MT. The code to all the experiments performed in this project are accessible on Github.

| Comments: | 7 pages, 2 figures, 4 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00106](https://arxiv.org/abs/2104.00106) [cs.CL]** |
|           | (or **[arXiv:2104.00106v1](https://arxiv.org/abs/2104.00106v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-4">4. Detecting over/under-translation errors for determining adequacy in human translations
</h2>

Title: [Detecting over/under-translation errors for determining adequacy in human translations](https://arxiv.org/abs/2104.00267)

Authors: [Prabhakar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Ridha Juneja](https://arxiv.org/search/cs?searchtype=author&query=Juneja%2C+R), [Anil Nelakanti](https://arxiv.org/search/cs?searchtype=author&query=Nelakanti%2C+A), [Tamojit Chatterjee](https://arxiv.org/search/cs?searchtype=author&query=Chatterjee%2C+T)

> We present a novel approach to detecting over and under translations (OT/UT) as part of adequacy error checks in translation evaluation. We do not restrict ourselves to machine translation (MT) outputs and specifically target applications with human generated translation pipeline. The goal of our system is to identify OT/UT errors from human translated video subtitles with high error recall. We achieve this without reference translations by learning a model on synthesized training data. We compare various classification networks that we trained on embeddings from pre-trained language model with our best hybrid network of GRU + CNN achieving 89.3% accuracy on high-quality human-annotated evaluation data in 8 languages.

| Comments: | 6 pages, 5 tables                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00267](https://arxiv.org/abs/2104.00267) [cs.CL]** |
|           | (or **[arXiv:2104.00267v1](https://arxiv.org/abs/2104.00267v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-5">5. Many-to-English Machine Translation Tools, Data, and Pretrained Models
</h2>

Title: [Many-to-English Machine Translation Tools, Data, and Pretrained Models](https://arxiv.org/abs/2104.00290)

Authors: [Thamme Gowda](https://arxiv.org/search/cs?searchtype=author&query=Gowda%2C+T), [Zhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Chris A Mattmann](https://arxiv.org/search/cs?searchtype=author&query=Mattmann%2C+C+A), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

> While there are more than 7000 languages in the world, most translation research efforts have targeted a few high-resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00290](https://arxiv.org/abs/2104.00290) [cs.CL]** |
|           | (or **[arXiv:2104.00290v1](https://arxiv.org/abs/2104.00290v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-6">6. Low-Resource Neural Machine Translation for South-Eastern African Languages
</h2>

Title: [Low-Resource Neural Machine Translation for South-Eastern African Languages](https://arxiv.org/abs/2104.00366)

Authors: [Evander Nyoni](https://arxiv.org/search/cs?searchtype=author&query=Nyoni%2C+E), [Bruce A. Bassett](https://arxiv.org/search/cs?searchtype=author&query=Bassett%2C+B+A)

> Low-resource African languages have not fully benefited from the progress in neural machine translation because of a lack of data. Motivated by this challenge we compare zero-shot learning, transfer learning and multilingual learning on three Bantu languages (Shona, isiXhosa and isiZulu) and English. Our main target is English-to-isiZulu translation for which we have just 30,000 sentence pairs, 28% of the average size of our other corpora. We show the importance of language similarity on the performance of English-to-isiZulu transfer learning based on English-to-isiXhosa and English-to-Shona parent models whose BLEU scores differ by 5.2. We then demonstrate that multilingual learning surpasses both transfer learning and zero-shot learning on our dataset, with BLEU score improvements relative to the baseline English-to-isiZulu model of 9.9, 6.1 and 2.0 respectively. Our best model also improves the previous SOTA BLEU score by more than 10.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2104.00366](https://arxiv.org/abs/2104.00366) [cs.CL]** |
|           | (or **[arXiv:2104.00366v1](https://arxiv.org/abs/2104.00366v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-7">7. WakaVT: A Sequential Variational Transformer for Waka Generation
</h2>

Title: [WakaVT: A Sequential Variational Transformer for Waka Generation](https://arxiv.org/abs/2104.00426)

Authors: [Yuka Takeishi](https://arxiv.org/search/cs?searchtype=author&query=Takeishi%2C+Y), [Mingxuan Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+M), [Jing Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Zhong Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Z), [Xinyu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X)

> Poetry generation has long been a challenge for artificial intelligence. In the scope of Japanese poetry generation, many researchers have paid attention to Haiku generation, but few have focused on Waka generation. To further explore the creative potential of natural language generation systems in Japanese poetry creation, we propose a novel Waka generation model, WakaVT, which automatically produces Waka poems given user-specified keywords. Firstly, an additive mask-based approach is presented to satisfy the form constraint. Secondly, the structures of Transformer and variational autoencoder are integrated to enhance the quality of generated content. Specifically, to obtain novelty and diversity, WakaVT employs a sequence of latent variables, which effectively captures word-level variability in Waka data. To improve linguistic quality in terms of fluency, coherence, and meaningfulness, we further propose the fused multilevel self-attention mechanism, which properly models the hierarchical linguistic structure of Waka. To the best of our knowledge, we are the first to investigate Waka generation with models based on Transformer and/or variational autoencoder. Both objective and subjective evaluation results demonstrate that our model outperforms baselines significantly.

| Comments: | This paper has been submitted to Neural Processing Letters   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2104.00426](https://arxiv.org/abs/2104.00426) [cs.CL]** |
|           | (or **[arXiv:2104.00426v1](https://arxiv.org/abs/2104.00426v1) [cs.CL]** for this version) |





<h2 id="2021-04-02-8">8. Sampling and Filtering of Neural Machine Translation Distillation Data
</h2>

Title: [Sampling and Filtering of Neural Machine Translation Distillation Data](https://arxiv.org/abs/2104.00664)

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V)

> In most of neural machine translation distillation or stealing scenarios, the goal is to preserve the performance of the target model (teacher). The highest-scoring hypothesis of the teacher model is commonly used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be upsampled and poor hypotheses either removed or undersampled.
> This paper explores the importance sampling method landscape (pruning, hypothesis upsampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful upsampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.

| Comments: | 6 pages (without references); to be published in NAACL-SRW   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2104.00664](https://arxiv.org/abs/2104.00664) [cs.CL]** |
|           | (or **[arXiv:2104.00664v1](https://arxiv.org/abs/2104.00664v1) [cs.CL]** for this version) |







# 2021-04-01

[Return to Index](#Index)



<h2 id="2021-04-01-1">1. An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation
</h2>

Title: [An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation](https://arxiv.org/abs/2103.16789)

Authors:[Lidia Kidane](https://arxiv.org/search/cs?searchtype=author&query=Kidane%2C+L), [Sachin Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y)

> It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, often requiring large amounts of auxiliary data to achieve competitive results. An effective method of generating auxiliary data is back-translation of target language sentences. In this work, we present a case study of Tigrinya where we investigate several back-translation methods to generate synthetic source sentences. We find that in low-resource conditions, back-translation by pivoting through a higher-resource language related to the target language proves most effective resulting in substantial improvements over baselines.

| Comments: | Accepted at AfricaNLP Workshop, EACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16789](https://arxiv.org/abs/2103.16789) [cs.CL]** |
|           | (or **[arXiv:2103.16789v1](https://arxiv.org/abs/2103.16789v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-2">2. Few-shot learning through contextual data augmentation
</h2>

Title: [Few-shot learning through contextual data augmentation](https://arxiv.org/abs/2103.16911)

Authors:[Farid Arthaud](https://arxiv.org/search/cs?searchtype=author&query=Arthaud%2C+F), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.

| Comments: | 14 pages includince 3 of appendices                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16911](https://arxiv.org/abs/2103.16911) [cs.CL]** |
|           | (or **[arXiv:2103.16911v1](https://arxiv.org/abs/2103.16911v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-3">3. UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language
</h2>

Title: [UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language](https://arxiv.org/abs/2103.16997)

Authors:[Oleksiy Syvokon](https://arxiv.org/search/cs?searchtype=author&query=Syvokon%2C+O), [Olena Nahorna](https://arxiv.org/search/cs?searchtype=author&query=Nahorna%2C+O)

> We present a corpus professionally annotated for grammatical error correction (GEC) and fluency edits in the Ukrainian language. To the best of our knowledge, this is the first GEC corpus for the Ukrainian language. We collected texts with errors (20,715 sentences) from a diverse pool of contributors, including both native and non-native speakers. The data cover a wide variety of writing domains, from text chats and essays to formal writing. Professional proofreaders corrected and annotated the corpus for errors relating to fluency, grammar, punctuation, and spelling. This corpus can be used for developing and evaluating GEC systems in Ukrainian. More generally, it can be used for researching multilingual and low-resource NLP, morphologically rich languages, document-level GEC, and fluency correction. The corpus is publicly available at [this https URL](https://github.com/grammarly/ua-gec)

| Comments: | See [this https URL](https://github.com/grammarly/ua-gec) for the dataset. Version 2 of the data is in progress |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2103.16997](https://arxiv.org/abs/2103.16997) [cs.CL]** |
|           | (or **[arXiv:2103.16997v1](https://arxiv.org/abs/2103.16997v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-4">4. Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources
</h2>

Title: [Divide and Rule: Training Context-Aware Multi-Encoder Translation Models with Little Resources](https://arxiv.org/abs/2103.17151)

Authors:[Lorenzo Lupo](https://arxiv.org/search/cs?searchtype=author&query=Lupo%2C+L), [Marco Dinarelli](https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Multi-encoder models are a broad family of context-aware Neural Machine Translation (NMT) systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we show that training these parameters takes large amount of data, since the contextual training signal is sparse. We propose an efficient alternative, based on splitting sentence pairs, that allows to enrich the training signal of a set of parallel sentences by breaking intra-sentential syntactic links, and thus frequently pushing the model to search the context for disambiguating clues. We evaluate our approach with BLEU and contrastive test sets, showing that it allows multi-encoder models to achieve comparable performances to a setting where they are trained with ×10 document-level data. We also show that our approach is a viable option to context-aware NMT for language pairs with zero document-level parallel data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2103.17151](https://arxiv.org/abs/2103.17151) [cs.CL]** |
|           | (or **[arXiv:2103.17151v1](https://arxiv.org/abs/2103.17151v1) [cs.CL]** for this version) |





<h2 id="2021-04-01-5">5. Leveraging Neural Machine Translation for Word Alignment
</h2>

Title: [Leveraging Neural Machine Translation for Word Alignment](https://arxiv.org/abs/2103.17250)

Authors:[Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Daria Pylypenko](https://arxiv.org/search/cs?searchtype=author&query=Pylypenko%2C+D)

> The most common tools for word-alignment rely on a large amount of parallel sentences, which are then usually processed according to one of the IBM model algorithms. The training data is, however, the same as for machine translation (MT) systems, especially for neural MT (NMT), which itself is able to produce word-alignments using the trained attention heads. This is convenient because word-alignment is theoretically a viable byproduct of any attention-based NMT, which is also able to provide decoder scores for a translated sentence pair.
> We summarize different approaches on how word-alignment can be extracted from alignment scores and then explore ways in which scores can be extracted from NMT, focusing on inferring the word-alignment scores based on output sentence and token probabilities. We compare this to the extraction of alignment scores from attention. We conclude with aggregating all of the sources of alignment scores into a simple feed-forward network which achieves the best results when combined alignment extractors are used.

| Comments: | 16 pages (without references). To be published in PBML 116   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2103.17250](https://arxiv.org/abs/2103.17250) [cs.CL]** |
|           | (or **[arXiv:2103.17250v1](https://arxiv.org/abs/2103.17250v1) [cs.CL]** for this version) |





