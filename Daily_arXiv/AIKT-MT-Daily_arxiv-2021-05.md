# Daily arXiv: Machine Translation - May, 2021

# Index


- [2021-05-31](#2021-05-31)

  - [1. Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation](#2021-05-31-1)
  - [2. Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](#2021-05-31-2)
  - [3. Hierarchical Transformer Encoders for Vietnamese Spelling Correction](#2021-05-31-3)
  - [4. Data Augmentation for Text Generation Without Any Augmented Data](#2021-05-31-4)
  - [5. How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation](#2021-05-31-5)
  - [6. Lightweight Cross-Lingual Sentence Representation Learning](#2021-05-31-6)
  - [7. Knowledge Inheritance for Pre-trained Language Models](#2021-05-31-7)
  - [8. Changing the World by Changing the Data](#2021-05-31-8)
- [2021-05-28](#2021-05-28)
  - [1. How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?](#2021-05-28-1)
  - [2. Selective Knowledge Distillation for Neural Machine Translation](#2021-05-28-2)
  - [3. Adaptive Nearest Neighbor Machine Translation](#2021-05-28-3)
  - [4. Extremely low-resource machine translation for closely related languages](#2021-05-28-4)
  - [5. TranSmart: A Practical Interactive Machine Translation System](#2021-05-28-5)
  - [6. Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models](#2021-05-28-6)
- [2021-05-27](#2021-05-27)
  - [1. Improving Sign Language Translation with Monolingual Data by Sign Back-Translation](#2021-05-27-1)
  - [2. IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion](#2021-05-27-2)
  - [3. Context-Sensitive Visualization of Deep Learning Natural Language Processing Models](#2021-05-27-3)
  - [4. Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction](#2021-05-27-4)
  - [5. Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking](#2021-05-27-5)
  - [6. Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](#2021-05-27-6)
- [2021-05-26](#2021-05-26)

  - [1. Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](#2021-05-26-1)
  - [2. ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](#2021-05-26-2)
- [2021-05-25](#2021-05-25)

  - [1. Prevent the Language Model from being Overconfident in Neural Machine Translation](#2021-05-25-1)
  - [2. Neural Machine Translation with Monolingual Translation Memory](#2021-05-25-2)
  - [3. True Few-Shot Learning with Language Models](#2021-05-25-3)
- [2021-05-24](2021-05-24)

  - [1. VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding](#2021-05-24-1)
  - [2. Pretrained Language Models for Text Generation: A Survey](#2021-05-24-2)
  - [3. Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](#2021-05-24-3)
- [2021-05-21](#2021-05-21-1)
- [1. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](#2021-05-21-1)
- [2021-05-20](#2021-05-20)
  - [1. Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing](#2021-05-20-1)
  - [2. Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder](#2021-05-20-2)
  - [3. Effective Attention Sheds Light On Interpretability](#2021-05-20-3)
  - [4. Investigating Math Word Problems using Pretrained Multilingual Language Models](#2021-05-20-4)
  - [5. Combining GCN and Transformer for Chinese Grammatical Error Detection](#2021-05-20-5)
  - [6. TableZa -- A classical Computer Vision approach to Tabular Extraction](#2021-05-20-6)
  - [7. Learning Language Specific Sub-network for Multilingual Machine Translation](#2021-05-20-7)
- [2021-05-19](#2021-05-19)
  - [1. Relative Positional Encoding for Transformers with Linear Complexity](#2021-05-19-1)
  - [2. Multi-Modal Image Captioning for the Visually Impaired](#2021-05-19-2)
  - [3. DRILL: Dynamic Representations for Imbalanced Lifelong Learning](#2021-05-19-3)
  - [4. Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](#2021-05-19-4)
- [2021-05-18](#2021-05-18)
  - [1. Rethinking Skip Connection with Layer Normalization in Transformers and ResNets](#2021-05-18-1)
  - [2. Conscious AI](#2021-05-18-2)
  - [3. Pay Attention to MLPs](#2021-05-18-3)
  - [4. DirectQE: Direct Pretraining for Machine Translation Quality Estimation](#2021-05-18-4)
  - [5. From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding](#2021-05-18-5)
  - [6. The Volctrans Neural Speech Translation System for IWSLT 2021](#2021-05-18-6 )
  - [7. Data Augmentation for Sign Language Gloss Translation](#2021-05-18-7)
  - [8. Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation](#2021-05-18-8)
  - [9. Stage-wise Fine-tuning for Graph-to-Text Generation](#2021-05-18-9)
- [2021-05-17](#2021-05-17)
  
  - [1. Distilling BERT for low complexity network training](#2021-05-17-1)
  - [2. Dynamic Multi-Branch Layers for On-Device Neural Machine Translation](#2021-05-17-2)
  - [3. Do Context-Aware Translation Models Pay the Right Attention?](#2021-05-17-3)
- [2021-05-14](#2021-05-14)
  - [1. Better than BERT but Worse than Baseline](#2021-05-14-1)
  - [2. Spelling Correction with Denoising Transformer](#2021-05-14-2)
  - [3. Designing Multimodal Datasets for NLP Challenges](#2021-05-14-3)
  - [4. Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](#2021-05-14-4)
- [2021-05-13]($2021-05-13)
  
  - [1. Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](#2021-05-13-1)
  - [2. Evaluating Gender Bias in Natural Language Inference](#2021-05-13-2)
  - [3. Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](#2021-05-13-3)
- [2021-05-12](#2021-05-21)
  
  - [1. Cross-Modal Generative Augmentation for Visual Question Answering](#2021-05-21-1)
  - [2. Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity](#2021-05-21-2)
  - [3. Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal](#2021-05-21-3)
  - [4. Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models](#2021-05-21-4)
  - [5. Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation](#2021-05-21-5)
  - [6. Can You Traducir This? Machine Translation for Code-Switched Input](#2021-05-21-6)
  - [7. BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](#2021-05-21-7)
  - [8. Towards transparency in NLP shared tasks](#2021-05-21-8)
  - [9. Including Signed Languages in Natural Language Processing](#2021-05-21-9)
- [2021-05-11](#2021-05-11-1)
  
  - [1. Duplex Sequence-to-Sequence Learning for Reversible Machine Translation](#2021-05-11-1)
  - [2. Measuring and Increasing Context Usage in Context-Aware Machine Translation](#2021-05-11-2)
  - [3. Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation](#2021-05-11-3)
  - [4. Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction](#2021-05-11-4)
  - [5. Self-Guided Curriculum Learning for Neural Machine Translation](#2021-05-11-5)
  - [6. UPC's Speech Translation System for IWSLT 2021](#2021-05-11-6)
- [2021-05-10](#2021-05-10)
  
  - [1. Adapting by Pruning: A Case Study on BERT](#2021-05-10-1)
  - [2. On-the-Fly Controlled Text Generation with Experts and Anti-Experts](#2021-05-10-2)
  - [3. Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](#2021-05-10-3)
  - [4. A Survey of Data Augmentation Approaches for NLP](#2021-05-10-4)
  - [5. Learning Shared Semantic Space for Speech-to-Text Translation](#2021-05-10-5)
  - [6. Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods](#2021-05-10-6)
  - [7. Are Pre-trained Convolutions Better than Pre-trained Transformers?](#2021-05-10-7)
  - [8. ∂-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization](#2021-05-10-8)
- [2021-05-07](#2021-05-07)
  - [1. XeroAlign: Zero-Shot Cross-lingual Transformer Alignment](#2021-05-07-1)
  - [2. Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases](#2021-05-07-2)
  - [3. Content4All Open Research Sign Language Translation Datasets](#2021-05-07-3)
  - [4. Reliability Testing for Natural Language Processing Systems](#2021-05-07-4)
- [2021-05-06](#2021-05-06)
  
  - [1. Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution](#2021-05-06-1)
  - [2. Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy](#2021-05-06-2)
- [2021-05-04](#2021-05-04)	
  - [1. AlloST: Low-resource Speech Translation without Source Transcription](#2021-05-04-1)
  - [2. Larger-Scale Transformers for Multilingual Masked Language Modeling](#2021-05-04-2)
  - [3. Transformers: "The End of History" for NLP?](#2021-05-04-3)
  - [4. BERT memorisation and pitfalls in low-resource scenarios](#2021-05-04-4)
  - [5. Natural Language Generation Using Link Grammar for General Conversational Intelligence](#2021-05-04-5)
- [Other Columns](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-05-31

[Return to Index](#Index)



<h2 id="2021-05-31-1">1. Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation
</h2>

Title: [Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation](https://arxiv.org/abs/2105.13573)

Authors: [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [AbdelRahim Elmadany](https://arxiv.org/search/cs?searchtype=author&query=Elmadany%2C+A), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M)

> Recent progress in neural machine translation (NMT) has made it possible to translate successfully between monolingual language pairs where large parallel data exist, with pre-trained models improving performance even further. Although there exists work on translating in code-mixed settings (where one of the pairs includes text from two or more languages), it is still unclear what recent success in NMT and language modeling exactly means for translating code-mixed text. We investigate one such context, namely MT from code-mixed Modern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop models under different conditions, employing both (i) standard end-to-end sequence-to-sequence (S2S) Transformers trained from scratch and (ii) pre-trained S2S language models (LMs). We are able to acquire reasonable performance using only MSA-EN parallel data with S2S models trained from scratch. We also find LMs fine-tuned on data from various Arabic dialects to help the MSAEA-EN task. Our work is in the context of the Shared Task on Machine Translation in Code-Switching. Our best model achieves **25.72** BLEU, placing us first on the official shared task evaluation for MSAEA-EN.

| Comments: | CALCS2021, colocated with NAACL-2021                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.13573](https://arxiv.org/abs/2105.13573) [cs.LG]** |
|           | (or **[arXiv:2105.13573v1](https://arxiv.org/abs/2105.13573v1) [cs.LG]** for this version) |





<h2 id="2021-05-31-2">2. Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort
</h2>

Title: [Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](https://arxiv.org/abs/2105.13385)

Authors: [Vânia Mendonça](https://arxiv.org/search/cs?searchtype=author&query=Mendonça%2C+V) (1 and 2), [Ricardo Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+R) (1 and 2 and 3), [Luisa Coheur](https://arxiv.org/search/cs?searchtype=author&query=Coheur%2C+L) (1 and 2), [Alberto Sardinha](https://arxiv.org/search/cs?searchtype=author&query=Sardinha%2C+A) (1 and 2), [Ana Lúcia Santos](https://arxiv.org/search/cs?searchtype=author&query=Santos%2C+A+L) (4 and 5) ((1) INESC-ID Lisboa, (2) Instituto Superior Técnico, (3) Unbabel AI, (4) Centro de Linguística da Universidade de Lisboa, (5) Faculdade de Letras da Universidade de Lisboa)

> In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging. Automatic metrics are not reliable when it comes to high performing systems. In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available. Our experiments on WMT'19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations.

| Comments: | Accepted to ACL-IJCNLP 2021 Main Conference (long paper)     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13385](https://arxiv.org/abs/2105.13385) [cs.CL]** |
|           | (or **[arXiv:2105.13385v1](https://arxiv.org/abs/2105.13385v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-3">3. Hierarchical Transformer Encoders for Vietnamese Spelling Correction
</h2>

Title: [Hierarchical Transformer Encoders for Vietnamese Spelling Correction](https://arxiv.org/abs/2105.13578)

Authors: [Hieu Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+H), [Cuong V. Dinh](https://arxiv.org/search/cs?searchtype=author&query=Dinh%2C+C+V), [Long Phan](https://arxiv.org/search/cs?searchtype=author&query=Phan%2C+L), [Son T. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+S+T)

> In this paper, we propose a Hierarchical Transformer model for Vietnamese spelling correction problem. The model consists of multiple Transformer encoders and utilizes both character-level and word-level to detect errors and make corrections. In addition, to facilitate future work in Vietnamese spelling correction tasks, we propose a realistic dataset collected from real-life texts for the problem. We compare our method with other methods and publicly available systems. The proposed method outperforms all of the contemporary methods in terms of recall, precision, and f1-score. A demo version is publicly available.

| Comments: | Accepted by The 34th International Conference on Industrial, Engineering & Other Applications of Applied Intelligent Systems(IEA/AIE 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13578](https://arxiv.org/abs/2105.13578) [cs.CL]** |
|           | (or **[arXiv:2105.13578v1](https://arxiv.org/abs/2105.13578v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-4">4. Data Augmentation for Text Generation Without Any Augmented Data
</h2>

Title: [Data Augmentation for Text Generation Without Any Augmented Data](https://arxiv.org/abs/2105.13650)

Authors: [Wei Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+W), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Jiacheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J)

> Data augmentation is an effective way to improve the performance of many neural text generation models. However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee. Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods.

| Comments: | Accepted into the main conference of ACL 2021                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.13650](https://arxiv.org/abs/2105.13650) [cs.CL]** |
|           | (or **[arXiv:2105.13650v1](https://arxiv.org/abs/2105.13650v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-5">5. How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation
</h2>

Title: [How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation](https://arxiv.org/abs/2105.13782)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Having recognized gender bias as a major issue affecting current translation technologies, researchers have primarily attempted to mitigate it by working on the data front. However, whether algorithmic aspects concur to exacerbate unwanted outputs remains so far under-investigated. In this work, we bring the analysis on gender bias in automatic translation onto a seemingly neutral yet critical component: word segmentation. Can segmenting methods influence the ability to translate gender? Do certain segmentation approaches penalize the representation of feminine linguistic markings? We address these questions by comparing 5 existing segmentation strategies on the target side of speech translation systems. Our results on two language pairs (English-Italian/French) show that state-of-the-art sub-word splitting (BPE) comes at the cost of higher gender bias. In light of this finding, we propose a combined approach that preserves BPE overall translation quality, while leveraging the higher ability of character-based segmentation to properly translate gender.

| Comments: | Accepted in Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13782](https://arxiv.org/abs/2105.13782) [cs.CL]** |
|           | (or **[arXiv:2105.13782v1](https://arxiv.org/abs/2105.13782v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-6">6. Lightweight Cross-Lingual Sentence Representation Learning
</h2>

Title: [Lightweight Cross-Lingual Sentence Representation Learning](https://arxiv.org/abs/2105.13856)

Authors: [Zhuoyuan Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&query=Jaggi%2C+M), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> Large-scale models for learning fixed-dimensional cross-lingual sentence representations like Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.13856](https://arxiv.org/abs/2105.13856) [cs.CL]** |
|           | (or **[arXiv:2105.13856v1](https://arxiv.org/abs/2105.13856v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-7">7. Knowledge Inheritance for Pre-trained Language Models
</h2>

Title: [Knowledge Inheritance for Pre-trained Language Models](https://arxiv.org/abs/2105.13880)

Authors: [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Jing Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+J), [Jiajie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Recent explorations of large-scale pre-trained language models (PLMs) such as GPT-3 have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, training a large-scale PLM requires tremendous amounts of computational resources, which is time-consuming and expensive. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring the availability of many existing well-trained PLMs. To this end, we explore the question that how can previously trained PLMs benefit training larger PLMs in future. Specifically, we introduce a novel pre-training framework named "knowledge inheritance" (KI), which combines both self-learning and teacher-guided learning to efficiently train larger PLMs. Sufficient experimental results demonstrate the feasibility of our KI framework. We also conduct empirical analyses to explore the effects of teacher PLMs' pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI can well support lifelong learning and knowledge transfer.

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.13880](https://arxiv.org/abs/2105.13880) [cs.CL]** |
|           | (or **[arXiv:2105.13880v1](https://arxiv.org/abs/2105.13880v1) [cs.CL]** for this version) |







<h2 id="2021-05-31-8">8. Changing the World by Changing the Data
</h2>

Title: [Changing the World by Changing the Data](https://arxiv.org/abs/2105.13947)

Authors: [Anna Rogers](https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A)

> NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.13947](https://arxiv.org/abs/2105.13947) [cs.CL]** |
|           | (or **[arXiv:2105.13947v1](https://arxiv.org/abs/2105.13947v1) [cs.CL]** for this version) |








# 2021-05-28

[Return to Index](#Index)



<h2 id="2021-05-28-1">1. How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?
</h2>

Title: [How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?](https://arxiv.org/abs/2105.12900)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While non-autoregressive (NAR) models are showing great promise for machine translation, their use is limited by their dependence on knowledge distillation from autoregressive models. To address this issue, we seek to understand why distillation is so effective. Prior work suggests that distilled training data is less complex than manual translations. Based on experiments with the Levenshtein Transformer and the Mask-Predict NAR models on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently.

| Comments: | Findings of ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.12900](https://arxiv.org/abs/2105.12900) [cs.CL]** |
|           | (or **[arXiv:2105.12900v1](https://arxiv.org/abs/2105.12900v1) [cs.CL]** for this version) |





<h2 id="2021-05-28-2">2. Selective Knowledge Distillation for Neural Machine Translation
</h2>

Title: [Selective Knowledge Distillation for Neural Machine Translation](https://arxiv.org/abs/2105.12967)

Authors: [Fusheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Jianhao Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model's performance by transferring teacher model's knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples' partitions. Based on above protocol, we conduct extensive experiments and find that the teacher's knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT'14 English->German and WMT'19 Chinese->English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.

| Comments: | Accepted as a long paper at ACL 2021. Code is available at [this https URL](https://github.com/LeslieOverfitting/selective_distillation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.12967](https://arxiv.org/abs/2105.12967) [cs.CL]** |
|           | (or **[arXiv:2105.12967v1](https://arxiv.org/abs/2105.12967v1) [cs.CL]** for this version) |





<h2 id="2021-05-28-3">3. Adaptive Nearest Neighbor Machine Translation
</h2>

Title: [Adaptive Nearest Neighbor Machine Translation](https://arxiv.org/abs/2105.13022)

Authors: [Xin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Junliang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the number of k for each target token. We achieve this by introducing a light-weight Meta-k Network, which can be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at [this https URL](https://github.com/zhengxxn/adaptive-knn-mt).

| Comments: | Accepted by ACL-IJCNLP 2021 main conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13022](https://arxiv.org/abs/2105.13022) [cs.CL]** |
|           | (or **[arXiv:2105.13022v1](https://arxiv.org/abs/2105.13022v1) [cs.CL]** for this version) |





<h2 id="2021-05-28-4">4. Extremely low-resource machine translation for closely related languages
</h2>

Title: [Extremely low-resource machine translation for closely related languages](https://arxiv.org/abs/2105.13065)

Authors: [Maali Tars](https://arxiv.org/search/cs?searchtype=author&query=Tars%2C+M), [Andre Tättar](https://arxiv.org/search/cs?searchtype=author&query=Tättar%2C+A), [Mark Fišel](https://arxiv.org/search/cs?searchtype=author&query=Fišel%2C+M)

> An effective method to improve extremely low-resource neural machine translation is multilingual training, which can be improved by leveraging monolingual data to create synthetic bilingual corpora using the back-translation method. This work focuses on closely related languages from the Uralic language family: from Estonian and Finnish geographical regions. We find that multilingual learning and synthetic corpora increase the translation quality in every language pair for which we have data. We show that transfer learning and fine-tuning are very effective for doing low-resource machine translation and achieve the best results. We collected new parallel data for Võro, North and South Saami and present first results of neural machine translation for these languages.

| Comments: | Accepted at Nodalida'2021                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13065](https://arxiv.org/abs/2105.13065) [cs.CL]** |
|           | (or **[arXiv:2105.13065v1](https://arxiv.org/abs/2105.13065v1) [cs.CL]** for this version) |





<h2 id="2021-05-28-5">5. TranSmart: A Practical Interactive Machine Translation System
</h2>

Title: [TranSmart: A Practical Interactive Machine Translation System](https://arxiv.org/abs/2105.13072)

Authors: [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Chengyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Automatic machine translation is super efficient to produce translations yet their quality is not guaranteed. This technique report introduces TranSmart, a practical human-machine interactive translation system that is able to trade off translation quality and efficiency. Compared to existing publicly available interactive translation systems, TranSmart supports three key features, word-level autocompletion, sentence-level autocompletion and translation memory. By word-level and sentence-level autocompletion, TranSmart allows users to interactively translate words in their own manners rather than the strict manner from left to right. In addition, TranSmart has the potential to avoid similar translation mistakes by using translated sentences in history as its memory. This report presents major functions of TranSmart, algorithms for achieving these functions, how to use the TranSmart APIs, and evaluation results of some key functions. TranSmart is publicly available at its homepage ([this https URL](https://transmart.qq.com/)).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.13072](https://arxiv.org/abs/2105.13072) [cs.CL]** |
|           | (or **[arXiv:2105.13072v1](https://arxiv.org/abs/2105.13072v1) [cs.CL]** for this version) |





<h2 id="2021-05-28-6">6. Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models
</h2>

Title: [Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models](https://arxiv.org/abs/2105.13318)

Authors: [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Shankar Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S)

> Synthetic data generation is widely known to boost the accuracy of neural grammatical error correction (GEC) systems, but existing methods often lack diversity or are too simplistic to generate the broad range of grammatical errors made by human writers. In this work, we use error type tags from automatic annotation tools such as ERRANT to guide synthetic data generation. We compare several models that can produce an ungrammatical sentence given a clean sentence and an error type tag. We use these models to build a new, large synthetic pre-training data set with error tag frequency distributions matching a given development set. Our synthetic data set yields large and consistent gains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We also show that our approach is particularly effective in adapting a GEC system, trained on mixed native and non-native English, to a native English test set, even surpassing real training data consisting of high-quality sentence pairs.

| Comments: | Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, 2021. [this https URL](https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.13318](https://arxiv.org/abs/2105.13318) [cs.CL]** |
|           | (or **[arXiv:2105.13318v1](https://arxiv.org/abs/2105.13318v1) [cs.CL]** for this version) |






# 2021-05-27

[Return to Index](#Index)



<h2 id="2021-05-27-1">1. Improving Sign Language Translation with Monolingual Data by Sign Back-Translation
</h2>

Title: [Improving Sign Language Translation with Monolingual Data by Sign Back-Translation](https://arxiv.org/abs/2105.12397)

Authors: [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Wengang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Weizhen Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W), [Junfu Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+J), [Houqiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Despite existing pioneering works on sign language translation (SLT), there is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text data. To tackle this parallel data bottleneck, we propose a sign back-translation (SignBT) approach, which incorporates massive spoken language texts into SLT training. With a text-to-gloss translation model, we first back-translate the monolingual text to its gloss sequence. Then, the paired sign sequence is generated by splicing pieces from an estimated gloss-to-sign bank at the feature level. Finally, the synthetic parallel data serves as a strong supplement for the end-to-end training of the encoder-decoder SLT framework.
> To promote the SLT research, we further contribute CSL-Daily, a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people's daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Extensive experimental results and analysis of SLT methods are reported on CSL-Daily. With the proposed sign back-translation method, we obtain a substantial improvement over previous state-of-the-art SLT methods.

| Comments: | To appear in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.12397](https://arxiv.org/abs/2105.12397) [cs.CV]** |
|           | (or **[arXiv:2105.12397v1](https://arxiv.org/abs/2105.12397v1) [cs.CV]** for this version) |





<h2 id="2021-05-27-2">2. IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion
</h2>

Title: [IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion](https://arxiv.org/abs/2105.12172)

Authors: [Dongjun Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D), [Junhyeong Ahn](https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+J), [Heesoo Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+H), [Jaemin Jo](https://arxiv.org/search/cs?searchtype=author&query=Jo%2C+J)

> We present IntelliCAT, an interactive translation interface with neural models that streamline the post-editing process on machine translation output. We leverage two quality estimation (QE) models at different granularities: sentence-level QE, to predict the quality of each machine-translated sentence, and word-level QE, to locate the parts of the machine-translated sentence that need correction. Additionally, we introduce a novel translation suggestion model conditioned on both the left and right contexts, providing alternatives for specific words or phrases for correction. Finally, with word alignments, IntelliCAT automatically preserves the original document's styles in the translated document. The experimental results show that post-editing based on the proposed QE and translation suggestions can significantly improve translation quality. Furthermore, a user study reveals that three features provided in IntelliCAT significantly accelerate the post-editing task, achieving a 52.9\% speedup in translation time compared to translating from scratch. The interface is publicly available at [this https URL](https://intellicat.beringlab.com/).

| Comments: | ACL 2021 (system demonstration)                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.12172](https://arxiv.org/abs/2105.12172) [cs.CL]** |
|           | (or **[arXiv:2105.12172v1](https://arxiv.org/abs/2105.12172v1) [cs.CL]** for this version) |





<h2 id="2021-05-27-3">3. Context-Sensitive Visualization of Deep Learning Natural Language Processing Models
</h2>

Title: [Context-Sensitive Visualization of Deep Learning Natural Language Processing Models](https://arxiv.org/abs/2105.12202)

Authors: [Andrew Dunn](https://arxiv.org/search/cs?searchtype=author&query=Dunn%2C+A), [Diana Inkpen](https://arxiv.org/search/cs?searchtype=author&query=Inkpen%2C+D), [Răzvan Andonie](https://arxiv.org/search/cs?searchtype=author&query=Andonie%2C+R)

> The introduction of Transformer neural networks has changed the landscape of Natural Language Processing (NLP) during the last years. So far, none of the visualization systems has yet managed to examine all the facets of the Transformers. This gave us the motivation of the current work. We propose a new NLP Transformer context-sensitive visualization method that leverages existing NLP tools to find the most significant groups of tokens (words) that have the greatest effect on the output, thus preserving some context from the original text. First, we use a sentence-level dependency parser to highlight promising word groups. The dependency parser creates a tree of relationships between the words in the sentence. Next, we systematically remove adjacent and non-adjacent tuples of \emph{n} tokens from the input text, producing several new texts with those tokens missing. The resulting texts are then passed to a pre-trained BERT model. The classification output is compared with that of the full text, and the difference in the activation strength is recorded. The modified texts that produce the largest difference in the target classification output neuron are selected, and the combination of removed words are then considered to be the most influential on the model's output. Finally, the most influential word combinations are visualized in a heatmap.

| Comments: | 9 pages, 10 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.12202](https://arxiv.org/abs/2105.12202) [cs.CL]** |
|           | (or **[arXiv:2105.12202v1](https://arxiv.org/abs/2105.12202v1) [cs.CL]** for this version) |





<h2 id="2021-05-27-4">4. Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction
</h2>

Title: [Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction](https://arxiv.org/abs/2105.12297)

Authors: [Hailong Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+H), [Tiejun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T)

> Great progress has been made in unsupervised bilingual lexicon induction (UBLI) by aligning the source and target word embeddings independently trained on monolingual corpora. The common assumption of most UBLI models is that the embedding spaces of two languages are approximately isomorphic. Therefore the performance is bound by the degree of isomorphism, especially on etymologically and typologically distant languages. To address this problem, we propose a transformation-based method to increase the isomorphism. Embeddings of two languages are made to match with each other by rotating and scaling. The method does not require any form of supervision and can be applied to any language pair. On a benchmark data set of bilingual lexicon induction, our approach can achieve competitive or superior performance compared to state-of-the-art methods, with particularly strong results being found on distant languages.

| Comments: | 6 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.12297](https://arxiv.org/abs/2105.12297) [cs.CL]** |
|           | (or **[arXiv:2105.12297v1](https://arxiv.org/abs/2105.12297v1) [cs.CL]** for this version) |





<h2 id="2021-05-27-5">5. Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking
</h2>

Title: [Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking](https://arxiv.org/abs/2105.12306)

Authors: [Heng-Da Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Zhongli Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Qingyu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Q), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Zizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yunbo Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X)

> Chinese Spell Checking (CSC) aims to detect and correct erroneous characters for user-generated text in the Chinese language. Most of the Chinese spelling errors are misused semantically, phonetically or graphically similar characters. Previous attempts noticed this phenomenon and try to use the similarity for this task. However, these methods use either heuristics or handcrafted confusion sets to predict the correct character. In this paper, we propose a Chinese spell checker called ReaLiSe, by directly leveraging the multimodal information of the Chinese characters. The ReaLiSe model tackles the CSC task by (1) capturing the semantic, phonetic and graphic information of the input characters, and (2) selectively mixing the information in these modalities to predict the correct output. Experiments on the SIGHAN benchmarks show that the proposed model outperforms strong baselines by a large margin.

| Comments: | In ACL Findings 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.12306](https://arxiv.org/abs/2105.12306) [cs.CL]** |
|           | (or **[arXiv:2105.12306v1](https://arxiv.org/abs/2105.12306v1) [cs.CL]** for this version) |





<h2 id="2021-05-27-6">6. Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation
</h2>

Title: [Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2105.12523)

Authors: [Yangyifan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Recently, token-level adaptive training has achieved promising improvement in machine translation, where the cross-entropy loss function is adjusted by assigning different training weights to different tokens, in order to alleviate the token imbalance problem. However, previous approaches only use static word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks like machine translation. In this paper, we propose a novel bilingual mutual information (BMI) based adaptive objective, which measures the learning difficulty for each target token from the perspective of bilingualism, and assigns an adaptive weight accordingly to improve token-level adaptive training. This method assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity.

| Comments: | Accepted by ACL-IJCNLP 2021 main conference (short paper)    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.12523](https://arxiv.org/abs/2105.12523) [cs.CL]** |
|           | (or **[arXiv:2105.12523v1](https://arxiv.org/abs/2105.12523v1) [cs.CL]** for this version) |









# 2021-05-26

[Return to Index](#Index)



<h2 id="2021-05-26-1">1. Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization
</h2>

Title: [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://arxiv.org/abs/2105.12002)

Authors:[Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [Simiao Zuo](https://arxiv.org/search/cs?searchtype=author&query=Zuo%2C+S), [Minshuo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Haoming Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Pengcheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+P), [Tuo Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of "lottery tickets", and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as "winning tickets", in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, generalization performance of the winning tickets can not only match, but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as "super tickets". We further show that the phase transition is task and model dependent -- as model size becomes larger and training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.

| Comments: | The 59th annual meeting of the Association for Computational Linguistics (ACL 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.12002](https://arxiv.org/abs/2105.12002) [cs.LG]** |
|           | (or **[arXiv:2105.12002v1](https://arxiv.org/abs/2105.12002v1) [cs.LG]** for this version) |





<h2 id="2021-05-26-2">2. ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer
</h2>

Title: [ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](https://arxiv.org/abs/2105.11741)

Authors:[Yuanmeng Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y), [Rumei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+R), [Sirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Fuzheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F), [Wei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Weiran Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W)

> Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.

| Comments: | Accepted by ACL2021, 10 pages, 7 figures, 4 tables           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.11741](https://arxiv.org/abs/2105.11741) [cs.CL]** |
|           | (or **[arXiv:2105.11741v1](https://arxiv.org/abs/2105.11741v1) [cs.CL]** for this version) |





# 2021-05-25

[Return to Index](#Index)



<h2 id="2021-05-25-1">1. Prevent the Language Model from being Overconfident in Neural Machine Translation
</h2>

Title: [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://arxiv.org/abs/2105.11098)

Authors: [Mengqi Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+M), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xiao-Hua Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.

| Comments: | Accepted as a long paper at ACL 2021. Code is available at: [this https URL](https://github.com/Mlair77/nmt_adequacy) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.11098](https://arxiv.org/abs/2105.11098) [cs.CL]** |
|           | (or **[arXiv:2105.11098v1](https://arxiv.org/abs/2105.11098v1) [cs.CL]** for this version) |





<h2 id="2021-05-25-2">2. Neural Machine Translation with Monolingual Translation Memory
</h2>

Title: [Neural Machine Translation with Monolingual Translation Memory](https://arxiv.org/abs/2105.11269)

Authors: [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Wai Lam](https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+W), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L)

> Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.

| Comments: | ACL2021                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.11269](https://arxiv.org/abs/2105.11269) [cs.CL]** |
|           | (or **[arXiv:2105.11269v1](https://arxiv.org/abs/2105.11269v1) [cs.CL]** for this version) |





<h2 id="2021-05-25-3">3. True Few-Shot Learning with Language Models
</h2>

Title: [True Few-Shot Learning with Language Models](https://arxiv.org/abs/2105.11447)

Authors: [Ethan Perez](https://arxiv.org/search/cs?searchtype=author&query=Perez%2C+E), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates ("prompts"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.

| Comments: | Code at [this https URL](https://github.com/ethanjperez/true_few_shot) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2105.11447](https://arxiv.org/abs/2105.11447) [cs.CL]** |
|           | (or **[arXiv:2105.11447v1](https://arxiv.org/abs/2105.11447v1) [cs.CL]** for this version) |





# 2021-05-24

[Return to Index](#Index)



<h2 id="2021-05-24-1">1. VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding
</h2>

Title: [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding](https://arxiv.org/abs/2105.09996)

Authors: [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Gargi Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+G), [Po-Yao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Prahal Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+P), [Masoumeh Aminzadeh](https://arxiv.org/search/cs?searchtype=author&query=Aminzadeh%2C+M), [Christoph Feichtenhofer](https://arxiv.org/search/cs?searchtype=author&query=Feichtenhofer%2C+C), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

> We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training.

| Comments: | 9 pages, ACL Findings 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.09996](https://arxiv.org/abs/2105.09996) [cs.CV]** |
|           | (or **[arXiv:2105.09996v1](https://arxiv.org/abs/2105.09996v1) [cs.CV]** for this version) |





<h2 id="2021-05-24-2">2. Pretrained Language Models for Text Generation: A Survey
</h2>

Title: [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/abs/2105.10311)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.

| Comments: | Accepted by IJCAI 2021 Survey Track                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.10311](https://arxiv.org/abs/2105.10311) [cs.CL]** |
|           | (or **[arXiv:2105.10311v1](https://arxiv.org/abs/2105.10311v1) [cs.CL]** for this version) |





<h2 id="2021-05-24-3">3. Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining
</h2>

Title: [Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](https://arxiv.org/abs/2105.10419)

Authors: [Ivana Kvapilıkova](https://arxiv.org/search/cs?searchtype=author&query=Kvapilıkova%2C+I), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.

| Comments:          | ACL SRW 2020                                                 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics - Student Research Workshop, pages 255-262, Association for Computational Linguistics, 2020 |
| DOI:               | [10.18653/v1/2020.acl-srw.34](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2020.acl-srw.34&v=659f8833) |
| Cite as:           | **[arXiv:2105.10419](https://arxiv.org/abs/2105.10419) [cs.CL]** |
|                    | (or **[arXiv:2105.10419v1](https://arxiv.org/abs/2105.10419v1) [cs.CL]** for this version) |






# 2021-05-21

[Return to Index](#Index)



<h2 id="2021-05-21-1">1. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation
</h2>

Title: [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://arxiv.org/abs/2105.09501)

Authors: [Xiao Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose \method, a training method to obtain a single unified multilingual translation model. mCOLT is empowered by two techniques: (i) a contrastive learning scheme to close the gap among representations of different languages, and (ii) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mCOLT achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mCOLT achieves an improvement of average 10+ BLEU compared with the multilingual baseline.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.09501](https://arxiv.org/abs/2105.09501) [cs.CL]** |
|           | (or **[arXiv:2105.09501v1](https://arxiv.org/abs/2105.09501v1) [cs.CL]** for this version) |










# 2021-05-20

[Return to Index](#Index)



<h2 id="2021-05-20-1">1. Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing
</h2>

Title: [Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing](https://arxiv.org/abs/2105.08807)

Authors: [Ganesh Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+G), [El Moatez Billah Nagoudi](https://arxiv.org/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M), [Laks V.S. Lakshmanan](https://arxiv.org/search/cs?searchtype=author&query=Lakshmanan%2C+L+V)

> We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task.

| Comments: | Computational Approaches to Linguistic Code-Switching (CALCS 2021) workshop |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08807](https://arxiv.org/abs/2105.08807) [cs.CL]** |
|           | (or **[arXiv:2105.08807v1](https://arxiv.org/abs/2105.08807v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-2">2. Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder
</h2>

Title: [Representation Learning in Sequence to Sequence Tasks: Multi-filter Gaussian Mixture Autoencoder](https://arxiv.org/abs/2105.08840)

Authors: [Yunhao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zhaokun Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+Z)

> Heterogeneity of sentences exists in sequence to sequence tasks such as machine translation. Sentences with largely varied meanings or grammatical structures may increase the difficulty of convergence while training the network. In this paper, we introduce a model to resolve the heterogeneity in the sequence to sequence task. The Multi-filter Gaussian Mixture Autoencoder (MGMAE) utilizes an autoencoder to learn the representations of the inputs. The representations are the outputs from the encoder, lying in the latent space whose dimension is the hidden dimension of the encoder. The representations of training data in the latent space are used to train Gaussian mixtures. The latent space representations are divided into several mixtures of Gaussian distributions. A filter (decoder) is tuned to fit the data in one of the Gaussian distributions specifically. Each Gaussian is corresponding to one filter so that the filter is responsible for the heterogeneity within this Gaussian. Thus the heterogeneity of the training data can be resolved. Comparative experiments are conducted on the Geo-query dataset and English-French translation. Our experiments show that compares to the traditional encoder-decoder model, this network achieves better performance on sequence to sequence tasks such as machine translation and question answering.

| Comments: | 7 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.08840](https://arxiv.org/abs/2105.08840) [cs.CL]** |
|           | (or **[arXiv:2105.08840v1](https://arxiv.org/abs/2105.08840v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-3">3. Effective Attention Sheds Light On Interpretability
</h2>

Title: [Effective Attention Sheds Light On Interpretability](https://arxiv.org/abs/2105.08855)

Authors: [Kaiser Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+K), [Ana Marasović](https://arxiv.org/search/cs?searchtype=author&query=Marasović%2C+A)

> An attention matrix of a transformer self-attention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer's behavior since it is more pertinent to the model output by design.

| Comments: | Accepted to Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08855](https://arxiv.org/abs/2105.08855) [cs.CL]** |
|           | (or **[arXiv:2105.08855v1](https://arxiv.org/abs/2105.08855v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-4">4. Investigating Math Word Problems using Pretrained Multilingual Language Models
</h2>

Title: [Investigating Math Word Problems using Pretrained Multilingual Language Models](https://arxiv.org/abs/2105.08928)

Authors: [Minghuan Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Lei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Lingxiao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J)

> In this paper, we revisit math word problems~(MWPs) from the cross-lingual and multilingual perspective. We construct our MWP solvers over pretrained multilingual language models using sequence-to-sequence model with copy mechanism. We compare how the MWP solvers perform in cross-lingual and multilingual scenarios. To facilitate the comparison of cross-lingual performance, we first adapt the large-scale English dataset MathQA as a counterpart of the Chinese dataset Math23K. Then we extend several English datasets to bilingual datasets through machine translation plus human annotation. Our experiments show that the MWP solvers may not be transferred to a different language even if the target expressions have the same operator set and constants. But for both cross-lingual and multilingual cases, it can be better generalized if problem types exist on both source language and target language.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08928](https://arxiv.org/abs/2105.08928) [cs.CL]** |
|           | (or **[arXiv:2105.08928v1](https://arxiv.org/abs/2105.08928v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-5">5. Combining GCN and Transformer for Chinese Grammatical Error Detection
</h2>

Title: [Combining GCN and Transformer for Chinese Grammatical Error Detection](https://arxiv.org/abs/2105.09085)

Authors: [Jinhong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

> This paper introduces our system at NLPTEA-2020 Task: Chinese Grammatical Error Diagnosis (CGED). CGED aims to diagnose four types of grammatical errors which are missing words (M), redundant words (R), bad word selection (S) and disordered words (W). The automatic CGED system contains two parts including error detection and error correction and our system is designed to solve the error detection problem. Our system is built on three models: 1) a BERT-based model leveraging syntactic information; 2) a BERT-based model leveraging contextual embeddings; 3) a lexicon-based graph neural network. We also design an ensemble mechanism to improve the performance of the single model. Finally, our system obtains the highest F1 scores at detection level and identification level among all teams participating in the CGED 2020 task.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.09085](https://arxiv.org/abs/2105.09085) [cs.CL]** |
|           | (or **[arXiv:2105.09085v1](https://arxiv.org/abs/2105.09085v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-6">6. TableZa -- A classical Computer Vision approach to Tabular Extraction
</h2>

Title: [TableZa -- A classical Computer Vision approach to Tabular Extraction](https://arxiv.org/abs/2105.09137)

Authors: [Saumya Banthia](https://arxiv.org/search/cs?searchtype=author&query=Banthia%2C+S), [Anantha Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A), [Ravi Mangipudi](https://arxiv.org/search/cs?searchtype=author&query=Mangipudi%2C+R)

> Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).

| Comments:    | 14 pages, 16 figures, 1 table                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR) |
| ACM classes: | I.5.1; I.5.2; I.5.4                                          |
| Cite as:     | **[arXiv:2105.09137](https://arxiv.org/abs/2105.09137) [cs.CL]** |
|              | (or **[arXiv:2105.09137v1](https://arxiv.org/abs/2105.09137v1) [cs.CL]** for this version) |





<h2 id="2021-05-20-7">7. Learning Language Specific Sub-network for Multilingual Machine Translation
</h2>

Title: [Learning Language Specific Sub-network for Multilingual Machine Translation](https://arxiv.org/abs/2105.09259)

Authors: [Zehui Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradation on rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy extension to new language pairs and zero-shot translation.LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at [this https URL](https://github.com/NLP-Playground/LaSS).

| Comments: | To appear at ACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.09259](https://arxiv.org/abs/2105.09259) [cs.CL]** |
|           | (or **[arXiv:2105.09259v1](https://arxiv.org/abs/2105.09259v1) [cs.CL]** for this version) |








# 2021-05-19

[Return to Index](#Index)



<h2 id="2021-05-19-1">1. Relative Positional Encoding for Transformers with Linear Complexity
</h2>

Title: [Relative Positional Encoding for Transformers with Linear Complexity](https://arxiv.org/abs/2105.08399)

Authors: [Antoine Liutkus](https://arxiv.org/search/cs?searchtype=author&query=Liutkus%2C+A), [Ondřej Cífka](https://arxiv.org/search/cs?searchtype=author&query=Cífka%2C+O), [Shih-Lun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Umut Şimşekli](https://arxiv.org/search/cs?searchtype=author&query=Şimşekli%2C+U), [Yi-Hsuan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Gaël Richard](https://arxiv.org/search/cs?searchtype=author&query=Richard%2C+G)

> Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.

| Comments: | Accepted to ICML 2021 (long talk). 23 pages                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2105.08399](https://arxiv.org/abs/2105.08399) [cs.LG]** |
|           | (or **[arXiv:2105.08399v1](https://arxiv.org/abs/2105.08399v1) [cs.LG]** for this version) |





<h2 id="2021-05-19-2">2. Multi-Modal Image Captioning for the Visually Impaired
</h2>

Title: [Multi-Modal Image Captioning for the Visually Impaired](https://arxiv.org/abs/2105.08106)

Authors: [Hiba Ahsan](https://arxiv.org/search/cs?searchtype=author&query=Ahsan%2C+H), [Nikita Bhalla](https://arxiv.org/search/cs?searchtype=author&query=Bhalla%2C+N), [Daivat Bhatt](https://arxiv.org/search/cs?searchtype=author&query=Bhatt%2C+D), [Kaivankumar Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+K)

> One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image captioning systems. Current work on captioning images for the visually impaired do not use the textual data present in the image when generating captions. This problem is critical as many visual scenes contain text. Moreover, up to 21% of the questions asked by blind people about the images they click pertain to the text present in them. In this work, we propose altering AoANet, a state-of-the-art image captioning model, to leverage the text detected in the image as an input feature. In addition, we use a pointer-generator mechanism to copy the detected text to the caption when tokens need to be reproduced accurately. Our model outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2% performance improvement on CIDEr and SPICE scores, respectively.

| Comments: | 8 pages, 2 figures, 2 tables, accepted to NAACL-HLT SRW 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.08106](https://arxiv.org/abs/2105.08106) [cs.CL]** |
|           | (or **[arXiv:2105.08106v1](https://arxiv.org/abs/2105.08106v1) [cs.CL]** for this version) |





<h2 id="2021-05-19-3">3. DRILL: Dynamic Representations for Imbalanced Lifelong Learning
</h2>

Title: [DRILL: Dynamic Representations for Imbalanced Lifelong Learning](https://arxiv.org/abs/2105.08445)

Authors: [Kyra Ahrens](https://arxiv.org/search/cs?searchtype=author&query=Ahrens%2C+K), [Fares Abawi](https://arxiv.org/search/cs?searchtype=author&query=Abawi%2C+F), [Stefan Wermter](https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S)

> Continual or lifelong learning has been a long-standing challenge in machine learning to date, especially in natural language processing (NLP). Although state-of-the-art language models such as BERT have ushered in a new era in this field due to their outstanding performance in multitask learning scenarios, they suffer from forgetting when being exposed to a continuous stream of data with shifting data distributions. In this paper, we introduce DRILL, a novel continual learning architecture for open-domain text classification. DRILL leverages a biologically inspired self-organizing neural architecture to selectively gate latent language representations from BERT in a task-incremental manner. We demonstrate in our experiments that DRILL outperforms current methods in a realistic scenario of imbalanced, non-stationary data without prior knowledge about task boundaries. To the best of our knowledge, DRILL is the first of its kind to use a self-organizing neural architecture for open-domain lifelong learning in NLP.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08445](https://arxiv.org/abs/2105.08445) [cs.CL]** |
|           | (or **[arXiv:2105.08445v1](https://arxiv.org/abs/2105.08445v1) [cs.CL]** for this version) |





<h2 id="2021-05-19-4">4. Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation
</h2>

Title: [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://arxiv.org/abs/2105.08504)

Authors: [Mathias Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+M), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search -- the de facto standard inference algorithm in NMT -- and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead.
> In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.

| Comments: | V1: ACL 2021 camera-ready                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.08504](https://arxiv.org/abs/2105.08504) [cs.CL]** |
|           | (or **[arXiv:2105.08504v1](https://arxiv.org/abs/2105.08504v1) [cs.CL]** for this version) |








# 2021-05-18

[Return to Index](#Index)



<h2 id="2021-05-18-1">1. Rethinking Skip Connection with Layer Normalization in Transformers and ResNets
</h2>

Title: [Rethinking Skip Connection with Layer Normalization in Transformers and ResNets](https://arxiv.org/abs/2105.07205)

Authors: [Fenglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Zhiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuexian Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y)

> Skip connection, is a widely-used technique to improve the performance and the convergence of deep neural networks, which is believed to relieve the difficulty in optimization due to non-linearity by propagating a linear component through the neural network layers. However, from another point of view, it can also be seen as a modulating mechanism between the input and the output, with the input scaled by a pre-defined value one. In this work, we investigate how the scale factors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale will lead to spurious gradient exploding or vanishing in line with the deepness of the models, which could be addressed by normalization, in particular, layer normalization, which induces consistent improvements over the plain skip connection. Inspired by the findings, we further propose to adaptively adjust the scale of the input by recursively applying skip connection with layer normalization, which promotes the performance substantially and generalizes well across diverse tasks including both machine translation and image classification datasets.

| Comments: | Accepted by COLING2020 (The 28th International Conference on Computational Linguistics (COLING 2020)) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2105.07205](https://arxiv.org/abs/2105.07205) [cs.LG]** |
|           | (or **[arXiv:2105.07205v1](https://arxiv.org/abs/2105.07205v1) [cs.LG]** for this version) |





<h2 id="2021-05-18-2">2. Conscious AI
</h2>

Title: [Conscious AI](https://arxiv.org/abs/2105.07879)

Authors: [Hadi Esmaeilzadeh](https://arxiv.org/search/cs?searchtype=author&query=Esmaeilzadeh%2C+H), [Reza Vaezi](https://arxiv.org/search/cs?searchtype=author&query=Vaezi%2C+R)

> Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07879](https://arxiv.org/abs/2105.07879) [cs.AI]** |
|           | (or **[arXiv:2105.07879v1](https://arxiv.org/abs/2105.07879v1) [cs.AI]** for this version) |





<h2 id="2021-05-18-3">3. Pay Attention to MLPs
</h2>

Title: [Pay Attention to MLPs](https://arxiv.org/abs/2105.08050)

Authors: [Hanxiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [David R. So](https://arxiv.org/search/cs?searchtype=author&query=So%2C+D+R), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.08050](https://arxiv.org/abs/2105.08050) [cs.LG]** |
|           | (or **[arXiv:2105.08050v1](https://arxiv.org/abs/2105.08050v1) [cs.LG]** for this version) |





<h2 id="2021-05-18-4">4. DirectQE: Direct Pretraining for Machine Translation Quality Estimation
</h2>

Title: [DirectQE: Direct Pretraining for Machine Translation Quality Estimation](https://arxiv.org/abs/2105.07149)

Authors: [Qu Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Q), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiahuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xiang Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+X), [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Machine Translation Quality Estimation (QE) is a task of predicting the quality of machine translations without relying on any reference. Recently, the predictor-estimator framework trains the predictor as a feature extractor, which leverages the extra parallel corpora without QE labels, achieving promising QE performance. However, we argue that there are gaps between the predictor and the estimator in both data quality and training objectives, which preclude QE models from benefiting from a large number of parallel corpora more directly. We propose a novel framework called DirectQE that provides a direct pretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo data that is closer to the real QE data, and a detector is pretrained on these data with novel objectives that are akin to the QE task. Experiments on widely used benchmarks show that DirectQE outperforms existing methods, without using any pretraining models such as BERT. We also give extensive analyses showing how fixing the two gaps contributes to our improvements.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07149](https://arxiv.org/abs/2105.07149) [cs.CL]** |
|           | (or **[arXiv:2105.07149v1](https://arxiv.org/abs/2105.07149v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-5">5. From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding
</h2>

Title: [From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding](https://arxiv.org/abs/2105.07316)

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Ibrahim Sharaf](https://arxiv.org/search/cs?searchtype=author&query=Sharaf%2C+I), [Aizhan Imankulova](https://arxiv.org/search/cs?searchtype=author&query=Imankulova%2C+A), [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Marija Stepanović](https://arxiv.org/search/cs?searchtype=author&query=Stepanović%2C+M), [Alan Ramponi](https://arxiv.org/search/cs?searchtype=author&query=Ramponi%2C+A), [Siti Oryza Khairunnisa](https://arxiv.org/search/cs?searchtype=author&query=Khairunnisa%2C+S+O), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M), [Barbara Plank](https://arxiv.org/search/cs?searchtype=author&query=Plank%2C+B)

> The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.

| Comments: | To appear in the proceedings of NAACL 2021                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.07316](https://arxiv.org/abs/2105.07316) [cs.CL]** |
|           | (or **[arXiv:2105.07316v1](https://arxiv.org/abs/2105.07316v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-6">6. The Volctrans Neural Speech Translation System for IWSLT 2021
</h2>

Title: [The Volctrans Neural Speech Translation System for IWSLT 2021](https://arxiv.org/abs/2105.07319)

Authors: [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Zhicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jian Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+J), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Rong Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+R), [Qianqian Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Jun Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> This paper describes the systems submitted to IWSLT 2021 by the Volctrans team. We participate in the offline speech translation and text-to-text simultaneous translation tracks. For offline speech translation, our best end-to-end model achieves 8.1 BLEU improvements over the benchmark on the MuST-C test set and is even approaching the results of a strong cascade solution. For text-to-text simultaneous translation, we explore the best practice to optimize the wait-k model. As a result, our final submitted systems exceed the benchmark at around 7 BLEU on the same latency regime. We will publish our code and model to facilitate both future research works and industrial applications.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07319](https://arxiv.org/abs/2105.07319) [cs.CL]** |
|           | (or **[arXiv:2105.07319v1](https://arxiv.org/abs/2105.07319v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-7">7. Data Augmentation for Sign Language Gloss Translation
</h2>

Title: [Data Augmentation for Sign Language Gloss Translation](https://arxiv.org/abs/2105.07476)

Authors: [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.

| Comments: | 4 pages, 1 page abstract                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.07476](https://arxiv.org/abs/2105.07476) [cs.CL]** |
|           | (or **[arXiv:2105.07476v1](https://arxiv.org/abs/2105.07476v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-8">8. Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation
</h2>

Title: [Ensemble-based Transfer Learning for Low-resource Machine Translation Quality Estimation](https://arxiv.org/abs/2105.07622)

Authors: [Ting-Wei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T), [Yung-An Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+Y), [Yi-Chieh Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Quality Estimation (QE) of Machine Translation (MT) is a task to estimate the quality scores for given translation outputs from an unknown MT system. However, QE scores for low-resource languages are usually intractable and hard to collect. In this paper, we focus on the Sentence-Level QE Shared Task of the Fifth Conference on Machine Translation (WMT20), but in a more challenging setting. We aim to predict QE scores of given translation outputs when barely none of QE scores of that paired languages are given during training. We propose an ensemble-based predictor-estimator QE model with transfer learning to overcome such QE data scarcity challenge by leveraging QE scores from other miscellaneous languages and translation results of targeted languages. Based on the evaluation results, we provide a detailed analysis of how each of our extension affects QE models on the reliability and the generalization ability to perform transfer learning under multilingual tasks. Finally, we achieve the best performance on the ensemble model combining the models pretrained by individual languages as well as different levels of parallel trained corpus with a Pearson's correlation of 0.298, which is 2.54 times higher than baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.07622](https://arxiv.org/abs/2105.07622) [cs.CL]** |
|           | (or **[arXiv:2105.07622v1](https://arxiv.org/abs/2105.07622v1) [cs.CL]** for this version) |





<h2 id="2021-05-18-9">9. Stage-wise Fine-tuning for Graph-to-Text Generation
</h2>

Title: [Stage-wise Fine-tuning for Graph-to-Text Generation](https://arxiv.org/abs/2105.08021)

Authors: [Qingyun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Semih Yavuz](https://arxiv.org/search/cs?searchtype=author&query=Yavuz%2C+S), [Victoria Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+V), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Nazneen Rajani](https://arxiv.org/search/cs?searchtype=author&query=Rajani%2C+N)

> Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.

| Comments: | 9 pages, Accepted by Proceedings of ACL-IJCNLP 2021 Student Research Workshop, Code and Resources at this [this https URL](https://github.com/EagleW/Stage-wise-Fine-tuning) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.08021](https://arxiv.org/abs/2105.08021) [cs.CL]** |
|           | (or **[arXiv:2105.08021v1](https://arxiv.org/abs/2105.08021v1) [cs.CL]** for this version) |







# 2021-05-17

[Return to Index](#Index)



<h2 id="2021-05-17-1">1. Distilling BERT for low complexity network training
</h2>

Title: [Distilling BERT for low complexity network training](https://arxiv.org/abs/2105.06514)

Authors: [Bansidhar Mangalwedhekar](https://arxiv.org/search/cs?searchtype=author&query=Mangalwedhekar%2C+B)

> This paper studies the efficiency of transferring BERT learnings to low complexity models like BiLSTM, BiLSTM with attention and shallow CNNs using sentiment analysis on SST-2 dataset. It also compares the complexity of inference of the BERT model with these lower complexity models and underlines the importance of these techniques in enabling high performance NLP models on edge devices like mobiles, tablets and MCU development boards like Raspberry Pi etc. and enabling exciting new applications.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.06514](https://arxiv.org/abs/2105.06514) [cs.CL]** |
|           | (or **[arXiv:2105.06514v1](https://arxiv.org/abs/2105.06514v1) [cs.CL]** for this version) |





<h2 id="2021-05-17-2">2. Dynamic Multi-Branch Layers for On-Device Neural Machine Translation
</h2>

Title: [Dynamic Multi-Branch Layers for On-Device Neural Machine Translation](https://arxiv.org/abs/2105.06679)

Authors: [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications such as neural machine translation (NMT) from cloud to mobile devices such as smartphones. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.6 times faster with the same number of parameters.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.06679](https://arxiv.org/abs/2105.06679) [cs.CL]** |
|           | (or **[arXiv:2105.06679v1](https://arxiv.org/abs/2105.06679v1) [cs.CL]** for this version) |





<h2 id="2021-05-17-3">3. Do Context-Aware Translation Models Pay the Right Attention?
</h2>

Title: [Do Context-Aware Translation Models Pay the Right Attention?](https://arxiv.org/abs/2105.06977)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Aditi Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+A), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.

| Comments: | Accepted to ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.06977](https://arxiv.org/abs/2105.06977) [cs.CL]** |
|           | (or **[arXiv:2105.06977v1](https://arxiv.org/abs/2105.06977v1) [cs.CL]** for this version) |








# 2021-05-14

[Return to Index](#Index)



<h2 id="2021-05-14-1">1. Better than BERT but Worse than Baseline
</h2>

Title: [Better than BERT but Worse than Baseline](https://arxiv.org/abs/2105.05915)

Authors: [Boxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Jiaji Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Xingyu Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+X), [Kenneth Church](https://arxiv.org/search/cs?searchtype=author&query=Church%2C+K)

> This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition Identification (ADI) task. ADI inputs a text and outputs short forms (abbreviations/acronyms) and long forms (expansions). BERT with reranking improves over BERT without reranking but fails to reach the Ab3P rule-based baseline. What is BERT missing? Reranking introduces two new features: charmatch and freq. The first feature identifies opportunities to take advantage of character constraints in acronyms and the second feature identifies opportunities to take advantage of frequency constraints across documents.

| Comments: | 6 pages, 2 figures, 5 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05915](https://arxiv.org/abs/2105.05915) [cs.CL]** |
|           | (or **[arXiv:2105.05915v1](https://arxiv.org/abs/2105.05915v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-2">2. Spelling Correction with Denoising Transformer
</h2>

Title: [Spelling Correction with Denoising Transformer](https://arxiv.org/abs/2105.05977)

Authors: [Alex Kuznetsov](https://arxiv.org/search/cs?searchtype=author&query=Kuznetsov%2C+A), [Hector Urdiales](https://arxiv.org/search/cs?searchtype=author&query=Urdiales%2C+H)

> We present a novel method of performing spelling correction on short input strings, such as search queries or individual words. At its core lies a procedure for generating artificial typos which closely follow the error patterns manifested by humans. This procedure is used to train the production spelling correction model based on a transformer architecture. This model is currently served in the HubSpot product search. We show that our approach to typo generation is superior to the widespread practice of adding noise, which ignores human patterns. We also demonstrate how our approach may be extended to resource-scarce settings and train spelling correction models for Arabic, Greek, Russian, and Setswana languages, without using any labeled data.

| Comments: | 9 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05977](https://arxiv.org/abs/2105.05977) [cs.CL]** |
|           | (or **[arXiv:2105.05977v1](https://arxiv.org/abs/2105.05977v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-3">3. Designing Multimodal Datasets for NLP Challenges
</h2>

Title: [Designing Multimodal Datasets for NLP Challenges](https://arxiv.org/abs/2105.05999)

Authors: [James Pustejovsky](https://arxiv.org/search/cs?searchtype=author&query=Pustejovsky%2C+J), [Eben Holderness](https://arxiv.org/search/cs?searchtype=author&query=Holderness%2C+E), [Jingxuan Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+J), [Parker Glenn](https://arxiv.org/search/cs?searchtype=author&query=Glenn%2C+P), [Kyeongmin Rim](https://arxiv.org/search/cs?searchtype=author&query=Rim%2C+K), [Kelley Lynch](https://arxiv.org/search/cs?searchtype=author&query=Lynch%2C+K), [Richard Brutti](https://arxiv.org/search/cs?searchtype=author&query=Brutti%2C+R)

> In this paper, we argue that the design and development of multimodal datasets for natural language processing (NLP) challenges should be enhanced in two significant respects: to more broadly represent commonsense semantic inferences; and to better reflect the dynamics of actions and events, through a substantive alignment of textual and visual information. We identify challenges and tasks that are reflective of linguistic and cognitive competencies that humans have when speaking and reasoning, rather than merely the performance of systems on isolated tasks. We introduce the distinction between challenge-based tasks and competence-based performance, and describe a diagnostic dataset, Recipe-to-Video Questions (R2VQ), designed for testing competence-based comprehension over a multimodal recipe collection ([this http URL](http://r2vq.org/)). The corpus contains detailed annotation supporting such inferencing tasks and facilitating a rich set of question families that we use to evaluate NLP systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.05999](https://arxiv.org/abs/2105.05999) [cs.CL]** |
|           | (or **[arXiv:2105.05999v1](https://arxiv.org/abs/2105.05999v1) [cs.CL]** for this version) |





<h2 id="2021-05-14-4">4. Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level
</h2>

Title: [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://arxiv.org/abs/2105.06020)

Authors: [Ruiqi Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+R), [Dhruba Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+D), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D), [Jacob Steinhardt](https://arxiv.org/search/cs?searchtype=author&query=Steinhardt%2C+J)

> Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-of-distribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%. We also find that finetuning noise increases with model size and that instance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium correlates with improvement from BERT-Medium to BERT-Large. Our findings suggest that instance-level predictions provide a rich source of information; we therefore, recommend that researchers supplement model weights with model predictions.

| Comments: | ACL 2021 Findings. Code and data: [this https URL](https://github.com/ruiqi-zhong/acl2021-instance-level) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.06020](https://arxiv.org/abs/2105.06020) [cs.CL]** |
|           | (or **[arXiv:2105.06020v1](https://arxiv.org/abs/2105.06020v1) [cs.CL]** for this version) |







# 2021-05-13

[Return to Index](#Index)



<h2 id="2021-05-13-1">1. Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction
</h2>

Title: [Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](https://arxiv.org/abs/2105.05498)

Authors: [Gyubok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+G), [Seongjun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Edward Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+E)

> Generating accurate terminology is a crucial component for the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases to appear in the translations. In many cases, however, those methods are evaluated on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. To encourage span-level representations in generation, we additionally impose a source-sentence conditioned masked span prediction loss in the decoder and observe improvements on both terminology translation as well as BLEU scores. Experimental results on three domain-specific corpora in two language pairs demonstrate that the proposed training scheme can improve the performance of existing lexically constrained methods that can operate both with or without a term dictionary at test time.

| Comments: | To appear in ACL 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05498](https://arxiv.org/abs/2105.05498) [cs.CL]** |
|           | (or **[arXiv:2105.05498v1](https://arxiv.org/abs/2105.05498v1) [cs.CL]** for this version) |





<h2 id="2021-05-13-2">2. Evaluating Gender Bias in Natural Language Inference
</h2>

Title: [Evaluating Gender Bias in Natural Language Inference](https://arxiv.org/abs/2105.05541)

Authors: [Shanya Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+S), [Manan Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+M), [Koustuv Sinha](https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+K)

> Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of gender bias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task that involves pairing gender-neutral premises against a gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are significantly prone to gender-induced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.

| Comments:    | NeurIPS 2020 Workshop on Dataset Curation and Security       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2105.05541](https://arxiv.org/abs/2105.05541) [cs.CL]** |
|              | (or **[arXiv:2105.05541v1](https://arxiv.org/abs/2105.05541v1) [cs.CL]** for this version) |





<h2 id="2021-05-13-3">3. Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders
</h2>

Title: [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://arxiv.org/abs/2105.05752)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Bojie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Yanyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yuhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [shen huang](https://arxiv.org/search/cs?searchtype=author&query=huang%2C+s), [Qi Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+Q), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De show that our method achieves the state-of-the-art performance of 18.3 and 25.2 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2105.05752](https://arxiv.org/abs/2105.05752) [cs.CL]** |
|           | (or **[arXiv:2105.05752v1](https://arxiv.org/abs/2105.05752v1) [cs.CL]** for this version) |







# 2021-05-12

[Return to Index](#Index)



<h2 id="2021-05-12-1">1. Cross-Modal Generative Augmentation for Visual Question Answering
</h2>

Title: [Cross-Modal Generative Augmentation for Visual Question Answering](https://arxiv.org/abs/2105.04780)

Authors: [Zixu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yishu Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> Data augmentation is an approach that can effectively improve the performance of multimodal machine learning. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low level operations with deterministic heuristics, our method proposes to learn an augmentation sampler that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly updated with a downstream pipeline. Experiments on Visual Question Answering tasks demonstrate the effectiveness of the proposed generative model, which is able to boost the strong UpDn-based models to the state-of-the-art performance.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.04780](https://arxiv.org/abs/2105.04780) [cs.CV]** |
|           | (or **[arXiv:2105.04780v1](https://arxiv.org/abs/2105.04780v1) [cs.CV]** for this version) |





<h2 id="2021-05-12-2">2. Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity
</h2>

Title: [Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity](https://arxiv.org/abs/2105.04616)

Authors: [Yingxue Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Mark-Jan Nederhof](https://arxiv.org/search/cs?searchtype=author&query=Nederhof%2C+M)

> By using a trigram model and fine-tuning a pretrained BERT model for sequence classification, we show that machine translation and human translation can be classified with an accuracy above chance level, which suggests that machine translation and human translation are different in a systematic way. The classification accuracy of machine translation is much higher than of human translation. We show that this may be explained by the difference in lexical diversity between machine translation and human translation. If machine translation has independent patterns from human translation, automatic metrics which measure the deviation of machine translation from human translation may conflate difference with quality. Our experiment with two different types of automatic metrics shows correlation with the result of the classification task. Therefore, we suggest the difference in lexical diversity between machine translation and human translation be given more attention in machine translation evaluation.

| Comments: | accepted by MoTra21, Nodalida 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04616](https://arxiv.org/abs/2105.04616) [cs.CL]** |
|           | (or **[arXiv:2105.04616v1](https://arxiv.org/abs/2105.04616v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-3">3. Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal
</h2>

Title: [Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal](https://arxiv.org/abs/2105.04633)

Authors: [Casey Kennington](https://arxiv.org/search/cs?searchtype=author&query=Kennington%2C+C)

> Humans' experience of the world is profoundly multimodal from the beginning, so why do existing state-of-the-art language models only use text as a modality to learn and represent semantic meaning? In this paper we review the literature on the role of embodiment and emotion in the interactive setting of spoken dialogue as necessary prerequisites for language learning for human children, including how words in child vocabularies are largely concrete, then shift to become more abstract as the children get older. We sketch a model of semantics that leverages current transformer-based models and a word-level grounded model, then explain the robot-dialogue system that will make use of our semantic model, the setting for the system to learn language, and existing benchmarks for evaluation.

| Comments: | 6 pages, ICLR 2021 Embodied Multimodal Learning Workshop     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04633](https://arxiv.org/abs/2105.04633) [cs.CL]** |
|           | (or **[arXiv:2105.04633v1](https://arxiv.org/abs/2105.04633v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-4">4. Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models
</h2>

Title: [Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models](https://arxiv.org/abs/2105.04688)

Authors: [Laura Pérez-Mayos](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Mayos%2C+L), [Alba Táboas García](https://arxiv.org/search/cs?searchtype=author&query=García%2C+A+T), [Simon Mille](https://arxiv.org/search/cs?searchtype=author&query=Mille%2C+S), [Leo Wanner](https://arxiv.org/search/cs?searchtype=author&query=Wanner%2C+L)

> Multilingual Transformer-based language models, usually pretrained on more than 100 languages, have been shown to achieve outstanding results in a wide range of cross-lingual transfer tasks. However, it remains unknown whether the optimization for different languages conditions the capacity of the models to generalize over syntactic structures, and how languages with syntactic phenomena of different complexity are affected. In this work, we explore the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa. More specifically, we evaluate the syntactic generalization potential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform.

| Comments: | To be published in Findings of ACL 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04688](https://arxiv.org/abs/2105.04688) [cs.CL]** |
|           | (or **[arXiv:2105.04688v1](https://arxiv.org/abs/2105.04688v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-5">5. Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation
</h2>

Title: [Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation](https://arxiv.org/abs/2105.04840)

Authors: [Shun-Po Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+S), [Yung-Sung Chuang](https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y), [Chih-Chiang Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+C), [Hung-yi Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H)

> We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.

| Comments: | Accepted in Findings of ACL-IJCNLP 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04840](https://arxiv.org/abs/2105.04840) [cs.CL]** |
|           | (or **[arXiv:2105.04840v1](https://arxiv.org/abs/2105.04840v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-6">6. Can You Traducir This? Machine Translation for Code-Switched Input
</h2>

Title: [Can You Traducir This? Machine Translation for Code-Switched Input](https://arxiv.org/abs/2105.04846)

Authors: [Jitao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J) (TLP), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F) (TLP)

> Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Workshop on Computational Approaches to Linguistic Code Switching, Jun 2021, Online, United States |
| Cite as:           | **[arXiv:2105.04846](https://arxiv.org/abs/2105.04846) [cs.CL]** |
|                    | (or **[arXiv:2105.04846v1](https://arxiv.org/abs/2105.04846v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-7">7. BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?
</h2>

Title: [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://arxiv.org/abs/2105.04949)

Authors: [Asahi Ushio](https://arxiv.org/search/cs?searchtype=author&query=Ushio%2C+A), [Luis Espinosa-Anke](https://arxiv.org/search/cs?searchtype=author&query=Espinosa-Anke%2C+L), [Steven Schockaert](https://arxiv.org/search/cs?searchtype=author&query=Schockaert%2C+S), [Jose Camacho-Collados](https://arxiv.org/search/cs?searchtype=author&query=Camacho-Collados%2C+J)

> Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as eye is to seeing what ear is to hearing, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations\footnote{Source code and data to reproduce our experimental results are available in the following repository: \url{[this https URL](https://github.com/asahi417/analogy-language-model)}}.

| Comments: | Accepted by ACL 2021 main conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.04949](https://arxiv.org/abs/2105.04949) [cs.CL]** |
|           | (or **[arXiv:2105.04949v1](https://arxiv.org/abs/2105.04949v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-8">8. Towards transparency in NLP shared tasks
</h2>

Title: [Towards transparency in NLP shared tasks](https://arxiv.org/abs/2105.05020)

Authors: [Carla Parra Escartín](https://arxiv.org/search/cs?searchtype=author&query=Escartín%2C+C+P), [Teresa Lynn](https://arxiv.org/search/cs?searchtype=author&query=Lynn%2C+T), [Joss Moorkens](https://arxiv.org/search/cs?searchtype=author&query=Moorkens%2C+J), [Jane Dunne](https://arxiv.org/search/cs?searchtype=author&query=Dunne%2C+J)

> This article reports on a survey carried out across the Natural Language Processing (NLP) community. The survey aimed to capture the opinions of the research community on issues surrounding shared tasks, with respect to both participation and organisation. Amongst the 175 responses received, both positive and negative observations were made. We carried out and report on an extensive analysis of these responses, which leads us to propose a Shared Task Organisation Checklist that could support future participants and organisers. The proposed Checklist is flexible enough to accommodate the wide diversity of shared tasks in our field and its goal is not to be prescriptive, but rather to serve as a tool that encourages shared task organisers to foreground ethical behaviour, beginning with the common issues that the 175 respondents deemed important. Its usage would not only serve as an instrument to reflect on important aspects of shared tasks, but would also promote increased transparency around them.

| Comments: | 38 pages, 26 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.05020](https://arxiv.org/abs/2105.05020) [cs.CL]** |
|           | (or **[arXiv:2105.05020v1](https://arxiv.org/abs/2105.05020v1) [cs.CL]** for this version) |





<h2 id="2021-05-12-9">9. Including Signed Languages in Natural Language Processing
</h2>

Title: [Including Signed Languages in Natural Language Processing](https://arxiv.org/abs/2105.05222)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Julie Hochgesang](https://arxiv.org/search/cs?searchtype=author&query=Hochgesang%2C+J), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Malihe Alikhani](https://arxiv.org/search/cs?searchtype=author&query=Alikhani%2C+M)

> Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.

| Comments: | Accepted as a Theme paper to ACL 2021                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.05222](https://arxiv.org/abs/2105.05222) [cs.CL]** |
|           | (or **[arXiv:2105.05222v1](https://arxiv.org/abs/2105.05222v1) [cs.CL]** for this version) |





# 2021-05-11

[Return to Index](#Index)



<h2 id="2021-05-11-1">1. Duplex Sequence-to-Sequence Learning for Reversible Machine Translation
</h2>

Title: [Duplex Sequence-to-Sequence Learning for Reversible Machine Translation](https://arxiv.org/abs/2105.03458)

Authors: [Zaixiang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Sequence-to-sequence (seq2seq) problems such as machine translation are bidirectional, which naturally derive a pair of directional tasks and two directional learning signals. However, typical seq2seq neural networks are {\em simplex} that only model one unidirectional task, which cannot fully exploit the potential of bidirectional learning signals from parallel data. To address this issue, we propose a {\em duplex} seq2seq neural network, REDER (Reversible Duplex Transformer), and apply it to machine translation. The architecture of REDER has two ends, each of which specializes in a language so as to read and yield sequences in that language. As a result, REDER can simultaneously learn from the bidirectional signals, and enables {\em reversible machine translation} by simply flipping the input and output ends, Experiments on widely-used machine translation benchmarks verify that REDER achieves the first success of reversible machine translation, which helps obtain considerable gains over several strong baselines.

| Comments: | Under review, 10 pages                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03458](https://arxiv.org/abs/2105.03458) [cs.CL]** |
|           | (or **[arXiv:2105.03458v1](https://arxiv.org/abs/2105.03458v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-2">2. Measuring and Increasing Context Usage in Context-Aware Machine Translation
</h2>

Title: [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://arxiv.org/abs/2105.03482)

Authors: [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

> Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03482](https://arxiv.org/abs/2105.03482) [cs.CL]** |
|           | (or **[arXiv:2105.03482v1](https://arxiv.org/abs/2105.03482v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-3">3. Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation
</h2>

Title: [Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation](https://arxiv.org/abs/2105.03953)

Authors: [Zihan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Fine-tuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pre-training mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the fine-tuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at [this https URL](https://github.com/zliucr/cpt-nmt).

| Comments: | Accepted in Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.03953](https://arxiv.org/abs/2105.03953) [cs.CL]** |
|           | (or **[arXiv:2105.03953v1](https://arxiv.org/abs/2105.03953v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-4">4. Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction
</h2>

Title: [Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction](https://arxiv.org/abs/2105.04443)

Authors: [Zhenghao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Xiaoyuan Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Liner Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Tat-Seng Chua](https://arxiv.org/search/cs?searchtype=author&query=Chua%2C+T)

> Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at [this https URL](https://github.com/thunlp/VERNet).

| Comments: | Accepted by NAACL2021, 9 pages, 5 figures                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04443](https://arxiv.org/abs/2105.04443) [cs.CL]** |
|           | (or **[arXiv:2105.04443v1](https://arxiv.org/abs/2105.04443v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-5">5. Self-Guided Curriculum Learning for Neural Machine Translation
</h2>

Title: [Self-Guided Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/2105.04475)

Authors: [Lei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Kevin Duh](https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K), [Ryohei Sasano](https://arxiv.org/search/cs?searchtype=author&query=Sasano%2C+R), [Koichi Takeda](https://arxiv.org/search/cs?searchtype=author&query=Takeda%2C+K)

> In the field of machine learning, the well-trained model is assumed to be able to recover the training labels, i.e. the synthetic labels predicted by the model should be as close to the ground-truth labels as possible. Inspired by this, we propose a self-guided curriculum strategy to encourage the learning of neural machine translation (NMT) models to follow the above recovery criterion, where we cast the recovery degree of each training example as its learning difficulty. Specifically, we adopt the sentence level BLEU score as the proxy of recovery degree. Different from existing curricula relying on linguistic prior knowledge or third-party language models, our chosen learning difficulty is more suitable to measure the degree of knowledge mastery of the NMT models. Experiments on translation benchmarks, including WMT14 English⇒German and WMT17 Chinese⇒English, demonstrate that our approach can consistently improve translation performance against strong baseline Transformer.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.04475](https://arxiv.org/abs/2105.04475) [cs.CL]** |
|           | (or **[arXiv:2105.04475v1](https://arxiv.org/abs/2105.04475v1) [cs.CL]** for this version) |





<h2 id="2021-05-11-6">6. UPC's Speech Translation System for IWSLT 2021
</h2>

Title: [UPC's Speech Translation System for IWSLT 2021](https://arxiv.org/abs/2105.04512)

Authors: [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I), [Ioannis Tsiamas](https://arxiv.org/search/cs?searchtype=author&query=Tsiamas%2C+I), [Carlos Escolano](https://arxiv.org/search/cs?searchtype=author&query=Escolano%2C+C), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> This paper describes the submission to the IWSLT 2021 offline speech translation task by the UPC Machine Translation group. The task consists of building a system capable of translating English audio recordings extracted from TED talks into German text. Submitted systems can be either cascade or end-to-end and use a custom or given segmentation. Our submission is an end-to-end speech translation system, which combines pre-trained models (Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder, and uses an efficient fine-tuning technique, which trains only 20% of its total parameters. We show that adding an Adapter to the system and pre-training it, can increase the convergence speed and the final result, with which we achieve a BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble that obtains 28.22 BLEU score on the same set. Our submission also uses a custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the given segmentation.

| Comments: | Submitted to IWSLT 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.04512](https://arxiv.org/abs/2105.04512) [cs.CL]** |
|           | (or **[arXiv:2105.04512v1](https://arxiv.org/abs/2105.04512v1) [cs.CL]** for this version) |







# 2021-05-10

[Return to Index](#Index)



<h2 id="2021-05-10-1">1. Adapting by Pruning: A Case Study on BERT
</h2>

Title: [Adapting by Pruning: A Case Study on BERT](https://arxiv.org/abs/2105.03343)

Authors: [Yang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Nicolo Colombo](https://arxiv.org/search/cs?searchtype=author&query=Colombo%2C+N), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W)

> Adapting pre-trained neural models to downstream tasks has become the standard practice for obtaining high-quality models. In this work, we propose a novel model adaptation paradigm, adapting by pruning, which prunes neural connections in the pre-trained model to optimise the performance on the target task; all remaining connections have their weights intact. We formulate adapting-by-pruning as an optimisation problem with a differentiable loss and propose an efficient algorithm to prune the model. We prove that the algorithm is near-optimal under standard assumptions and apply the algorithm to adapt BERT to some GLUE tasks. Results suggest that our method can prune up to 50% weights in BERT while yielding similar performance compared to the fine-tuned full model. We also compare our method with other state-of-the-art pruning methods and study the topological differences of their obtained sub-networks.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.03343](https://arxiv.org/abs/2105.03343) [cs.LG]** |
|           | (or **[arXiv:2105.03343v1](https://arxiv.org/abs/2105.03343v1) [cs.LG]** for this version) |





<h2 id="2021-05-10-2">2. On-the-Fly Controlled Text Generation with Experts and Anti-Experts
</h2>

Title: [On-the-Fly Controlled Text Generation with Experts and Anti-Experts](https://arxiv.org/abs/2105.03023)

Authors: [Alisa Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Maarten Sap](https://arxiv.org/search/cs?searchtype=author&query=Sap%2C+M), [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Swabha Swayamdipta](https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S), [Chandra Bhagavatula](https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation which combines a pretrained language model with experts and/or anti-experts in an ensemble of language models. Intuitively, under our ensemble, output tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Our work highlights the promise of using LMs trained on text with (un)desired attributes for efficient decoding-time controlled language generation.

| Comments: | Accepted to ACL 2021, camera-ready version coming soon       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03023](https://arxiv.org/abs/2105.03023) [cs.CL]** |
|           | (or **[arXiv:2105.03023v1](https://arxiv.org/abs/2105.03023v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-3">3. Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates
</h2>

Title: [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://arxiv.org/abs/2105.03048)

Authors: [Yuqing Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y), [Yi-an Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Y), [Yuanjun Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+Y), [Yi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Stefano Soatto](https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S)

> Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.

| Comments: | 13 pages, 3 figures, Accepted at ACL 2021 main conference    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03048](https://arxiv.org/abs/2105.03048) [cs.CL]** |
|           | (or **[arXiv:2105.03048v1](https://arxiv.org/abs/2105.03048v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-4">4. A Survey of Data Augmentation Approaches for NLP
</h2>

Title: [A Survey of Data Augmentation Approaches for NLP](https://arxiv.org/abs/2105.03075)

Authors: [Steven Y. Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S+Y), [Varun Gangal](https://arxiv.org/search/cs?searchtype=author&query=Gangal%2C+V), [Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Sarath Chandar](https://arxiv.org/search/cs?searchtype=author&query=Chandar%2C+S), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S), [Teruko Mitamura](https://arxiv.org/search/cs?searchtype=author&query=Mitamura%2C+T), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.

| Comments: | Accepted to ACL 2021 Findings                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.03075](https://arxiv.org/abs/2105.03075) [cs.CL]** |
|           | (or **[arXiv:2105.03075v1](https://arxiv.org/abs/2105.03075v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-5">5. Learning Shared Semantic Space for Speech-to-Text Translation
</h2>

Title: [Learning Shared Semantic Space for Speech-to-Text Translation](https://arxiv.org/abs/2105.03095)

Authors: [Chi Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Having numerous potential applications and great impact, end-to-end speech translation (ST) has long been treated as an independent task, failing to fully draw strength from the rapid advances of its sibling - text machine translation (MT). With text and audio inputs represented differently, the modality gap has rendered MT data and its end-to-end models incompatible with their ST counterparts. In observation of this obstacle, we propose to bridge this representation gap with Chimera. By projecting audio and text features to a common semantic representation, Chimera unifies MT and ST tasks and boosts the performance on ST benchmark, MuST-C, to a new state-of-the-art. Specifically, Chimera obtains 26.3 BLEU on EN-DE, improving the SOTA by a +2.7 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities.

| Comments: | 8 pages, 5 figures, Accepted by Findings of ACL 2021         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03095](https://arxiv.org/abs/2105.03095) [cs.CL]** |
|           | (or **[arXiv:2105.03095v1](https://arxiv.org/abs/2105.03095v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-6">6. Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods
</h2>

Title: [Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods](https://arxiv.org/abs/2105.03311)

Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Gareth J. F. Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+G+J+F), [Alan F. Smeaton](https://arxiv.org/search/cs?searchtype=author&query=Smeaton%2C+A+F)

> To facilitate effective translation modeling and translation studies, one of the crucial questions to address is how to assess translation quality. From the perspectives of accuracy, reliability, repeatability and cost, translation quality assessment (TQA) itself is a rich and challenging task. In this work, we present a high-level and concise survey of TQA methods, including both manual judgement criteria and automated evaluation metrics, which we classify into further detailed sub-categories. We hope that this work will be an asset for both translation model researchers and quality assessment researchers. In addition, we hope that it will enable practitioners to quickly develop a better understanding of the conventional TQA field, and to find corresponding closely relevant evaluation solutions for their own needs. This work may also serve inspire further development of quality assessment and evaluation methodologies for other natural language processing (NLP) tasks in addition to machine translation (MT), such as automatic text summarization (ATS), natural language understanding (NLU) and natural language generation (NLG).

| Comments: | Accepted to 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021): Workshop on Modelling Translation: Translatology in the Digital Age (MoTra21). arXiv admin note: substantial text overlap with [arXiv:1605.04515](https://arxiv.org/abs/1605.04515) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.03311](https://arxiv.org/abs/2105.03311) [cs.CL]** |
|           | (or **[arXiv:2105.03311v1](https://arxiv.org/abs/2105.03311v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-7">7. Are Pre-trained Convolutions Better than Pre-trained Transformers?
</h2>

Title: [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://arxiv.org/abs/2105.03322)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [Jai Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+J), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Vamsi Aribandi](https://arxiv.org/search/cs?searchtype=author&query=Aribandi%2C+V), [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.03322](https://arxiv.org/abs/2105.03322) [cs.CL]** |
|           | (or **[arXiv:2105.03322v1](https://arxiv.org/abs/2105.03322v1) [cs.CL]** for this version) |





<h2 id="2021-05-10-8">8. ∂-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization
</h2>

Title: [∂-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization](https://arxiv.org/abs/2105.03417)

Authors: [Mokanarangan Thayaparan](https://arxiv.org/search/cs?searchtype=author&query=Thayaparan%2C+M), [Marco Valentino](https://arxiv.org/search/cs?searchtype=author&query=Valentino%2C+M), [Deborah Ferreira](https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+D), [Julia Rozanova](https://arxiv.org/search/cs?searchtype=author&query=Rozanova%2C+J), [André Freitas](https://arxiv.org/search/cs?searchtype=author&query=Freitas%2C+A)

> Constrained optimization solvers with Integer Linear programming (ILP) have been the cornerstone for explainable natural language inference during its inception. ILP based approaches provide a way to encode explicit and controllable assumptions casting natural language inference as an abductive reasoning problem, where the solver constructs a plausible explanation for a given hypothesis. While constrained based solvers provide explanations, they are often limited by the use of explicit constraints and cannot be integrated as part of broader deep neural architectures. In contrast, state-of-the-art transformer-based models can learn from data and implicitly encode complex constraints. However, these models are intrinsically black boxes. This paper presents a novel framework named ∂-Explainer (Diff-Explainer) that combines the best of both worlds by casting the constrained optimization as part of a deep neural network via differentiable convex optimization and fine-tuning pre-trained transformers for downstream explainable NLP tasks. To demonstrate the efficacy of the framework, we transform the constraints presented by TupleILP and integrate them with sentence embedding transformers for the task of explainable science QA. Our experiments show up to ≈10% improvement over non-differentiable solver while still providing explanations for supporting its inference.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.03417](https://arxiv.org/abs/2105.03417) [cs.CL]** |
|           | (or **[arXiv:2105.03417v1](https://arxiv.org/abs/2105.03417v1) [cs.CL]** for this version) |






# 2021-05-07

[Return to Index](#Index)



<h2 id="2021-05-07-1">1. XeroAlign: Zero-Shot Cross-lingual Transformer Alignment
</h2>

Title: [XeroAlign: Zero-Shot Cross-lingual Transformer Alignment](https://arxiv.org/abs/2105.02472)

Authors: [Milan Gritta](https://arxiv.org/search/cs?searchtype=author&query=Gritta%2C+M), [Ignacio Iacobacci](https://arxiv.org/search/cs?searchtype=author&query=Iacobacci%2C+I)

> The introduction of pretrained cross-lingual language models brought decisive improvements to multilingual NLP tasks. However, the lack of labelled task data necessitates a variety of methods aiming to close the gap to high-resource languages. Zero-shot methods in particular, often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign, a simple method for task-specific alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows strong improvements over the baseline models to achieve state-of-the-art zero-shot results on three multilingual natural language understanding tasks. XLM-RA's text classification accuracy exceeds that of XLM-R trained with labelled data and performs on par with state-of-the-art models on a cross-lingual adversarial paraphrasing task.

| Comments: | Accepted as long paper at Findings of ACL 2021               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.02472](https://arxiv.org/abs/2105.02472) [cs.CL]** |
|           | (or **[arXiv:2105.02472v1](https://arxiv.org/abs/2105.02472v1) [cs.CL]** for this version) |





<h2 id="2021-05-07-2">2. Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases
</h2>

Title: [Quantitative Evaluation of Alternative Translations in a Corpus of Highly Dissimilar Finnish Paraphrases](https://arxiv.org/abs/2105.02477)

Authors: [Li-Hsin Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+L), [Sampo Pyysalo](https://arxiv.org/search/cs?searchtype=author&query=Pyysalo%2C+S), [Jenna Kanerva](https://arxiv.org/search/cs?searchtype=author&query=Kanerva%2C+J), [Filip Ginter](https://arxiv.org/search/cs?searchtype=author&query=Ginter%2C+F)

> In this paper, we present a quantitative evaluation of differences between alternative translations in a large recently released Finnish paraphrase corpus focusing in particular on non-trivial variation in translation. We combine a series of automatic steps detecting systematic variation with manual analysis to reveal regularities and identify categories of translation differences. We find the paraphrase corpus to contain highly non-trivial translation variants difficult to recognize through automatic approaches.

| Comments: | Accepted to Workshop on MOdelling TRAnslation: Translatology in the Digital Age |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.02477](https://arxiv.org/abs/2105.02477) [cs.CL]** |
|           | (or **[arXiv:2105.02477v1](https://arxiv.org/abs/2105.02477v1) [cs.CL]** for this version) |





<h2 id="2021-05-07-3">3. Content4All Open Research Sign Language Translation Datasets
</h2>

Title: [Content4All Open Research Sign Language Translation Datasets](https://arxiv.org/abs/2105.02351)

Authors: [Necati Cihan Camgoz](https://arxiv.org/search/cs?searchtype=author&query=Camgoz%2C+N+C), [Ben Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+B), [Guillaume Rochette](https://arxiv.org/search/cs?searchtype=author&query=Rochette%2C+G), [Marco Giovanelli](https://arxiv.org/search/cs?searchtype=author&query=Giovanelli%2C+M), [Giacomo Inches](https://arxiv.org/search/cs?searchtype=author&query=Inches%2C+G), [Robin Nachtrab-Ribback](https://arxiv.org/search/cs?searchtype=author&query=Nachtrab-Ribback%2C+R), [Richard Bowden](https://arxiv.org/search/cs?searchtype=author&query=Bowden%2C+R)

> Computational sign language research lacks the large-scale datasets that enables the creation of useful reallife applications. To date, most research has been limited to prototype systems on small domains of discourse, e.g. weather forecasts. To address this issue and to push the field forward, we release six datasets comprised of 190 hours of footage on the larger domain of news. From this, 20 hours of footage have been annotated by Deaf experts and interpreters and is made publicly available for research purposes. In this paper, we share the dataset collection process and tools developed to enable the alignment of sign language video and subtitles, as well as baseline translation results to underpin future research.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.02351](https://arxiv.org/abs/2105.02351) [cs.CV]** |
|           | (or **[arXiv:2105.02351v1](https://arxiv.org/abs/2105.02351v1) [cs.CV]** for this version) |





<h2 id="2021-05-07-4">4. Reliability Testing for Natural Language Processing Systems
</h2>

Title: [Reliability Testing for Natural Language Processing Systems](https://arxiv.org/abs/2105.02590)

Authors: [Samson Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Kathy Baxter](https://arxiv.org/search/cs?searchtype=author&query=Baxter%2C+K), [Araz Taeihagh](https://arxiv.org/search/cs?searchtype=author&query=Taeihagh%2C+A), [Gregory A. Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+G+A), [Min-Yen Kan](https://arxiv.org/search/cs?searchtype=author&query=Kan%2C+M)

> Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing -- with an emphasis on interdisciplinary collaboration -- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.

| Comments: | Accepted to ACL-IJCNLP 2021 (main conference). Final camera-ready version to follow shortly |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.02590](https://arxiv.org/abs/2105.02590) [cs.LG]** |
|           | (or **[arXiv:2105.02590v1](https://arxiv.org/abs/2105.02590v1) [cs.LG]** for this version) |









# 2021-05-06

[Return to Index](#Index)



<h2 id="2021-05-06-1">1. Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution
</h2>

Title: [Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution](https://arxiv.org/abs/2105.01691)

Authors: [Toan Q. Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+Q), [Kenton Murray](https://arxiv.org/search/cs?searchtype=author&query=Murray%2C+K), [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D)

> In this paper, we investigate the driving factors behind concatenation, a simple but effective data augmentation method for low-resource neural machine translation. Our experiments suggest that discourse context is unlikely the cause for the improvement of about +1 BLEU across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.01691](https://arxiv.org/abs/2105.01691) [cs.CL]** |
|           | (or **[arXiv:2105.01691v1](https://arxiv.org/abs/2105.01691v1) [cs.CL]** for this version) |



<h2 id="2021-05-06-2">2. Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy
</h2>

Title: [Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy](https://arxiv.org/abs/2105.01893)

Authors: [Zhengxin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> Simultaneous translation, which starts translating each sentence after receiving only a few words in source sentence, has a vital role in many scenarios. Although the previous prefix-to-prefix framework is considered suitable for simultaneous translation and achieves good performance, it still has two inevitable drawbacks: the high computational resource costs caused by the need to train a separate model for each latency k and the insufficient ability to encode information because each target token can only attend to a specific source prefix. We propose a novel framework that adopts a simple but effective decoding strategy which is designed for full-sentence models. Within this framework, training a single full-sentence model can achieve arbitrary given latency and save computational resources. Besides, with the competence of the full-sentence model to encode the whole sentence, our decoding strategy can enhance the information maintained in the decoded states in real time. Experimental results show that our method achieves better translation quality than baselines on 4 directions: Zh→En, En→Ro and En↔De.

| Comments: | 8 pages, 5 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.01893](https://arxiv.org/abs/2105.01893) [cs.CL]** |
|           | (or **[arXiv:2105.01893v1](https://arxiv.org/abs/2105.01893v1) [cs.CL]** for this version) |









# 2021-05-04

[Return to Index](#Index)



<h2 id="2021-05-04-1">1. AlloST: Low-resource Speech Translation without Source Transcription
</h2>


Title: [AlloST: Low-resource Speech Translation without Source Transcription](https://arxiv.org/abs/2105.00171)

Authors: [Yao-Fei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Hung-Shin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Hsin-Min Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> The end-to-end architecture has made promising progress in speech translation (ST). However, the ST task is still challenging under low-resource conditions. Most ST models have shown unsatisfactory results, especially in the absence of word information from the source speech utterance. In this study, we survey methods to improve ST performance without using source transcription, and propose a learning framework that utilizes a language-independent universal phone recognizer. The framework is based on an attention-based sequence-to-sequence model, where the encoder generates the phonetic embeddings and phone-aware acoustic representations, and the decoder controls the fusion of the two embedding streams to produce the target token sequence. In addition to investigating different fusion strategies, we explore the specific usage of byte pair encoding (BPE), which compresses a phone sequence into a syllable-like segmented sequence with semantic information. Experiments conducted on the Fisher Spanish-English and Taigi-Mandarin drama corpora show that our method outperforms the conformer-based baseline, and the performance is close to that of the existing best method using source transcription.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.00171](https://arxiv.org/abs/2105.00171) [cs.CL]** |
|           | (or **[arXiv:2105.00171v1](https://arxiv.org/abs/2105.00171v1) [cs.CL]** for this version) |



<h2 id="2021-05-04-2">2. Larger-Scale Transformers for Multilingual Masked Language Modeling
</h2>


Title: [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572)

Authors: [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Giri Anantharaman](https://arxiv.org/search/cs?searchtype=author&query=Anantharaman%2C+G), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A)

> Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.00572](https://arxiv.org/abs/2105.00572) [cs.CL]** |
|           | (or **[arXiv:2105.00572v1](https://arxiv.org/abs/2105.00572v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-3">3. Transformers: "The End of History" for NLP?
</h2>


Title: [Transformers: "The End of History" for NLP?](https://arxiv.org/abs/2105.00813)

Authors: [Anton Chernyavskiy](https://arxiv.org/search/cs?searchtype=author&query=Chernyavskiy%2C+A), [Dmitry Ilvovsky](https://arxiv.org/search/cs?searchtype=author&query=Ilvovsky%2C+D), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

> Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state-of-the-art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed some light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures.

| Comments:    | Transformers, NLP, BERT, RoBERTa, XLNet                      |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2105.00813](https://arxiv.org/abs/2105.00813) [cs.CL]** |
|              | (or **[arXiv:2105.00813v1](https://arxiv.org/abs/2105.00813v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-4">4. BERT memorisation and pitfalls in low-resource scenarios
</h2>


Title: [BERT memorisation and pitfalls in low-resource scenarios](https://arxiv.org/abs/2105.00828)

Authors: [Michael Tänzer](https://arxiv.org/search/cs?searchtype=author&query=Tänzer%2C+M), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M)

> State-of-the-art pre-trained models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal performances even on extremely noisy datasets. Conversely, we also find that they completely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose a novel architecture based on BERT and prototypical networks that improves performance in low-resource named entity recognition tasks.

| Comments: | 14 pages, 24 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.00828](https://arxiv.org/abs/2105.00828) [cs.CL]** |
|           | (or **[arXiv:2105.00828v1](https://arxiv.org/abs/2105.00828v1) [cs.CL]** for this version) |





<h2 id="2021-05-04-5">5. Natural Language Generation Using Link Grammar for General Conversational Intelligence
</h2>


Title: [Natural Language Generation Using Link Grammar for General Conversational Intelligence](https://arxiv.org/abs/2105.00830)

Authors: [Vignav Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+V), [Anton Kolonin](https://arxiv.org/search/cs?searchtype=author&query=Kolonin%2C+A)

> Many current artificial general intelligence (AGI) and natural language processing (NLP) architectures do not possess general conversational intelligence--that is, they either do not deal with language or are unable to convey knowledge in a form similar to the human language without manual, labor-intensive methods such as template-based customization. In this paper, we propose a new technique to automatically generate grammatically valid sentences using the Link Grammar database. This natural language generation method far outperforms current state-of-the-art baselines and may serve as the final component in a proto-AGI question answering pipeline that understandably handles natural language material.

| Comments: | 17 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.00830](https://arxiv.org/abs/2105.00830) [cs.CL]** |
|           | (or **[arXiv:2105.00830v1](https://arxiv.org/abs/2105.00830v1) [cs.CL]** for this version) |

