# Daily arXiv: Machine Translation - Jun., 2019

### Index

- [2019-06-27](#2019-06-27)
  - [1. Sharing Attention Weights for Fast Transformer](#2019-06-27-1)

- [2019-06-26](#2019-06-26)
  - [1. Saliency-driven Word Alignment Interpretation for Neural Machine Translation](#2019-06-26-1)
  - [2. Benchmarking Neural Machine Translation for Southern African Languages](#2019-06-26-2)

- [2019-06-25](#2019-06-25)
  - [1. Neural Machine Translating from Natural Language to SPARQL](#2019-06-25-1)
  - [2. Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation](#2019-06-25-2)
  - [3. Variational Sequential Labelers for Semi-Supervised Learning](#2019-06-25-3)
  - [4. Sequence Generation: From Both Sides to the Middle](#2019-06-25-4)
  - [5. Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models](#2019-06-25-5)
  - [6. A Tensorized Transformer for Language Modeling](#2019-06-25-6)
  - [7. Translationese in Machine Translation Evaluation](#2019-06-25-7)
  
- [2019-06-24](#2019-06-24)
  - [1. Meta-learning of textual representations](#2019-06-24-1)
  - [2. Low-Resource Corpus Filtering using Multilingual Sentence Embeddings](#2019-06-24-2)
  - [3. Learning Bilingual Word Embeddings Using Lexical Definitions](#2019-06-24-3)
  - [4. Incremental Adaptation of NMT for Professional Post-editors: A User Study](#2019-06-24-4)
  - [5. Demonstration of a Neural Machine Translation System with Online Learning for Translators](#2019-06-24-5)
  - [6. CUNI System for the WMT19 Robustness Task](#2019-06-24-6)

- [2019-06-21](#2019-06-21)
  - [1. Robust Machine Translation with Domain Sensitive Pseudo-Sources: Baidu-OSU WMT19 MT Robustness Shared Task System Report](#2019-06-21-1)
  - [2. Improving Zero-shot Translation with Language-Independent Constraints](#2019-06-21-2)
- [2019-06-20](#2019-06-20)
  - [1. Adaptation of Machine Translation Models with Back-translated Data using Transductive Data Selection Methods](#2019-06-20-1)
  - [2. Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation](#2019-06-20-2)
  - [3. The Effect of Translationese in Machine Translation Test Sets](#2019-06-20-3)
  - [4. Pre-Training with Whole Word Masking for Chinese BERT](#2019-06-20-4)
  - [5. XLNet: Generalized Autoregressive Pretraining for Language Understanding](#2019-06-20-5)
- [2019-06-19](#2019-06-19)
  - [1. Generalizing Back-Translation in Neural Machine Translation](#2019-06-19-1)
  - [2. Scheduled Sampling for Transformers](#2019-06-19-2)
  - [3. Distilling Translations with Visual Awareness](#2019-06-19-3)
- [2019-06-18](#2019-06-18)
  - [1. Fixing Gaussian Mixture VAEs for Interpretable Text Generation](#2019-06-18-1)
  - [2. Tagged Back-Translation](#2019-06-18-2)
  - [3. Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks](#2019-06-18-3)
  - [4. Context is Key: Grammatical Error Detection with Contextual Word Representations](#2019-06-18-4)
- [2019-06-17](#2019-06-17)
  - [1. A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning](#2019-06-17-1)
- [2019-06-14](#2019-06-14)
  - [1. UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles](#2019-06-14-1)
  - [2. A Focus on Neural Machine Translation for African Languages](#2019-06-14-2)
  - [3. Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation](#2019-06-14-3)
  - [4. Lattice Transformer for Speech Translation](#2019-06-14-4)
  - [5. Cued@wmt19:ewc&lms](#2019-06-14-5)
  - [6. Analyzing the Limitations of Cross-lingual Word Embedding Mappings](#2019-06-14-6)
  - [7. Compositional generalization through meta sequence-to-sequence learning](#2019-06-14-7)
  - [8. A Multiscale Visualization of Attention in the Transformer Model](#2019-06-14-8)
- [2019-06-13](#2019-06-13)
  - [1. Continual and Multi-Task Architecture Search](#2019-06-13-1)
  - [2. Monotonic Infinite Lookback Attention for Simultaneous Machine Translation](#2019-06-13-2)
- [2019-06-12](#2019-06-12)
  - [1. What Does BERT Look At? An Analysis of BERT's Attention](#2019-06-12-1)
  - [2. Parallel Scheduled Sampling](#2019-06-12-2)
  - [3. Analyzing the Structure of Attention in a Transformer Language Model](#2019-06-12-3)
- [2019-06-11](#2019-06-11)
  - [1. The University of Helsinki submissions to the WMT19 news translation task](#2019-06-11-1)
  - [2. Generalized Data Augmentation for Low-Resource Translation](#2019-06-11-2)
  - [3. Is Attention Interpretable?](#2019-06-11-3)
  - [4. Making Asynchronous Stochastic Gradient Descent Work for Transformers](#2019-06-11-4)
  - [5. Assessing incrementality in sequence-to-sequence models](#2019-06-11-5)
  - [6. Syntax-Infused Variational Autoencoder for Text Generation](#2019-06-11-6)
- [2019-06-10](#2019-06-10)
  - [1. Word-based Domain Adaptation for Neural Machine Translation](#2019-06-10-1)
  - [2. Shared-Private Bilingual Word Embeddings for Neural Machine Translation](#2019-06-10-2)
  - [3. Syntactically Supervised Transformers for Faster Neural Machine Translation](#2019-06-10-3)
- [2019-06-06](#2019-06-06)
  - [1. Imitation Learning for Non-Autoregressive Neural Machine Translation](#2019-06-06-1)
  - [2. The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction](#2019-06-06-2)
  - [3. Learning Deep Transformer Models for Machine Translation](#2019-06-06-3)
  - [4. Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron](#2019-06-06-4)
- [2019-06-05](#2019-06-05)
  - [1. Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations](#2019-06-05-1)
  - [2. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation](#2019-06-05-2)
  - [3. Exploiting Sentential Context for Neural Machine Translation](#2019-06-05-3)
  - [4. Lattice-Based Transformer Encoder for Neural Machine Translation](#2019-06-05-4)
- [2019-06-04](#2019-06-04)
  - [1. Thinking Slow about Latency Evaluation for Simultaneous Machine Translation](#2019-06-04-1)
  - [2. Domain Adaptation of Neural Machine Translation by Lexicon Induction](#2019-06-04-2)
  - [3. Domain Adaptive Inference for Neural Machine Translation](#2019-06-04-3)
  - [4. Fluent Translations from Disfluent Speech in End-to-End Speech Translation](#2019-06-04-4)
  - [5. Evaluating Gender Bias in Machine Translation](#2019-06-04-5)
  - [6. From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots](#2019-06-04-6)
- [2019-06-03](#2019-06-03)
  - [1. DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation](#2019-06-03)

* [2019-05](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-05.md)
* [2019-04](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-04.md)
* [2019-03](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-03.md)
* [2019-02](https://github.com/SFFAI-AIKT/AIKT-Natural_Language_Processing/blob/master/Daily_arXiv/AIKT-MT-Daily_arXiv-2019-02.md)



# 2019-06-27

[Return to Index](#Index)

<h2 id="2019-06-27-1">1. Sharing Attention Weights for Fast Transformer</h2>

Title: [Sharing Attention Weights for Fast Transformer](https://arxiv.org/abs/1906.11024)

Authors: [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Yinqiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Zhengtao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z), [Tongran Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

*(Submitted on 26 Jun 2019)*

> Recently, the Transformer machine translation system has shown strong results by stacking attention layers on both the source and target-language sides. But the inference of this model is slow due to the heavy use of dot-product attention in auto-regressive decoding. In this paper we speed up Transformer via a fast and lightweight attention model. More specifically, we share attention weights in adjacent layers and enable the efficient re-use of hidden states in a vertical manner. Moreover, the sharing policy can be jointly learned with the MT model. We test our approach on ten WMT and NIST OpenMT tasks. Experimental results show that it yields an average of 1.3X speed-up (with almost no decrease in BLEU) on top of a state-of-the-art implementation that has already adopted a cache for fast inference. Also, our approach obtains a 1.8X speed-up when it works with the \textsc{Aan} model. This is even 16 times faster than the baseline with no use of the attention cache.

| Comments: | IJCAI 2019                                           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.11024 [cs.CL]**                         |
|           | (or **arXiv:1906.11024v1 [cs.CL]** for this version) |



# 2019-06-26

[Return to Index](#Index)

<h2 id="2019-06-26-1">1. Saliency-driven Word Alignment Interpretation for Neural Machine Translation</h2>
Title: [Saliency-driven Word Alignment Interpretation for Neural Machine Translation](https://arxiv.org/abs/1906.10282)

Authors:  [Shuoyang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Hainan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

*(Submitted on 25 Jun 2019)*

> Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools.

| Comments: | Accepted to WMT 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.10282 [cs.CL]**                         |
|           | (or **arXiv:1906.10282v1 [cs.CL]** for this version) |

<h2 id="2019-06-26-2">2. Benchmarking Neural Machine Translation for Southern African Languages</h2>
Title: [Benchmarking Neural Machine Translation for Southern African Languages](https://arxiv.org/abs/1906.10511)

Authors:  [Laura Martinus](https://arxiv.org/search/cs?searchtype=author&query=Martinus%2C+L), [Jade Z. Abbott](https://arxiv.org/search/cs?searchtype=author&query=Abbott%2C+J+Z)

*(Submitted on 17 Jun 2019)*

> Unlike major Western languages, most African languages are very low-resourced. Furthermore, the resources that do exist are often scattered and difficult to obtain and discover. As a result, the data and code for existing research has rarely been shared. This has lead a struggle to reproduce reported results, and few publicly available benchmarks for African machine translation models exist. To start to address these problems, we trained neural machine translation models for 5 Southern African languages on publicly-available datasets. Code is provided for training the models and evaluate the models on a newly released evaluation set, with the aim of spur future research in the field for Southern African languages.

| Comments: | arXiv admin note: text overlap with [arXiv:1906.05685](https://arxiv.org/abs/1906.05685) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1906.10511 [cs.CL]**                                 |
|           | (or **arXiv:1906.10511v1 [cs.CL]** for this version)         |



# 2019-06-25

[Return to Index](#Index)

<h2 id="2019-06-25-1">1. Neural Machine Translating from Natural Language to SPARQL</h2>
Title: [Neural Machine Translating from Natural Language to SPARQL](https://arxiv.org/abs/1906.09302)

Authors: [Xiaoyu Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+X), [Dagmar Gromann](https://arxiv.org/search/cs?searchtype=author&query=Gromann%2C+D), [Sebastian Rudolph](https://arxiv.org/search/cs?searchtype=author&query=Rudolph%2C+S)

 *(Submitted on 21 Jun 2019)*

>   SPARQL is a highly powerful query language for an ever-growing number of Linked Data resources and Knowledge Graphs. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language's syntax and semantics, none of which average human web users can be assumed to possess. To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a CNN-based architecture with a BLEU score of up to 98 and accuracy of up to 94%. 

| Subjects: | Computation and Language (cs.CL)                             |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1906.09302](https://arxiv.org/abs/1906.09302) [cs.CL] |
|           | (or                [arXiv:1906.09302v1](https://arxiv.org/abs/1906.09302v1) [cs.CL] for this version) |

<h2 id="2019-06-25-2">2. Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation</h2>
Title: [Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/1906.09444)

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Xilin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

 *(Submitted on 22 Jun 2019)*

>   Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup. 

| Comments: | 12 pages, 4 figures, ACL 2019 long paper                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1906.09444](https://arxiv.org/abs/1906.09444) [cs.CL] |
|           | (or                [arXiv:1906.09444v1](https://arxiv.org/abs/1906.09444v1) [cs.CL] for this version) |

<h2 id="2019-06-25-3">3. Variational Sequential Labelers for Semi-Supervised Learning</h2>
Title: [Variational Sequential Labelers for Semi-Supervised Learning](https://arxiv.org/abs/1906.09535)

Authors: [Mingda Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Qingming Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Q), [Karen Livescu](https://arxiv.org/search/cs?searchtype=author&query=Livescu%2C+K), [Kevin Gimpel](https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K)

  *(Submitted on 23 Jun 2019)*

>   We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data. 

| Comments: | Appeared in EMNLP 2018 Long                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL)                             |
| Cite as:  | [arXiv:1906.09535](https://arxiv.org/abs/1906.09535) [cs.CL] |
|           | (or                [arXiv:1906.09535v1](https://arxiv.org/abs/1906.09535v1) [cs.CL] for this version) |

<h2 id="2019-06-25-4">4. Sequence Generation: From Both Sides to the Middle</h2>
Title: [Sequence Generation: From Both Sides to the Middle](https://arxiv.org/abs/1906.09601)

Authors: [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C), [Heng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H)

 *(Submitted on 23 Jun 2019)*

>   The encoder-decoder framework has achieved promising process for many sequence generation tasks, such as neural machine translation and text summarization. Such a framework usually generates a sequence token by token from left to right, hence (1) this autoregressive decoding procedure is time-consuming when the output sentence becomes longer, and (2) it lacks the guidance of future context which is crucial to avoid under translation. To alleviate these issues, we propose a synchronous bidirectional sequence generation (SBSG) model which predicts its outputs from both sides to the middle simultaneously. In the SBSG model, we enable the left-to-right (L2R) and right-to-left (R2L) generation to help and interact with each other by leveraging interactive bidirectional attention network. Experiments on neural machine translation (En-De, Ch-En, and En-Ro) and text summarization tasks show that the proposed model significantly speeds up decoding while improving the generation quality compared to the autoregressive Transformer. 

| Comments: | Accepted by IJCAI 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | [arXiv:1906.09601](https://arxiv.org/abs/1906.09601) [cs.CL] |
|           | (or                [arXiv:1906.09601v1](https://arxiv.org/abs/1906.09601v1) [cs.CL] for this version) |

<h2 id="2019-06-25-5">5. Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models</h2>
Title: [Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models](https://arxiv.org/abs/1906.09675)

Authors: [Chris Hokamp](https://arxiv.org/search/cs?searchtype=author&query=Hokamp%2C+C), [John Glover](https://arxiv.org/search/cs?searchtype=author&query=Glover%2C+J), [Demian Gholipour](https://arxiv.org/search/cs?searchtype=author&query=Gholipour%2C+D)

 *(Submitted on 24 Jun 2019)*

>   We study several methods for full or partial sharing of the decoder parameters of multilingual NMT models. We evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions using only the WMT 2019 shared task parallel datasets for training. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the diversity of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks. 

| Subjects: | Computation and Language (cs.CL)                             |
| --------- | ------------------------------------------------------------ |
| Cite as:  | [arXiv:1906.09675](https://arxiv.org/abs/1906.09675) [cs.CL] |
|           | (or                [arXiv:1906.09675v1](https://arxiv.org/abs/1906.09675v1) [cs.CL] for this version) |

<h2 id="2019-06-25-6">6. A Tensorized Transformer for Language Modeling</h2>
Title: [A Tensorized Transformer for Language Modeling](https://arxiv.org/abs/1906.09777)

Authors: [Xindian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Peng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Shuai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Yuexian Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Dawei Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

   *(Submitted on 24 Jun 2019)*

>   Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a limited resource setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition. 

| Comments: | Submitted to NeurIPS 2019                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL)                             |
| Cite as:  | [arXiv:1906.09777](https://arxiv.org/abs/1906.09777) [cs.CL] |
|           | (or                [arXiv:1906.09777v1](https://arxiv.org/abs/1906.09777v1) [cs.CL] for this version) |

<h2 id="2019-06-25-7">7. Translationese in Machine Translation Evaluation</h2>
Title: [Translationese in Machine Translation Evaluation](https://arxiv.org/abs/1906.09833)

Authors: [Yvette Graham](https://arxiv.org/search/cs?searchtype=author&query=Graham%2C+Y), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

 *(Submitted on 24 Jun 2019)*

>   The term translationese has been used to describe the presence of unusual features of translated text. In this paper, we provide a detailed analysis of the adverse effects of translationese on machine translation evaluation results. Our analysis shows evidence to support differences in text originally written in a given language relative to translated text and this can potentially negatively impact the accuracy of machine translation evaluations. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past high-profile machine translation evaluation claiming human-parity of MT, as well as analysis of the since re-evaluations of it. We find potential ways of improving the reliability of all three past evaluations. One important issue not previously considered is the statistical power of significance tests applied in past evaluations that aim to investigate human-parity of MT. Since the very aim of such evaluations is to reveal legitimate ties between human and MT systems, power analysis is of particular importance, where low power could result in claims of human parity that in fact simply correspond to Type II error. We therefore provide a detailed power analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size of translations for such studies. Subsequently, since no past evaluation that aimed to investigate claims of human parity ticks all boxes in terms of accuracy and reliability, we rerun the evaluation of the systems claiming human parity. Finally, we provide a comprehensive check-list for future machine translation evaluation. 

| Comments: | 17 pages, 8 figures, 9 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI) |



# 2019-06-24
[Return to Index](#Index)

<h2 id="2019-06-24-1">1. Meta-learning of textual representations</h2>
Title: [Meta-learning of textual representations](https://arxiv.org/abs/1906.08934)
Authors:[Jorge Madrid](https://arxiv.org/search/cs?searchtype=author&query=Madrid%2C+J), [Hugo Jair Escalante](https://arxiv.org/search/cs?searchtype=author&query=Escalante%2C+H+J), [Eduardo Morales](https://arxiv.org/search/cs?searchtype=author&query=Morales%2C+E)

*(Submitted on 21 Jun 2019)*

> Recent progress in AutoML has lead to state-of-the-art methods (e.g., AutoSKLearn) that can be readily used by non-experts to approach any supervised learning problem. Whereas these methods are quite effective, they are still limited in the sense that they work for tabular (matrix formatted) data only. This paper describes one step forward in trying to automate the design of supervised learning methods in the context of text mining. We introduce a meta learning methodology for automatically obtaining a representation for text mining tasks starting from raw text. We report experiments considering 60 different textual representations and more than 80 text mining datasets associated to a wide variety of tasks. Experimental results show the proposed methodology is a promising solution to obtain highly effective off the shell text classification pipelines.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.08934 [cs.LG]**                                 |
|           | (or **arXiv:1906.08934v1 [cs.LG]** for this version)         |

<h2 id="2019-06-24-2">2. Low-Resource Corpus Filtering using Multilingual Sentence Embeddings</h2>
Title: [Low-Resource Corpus Filtering using Multilingual Sentence Embeddings](https://arxiv.org/abs/1906.08885)
Authors: [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

*(Submitted on 20 Jun 2019)*

> In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios.

| Comments:          | 6 pages, WMT 2019                                    |
| ------------------ | ---------------------------------------------------- |
| Subjects:          | **Computation and Language (cs.CL)**                 |
| Journal reference: | Conference on Machine Translation (WMT) 2019         |
| Cite as:           | **arXiv:1906.08885 [cs.CL]**                         |
|                    | (or **arXiv:1906.08885v1 [cs.CL]** for this version) |

<h2 id="2019-06-24-3">3. Learning Bilingual Word Embeddings Using Lexical Definitions</h2>
Title: [Learning Bilingual Word Embeddings Using Lexical Definitions](https://arxiv.org/abs/1906.08939)
Authors: [Weijia Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W), [Muhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Yingtao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)

*(Submitted on 21 Jun 2019)*

> Bilingual word embeddings, which representlexicons of different languages in a shared em-bedding space, are essential for supporting se-mantic and knowledge transfers in a variety ofcross-lingual NLP tasks. Existing approachesto training bilingual word embeddings requireoften require pre-defined seed lexicons that areexpensive to obtain, or parallel sentences thatcomprise coarse and noisy alignment. In con-trast, we propose BilLex that leverages pub-licly available lexical definitions for bilingualword embedding learning. Without the needof predefined seed lexicons, BilLex comprisesa novel word pairing strategy to automati-cally identify and propagate the precise fine-grained word alignment from lexical defini-tions. We evaluate BilLex in word-level andsentence-level translation tasks, which seek tofind the cross-lingual counterparts of wordsand sentences respectively.BilLex signifi-cantly outperforms previous embedding meth-ods on both tasks.

| Comments: | ACL 2019 RepL4NLP                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.08939 [cs.CL]**                                 |
|           | (or **arXiv:1906.08939v1 [cs.CL]** for this version)         |

<h2 id="2019-06-24-4">4. Incremental Adaptation of NMT for Professional Post-editors: A User Study</h2>
Title: [Incremental Adaptation of NMT for Professional Post-editors: A User Study](https://arxiv.org/abs/1906.08996)
Authors: [Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Mercedes García-Martínez](https://arxiv.org/search/cs?searchtype=author&query=García-Martínez%2C+M), [Álvaro Peris](https://arxiv.org/search/cs?searchtype=author&query=Peris%2C+Á), [Alexandre Helle](https://arxiv.org/search/cs?searchtype=author&query=Helle%2C+A), [Amando Estela](https://arxiv.org/search/cs?searchtype=author&query=Estela%2C+A), [Laurent Bié](https://arxiv.org/search/cs?searchtype=author&query=Bié%2C+L), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F), [Manuel Herranz](https://arxiv.org/search/cs?searchtype=author&query=Herranz%2C+M)

*(Submitted on 21 Jun 2019)*

> A common use of machine translation in the industry is providing initial translation hypotheses, which are later supervised and post-edited by a human expert. During this revision process, new bilingual data are continuously generated. Machine translation systems can benefit from these new data, incrementally updating the underlying models under an online learning paradigm. We conducted a user study on this scenario, for a neural machine translation system. The experimentation was carried out by professional translators, with a vast experience in machine translation post-editing. The results showed a reduction in the required amount of human effort needed when post-editing the outputs of the system, improvements in the translation quality and a positive perception of the adaptive system by the users.

| Comments: | Accepted for publication in MT Summit 2019           |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.08996 [cs.CL]**                         |
|           | (or **arXiv:1906.08996v1 [cs.CL]** for this version) |

<h2 id="2019-06-24-5">5. Demonstration of a Neural Machine Translation System with Online Learning for Translators</h2>
Title: [Demonstration of a Neural Machine Translation System with Online Learning for Translators](https://arxiv.org/abs/1906.09000)
Authors: [Miguel Domingo](https://arxiv.org/search/cs?searchtype=author&query=Domingo%2C+M), [Mercedes García-Martínez](https://arxiv.org/search/cs?searchtype=author&query=García-Martínez%2C+M), [Amando Estela](https://arxiv.org/search/cs?searchtype=author&query=Estela%2C+A), [Laurent Bié](https://arxiv.org/search/cs?searchtype=author&query=Bié%2C+L), [Alexandre Helle](https://arxiv.org/search/cs?searchtype=author&query=Helle%2C+A), [Álvaro Peris](https://arxiv.org/search/cs?searchtype=author&query=Peris%2C+Á), [Francisco Casacuberta](https://arxiv.org/search/cs?searchtype=author&query=Casacuberta%2C+F), [Manuerl Herranz](https://arxiv.org/search/cs?searchtype=author&query=Herranz%2C+M)

*(Submitted on 21 Jun 2019)*

> We introduce a demonstration of our system, which implements online learning for neural machine translation in a production environment. These techniques allow the system to continuously learn from the corrections provided by the translators. We implemented an end-to-end platform integrating our machine translation servers to one of the most common user interfaces for professional translators: SDL Trados Studio. Our objective was to save post-editing effort as the machine is continuously learning from human choices and adapting the models to a specific domain or user style.

| Comments: | Accepted for publication in ACL 2019                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.09000 [cs.CL]**                         |
|           | (or **arXiv:1906.09000v1 [cs.CL]** for this version) |

<h2 id="2019-06-24-6">6. CUNI System for the WMT19 Robustness Task</h2>
Title: [CUNI System for the WMT19 Robustness Task](https://arxiv.org/abs/1906.09246)
Authors:





# 2019-06-21

[Return to Index](#Index)

<h2 id="2019-06-21-1">1. Robust Machine Translation with Domain Sensitive Pseudo-Sources: Baidu-OSU WMT19 MT Robustness Shared Task System Report</h2>
Title: [Robust Machine Translation with Domain Sensitive Pseudo-Sources: Baidu-OSU WMT19 MT Robustness Shared Task System Report](https://arxiv.org/abs/1906.08393)

Authors: [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Hairong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Baigong Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

*(Submitted on 19 Jun 2019)*

> This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task. Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises. To make it worse, the amount of social media parallel corpora is extremely limited. In this paper, we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media. Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way. We achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods.

| Comments: | accepted by WMT 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.08393 [cs.CL]**                         |
|           | (or **arXiv:1906.08393v1 [cs.CL]** for this version) |

<h2 id="2019-06-21-2">2. Improving Zero-shot Translation with Language-Independent Constraints</h2>
Title: [Improving Zero-shot Translation with Language-Independent Constraints](https://arxiv.org/abs/1906.08584)

Authors: [Ngoc-Quan Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+N), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J), [Thanh-Le Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+T), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

*(Submitted on 20 Jun 2019)*

> An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. 
> In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.

| Comments: | 10 pages version accepted in WMT 2019                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.08584 [cs.CL]**                         |
|           | (or **arXiv:1906.08584v1 [cs.CL]** for this version) |






# 2019-06-20

[Return to Index](#Index)

<h2 id="2019-06-20-1">1. Adaptation of Machine Translation Models with Back-translated Data using Transductive Data Selection Methods</h2>
Title: [Adaptation of Machine Translation Models with Back-translated Data using Transductive Data Selection Methods](https://arxiv.org/abs/1906.07808#)
Authors: [Alberto Poncelas](https://arxiv.org/search/cs?searchtype=author&query=Poncelas%2C+A), [Gideon Maillette de Buy Wenniger](https://arxiv.org/search/cs?searchtype=author&query=de+Buy+Wenniger%2C+G+M), [Andy Way](https://arxiv.org/search/cs?searchtype=author&query=Way%2C+A)

*(Submitted on 18 Jun 2019)*

> Data selection has proven its merit for improving Neural Machine Translation (NMT), when applied to authentic data. But the benefit of using synthetic data in NMT training, produced by the popular back-translation technique, raises the question if data selection could also be useful for synthetic data? 
> In this work we use Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA), two transductive data selection methods to obtain subsets of sentences from synthetic data. These methods ensure that selected sentences share n-grams with the test set so the NMT model can be adapted to translate it. 
> Performing data selection on back-translated data creates new challenges as the source-side may contain noise originated by the model used in the back-translation. Hence, finding n-grams present in the test set become more difficult. Despite that, in our work we show that adapting a model with a selection of synthetic data is an useful approach.

| Comments: | Accepted in CICLing 2019                             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.07808 [cs.CL]**                         |
|           | (or **arXiv:1906.07808v1 [cs.CL]** for this version) |

<h2 id="2019-06-20-2">2. Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation</h2>
Title: [Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation](https://arxiv.org/abs/1906.07978)
Authors: [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R)

*(Submitted on 19 Jun 2019)*

> In this paper, we propose two novel methods for domain adaptation for the attention-only neural machine translation (NMT) model, i.e., the Transformer. Our methods focus on training a single translation model for multiple domains by either learning domain specialized hidden state representations or predictor biases for each domain. We combine our methods with a previously proposed black-box method called mixed fine tuning, which is known to be highly effective for domain adaptation. In addition, we incorporate multilingualism into the domain adaptation framework. Experiments show that multilingual multi-domain adaptation can significantly improve both resource-poor in-domain and resource-rich out-of-domain translations, and the combination of our methods with mixed fine tuning achieves the best performance.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.07978 [cs.CL]**                         |
|           | (or **arXiv:1906.07978v1 [cs.CL]** for this version) |

<h2 id="2019-06-20-3">3. The Effect of Translationese in Machine Translation Test Sets</h2>
Title: [The Effect of Translationese in Machine Translation Test Sets](https://arxiv.org/abs/1906.08069)
Authors: [Mike Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A)

*(Submitted on 19 Jun 2019)*

> The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT's news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.

| Comments: | 9 pages, 10 pages appendix, 3 figures, 20 tables, accepted in WMT19 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **arXiv:1906.08069 [cs.CL]**                                 |
|           | (or **arXiv:1906.08069v1 [cs.CL]** for this version)         |

<h2 id="2019-06-20-4">4. Pre-Training with Whole Word Masking for Chinese BERT</h2>
Title: [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)
Authors: [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Guoping Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+G)

*(Submitted on 19 Jun 2019)*

> Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks. Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT. In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task. The model was trained on the latest Chinese Wikipedia dump. We aim to provide easy extensibility and better performance for Chinese BERT without changing any neural architecture or even hyper-parameters. The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC). Experimental results on these datasets show that the whole word masking could bring another significant gain. Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm. We release the pre-trained model (both TensorFlow and PyTorch) on GitHub: [this https URL](https://github.com/ymcui/Chinese-BERT-wwm)

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.08101 [cs.CL]**                                 |
|           | (or **arXiv:1906.08101v1 [cs.CL]** for this version)         |

<h2 id="2019-06-20-5">5. XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2>
Title: [ XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
Authors: [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Jaime Carbonell](https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

*(Submitted on 19 Jun 2019)*

> With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.

| Comments: | Pretrained models and code are available at [this https URL](https://github.com/zihangdai/xlnet) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.08237 [cs.CL]**                                 |
|           | (or **arXiv:1906.08237v1 [cs.CL]** for this version)         |



# 2019-06-19

[Return to Index](#Index)

<h2 id="2019-06-19-1">1. Generalizing Back-Translation in Neural Machine Translation</h2>
Title: [Generalizing Back-Translation in Neural Machine Translation](https://arxiv.org/abs/1906.07286)

Authors: [Miguel Graça](https://arxiv.org/search/cs?searchtype=author&query=Graça%2C+M), [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Julian Schamper](https://arxiv.org/search/cs?searchtype=author&query=Schamper%2C+J), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 17 Jun 2019)*

> Back-translation - data augmentation by translating target monolingual data - is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German - English news translation task.

| Comments: | 4th Conference on Machine Translation (WMT 2019) camera-ready |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.07286 [cs.CL]**                                 |
|           | (or **arXiv:1906.07286v1 [cs.CL]** for this version)         |

<h2 id="2019-06-19-2">2. Scheduled Sampling for Transformers</h2>
Title: [Scheduled Sampling for Transformers](https://arxiv.org/abs/1906.07651)

Authors: [Tsvetomila Mihaylova](https://arxiv.org/search/cs?searchtype=author&query=Mihaylova%2C+T), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

*(Submitted on 18 Jun 2019)*

> Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation: exposure bias. It consists of feeding the model a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The technique has been used for improving the model performance with recurrent neural networks (RNN). In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architecture, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.07651 [cs.CL]**                         |
|           | (or **arXiv:1906.07651v1 [cs.CL]** for this version) |

<h2 id="2019-06-19-3">3. Distilling Translations with Visual Awareness</h2>
Title: [Distilling Translations with Visual Awareness](https://arxiv.org/abs/1906.07701)

Authors: [Julia Ive](https://arxiv.org/search/cs?searchtype=author&query=Ive%2C+J), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

*(Submitted on 18 Jun 2019)*

> Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.

| Comments: | accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.07701 [cs.CL]**                         |
|           | (or **arXiv:1906.07701v1 [cs.CL]** for this version) |


# 2019-06-18
[Return to Index](#Index)

<h2 id="2019-06-18-1">1. Fixing Gaussian Mixture VAEs for Interpretable Text Generation</h2>
Title: [Fixing Gaussian Mixture VAEs for Interpretable Text Generation](https://arxiv.org/abs/1906.06719)
Authors: [Wenxian Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Ning Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+N), [Shenjian Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

*(Submitted on 16 Jun 2019)*

> Variational auto-encoder (VAE) with Gaussian priors is effective in text generation. To improve the controllability and interpretability, we propose to use Gaussian mixture distribution as the prior for VAE (GMVAE), since it includes an extra discrete latent variable in addition to the continuous one. Unfortunately, training GMVAE using standard variational approximation often leads to the mode-collapse problem. We theoretically analyze the root cause --- maximizing the evidence lower bound of GMVAE implicitly aggregates the means of multiple Gaussian priors. We propose Dispersed-GMVAE (DGMVAE), an improved model for text generation. It introduces two extra terms to alleviate mode-collapse and to induce a better structured latent space. Experimental results show that DGMVAE outperforms strong baselines in several language modeling and text generation benchmarks.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.06719 [cs.LG]**                                 |
|           | (or **arXiv:1906.06719v1 [cs.LG]** for this version)         |


<h2 id="2019-06-18-2">2. Tagged Back-Translation</h2>
Title: [Tagged Back-Translation](https://arxiv.org/abs/1906.06442)
Authors: [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

*(Submitted on 15 Jun 2019)*

> Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data. We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic. We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, re-defining state-of-the-art in the former.

| Comments: | Accepted as oral presentation in WMT 2019; 9 pages; 9 tables; 1 figure |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **arXiv:1906.06442 [cs.CL]**                                 |
|           | (or **arXiv:1906.06442v1 [cs.CL]** for this version)         |

<h2 id="2019-06-18-3">3. Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks
</h2>

Title: [Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks](https://arxiv.org/abs/1906.06550)
Authors: [Ahmad Aghaebrahimian](https://arxiv.org/search/cs?searchtype=author&query=Aghaebrahimian%2C+A), [Mark Cieliebak](https://arxiv.org/search/cs?searchtype=author&query=Cieliebak%2C+M)

*(Submitted on 15 Jun 2019)*

> We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin, and to obtain competitive results on all other ones.

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.06550 [cs.CL]**                         |
|           | (or **arXiv:1906.06550v1 [cs.CL]** for this version) |

<h2 id="2019-06-18-4">4. Context is Key: Grammatical Error Detection with Contextual Word Representations
</h2>

Title: [Context is Key: Grammatical Error Detection with Contextual Word Representations](https://arxiv.org/abs/1906.06593)
Authors: [Samuel Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+S), [Helen Yannakoudakis](https://arxiv.org/search/cs?searchtype=author&query=Yannakoudakis%2C+H), [Marek Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+M)

*(Submitted on 15 Jun 2019)*

> Grammatical error detection (GED) in non-native writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.06593 [cs.CL]**                                 |
|           | (or **arXiv:1906.06593v1 [cs.CL]** for this version)         |

# 2019-06-17
[Return to Index](#Index)
<h2 id="2019-06-17-1">1. A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning</h2>
Title: [A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning](https://arxiv.org/abs/1906.06253)
Authors: [Gonçalo M. Correia](https://arxiv.org/search/cs?searchtype=author&query=Correia%2C+G+M), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

*(Submitted on 14 Jun 2019)*

> Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training an MT system from scratch. In this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU, we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data, our method obtains state-of-the-art results.

| Comments: | In proceedings of ACL 2019                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.06253 [cs.CL]**                                 |
|           | (or **arXiv:1906.06253v1 [cs.CL]** for this version)         |



# 2019-06-14
[Return to Index](#Index)
<h2 id="2019-06-14-1">1. UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles</h2>
Title: [UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles](https://arxiv.org/abs/1906.05786)
Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 13 Jun 2019)*

> The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experiment with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.

| Comments: | To appear at WMT19                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05786 [cs.CL]**                         |
|           | (or **arXiv:1906.05786v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-2">2. A Focus on Neural Machine Translation for African Languages</h2>
Title: [A Focus on Neural Machine Translation for African Languages](https://arxiv.org/abs/1906.05685)
Authors:[Laura Martinus](https://arxiv.org/search/cs?searchtype=author&query=Martinus%2C+L), [Jade Z. Abbott](https://arxiv.org/search/cs?searchtype=author&query=Abbott%2C+J+Z)

*(Submitted on 11 Jun 2019)*

> African languages are numerous, complex and low-resourced. The datasets required for machine translation are difficult to discover, and existing research is hard to reproduce. Minimal attention has been given to machine translation for African languages so there is scant research regarding the problems that arise when using machine translation techniques. To begin addressing these problems, we trained models to translate English to five of the official South African languages (Afrikaans, isiZulu, Northern Sotho, Setswana, Xitsonga), making use of modern neural machine translation techniques. The results obtained show the promise of using neural machine translation techniques for African languages. By providing reproducible publicly-available data, code and results, this research aims to provide a starting point for other researchers in African machine translation to compare to and build upon.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.05685 [cs.CL]**                                 |
|           | (or **arXiv:1906.05685v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-3">3. Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation</h2>
Title: [Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation](https://arxiv.org/abs/1906.05683)
Authors: [Nima Pourdamghani](https://arxiv.org/search/cs?searchtype=author&query=Pourdamghani%2C+N), [Nada Aldarrab](https://arxiv.org/search/cs?searchtype=author&query=Aldarrab%2C+N), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Kevin Knight](https://arxiv.org/search/cs?searchtype=author&query=Knight%2C+K), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

*(Submitted on 11 Jun 2019)*

> Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then `translating' the resulting pseudo-translation, or `Translationese' into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.

| Comments: | Accepted in ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05683 [cs.CL]**                         |
|           | (or **arXiv:1906.05683v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-4">4. Lattice Transformer for Speech Translation</h2>
Title: [Lattice Transformer for Speech Translation](https://arxiv.org/abs/1906.05551)
Authors: [Pei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Niyu Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+N), [Kai Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+K)

*(Submitted on 13 Jun 2019)*

> Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.

| Comments: | accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05551 [cs.CL]**                         |
|           | (or **arXiv:1906.05551v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-5">5. Cued@wmt19:ewc&lms</h2>
Title: [Cued@wmt19:ewc&lms](https://arxiv.org/abs/1906.05447)
Authors: [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Adria de Gispert](https://arxiv.org/search/cs?searchtype=author&query=de+Gispert%2C+A), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 11 Jun 2019)*

> Two techniques provide the fabric of the Cambridge University Engineering Department's (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.

| Comments: | WMT2019 system description (University of Cambridge) |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05447 [cs.CL]**                         |
|           | (or **arXiv:1906.05447v1 [cs.CL]** for this version) |

<h2 id="2019-06-14-6">6. Analyzing the Limitations of Cross-lingual Word Embedding Mappings</h2>
Title: [Analyzing the Limitations of Cross-lingual Word Embedding Mappings](https://arxiv.org/abs/1906.05407)
Authors: [Aitor Ormazabal](https://arxiv.org/search/cs?searchtype=author&query=Ormazabal%2C+A), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Gorka Labaka](https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G), [Aitor Soroa](https://arxiv.org/search/cs?searchtype=author&query=Soroa%2C+A), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E)

*(Submitted on 12 Jun 2019)*

> Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.

| Comments: | ACL 2019                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05407 [cs.CL]**                                 |
|           | (or **arXiv:1906.05407v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-7">7. Compositional generalization through meta sequence-to-sequence learning</h2>
Title: [Compositional generalization through meta sequence-to-sequence learning](https://arxiv.org/abs/1906.05381)
Authors: [Brenden M. Lake](https://arxiv.org/search/cs?searchtype=author&query=Lake%2C+B+M)

*(Submitted on 12 Jun 2019)*

> People can learn a new concept and use it compositionally, understanding how to "blicket twice" after learning how to "blicket." In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show that neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply rules to variables.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.05381 [cs.CL]**                                 |
|           | (or **arXiv:1906.05381v1 [cs.CL]** for this version)         |

<h2 id="2019-06-14-8">8. A Multiscale Visualization of Attention in the Transformer Model</h2>
Title: [A Multiscale Visualization of Attention in the Transformer Model](https://arxiv.org/abs/1906.05714)
Authors: [Jesse Vig](https://arxiv.org/search/cs?searchtype=author&query=Vig%2C+J)

*(Submitted on 12 Jun 2019)*

> The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.

| Comments: | To appear in ACL 2019 (System Demonstrations). arXiv admin note: substantial text overlap with [arXiv:1904.02679](https://arxiv.org/abs/1904.02679) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Human-Computer Interaction (cs.HC)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05714 [cs.HC]**                                 |
|           | (or **arXiv:1906.05714v1 [cs.HC]** for this version)         |




# 2019-06-13
[Return to Index](#Index)
<h2 id="2019-06-13-1">1. Continual and Multi-Task Architecture Search</h2>
Title: [Continual and Multi-Task Architecture Search](https://arxiv.org/abs/1906.05226)
Authors: [Ramakanth Pasunuru](https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

*(Submitted on 12 Jun 2019)*

> Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.

| Comments: | ACL 2019 (12 pages)                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.05226 [cs.CL]**                                 |
|           | (or **arXiv:1906.05226v1 [cs.CL]** for this version)         |

<h2 id="2019-06-13-2">2. Monotonic Infinite Lookback Attention for Simultaneous Machine Translation</h2>
Title: [Monotonic Infinite Lookback Attention for Simultaneous Machine Translation](https://arxiv.org/abs/1906.05218)
Authors: [Naveen Arivazhagan](https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W), [Chung-Cheng Chiu](https://arxiv.org/search/cs?searchtype=author&query=Chiu%2C+C), [Semih Yavuz](https://arxiv.org/search/cs?searchtype=author&query=Yavuz%2C+S), [Ruoming Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R), [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C)

*(Submitted on 12 Jun 2019)*

> Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk's adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.

| Comments: | Accepted for publication at ACL 2019                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.05218 [cs.CL]**                         |
|           | (or **arXiv:1906.05218v1 [cs.CL]** for this version) |



# 2019-06-12
[Return to Index](#Index)
<h2 id="2019-06-12-1">1. What Does BERT Look At? An Analysis of BERT's Attention</h2>
Title: [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)
Authors: [Kevin Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+K), [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

*(Submitted on 11 Jun 2019)*

> Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.

| Comments: | BlackBoxNLP 2019                                     |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.04341 [cs.CL]**                         |
|           | (or **arXiv:1906.04341v1 [cs.CL]** for this version) |

<h2 id="2019-06-12-2">2. Parallel Scheduled Sampling</h2>
Title: [Parallel Scheduled Sampling](https://arxiv.org/abs/1906.04331)
Authors:[Daniel Duckworth](https://arxiv.org/search/cs?searchtype=author&query=Duckworth%2C+D), [Arvind Neelakantan](https://arxiv.org/search/cs?searchtype=author&query=Neelakantan%2C+A), [Ben Goodrich](https://arxiv.org/search/cs?searchtype=author&query=Goodrich%2C+B), [Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+L), [Samy Bengio](https://arxiv.org/search/cs?searchtype=author&query=Bengio%2C+S)

*(Submitted on 11 Jun 2019)*

> Auto-regressive models are widely used in sequence generation problems. The output sequence is typically generated in a predetermined order, one discrete unit (pixel or word or character) at a time. The models are trained by teacher-forcing where ground-truth history is fed to the model as input, which at test time is replaced by the model prediction. Scheduled Sampling aims to mitigate this discrepancy between train and test time by randomly replacing some discrete units in the history with the model's prediction. While teacher-forced training works well with ML accelerators as the computation can be parallelized across time, Scheduled Sampling involves undesirable sequential processing. In this paper, we introduce a simple technique to parallelize Scheduled Sampling across time. We find that in most cases our technique leads to better empirical performance on summarization and dialog generation tasks compared to teacher-forced training. Further, we discuss the effects of different hyper-parameters associated with Scheduled Sampling on the model performance.

| Comments: | Initial submission                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.04331 [cs.CL]**                                 |
|           | (or **arXiv:1906.04331v1 [cs.CL]** for this version)         |

<h2 id="2019-06-12-3">3. Analyzing the Structure of Attention in a Transformer Language Model</h2>
Title: [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284)
Authors: [Jesse Vig](https://arxiv.org/search/cs?searchtype=author&query=Vig%2C+J), [Yonatan Belinkov](https://arxiv.org/search/cs?searchtype=author&query=Belinkov%2C+Y)

*(Submitted on 7 Jun 2019)*

> The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.

| Comments: | To appear in ACL BlackboxNLP workshop                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **arXiv:1906.04284 [cs.CL]**                                 |
|           | (or **arXiv:1906.04284v1 [cs.CL]** for this version)         |




# 2019-06-11
[Return to Index](#Index)
<h2 id="2019-06-11-1">1. The University of Helsinki submissions to the WMT19 news translation task</h2>
Title: [The University of Helsinki submissions to the WMT19 news translation task](https://arxiv.org/abs/1906.04040)
Authors: [Aarne Talman](https://arxiv.org/search/cs?searchtype=author&query=Talman%2C+A), [Umut Sulubacak](https://arxiv.org/search/cs?searchtype=author&query=Sulubacak%2C+U), [Raúl Vázquez](https://arxiv.org/search/cs?searchtype=author&query=Vázquez%2C+R), [Yves Scherrer](https://arxiv.org/search/cs?searchtype=author&query=Scherrer%2C+Y), [Sami Virpioja](https://arxiv.org/search/cs?searchtype=author&query=Virpioja%2C+S), [Alessandro Raganato](https://arxiv.org/search/cs?searchtype=author&query=Raganato%2C+A), [Arvi Hurskainen](https://arxiv.org/search/cs?searchtype=author&query=Hurskainen%2C+A), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

*(Submitted on 10 Jun 2019)*

> In this paper, we present the University of Helsinki submissions to the WMT 2019 shared task on news translation in three language pairs: English-German, English-Finnish and Finnish-English. This year, we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German, we trained both sentence-level transformer models and compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches, and we also included a rule-based system for English-Finnish.

| Comments: | To appear in WMT19                                   |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.04040 [cs.CL]**                         |
|           | (or **arXiv:1906.04040v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-2">2. Generalized Data Augmentation for Low-Resource Translation</h2>
Title: [Generalized Data Augmentation for Low-Resource Translation](https://arxiv.org/abs/1906.03785)
Authors: [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Xiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+X), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

*(Submitted on 10 Jun 2019)*

> Translation to or from low-resource languages LRLs poses challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing large amounts of monolingual data is regarded as an effective way to alleviate these problems. In this paper, we propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data, but also pivots through a related high-resource language HRL. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the LRL, making use of available resources to better approximate the true data distribution of the LRL. First, we inject LRL words into HRL sentences through an induced bilingual dictionary. Second, we further edit these modified sentences using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to~1.5 to~8 BLEU points compared to supervised back-translation baselines

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.03785 [cs.CL]**                         |
|           | (or **arXiv:1906.03785v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-3">3. Is Attention Interpretable?</h2>
Title: [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731)
Authors: [Sofia Serrano](https://arxiv.org/search/cs?searchtype=author&query=Serrano%2C+S), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

*(Submitted on 9 Jun 2019)*

> Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.

| Comments: | To appear at ACL 2019                                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.03731 [cs.CL]**                         |
|           | (or **arXiv:1906.03731v1 [cs.CL]** for this version) |

<h2 id="2019-06-11-4">4. Making Asynchronous Stochastic Gradient Descent Work for Transformers</h2>
Title: [Making Asynchronous Stochastic Gradient Descent Work for Transformers](https://arxiv.org/abs/1906.03496)
Authors: [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K)

*(Submitted on 8 Jun 2019)*

> Asynchronous stochastic gradient descent (SGD) is attractive from a speed perspective because workers do not wait for synchronization. However, the Transformer model converges poorly with asynchronous SGD, resulting in substantially lower quality compared to synchronous SGD. To investigate why this is the case, we isolate differences between asynchronous and synchronous methods to investigate batch size and staleness effects. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this hybrid method, Transformer training for neural machine translation task reaches a near-convergence level 1.36x faster in single-node multi-GPU training with no impact on model quality.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **arXiv:1906.03496 [cs.CL]**                                 |
|           | (or **arXiv:1906.03496v1 [cs.CL]** for this version)         |

<h2 id="2019-06-11-5">5. Assessing incrementality in sequence-to-sequence models</h2>
Title: [Assessing incrementality in sequence-to-sequence models](https://arxiv.org/abs/1906.03293)
Authors: [Dennis Ulmer](https://arxiv.org/search/cs?searchtype=author&query=Ulmer%2C+D), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D), [Elia Bruni](https://arxiv.org/search/cs?searchtype=author&query=Bruni%2C+E)

*(Submitted on 7 Jun 2019)*

> Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.

| Comments: | Accepted at Repl4NLP, ACL                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.03293 [cs.CL]**                                 |
|           | (or **arXiv:1906.03293v1 [cs.CL]** for this version)         |

<h2 id="2019-06-11-6">6. Syntax-Infused Variational Autoencoder for Text Generation</h2>
Title: [Syntax-Infused Variational Autoencoder for Text Generation](https://arxiv.org/abs/1906.02181)
Authors: [Xinyuan Zhang](https://arxiv.org/search/stat?searchtype=author&query=Zhang%2C+X), [Yi Yang](https://arxiv.org/search/stat?searchtype=author&query=Yang%2C+Y), [Siyang Yuan](https://arxiv.org/search/stat?searchtype=author&query=Yuan%2C+S), [Dinghan Shen](https://arxiv.org/search/stat?searchtype=author&query=Shen%2C+D), [Lawrence Carin](https://arxiv.org/search/stat?searchtype=author&query=Carin%2C+L)

*(Submitted on 5 Jun 2019)*

> We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.

| Comments: | Accepted by ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (stat.ML)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.02181 [stat.ML]**                               |
|           | (or **arXiv:1906.02181v1 [stat.ML]** for this version)       |




# 2019-06-10

[Return to Index](#Index)
<h2 id="2019-06-10-1">1. Word-based Domain Adaptation for Neural Machine Translation</h2>
Title: [Word-based Domain Adaptation for Neural Machine Translation](https://arxiv.org/abs/1906.03129)
Authors: [Shen Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+S), [Leonard Dahlmann](https://arxiv.org/search/cs?searchtype=author&query=Dahlmann%2C+L), [Pavel Petrushkov](https://arxiv.org/search/cs?searchtype=author&query=Petrushkov%2C+P), [Sanjika Hewavitharana](https://arxiv.org/search/cs?searchtype=author&query=Hewavitharana%2C+S), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S)

*(Submitted on 7 Jun 2019)*

> In this paper, we empirically investigate applying word-level weights to adapt neural machine translation to e-commerce domains, where small e-commerce datasets and large out-of-domain datasets are available. In order to mine in-domain like words in the out-of-domain datasets, we compute word weights by using a domain-specific and a non-domain-specific language model followed by smoothing and binary quantization. The baseline model is trained on mixed in-domain and out-of-domain datasets. Experimental results on English to Chinese e-commerce domain translation show that compared to continuing training without word weights, it improves MT quality by up to 2.11% BLEU absolute and 1.59% TER. We have also trained models using fine-tuning on the in-domain data. Pre-training a model with word weights improves fine-tuning up to 1.24% BLEU absolute and 1.64% TER, respectively.

| Comments:          | Published on the proceedings of the International Workshop on Spoken Language Translation (IWSLT), 2018 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | Proceedings of the 15th International Workshop on Spoken Language Translation, Bruges, Belgium, October 29-30, 2018 |
| Cite as:           | **arXiv:1906.03129 [cs.CL]**                                 |
|                    | (or **arXiv:1906.03129v1 [cs.CL]** for this version)         |

<h2 id="2019-06-10-2">2. Shared-Private Bilingual Word Embeddings for Neural Machine Translation</h2>
Title: [Shared-Private Bilingual Word Embeddings for Neural Machine Translation](https://arxiv.org/abs/1906.03100)
Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

*(Submitted on 7 Jun 2019)*

> Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.

| Comments: | Accepted to ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **arXiv:1906.03100 [cs.CL]**                                 |
|           | (or **arXiv:1906.03100v1 [cs.CL]** for this version)         |

<h2 id="2019-06-10-3">3. Syntactically Supervised Transformers for Faster Neural Machine Translation</h2>
Title: [Syntactically Supervised Transformers for Faster Neural Machine Translation](https://arxiv.org/abs/1906.02780)
Authors: 	[Nader Akoury](https://arxiv.org/search/cs?searchtype=author&query=Akoury%2C+N), [Kalpesh Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+K), [Mohit Iyyer](https://arxiv.org/search/cs?searchtype=author&query=Iyyer%2C+M)

*(Submitted on 6 Jun 2019)*

> Standard decoders for neural machine translation autoregressively generate a single target token per time step, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi- autoregressive decoding produce multiple tokens per time step independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~ 5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.

| Comments: | 9 pages, 5 figures, accepted to ACL 2019             |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.02780 [cs.CL]**                         |
|           | (or **arXiv:1906.02780v1 [cs.CL]** for this version) |

# 2019-06-06

[Return to Index](#Index)
<h2 id="2019-06-06-1">1. Imitation Learning for Non-Autoregressive Neural Machine Translation</h2>
Title: [Imitation Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/1906.02041)

Authors: [Bingzhen Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+B), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Junyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

*(Submitted on 5 Jun 2019)*

> Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.02041 [cs.CL]**                         |
|           | (or **arXiv:1906.02041v1 [cs.CL]** for this version) |

<h2 id="2019-06-06-2">2. The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction</h2>
Title: [The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction](https://arxiv.org/abs/1906.01733)

Authors: [Dimitrios Alikaniotis](https://arxiv.org/search/cs?searchtype=author&query=Alikaniotis%2C+D), [Vipul Raheja](https://arxiv.org/search/cs?searchtype=author&query=Raheja%2C+V)

*(Submitted on 4 Jun 2019)*

> Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.

| Comments: | 7 pages, 3 tables, accepted at the 14th Workshop on Innovative Use of NLP for Building Educational Applications |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **arXiv:1906.01733 [cs.CL]**                                 |
|           | (or **arXiv:1906.01733v1 [cs.CL]** for this version)         |

<h2 id="2019-06-06-3">3. Learning Deep Transformer Models for Machine Translation</h2>
Title: [Learning Deep Transformer Models for Machine Translation](https://arxiv.org/abs/1906.01787)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Changliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

*(Submitted on 5 Jun 2019)*

> Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.

| Comments: | Accepted by ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.01787 [cs.CL]**                                 |
|           | (or **arXiv:1906.01787v1 [cs.CL]** for this version)         |

<h2 id="2019-06-06-4">4. Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron</h2>
Title: [Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron](https://arxiv.org/abs/1906.01942)

Authors: [Yunsu Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Hendrik Rosendahl](https://arxiv.org/search/cs?searchtype=author&query=Rosendahl%2C+H), [Nick Rossenbach](https://arxiv.org/search/cs?searchtype=author&query=Rossenbach%2C+N), [Jan Rosendahl](https://arxiv.org/search/cs?searchtype=author&query=Rosendahl%2C+J), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

*(Submitted on 5 Jun 2019)*

> We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.

| Comments: | ACL 2019 Repl4NLP camera-ready                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **arXiv:1906.01942 [cs.CL]**                                 |
|           | (or **arXiv:1906.01942v1 [cs.CL]** for this version)         |


# 2019-06-05

[Return to Index](#Index)
<h2 id="2019-06-05-1">1. Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations</h2>
Title: [Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations](https://arxiv.org/abs/1906.01181)

Authors: [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Victor O.K. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+V+O)

*(Submitted on 4 Jun 2019)*

> Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4~22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01181 [cs.CL]**                         |
|           | (or **arXiv:1906.01181v1 [cs.CL]** for this version) |

<h2 id="2019-06-05-2">2. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</h2>
Title: [Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation](https://arxiv.org/abs/1906.01199)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

*(Submitted on 4 Jun 2019)*

> Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.

| Comments: | Accepted to ACL 2019                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **arXiv:1906.01199 [cs.CL]**                                 |
|           | (or **arXiv:1906.01199v1 [cs.CL]** for this version)         |

<h2 id="2019-06-05-3">3. Exploiting Sentential Context for Neural Machine Translation</h2>
Title: [Exploiting Sentential Context for Neural Machine Translation](https://arxiv.org/abs/1906.01268)

Authors: [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

*(Submitted on 4 Jun 2019)*

> In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we first show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-to-German and English-to-French benchmarks show that our model consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01268 [cs.CL]**                         |
|           | (or **arXiv:1906.01268v1 [cs.CL]** for this version) |

<h2 id="2019-06-05-4">4. Lattice-Based Transformer Encoder for Neural Machine Translation</h2>
Title: [Lattice-Based Transformer Encoder for Neural Machine Translation](https://arxiv.org/abs/1906.01282)

Authors: [Fengshun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F), [Jiangtong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Kehai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K)

*(Submitted on 4 Jun 2019)*

> Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.

| Comments: | Accepted by ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.01282 [cs.CL]**                         |
|           | (or **arXiv:1906.01282v1 [cs.CL]** for this version) |



# 2019-06-04

[Return to Index](#Index)
<h2 id="2019-06-04-1">1. Thinking Slow about Latency Evaluation for Simultaneous Machine Translation</h2>
Title: [Thinking Slow about Latency Evaluation for Simultaneous Machine Translation](https://arxiv.org/abs/1906.00048)

Authors: [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [George Foster](https://arxiv.org/search/cs?searchtype=author&query=Foster%2C+G)

*(Submitted on 31 May 2019)*

> Simultaneous machine translation attempts to translate a source sentence before it is finished being spoken, with applications to translation of spoken language for live streaming and conversation. Since simultaneous systems trade quality to reduce latency, having an effective and interpretable latency metric is crucial. We introduce a variant of the recently proposed Average Lagging (AL) metric, which we call Differentiable Average Lagging (DAL). It distinguishes itself by being differentiable and internally consistent to its underlying mathematical model.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1906.00048 [cs.CL]**                         |
|           | (or **arXiv:1906.00048v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-2">2. Domain Adaptation of Neural Machine Translation by Lexicon Induction</h2>
Title: [Domain Adaptation of Neural Machine Translation by Lexicon Induction](https://arxiv.org/abs/1906.00376)

Authors:[Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Jaime Carbonell](https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J)

*(Submitted on 2 Jun 2019)*

> It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | published at the 57th Annual Meeting of the Association for Computational Linguistics (ACL). July 2019 |
| Cite as:           | **arXiv:1906.00376 [cs.CL]**                                 |
|                    | (or **arXiv:1906.00376v1 [cs.CL]** for this version)         |

 





<h2 id="2019-06-04-3">3. Domain Adaptive Inference for Neural Machine Translation</h2>
Title: [Domain Adaptive Inference for Neural Machine Translation](https://arxiv.org/abs/1906.00408)

Authors: [Danielle Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+D), [Felix Stahlberg](https://arxiv.org/search/cs?searchtype=author&query=Stahlberg%2C+F), [Adria de Gispert](https://arxiv.org/search/cs?searchtype=author&query=de+Gispert%2C+A), [Bill Byrne](https://arxiv.org/search/cs?searchtype=author&query=Byrne%2C+B)

*(Submitted on 2 Jun 2019)*

> We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and show strong improvements across test domains without access to the domain label.

| Comments: | To appear at ACL 2019                                |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00408 [cs.CL]**                         |
|           | (or **arXiv:1906.00408v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-4">4. Fluent Translations from Disfluent Speech in End-to-End Speech Translation</h2>
Title: [Fluent Translations from Disfluent Speech in End-to-End Speech Translation](https://arxiv.org/abs/1906.00556)

Authors: [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Matthias Sperber](https://arxiv.org/search/cs?searchtype=author&query=Sperber%2C+M), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

*(Submitted on 3 Jun 2019)*

> Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected `copy-edited' references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, the translation of conversational speech with joint removal of disfluencies.

| Comments: | Accepted at NAACL 2019                               |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00556 [cs.CL]**                         |
|           | (or **arXiv:1906.00556v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-5">5. Evaluating Gender Bias in Machine Translation</h2>
Title: [Evaluating Gender Bias in Machine Translation](https://arxiv.org/abs/1906.00591)

Authors: [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

*(Submitted on 3 Jun 2019)*

> We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., "The doctor asked the nurse to help her in the operation"). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word "doctor"). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are made publicly available.

| Comments: | Accepted to ACL 2019                                 |
| --------- | ---------------------------------------------------- |
| Subjects: | **Computation and Language (cs.CL)**                 |
| Cite as:  | **arXiv:1906.00591 [cs.CL]**                         |
|           | (or **arXiv:1906.00591v1 [cs.CL]** for this version) |



<h2 id="2019-06-04-6">6. From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots</h2>
Title: [From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots](https://arxiv.org/abs/1906.00872)

Authors: [Shizhe Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Qin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q), [Jianlong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J)

*(Submitted on 3 Jun 2019)*

> The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behavior, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image-pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach significantly outperforms other state-of-the-art methods.

| Comments: | Accepted by IJCAI 2019                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **arXiv:1906.00872 [cs.CL]**                                 |
|           | (or **arXiv:1906.00872v1 [cs.CL]** for this version)         |



# 2019-06-03

[Return to Index](#Index)
<h2 id="2019-06-03-1">1. DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation</h2>
Title: [DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation](https://arxiv.org/abs/1905.13354)

Authors: [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Sophie Rosset](https://arxiv.org/search/cs?searchtype=author&query=Rosset%2C+S), [Thomas Lavergne](https://arxiv.org/search/cs?searchtype=author&query=Lavergne%2C+T), [Eric Bilinski](https://arxiv.org/search/cs?searchtype=author&query=Bilinski%2C+E)

*(Submitted on 30 May 2019)*

> We present a new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and reference translations produced a posteriori. The motivation for the corpus is two-fold: to provide (i) a unique resource for evaluating MT models, and (ii) a corpus for the analysis of MT-mediated communication. We provide a preliminary analysis of the corpus to confirm that the participants' judgments reveal perceptible differences in MT quality between the two MT systems used.

| Subjects: | **Computation and Language (cs.CL)**                 |
| --------- | ---------------------------------------------------- |
| Cite as:  | **arXiv:1905.13354 [cs.CL]**                         |
|           | (or **arXiv:1905.13354v1 [cs.CL]** for this version) |
